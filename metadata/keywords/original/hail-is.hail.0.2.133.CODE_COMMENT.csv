id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/auth.py:538,Security,validat,validation,538,"# The below are used by gateway / Envoy reverse proxies for auth checks, but; # Envoy calls those auth endpoints with the same HTTP method as the original; # user's request. In the case where a user is trying to POST to a protected; # service, that will additionally trigger a CSRF check on the auth endpoint; # which cannot always be conducted if, for example, the backend service is; # Grafana which conducts its own CSRF mitigations separate from our own.; # These auth endpoints are not CSRF-vulnerable so we opt out of CSRF-token; # validation.; # See: https://github.com/envoyproxy/envoy/issues/5357",MatchSource.CODE_COMMENT,auth/auth/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/auth.py
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/__main__.py:2,Modifiability,config,configure,2,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,auth/auth/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/__main__.py:12,Testability,log,logging,12,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,auth/auth/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/driver.py:19,Testability,test,test,19,"# auth services in test namespaces cannot/should not be creating and deleting namespaces",MatchSource.CODE_COMMENT,auth/auth/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/driver.py
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/driver.py:19,Testability,test,test,19,"# auth services in test namespaces cannot/should not be creating and deleting namespaces",MatchSource.CODE_COMMENT,auth/auth/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/driver.py
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/driver.py:40,Modifiability,config,config,40,"# don't bother deleting database-server-config since we're; # deleting the namespace",MatchSource.CODE_COMMENT,auth/auth/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/driver.py
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/__main__.py:2,Modifiability,config,configure,2,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,auth/auth/driver/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/__main__.py:12,Testability,log,logging,12,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,auth/auth/driver/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/auth/auth/driver/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py:127,Integrability,interface,interface-files,127,"# See below for a nice breakdown of the cpu cgroupv2:; # https://facebookmicrosites.github.io/cgroup2/docs/cpu-controller.html#interface-files; #; # and here for the authoritative source:; # https://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup.git/tree/Documentation/admin-guide/cgroup-v2.rst#n1038",MatchSource.CODE_COMMENT,batch/batch/resource_usage.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py:278,Usability,guid,guide,278,"# See below for a nice breakdown of the cpu cgroupv2:; # https://facebookmicrosites.github.io/cgroup2/docs/cpu-controller.html#interface-files; #; # and here for the authoritative source:; # https://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup.git/tree/Documentation/admin-guide/cgroup-v2.rst#n1038",MatchSource.CODE_COMMENT,batch/batch/resource_usage.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py:138,Integrability,interface,interface-files,138,"# See below for a nice breakdown of the memory cgroupv2:; # https://facebookmicrosites.github.io/cgroup2/docs/memory-controller.html#core-interface-files; #; # and here for the authoritative source:; # https://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup.git/tree/Documentation/admin-guide/cgroup-v2.rst#n1156",MatchSource.CODE_COMMENT,batch/batch/resource_usage.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py:289,Usability,guid,guide,289,"# See below for a nice breakdown of the memory cgroupv2:; # https://facebookmicrosites.github.io/cgroup2/docs/memory-controller.html#core-interface-files; #; # and here for the authoritative source:; # https://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup.git/tree/Documentation/admin-guide/cgroup-v2.rst#n1156",MatchSource.CODE_COMMENT,batch/batch/resource_usage.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/resource_usage.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/resources.py:89,Energy Efficiency,power,power,89,"# Azure bills for specific disk sizes so we must round the storage_in_gib to the nearest power of two",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/resources.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/resources.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/resources.py:89,Energy Efficiency,power,power,89,"# Azure bills for specific disk sizes so we must round the storage_in_gib to the nearest power of two",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/resources.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/resources.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/driver/driver.py:54,Deployability,deploy,deployments,54,"# https://docs.microsoft.com/en-us/rest/api/resources/deployments/list-by-resource-group#provisioningstate",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/driver/driver.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/worker/worker_api.py:79,Security,authenticat,authentication,79,"# https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli#az-acr-login-with---expose-token",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/worker/worker_api.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/worker/worker_api.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/worker/worker_api.py:129,Security,expose,expose-token,129,"# https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli#az-acr-login-with---expose-token",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/worker/worker_api.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/worker/worker_api.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/worker/worker_api.py:116,Testability,log,login-with---expose-token,116,"# https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication?tabs=azure-cli#az-acr-login-with---expose-token",MatchSource.CODE_COMMENT,batch/batch/cloud/azure/worker/worker_api.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/azure/worker/worker_api.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/create_instance.py:276,Availability,echo,echo,276,"""""""; #!/bin/bash; set -x. NAME=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google'); ZONE=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google'). if [ -f ""/started"" ]; then; echo ""instance $NAME has previously been started""; while true; do; gcloud -q compute instances delete $NAME --zone=$ZONE; sleep 1; done; exit; else; touch /started; fi. curl -s -H ""Metadata-Flavor: Google"" ""http://metadata.google.internal/computeMetadata/v1/instance/attributes/run_script"" >./run.sh. nohup /bin/bash run.sh >run.log 2>&1 &; """"""",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/create_instance.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/create_instance.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/create_instance.py:605,Testability,log,log,605,"""""""; #!/bin/bash; set -x. NAME=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google'); ZONE=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/zone -H 'Metadata-Flavor: Google'). if [ -f ""/started"" ]; then; echo ""instance $NAME has previously been started""; while true; do; gcloud -q compute instances delete $NAME --zone=$ZONE; sleep 1; done; exit; else; touch /started; fi. curl -s -H ""Metadata-Flavor: Google"" ""http://metadata.google.internal/computeMetadata/v1/instance/attributes/run_script"" >./run.sh. nohup /bin/bash run.sh >run.log 2>&1 &; """"""",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/create_instance.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/create_instance.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/create_instance.py:301,Testability,log,log,301,"""""""; set -x. INSTANCE_ID=$(curl -s -H ""Metadata-Flavor: Google"" ""http://metadata.google.internal/computeMetadata/v1/instance/attributes/instance_id""); NAME=$(curl -s http://metadata.google.internal/computeMetadata/v1/instance/name -H 'Metadata-Flavor: Google'). journalctl -u docker.service > dockerd.log; """"""",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/create_instance.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/create_instance.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py:382,Deployability,deploy,deployments,382,"# The project-wide logging quota is 60 request/m. The event; # loop sleeps 15s per iteration, so the max rate is 4; # iterations/m. Note, the event loop could make multiple; # logging requests per iteration, so these numbers are not; # quite comparable. I didn't want to consume the entire quota; # since there will be other users of the logging API (us at; # the web console, test deployments, etc.)",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py:19,Testability,log,logging,19,"# The project-wide logging quota is 60 request/m. The event; # loop sleeps 15s per iteration, so the max rate is 4; # iterations/m. Note, the event loop could make multiple; # logging requests per iteration, so these numbers are not; # quite comparable. I didn't want to consume the entire quota; # since there will be other users of the logging API (us at; # the web console, test deployments, etc.)",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py:176,Testability,log,logging,176,"# The project-wide logging quota is 60 request/m. The event; # loop sleeps 15s per iteration, so the max rate is 4; # iterations/m. Note, the event loop could make multiple; # logging requests per iteration, so these numbers are not; # quite comparable. I didn't want to consume the entire quota; # since there will be other users of the logging API (us at; # the web console, test deployments, etc.)",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py:338,Testability,log,logging,338,"# The project-wide logging quota is 60 request/m. The event; # loop sleeps 15s per iteration, so the max rate is 4; # iterations/m. Note, the event loop could make multiple; # logging requests per iteration, so these numbers are not; # quite comparable. I didn't want to consume the entire quota; # since there will be other users of the logging API (us at; # the web console, test deployments, etc.)",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py:377,Testability,test,test,377,"# The project-wide logging quota is 60 request/m. The event; # loop sleeps 15s per iteration, so the max rate is 4; # iterations/m. Note, the event loop could make multiple; # logging requests per iteration, so these numbers are not; # quite comparable. I didn't want to consume the entire quota; # since there will be other users of the logging API (us at; # the web console, test deployments, etc.)",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/driver.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/driver.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/zones.py:100,Modifiability,variab,variable,100,"# FIXME: data_disk_size_gb is assumed to be constant across all instances, but it is; # passed as a variable parameter to this function!!",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/driver/zones.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/driver/zones.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/worker/metadata_server.py:190,Security,access,access,190,"# https://cloud.google.com/compute/docs/metadata/querying-metadata; # token is not included in the recursive version, presumably as that; # is not simple metadata but requires requesting an access token",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/worker/metadata_server.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/worker/metadata_server.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/worker/metadata_server.py:147,Usability,simpl,simple,147,"# https://cloud.google.com/compute/docs/metadata/querying-metadata; # token is not included in the recursive version, presumably as that; # is not simple metadata but requires requesting an access token",MatchSource.CODE_COMMENT,batch/batch/cloud/gcp/worker/metadata_server.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/cloud/gcp/worker/metadata_server.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/billing_manager.py:63,Availability,redundant,redundant,63,"# this prevents having too many resources in the database with redundant information",MatchSource.CODE_COMMENT,batch/batch/driver/billing_manager.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/billing_manager.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/billing_manager.py:63,Safety,redund,redundant,63,"# this prevents having too many resources in the database with redundant information",MatchSource.CODE_COMMENT,batch/batch/driver/billing_manager.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/billing_manager.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance.py:9,Energy Efficiency,charge,charges,9,"""""""; The charges incurred from the cloud for this instance, ignoring attached disks.; """"""",MatchSource.CODE_COMMENT,batch/batch/driver/instance.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/job.py:18,Energy Efficiency,schedul,scheduling,18,"# may also create scheduling opportunities, set above",MatchSource.CODE_COMMENT,batch/batch/driver/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/job.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/__main__.py:2,Modifiability,config,configure,2,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,batch/batch/driver/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/__main__.py:12,Testability,log,logging,12,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,batch/batch/driver/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance_collection/pool.py:29,Energy Efficiency,schedul,scheduled,29,"# estimate of number of jobs scheduled per scheduling loop approximately every second",MatchSource.CODE_COMMENT,batch/batch/driver/instance_collection/pool.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance_collection/pool.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance_collection/pool.py:43,Energy Efficiency,schedul,scheduling,43,"# estimate of number of jobs scheduled per scheduling loop approximately every second",MatchSource.CODE_COMMENT,batch/batch/driver/instance_collection/pool.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance_collection/pool.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance_collection/pool.py:82,Energy Efficiency,schedul,scheduler,82,"# job_group_id must be part of the ordering when selecting records; # because the scheduler selects records by job group in order",MatchSource.CODE_COMMENT,batch/batch/driver/instance_collection/pool.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/driver/instance_collection/pool.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:74,Deployability,deploy,deployments,74,"# Clients stopped using `mount_tokens` prior to the introduction of terra deployments",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:22,Deployability,update,updates,22,"# jobs in non-initial updates of a batch always start out as pending; # because they may have currently running parents in previous updates; # and we dont take those into account here when calculating the number; # of pending parents",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:132,Deployability,update,updates,132,"# jobs in non-initial updates of a batch always start out as pending; # because they may have currently running parents in previous updates; # and we dont take those into account here when calculating the number; # of pending parents",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:67,Availability,error,error-reference,67,"# 1062 ER_DUP_ENTRY https://dev.mysql.com/doc/refman/5.7/en/server-error-reference.html#error_er_dup_entry",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:67,Availability,error,error-reference,67,"# 1062 ER_DUP_ENTRY https://dev.mysql.com/doc/refman/5.7/en/server-error-reference.html#error_er_dup_entry",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:35,Availability,error,error,35,"# IMPORTANT: If cancellation or an error prevents writing the spec to the cloud, then we; # must rollback. See https://github.com/hail-is/hail-production-issues/issues/9",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:97,Availability,rollback,rollback,97,"# IMPORTANT: If cancellation or an error prevents writing the spec to the cloud, then we; # must rollback. See https://github.com/hail-is/hail-production-issues/issues/9",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:97,Deployability,rollback,rollback,97,"# IMPORTANT: If cancellation or an error prevents writing the spec to the cloud, then we; # must rollback. See https://github.com/hail-is/hail-production-issues/issues/9",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:47,Deployability,update,update,47,"# We use FOR UPDATE so that we serialize batch update insertions; # This is necessary to reserve job id and job group id ranges.; # We don't allow updates to batches that have been cancelled; # but do allow updates to batches with jobs that have been cancelled.",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:147,Deployability,update,updates,147,"# We use FOR UPDATE so that we serialize batch update insertions; # This is necessary to reserve job id and job group id ranges.; # We don't allow updates to batches that have been cancelled; # but do allow updates to batches with jobs that have been cancelled.",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:207,Deployability,update,updates,207,"# We use FOR UPDATE so that we serialize batch update insertions; # This is necessary to reserve job id and job group id ranges.; # We don't allow updates to batches that have been cancelled; # but do allow updates to batches with jobs that have been cancelled.",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:24,Availability,avail,available,24,"# empty response if not available yet",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:10,Testability,log,logs,10,"# Not all logs will be proper utf-8 but we attempt to show them as; # str or else Jinja will present them surrounded by b''",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:41,Safety,avoid,avoid,41,"# we want to be case-insensitive here to avoid duplicates with existing records",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py:13,Safety,avoid,avoid,13,"# we want to avoid having billing projects with different cases but the same name",MatchSource.CODE_COMMENT,batch/batch/front_end/front_end.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/front_end.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/validate.py:8,Security,validat,validate,8,"# FIXME validate image; # https://github.com/docker/distribution/blob/master/reference/regexp.go#L68",MatchSource.CODE_COMMENT,batch/batch/front_end/validate.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/validate.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/__main__.py:2,Modifiability,config,configure,2,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,batch/batch/front_end/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/__main__.py:12,Testability,log,logging,12,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,batch/batch/front_end/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v1.py:25,Security,validat,validated,25,"# batch has already been validated",MatchSource.CODE_COMMENT,batch/batch/front_end/query/query_v1.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v1.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py:2,Testability,log,logic,2,"# logic to make time interval queries fast",MatchSource.CODE_COMMENT,batch/batch/front_end/query/query_v2.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py:25,Security,validat,validated,25,"# batch has already been validated",MatchSource.CODE_COMMENT,batch/batch/front_end/query/query_v2.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py:2,Testability,log,logic,2,"# logic to make time interval queries fast",MatchSource.CODE_COMMENT,batch/batch/front_end/query/query_v2.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py:25,Security,validat,validated,25,"# batch has already been validated",MatchSource.CODE_COMMENT,batch/batch/front_end/query/query_v2.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/front_end/query/query_v2.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:5,Deployability,patch,patched,5,"# We patched aiodocker's utility function `compose_auth_header` because it does not base64 encode strings; # in urlsafe mode which is required for Azure's credentials.; # https://github.com/aio-libs/aiodocker/blob/17e08844461664244ea78ecd08d1672b1779acc1/aiodocker/utils.py#L297",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:42,Security,access,access,42,"# Jobs on the private network should have access to the metadata server; # and our vdc. The public network should not so we use google's public; # resolver.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:45,Security,expose,exposed,45,"# Appending to PREROUTING means this is only exposed to external traffic.; # To expose for locally created packets, we would append instead to the OUTPUT chain.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:80,Security,expose,expose,80,"# Appending to PREROUTING means this is only exposed to external traffic.; # To expose for locally created packets, we would append instead to the OUTPUT chain.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:130,Security,authoriz,authorized,130,"# Mounts that can be shared across jobs by the same user; # Only sharing within jobs of the same user ensures that; # the user is authorized to access the bucket. A user only has a single; # set of credentials for cloudfuse so if they have successfully mounted; # a bucket we can ignore the passed-in credentials and reuse the previous; # mount.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:144,Security,access,access,144,"# Mounts that can be shared across jobs by the same user; # Only sharing within jobs of the same user ensures that; # the user is authorized to access the bucket. A user only has a single; # set of credentials for cloudfuse so if they have successfully mounted; # a bucket we can ignore the passed-in credentials and reuse the previous; # mount.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:77,Performance,perform,performance,77,"# Pull to verify this user has access to this; # image.; # FIXME improve the performance of this with a; # per-user image cache.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:122,Performance,cache,cache,122,"# Pull to verify this user has access to this; # image.; # FIXME improve the performance of this with a; # per-user image cache.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:31,Security,access,access,31,"# Pull to verify this user has access to this; # image.; # FIXME improve the performance of this with a; # per-user image cache.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:110,Performance,cache,cached,110,"# FIXME Authentication is entangled with pulling images. We need a way to test; # that a user has access to a cached image without pulling.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:98,Security,access,access,98,"# FIXME Authentication is entangled with pulling images. We need a way to test; # that a user has access to a cached image without pulling.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:74,Testability,test,test,74,"# FIXME Authentication is entangled with pulling images. We need a way to test; # that a user has access to a cached image without pulling.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:61,Modifiability,config,config,61,"# https://github.com/opencontainers/runtime-spec/blob/master/config.md",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:59,Modifiability,config,config-linux,59,"# https://github.com/opencontainers/runtime-spec/blob/main/config-linux.md",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:89,Modifiability,config,config-linux,89,"# Recommended filesystems:; # https://github.com/opencontainers/runtime-spec/blob/master/config-linux.md#default-filesystems",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:19,Modifiability,variab,variables,19,"# User-defined env variables should take precedence",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:63,Availability,error,error,63,"# {; # name: str,; # state: str, (pending, running, succeeded, error, failed); # timing: dict(str, float),; # error: str, (optional); # short_error: str, (optional); # container_status: {; # state: str,; # started_at: int, (date); # finished_at: int, (date); # out_of_memory: bool,; # exit_code: int; # }; # }",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:110,Availability,error,error,110,"# {; # name: str,; # state: str, (pending, running, succeeded, error, failed); # timing: dict(str, float),; # error: str, (optional); # short_error: str, (optional); # container_status: {; # state: str,; # started_at: int, (date); # finished_at: int, (date); # out_of_memory: bool,; # exit_code: int; # }; # }",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:171,Performance,cache,caches,171,"# The reason for not giving each job 5 Gi (for example) is the; # maximum number of simultaneous jobs on a worker is 64 which; # basically fills the disk not allowing for caches etc. Most jobs; # would need an external disk in that case.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:186,Availability,error,error,186,"# {; # version: int,; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # job_group_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int,; # resources: list of dict, {name: str, quantity: int}; # region: str # type: ignore; # }",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:227,Availability,error,error,227,"# {; # version: int,; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # job_group_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int,; # resources: list of dict, {name: str, quantity: int}; # region: str # type: ignore; # }",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:47,Safety,avoid,avoid,47,"# Make sure this path isn't in self.scratch to avoid accidental bucket deletions!",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:47,Safety,avoid,avoid,47,"# Make sure this path isn't in self.scratch to avoid accidental bucket deletions!",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:47,Safety,avoid,avoid,47,"# Make sure this path isn't in self.scratch to avoid accidental bucket deletions!",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:78,Deployability,deploy,deploy,78,"# Necessary for backward compatibility for Hail Query jars that expect; # the deploy config at this path and not at `/deploy-config/deploy-config.json`",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:118,Deployability,deploy,deploy-config,118,"# Necessary for backward compatibility for Hail Query jars that expect; # the deploy config at this path and not at `/deploy-config/deploy-config.json`",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:132,Deployability,deploy,deploy-config,132,"# Necessary for backward compatibility for Hail Query jars that expect; # the deploy config at this path and not at `/deploy-config/deploy-config.json`",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:85,Modifiability,config,config,85,"# Necessary for backward compatibility for Hail Query jars that expect; # the deploy config at this path and not at `/deploy-config/deploy-config.json`",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:125,Modifiability,config,config,125,"# Necessary for backward compatibility for Hail Query jars that expect; # the deploy config at this path and not at `/deploy-config/deploy-config.json`",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:139,Modifiability,config,config,139,"# Necessary for backward compatibility for Hail Query jars that expect; # the deploy config at this path and not at `/deploy-config/deploy-config.json`",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:47,Safety,avoid,avoid,47,"# Make sure this path isn't in self.scratch to avoid accidental bucket deletions!",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:47,Safety,avoid,avoid,47,"# Make sure this path isn't in self.scratch to avoid accidental bucket deletions!",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:47,Safety,avoid,avoid,47,"# Make sure this path isn't in self.scratch to avoid accidental bucket deletions!",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:11,Deployability,configurat,configuration,11,"# We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwards compatible. In anticipation; # of future additional job parameters, we decided to write the batch configuration to a; # file with explicit versioning to make sure we maintain backwards compatibility.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:89,Deployability,configurat,configuration,89,"# We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwards compatible. In anticipation; # of future additional job parameters, we decided to write the batch configuration to a; # file with explicit versioning to make sure we maintain backwards compatibility.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:411,Deployability,configurat,configuration,411,"# We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwards compatible. In anticipation; # of future additional job parameters, we decided to write the batch configuration to a; # file with explicit versioning to make sure we maintain backwards compatibility.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:11,Modifiability,config,configuration,11,"# We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwards compatible. In anticipation; # of future additional job parameters, we decided to write the batch configuration to a; # file with explicit versioning to make sure we maintain backwards compatibility.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:54,Modifiability,variab,variables,54,"# We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwards compatible. In anticipation; # of future additional job parameters, we decided to write the batch configuration to a; # file with explicit versioning to make sure we maintain backwards compatibility.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:89,Modifiability,config,configuration,89,"# We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwards compatible. In anticipation; # of future additional job parameters, we decided to write the batch configuration to a; # file with explicit versioning to make sure we maintain backwards compatibility.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:183,Modifiability,variab,variables,183,"# We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwards compatible. In anticipation; # of future additional job parameters, we decided to write the batch configuration to a; # file with explicit versioning to make sure we maintain backwards compatibility.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:411,Modifiability,config,configuration,411,"# We use a configuration file (instead of environment variables) to pass job-specific; # configuration options for a JVMJob because we cannot alter the JVM container's; # environment variables after it has been started and it is difficult to make; # passing additional command line arguments to the job backwards compatible. In anticipation; # of future additional job parameters, we decided to write the batch configuration to a; # file with explicit versioning to make sure we maintain backwards compatibility.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:164,Availability,error,error,164,"# {; # version: int,; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed, cancelled); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int,; # resources: list of dict, {name: str, quantity: int},; # jvm: str; # }",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:216,Availability,error,error,216,"# {; # version: int,; # worker: str,; # batch_id: int,; # job_id: int,; # attempt_id: int,; # user: str,; # state: str, (pending, initializing, running, succeeded, error, failed, cancelled); # format_version: int; # error: str, (optional); # container_statuses: [Container.status],; # start_time: int,; # end_time: int,; # resources: list of dict, {name: str, quantity: int},; # jvm: str; # }",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:5,Energy Efficiency,allocate,allocate,5,"# We allocate 60% of memory per core to off heap memory",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:30,Testability,log,logs,30,"# NOTE we assume that the JVM logs hail emits will be valid utf-8",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:25,Testability,log,log,25,"# Do not include the JVM log in the exception as this is sent to the user and; # the JVM log might inadvetantly contain sensitive information.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:89,Testability,log,log,89,"# Do not include the JVM log in the exception as this is sent to the user and; # the JVM log might inadvetantly contain sensitive information.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:39,Availability,down,down,39,"# check worker hasn't started shutting down",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:77,Availability,recover,recover,77,"# Don't retry. If it doesn't go through, the driver; # monitoring loops will recover. If the driver is; # gone (e.g. testing a PR), this would go into an; # infinite loop and the instance won't be deleted.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:55,Energy Efficiency,monitor,monitoring,55,"# Don't retry. If it doesn't go through, the driver; # monitoring loops will recover. If the driver is; # gone (e.g. testing a PR), this would go into an; # infinite loop and the instance won't be deleted.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:77,Safety,recover,recover,77,"# Don't retry. If it doesn't go through, the driver; # monitoring loops will recover. If the driver is; # gone (e.g. testing a PR), this would go into an; # infinite loop and the instance won't be deleted.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py:117,Testability,test,testing,117,"# Don't retry. If it doesn't go through, the driver; # monitoring loops will recover. If the driver is; # gone (e.g. testing a PR), this would go into an; # infinite loop and the instance won't be deleted.",MatchSource.CODE_COMMENT,batch/batch/worker/worker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/batch/worker/worker.py
https://github.com/hail-is/hail/tree/0.2.133/batch/load-test/test-max-scheduling-rate.py:32,Energy Efficiency,schedul,scheduling,32,"# the number of jobs is the max scheduling rate times the scheduling duration",MatchSource.CODE_COMMENT,batch/load-test/test-max-scheduling-rate.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/load-test/test-max-scheduling-rate.py
https://github.com/hail-is/hail/tree/0.2.133/batch/load-test/test-max-scheduling-rate.py:58,Energy Efficiency,schedul,scheduling,58,"# the number of jobs is the max scheduling rate times the scheduling duration",MatchSource.CODE_COMMENT,batch/load-test/test-max-scheduling-rate.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/load-test/test-max-scheduling-rate.py
https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py:97,Security,firewall,firewall-fees,97,"'''; INSERT INTO latest_product_versions (product, version); VALUES ('gcp-support-logs-specs-and-firewall-fees', '1');; '''",MatchSource.CODE_COMMENT,batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py
https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py:82,Testability,log,logs-specs-and-firewall-fees,82,"'''; INSERT INTO latest_product_versions (product, version); VALUES ('gcp-support-logs-specs-and-firewall-fees', '1');; '''",MatchSource.CODE_COMMENT,batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py
https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py:81,Security,firewall,firewall-fees,81,"'''; INSERT INTO resources (resource, rate); VALUES ('gcp-support-logs-specs-and-firewall-fees/1', 0.000000000001388888888888889);; '''",MatchSource.CODE_COMMENT,batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py
https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py:66,Testability,log,logs-specs-and-firewall-fees,66,"'''; INSERT INTO resources (resource, rate); VALUES ('gcp-support-logs-specs-and-firewall-fees/1', 0.000000000001388888888888889);; '''",MatchSource.CODE_COMMENT,batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py
https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py:107,Security,firewall,firewall-fees,107,'''; UPDATE resources; SET deduped_resource_id = resource_id; WHERE resource = 'gcp-support-logs-specs-and-firewall-fees/1';; ''',MatchSource.CODE_COMMENT,batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py
https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py:92,Testability,log,logs-specs-and-firewall-fees,92,'''; UPDATE resources; SET deduped_resource_id = resource_id; WHERE resource = 'gcp-support-logs-specs-and-firewall-fees/1';; ''',MatchSource.CODE_COMMENT,batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/add-gcp-support-logs-specs-and-firewall-fees.py
https://github.com/hail-is/hail/tree/0.2.133/batch/sql/insert_globals.py:36,Testability,log,log,36,"# 22 is 16 bytes of entropy; # math.log((2**8)**16, 62) = 21.497443706501368",MatchSource.CODE_COMMENT,batch/sql/insert_globals.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/insert_globals.py
https://github.com/hail-is/hail/tree/0.2.133/batch/sql/populate_agg_billing_by_date.py:9,Security,audit,audit,9,"# cannot audit billing project records because they are partially filled in from batches with format version < 3",MatchSource.CODE_COMMENT,batch/sql/populate_agg_billing_by_date.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/sql/populate_agg_billing_by_date.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py:2,Testability,test,test,2,"# test idempotent",MatchSource.CODE_COMMENT,batch/test/test_accounts.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py:2,Testability,test,test,2,"# test idempotent",MatchSource.CODE_COMMENT,batch/test/test_accounts.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py:2,Testability,test,test,2,"# test idempotent",MatchSource.CODE_COMMENT,batch/test/test_accounts.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py:2,Testability,test,test,2,"# test idempotent",MatchSource.CODE_COMMENT,batch/test/test_accounts.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py:2,Testability,test,test,2,"# test idempotent",MatchSource.CODE_COMMENT,batch/test/test_accounts.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py:2,Testability,test,test,2,"# test idempotent",MatchSource.CODE_COMMENT,batch/test/test_accounts.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_accounts.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:52,Testability,test,tests,52,"# list_batches returns all batches for all prev run tests so we set a limit",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:52,Testability,test,tests,52,"# list_batches returns all batches for all prev run tests so we set a limit",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:23,Testability,log,log,23,"# cancelled job has no log",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:19,Testability,log,login,19,"# redirect to auth/login",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:5,Availability,echo,echo,5,"""""""; echo $HAIL_BATCH_WORKER_PORT; echo $HAIL_BATCH_WORKER_IP; """"""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:35,Availability,echo,echo,35,"""""""; echo $HAIL_BATCH_WORKER_PORT; echo $HAIL_BATCH_WORKER_IP; """"""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:51,Deployability,deploy,deploy-config,51,"""""""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:65,Deployability,deploy,deploy-config,65,"""""""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:109,Deployability,deploy,deploy-config,109,"""""""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:123,Deployability,deploy,deploy-config,123,"""""""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:58,Modifiability,config,config,58,"""""""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:72,Modifiability,config,config,72,"""""""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:116,Modifiability,config,config,116,"""""""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:130,Modifiability,config,config,130,"""""""; set -ex; jq '.default_namespace = ""default""' /deploy-config/deploy-config.json > tmp.json; mv tmp.json /deploy-config/deploy-config.json""""""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:39,Availability,echo,echo,39,"""""""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 localhost 5000; """"""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:39,Availability,echo,echo,39,"""""""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 127.0.0.1 5000; """"""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:39,Availability,echo,echo,39,"""""""; set -e; nc -l -p 5000 &; sleep 5; echo ""hello"" | nc -q 1 $(hostname -i) 5000; """"""",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:30,Availability,avail,available,30,"# G2 instances are not always available within a time window; # acceptable for CI. This test is permitted to time out; # but not otherwise fail",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:88,Testability,test,test,88,"# G2 instances are not always available within a time window; # acceptable for CI. This test is permitted to time out; # but not otherwise fail",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py:16,Deployability,update,update,16,"# do not commit update",MatchSource.CODE_COMMENT,batch/test/test_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_batch.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_time_limited_max_size_cache.py:60,Safety,timeout,timeout,60,"# Python should execute these lookups before the one second timeout on keys 3 and 10, but this; # code is unavoidably a data race. This test can fail under extraordinary conditions which cause; # a stall in the Python interpreter.",MatchSource.CODE_COMMENT,batch/test/test_time_limited_max_size_cache.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_time_limited_max_size_cache.py
https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_time_limited_max_size_cache.py:136,Testability,test,test,136,"# Python should execute these lookups before the one second timeout on keys 3 and 10, but this; # code is unavoidably a data race. This test can fail under extraordinary conditions which cause; # a stall in the Python interpreter.",MatchSource.CODE_COMMENT,batch/test/test_time_limited_max_size_cache.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/batch/test/test_time_limited_max_size_cache.py
https://github.com/hail-is/hail/tree/0.2.133/ci/bootstrap.py:13,Performance,cache,cache,13,"# Reboot the cache on each use. The kube client isn't; # refreshing tokens correctly.; # https://github.com/kubernetes-client/python/issues/741; # Note, that is in the kubenetes-client repo, the; # kubernetes_asyncio. I'm assuming it has the same; # issue.",MatchSource.CODE_COMMENT,ci/bootstrap.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/bootstrap.py
https://github.com/hail-is/hail/tree/0.2.133/ci/bootstrap_create_accounts.py:7,Modifiability,config,config,7,"# kube.config.load_incluster_config()",MatchSource.CODE_COMMENT,ci/bootstrap_create_accounts.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/bootstrap_create_accounts.py
https://github.com/hail-is/hail/tree/0.2.133/ci/create_database.py:9,Security,checksum,checksum,9,"# verify checksum",MatchSource.CODE_COMMENT,ci/create_database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/create_database.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:47,Integrability,depend,dependencies,47,"# transitively close requested_step_names over dependencies",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:41,Deployability,deploy,deploy,41,"# CIs that don't live in default doing a deploy; # should not clobber the main `cache` tag",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:80,Performance,cache,cache,80,"# CIs that don't live in default doing a deploy; # should not clobber the main `cache` tag",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:8,Deployability,configurat,configuration,8,"# FIXME configuration",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:8,Modifiability,config,configuration,8,"# FIXME configuration",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:8,Deployability,configurat,configuration,8,"# FIXME configuration",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:8,Modifiability,config,configuration,8,"# FIXME configuration",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:8,Deployability,configurat,configuration,8,"# FIXME configuration",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:8,Modifiability,config,configuration,8,"# FIXME configuration",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py:8,Security,validat,validate,8,"# FIXME validate",MatchSource.CODE_COMMENT,ci/ci/build.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/build.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/ci.py:36,Testability,log,log,36,"# FIXME generate links to the merge log",MatchSource.CODE_COMMENT,ci/ci/ci.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/ci.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/ci.py:15,Deployability,deploy,deploy,15,"# FIXME recent deploy history",MatchSource.CODE_COMMENT,ci/ci/ci.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/ci.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/ci.py:36,Testability,log,log,36,"# FIXME generate links to the merge log",MatchSource.CODE_COMMENT,ci/ci/ci.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/ci.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/envoy.py:6,Modifiability,config,config,6,"# The config is set per load balancer pod, so we must account for; # multiple replicas of the load balancer",MatchSource.CODE_COMMENT,ci/ci/envoy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/envoy.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/envoy.py:24,Performance,load,load,24,"# The config is set per load balancer pod, so we must account for; # multiple replicas of the load balancer",MatchSource.CODE_COMMENT,ci/ci/envoy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/envoy.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/envoy.py:94,Performance,load,load,94,"# The config is set per load balancer pod, so we must account for; # multiple replicas of the load balancer",MatchSource.CODE_COMMENT,ci/ci/envoy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/envoy.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:33,Availability,failure,failure,33,"# record the context for a merge failure",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:3,Availability,error,error,3,"# 'error', 'success', 'failure', None",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:23,Availability,failure,failure,23,"# 'error', 'success', 'failure', None",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:57,Deployability,configurat,configuration,57,"# This probably means the repo has no ""required reviews"" configuration. But CI shouldn't merge without; # at least one approval, so we'll treat this as ""pending"":",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:57,Modifiability,config,configuration,57,"# This probably means the repo has no ""required reviews"" configuration. But CI shouldn't merge without; # at least one approval, so we'll treat this as ""pending"":",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:2,Usability,clear,clear,2,"# clear current batch",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:51,Availability,failure,failure,51,"# pylint: disable=broad-except; # FIXME save merge failure output for UI",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:11,Availability,failure,failure,11,"# success, failure, pending",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:2,Deployability,update,update,2,"# update everything",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py:6,Deployability,deploy,deploying,6,"# not deploying",MatchSource.CODE_COMMENT,ci/ci/github.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/github.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/__main__.py:2,Modifiability,config,configure,2,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,ci/ci/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/ci/ci/__main__.py:12,Testability,log,logging,12,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,ci/ci/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/ci/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/ci/sql/fix-default-namespace.py:37,Testability,test,test,37,"# This fix is only necessary for dev/test namespaces",MatchSource.CODE_COMMENT,ci/sql/fix-default-namespace.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/ci/sql/fix-default-namespace.py
https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.GTEx.py:877,Testability,test,test,877,"""""""Import a GTF file. The GTF file format is identical to the GFF version 2 file format,; and so this function can be used to import GFF version 2 files as; well. See https://www.ensembl.org/info/website/upload/gff.html for more; details on the GTF/GFF2 file format. The :class:`.Table` returned by this function will include the following; row fields:. .. code-block:: text. 'seqname': str; 'source': str; 'feature': str; 'start': int32; 'end': int32; 'score': float64; 'strand': str; 'frame': int32. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. .. note::. The ""end"" field in the table will be incremented by 1 in; comparison to the value found in the GTF file, as the end; coordinate in a GTF file is inclusive while the end; coordinate in Hail is exclusive. Example; -------. >>> ht = hl.experimental.import_gtf('data/test.gtf', key='gene_id'); >>> ht.describe(). .. code-block:: text. ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'seqname': str; 'source': str; 'feature': str; 'start': int32; 'end': int32; 'score': float64; 'strand': str; 'frame': int32; 'havana_gene': str; 'exon_id': str; 'havana_transcript': str; 'transcript_name': str; 'gene_type': str; 'tag': str; 'transcript_status': str; 'exon_number': str; 'level': str; 'transcript_id': str; 'transcript_type': str; 'gene_id': str; 'gene_name': str; 'gene_status': str; ----------------------------------------; Key: ['gene_id']; ----------------------------------------. Parameters; ----------. path : :obj:`str`; File to import.; key : :obj:`str` or :obj:`list` of :obj:`str`; Key field(s). Can be tag name(s) found in the attribute field; of the GTF file. Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,datasets/load/load.GTEx.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.GTEx.py
https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.GTEx_v7.py:1496,Testability,test,test,1496,"rame': int32; 'interval': interval<>. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. Note; ----. This function will return an ``interval`` field of type :class:`.tinterval`; constructed from the ``seqname``, ``start``, and ``end`` fields in the; GTF file. This interval is inclusive of both the start and end positions; in the GTF file. . If the ``reference_genome`` parameter is specified, the start and end; points of the ``interval`` field will be of type :class:`.tlocus`.; Otherwise, the start and end points of the ``interval`` field will be of; type :class:`.tstruct` with fields ``seqname`` (type :class:`str`) and; ``position`` (type :class:`.tint32`). Furthermore, if the ``reference_genome`` parameter is specified and; ``skip_invalid_contigs`` is ``True``, this import function will skip; lines in the GTF where ``seqname`` is not consistent with the reference; genome specified. Example; -------. >>> ht = hl.experimental.import_gtf('data/test.gtf', ; reference_genome='GRCh37',; skip_invalid_contigs=True); >>> ht.describe(). .. code-block:: text. ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :obj:`str`; File to import.; reference_genome : :obj:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqna",MatchSource.CODE_COMMENT,datasets/load/load.GTEx_v7.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/datasets/load/load.GTEx_v7.py
https://github.com/hail-is/hail/tree/0.2.133/devbin/dev_proxy.py:86,Modifiability,rewrite,rewrite,86,"# Make links point back to the local dev server and not use; # the dev namespace path rewrite shenanigans.",MatchSource.CODE_COMMENT,devbin/dev_proxy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/devbin/dev_proxy.py
https://github.com/hail-is/hail/tree/0.2.133/gear/gear/auth.py:11,Integrability,rout,routes,11,"# Only web routes should redirect by default",MatchSource.CODE_COMMENT,gear/gear/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/auth.py
https://github.com/hail-is/hail/tree/0.2.133/gear/gear/csrf.py:79,Security,authenticat,authentication,79,"# CSRF prevention is only relevant to requests that use browser; # cookies for authentication.",MatchSource.CODE_COMMENT,gear/gear/csrf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/csrf.py
https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py:19,Safety,timeout,timeout,19,"# 1205 - Lock wait timeout exceeded; try restarting transaction",MatchSource.CODE_COMMENT,gear/gear/database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py
https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py:53,Testability,log,logging,53,"""""""If a database exception is retryable, return its `logging` log level.""""""",MatchSource.CODE_COMMENT,gear/gear/database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py
https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py:62,Testability,log,log,62,"""""""If a database exception is retryable, return its `logging` log level.""""""",MatchSource.CODE_COMMENT,gear/gear/database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py
https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py:131,Availability,rollback,rollback,131,"# pylint: disable=unused-argument; # cancelling cleanup could leak a connection; # await shield becuase we want to wait for commit/rollback to finish",MatchSource.CODE_COMMENT,gear/gear/database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py
https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py:131,Deployability,rollback,rollback,131,"# pylint: disable=unused-argument; # cancelling cleanup could leak a connection; # await shield becuase we want to wait for commit/rollback to finish",MatchSource.CODE_COMMENT,gear/gear/database.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/database.py
https://github.com/hail-is/hail/tree/0.2.133/gear/gear/metrics.py:34,Integrability,rout,route,34,"# Use the path template given to @route.<METHOD>, not the fully resolved one",MatchSource.CODE_COMMENT,gear/gear/metrics.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/gear/gear/metrics.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:39,Testability,benchmark,benchmark,39,"# Initialise hail before running every benchmark for two reasons:; # - each benchmark runs in a clean hail session; # - our means of getting max task memory is quite crude (regex on logs); # and a fresh session provides a new log",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:76,Testability,benchmark,benchmark,76,"# Initialise hail before running every benchmark for two reasons:; # - each benchmark runs in a clean hail session; # - our means of getting max task memory is quite crude (regex on logs); # and a fresh session provides a new log",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:182,Testability,log,logs,182,"# Initialise hail before running every benchmark for two reasons:; # - each benchmark runs in a clean hail session; # - our means of getting max task memory is quite crude (regex on logs); # and a fresh session provides a new log",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:226,Testability,log,log,226,"# Initialise hail before running every benchmark for two reasons:; # - each benchmark runs in a clean hail session; # - our means of getting max task memory is quite crude (regex on logs); # and a fresh session provides a new log",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:65,Availability,down,down,65,"# `nextitem` is used to determine which fixtures need to be torn-down; # after the test finishes. For example, if `nextitem` is `None`, then; # all fixtures (including session fixtures) will be finalised.; # Since we're invoking this benchmark repeatedly, we want to tear-down; # function/method level fixtures only, leaving module and session; # fixtures in place; `item.parent` is one such `Item` that represents this.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:272,Availability,down,down,272,"# `nextitem` is used to determine which fixtures need to be torn-down; # after the test finishes. For example, if `nextitem` is `None`, then; # all fixtures (including session fixtures) will be finalised.; # Since we're invoking this benchmark repeatedly, we want to tear-down; # function/method level fixtures only, leaving module and session; # fixtures in place; `item.parent` is one such `Item` that represents this.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:83,Testability,test,test,83,"# `nextitem` is used to determine which fixtures need to be torn-down; # after the test finishes. For example, if `nextitem` is `None`, then; # all fixtures (including session fixtures) will be finalised.; # Since we're invoking this benchmark repeatedly, we want to tear-down; # function/method level fixtures only, leaving module and session; # fixtures in place; `item.parent` is one such `Item` that represents this.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:234,Testability,benchmark,benchmark,234,"# `nextitem` is used to determine which fixtures need to be torn-down; # after the test finishes. For example, if `nextitem` is `None`, then; # all fixtures (including session fixtures) will be finalised.; # Since we're invoking this benchmark repeatedly, we want to tear-down; # function/method level fixtures only, leaving module and session; # fixtures in place; `item.parent` is one such `Item` that represents this.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:26,Performance,perform,perform,26,"# on the final iteration, perform the required teardown for the test",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:64,Testability,test,test,64,"# on the final iteration, perform the required teardown for the test",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:16,Modifiability,plugin,plugins,16,"# prevent other plugins running that might invoke the benchmark again",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:54,Testability,benchmark,benchmark,54,"# prevent other plugins running that might invoke the benchmark again",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:16,Modifiability,plugin,plugins,16,"# prevent other plugins running that might invoke the benchmark again",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:54,Testability,benchmark,benchmark,54,"# prevent other plugins running that might invoke the benchmark again",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:67,Testability,test,tests,67,"# make fixtures discoverable to `pytest --fixtures` as well as all tests; # within benchmark/hail without explict import.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py:83,Testability,benchmark,benchmark,83,"# make fixtures discoverable to `pytest --fixtures` as well as all tests; # within benchmark/hail without explict import.",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/utils.py:45,Safety,timeout,timeout-on-a-function-call,45,"# https://stackoverflow.com/questions/492519/timeout-on-a-function-call/494273#494273",MatchSource.CODE_COMMENT,hail/python/benchmark/hail/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/benchmark/hail/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/cluster-tests/cluster-start-stop.py:119,Performance,race condition,race condition,119,"# turn it back on (Google Dataproc with Spark 3 has issues immediately restarting context after stopping, some kind of race condition in yarn 3); # hl.init()",MatchSource.CODE_COMMENT,hail/python/cluster-tests/cluster-start-stop.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/cluster-tests/cluster-start-stop.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/cluster-tests/cluster-vep-check-GRCh37.py:130,Testability,assert,assert,130,"# vep_result_agrees = actual._same(expected); # if vep_result_agrees:; # print('TEST PASSED'); # else:; # print('TEST FAILED'); # assert vep_result_agrees",MatchSource.CODE_COMMENT,hail/python/cluster-tests/cluster-vep-check-GRCh37.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/cluster-tests/cluster-vep-check-GRCh37.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/cluster-tests/cluster-vep-check-GRCh38.py:130,Testability,assert,assert,130,"# vep_result_agrees = actual._same(expected); # if vep_result_agrees:; # print('TEST PASSED'); # else:; # print('TEST FAILED'); # assert vep_result_agrees",MatchSource.CODE_COMMENT,hail/python/cluster-tests/cluster-vep-check-GRCh38.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/cluster-tests/cluster-vep-check-GRCh38.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py:21,Testability,test,test,21,"# FIXME: remove once test output matches docs",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py:47,Performance,race condition,race conditions,47,"# This gets run once per process -- must avoid race conditions",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py:41,Safety,avoid,avoid,41,"# This gets run once per process -- must avoid race conditions",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py:928,Availability,checkpoint,checkpoint,928,"# ds = hl.import_vcf('data/sample.vcf.bgz'); # ds = ds.sample_rows(0.035); # ds = ds.annotate_rows(use_as_marker=hl.rand_bool(0.5),; # panel_maf=0.1,; # anno1=5,; # anno2=0,; # consequence=""LOF"",; # gene=""A"",; # score=5.0); # ds = ds.annotate_rows(a_index=1); # ds = hl.sample_qc(hl.variant_qc(ds)); # ds = ds.annotate_cols(is_case=True,; # pheno=hl.struct(is_case=hl.rand_bool(0.5),; # is_female=hl.rand_bool(0.5),; # age=hl.rand_norm(65, 10),; # height=hl.rand_norm(70, 10),; # blood_pressure=hl.rand_norm(120, 20),; # cohort_name=""cohort1""),; # cov=hl.struct(PC1=hl.rand_norm(0, 1)),; # cov1=hl.rand_norm(0, 1),; # cov2=hl.rand_norm(0, 1),; # cohort=""SIGMA""); # ds = ds.annotate_globals(global_field_1=5,; # global_field_2=10,; # pli={'SCN1A': 0.999, 'SONIC': 0.014},; # populations=['AFR', 'EAS', 'EUR', 'SAS', 'AMR', 'HIS']); # ds = ds.annotate_rows(gene=['TTN']); # ds = ds.annotate_cols(cohorts=['1kg'], pop='EAS'); # ds.checkpoint('data/example.mt', overwrite=True)",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py:59,Availability,checkpoint,checkpoint,59,"# small_mt = hl.balding_nichols_model(3, 4, 4); # small_mt.checkpoint('data/small.mt', overwrite=True)",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py:493,Availability,checkpoint,checkpoint,493,"# burden_ds = hl.import_vcf('data/example_burden.vcf'); # burden_kt = hl.import_table('data/example_burden.tsv', key='Sample', impute=True); # burden_ds = burden_ds.annotate_cols(burden=burden_kt[burden_ds.s]); # burden_ds = burden_ds.annotate_rows(weight=hl.float64(burden_ds.locus.position)); # burden_ds = hl.variant_qc(burden_ds); # genekt = hl.import_locus_intervals('data/gene.interval_list'); # burden_ds = burden_ds.annotate_rows(gene=genekt[burden_ds.locus]); # burden_ds = burden_ds.checkpoint('data/example_burden.mt', overwrite=True)",MatchSource.CODE_COMMENT,hail/python/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:2337,Availability,avail,available,2337,"sh` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:5579,Availability,error,error,5579,"Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:141,Deployability,configurat,configuration,141,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:485,Deployability,configurat,configuration,485,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:672,Deployability,configurat,configuration,672,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:1862,Deployability,pipeline,pipeline,1862," # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:2500,Deployability,configurat,configuration,2500,"hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Sp",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:3515,Deployability,configurat,configuration,3515,"lass:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:3617,Deployability,configurat,configuration,3617,"ogle Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:5122,Deployability,configurat,configuration,5122,"Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:2691,Integrability,message,messages,2691,"; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`s",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:18,Modifiability,config,configure,18,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:141,Modifiability,config,configuration,141,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:485,Modifiability,config,configuration,485,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:601,Modifiability,variab,variable,601,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:638,Modifiability,config,config,638,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:672,Modifiability,config,configuration,672,"""""""Initialize and configure Hail. This function will be called with default arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batc",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:1499,Modifiability,config,config,1499,"is function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local pat",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:1551,Modifiability,config,config,1551,"is function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local pat",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:2500,Modifiability,config,configuration,2500,"hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Sp",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:3515,Modifiability,config,configuration,3515,"lass:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:3617,Modifiability,config,configuration,3617,"ogle Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:4601,Modifiability,config,configure,4601,"obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_d",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:4729,Modifiability,config,configure,4729,"Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:5122,Modifiability,config,configuration,5122,"Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:5164,Modifiability,config,config,5164,"Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:1056,Security,access,accessing,1056,"ault arguments if any Hail functionality is used. If you; need custom configuration, you must explicitly call this function before using Hail. For; example, to set the global random seed to 0, import Hail and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifyin",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:1254,Security,access,accessing,1254,"and immediately call; :func:`.init`:. >>> import hail as hl; >>> hl.init(global_seed=0) # doctest: +SKIP. Hail has two backends, ``spark`` and ``batch``. Hail selects a backend by consulting, in order,; these configuration locations:. 1. The ``backend`` parameter of this function.; 2. The ``HAIL_QUERY_BACKEND`` environment variable.; 3. The value of ``hailctl config get query/backend``. If no configuration is found, Hail will select the Spark backend. Examples; --------; Configure Hail to use the Batch backend:. >>> import hail as hl; >>> hl.init(backend='batch') # doctest: +SKIP. If a :class:`pyspark.SparkContext` is already running, then Hail must be; initialized with it as an argument:. >>> hl.init(sc=sc) # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing any Google Cloud Storage bucket that has; requester pays enabled:. >>> hl.init(gcs_requester_pays_configuration='my-project') # doctest: +SKIP. Configure Hail to bill to `my_project` when accessing the Google Cloud Storage buckets named; `bucket_of_fish` and `bucket_of_eels`:. >>> hl.init(; ... gcs_requester_pays_configuration=('my-project', ['bucket_of_fish', 'bucket_of_eels']); ... ) # doctest: +SKIP. You may also use `hailctl config set gcs_requester_pays/project` and `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or ",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:2516,Testability,log,log,2516,"nd `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:2556,Testability,log,log,2556,"nd `hailctl config set; gcs_requester_pays/buckets` to achieve the same effect. See Also; --------; :func:`.stop`. Parameters; ----------; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:2687,Testability,log,log,2687,"; sc : pyspark.SparkContext, optional; Spark Backend only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`s",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:2749,Testability,log,log,2749,"only. Spark context. If not specified, the Spark backend will create a new; Spark context.; app_name : :class:`str`; A name for this pipeline. In the Spark backend, this becomes the Spark application name. In; the Batch backend, this is a prefix for the name of every Batch.; master : :class:`str`, optional; Spark Backend only. URL identifying the Spark leader (master) node or `local[N]` for local; clusters.; local : :class:`str`; Spark Backend only. Local-mode core limit indicator. Must either be `local[N]` where N is a; positive integer or `local[*]`. The latter indicates Spark should use all cores; available. `local[*]` does not respect most containerization CPU limits. This option is only; used if `master` is unset and `spark.master` is not set in the Spark configuration.; log : :class:`str`; Local path for Hail log file. Does not currently support distributed file systems like; Google Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on dri",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:3609,Testability,log,logging,3609,"ogle Storage, S3, or HDFS.; quiet : :obj:`bool`; Print fewer log messages.; append : :obj:`bool`; Append to the end of the log file.; min_block_size : :obj:`int`; Minimum file block size in MB.; branching_factor : :obj:`int`; Branching factor for tree aggregation.; tmp_dir : :class:`str`, optional; Networked temporary directory. Must be a network-visible file; path. Defaults to /tmp in the default scheme.; default_reference : :class:`str`; *Deprecated*. Please use :func:`.default_reference` to set the default reference genome. Default reference genome. Either ``'GRCh37'``, ``'GRCh38'``,; ``'GRCm38'``, or ``'CanFam3'``.; idempotent : :obj:`bool`; If ``True``, calling this function is a no-op if Hail has already been initialized.; global_seed : :obj:`int`, optional; Global random seed.; spark_conf : :obj:`dict` of :class:`str` to :class`str`, optional; Spark backend only. Spark configuration parameters.; skip_logging_configuration : :obj:`bool`; Spark Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:5532,Testability,log,log,5532,"Backend only. Skip logging configuration in java and python.; local_tmpdir : :class:`str`, optional; Local temporary directory. Used on driver and executor nodes.; Must use the file scheme. Defaults to TMPDIR, or /tmp.; driver_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the driver process. May be 1, 2, 4, or 8. Default is; 1.; driver_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the driver process. May be standard or; highmem. Default is standard.; worker_cores : :class:`str` or :class:`int`, optional; Batch backend only. Number of cores to use for the worker processes. May be 1, 2, 4, or 8. Default is; 1.; worker_memory : :class:`str`, optional; Batch backend only. Memory tier to use for the worker processes. May be standard or; highmem. Default is standard.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list. See examples above.; regions : :obj:`list` of :class:`str`, optional; List of regions to run jobs in when using the Batch backend. Use :data:`.ANY_REGION` to specify any region is allowed; or use `None` to use the underlying default regions from the hailctl environment configuration. For example, use; `hailctl config set batch/regions region1,region2` to set the default regions to use.; gcs_bucket_allow_list:; A list of buckets that Hail should be permitted to read from or write to, even if their default policy is to; use ""cold"" storage. Should look like ``[""bucket1"", ""bucket2""]``.; copy_spark_log_on_error: :class:`bool`, optional; Spark backend only. If `True`, copy the log from the spark driver node to `tmp_dir` on error.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:11,Deployability,install,installed,11,"""""""Get the installed Hail version. Returns; -------; str; """"""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:11,Deployability,install,installed,11,"""""""Get the installed Hail git revision. Returns; -------; str; """"""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:6,Deployability,install,installed,6,"# pip installed",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:609,Performance,load,loaded,609,"""""""Returns the reference genome corresponding to `name`. Notes; -----. Hail's built-in references are ``'GRCh37'``, ``GRCh38'``, ``'GRCm38'``, and; ``'CanFam3'``.; The contig names and lengths come from the GATK resource bundle:; `human_g1k_v37.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37/human_g1k_v37.dict>`__; and `Homo_sapiens_assembly38.dict; <ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/hg38/Homo_sapiens_assembly38.dict>`__. If ``name='default'``, the value of :func:`.default_reference` is returned. Parameters; ----------; name : :class:`str`; Name of a previously loaded reference genome or one of Hail's built-in; references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``, ``'CanFam3'``, and; ``'default'``. Returns; -------; :class:`.ReferenceGenome`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py:50,Testability,test,test,50,"""""""Restore global randomness to initial state for test reproducibility.""""""",MatchSource.CODE_COMMENT,hail/python/hail/context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:786,Availability,down,downstream,786,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:178,Deployability,pipeline,pipeline,178,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:449,Deployability,pipeline,pipeline,449,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:684,Energy Efficiency,reduce,reduces,684,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:379,Performance,optimiz,optimizer,379,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.MatrixTable.group_rows_by` /; :meth:`.GroupedMatrixTable.aggregate` pipeline:. >>> dataset_result = (dataset.group_rows_by(dataset.gene); ... .partition_hint(5); ... .aggregate(n_non_ref = hl.agg.count_where(dataset.GT.is_non_ref()))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedMatrixTable.aggregate` is; the number of partitions in the upstream dataset. If the aggregation greatly; reduces the size of the dataset, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedMatrixTable`; Same grouped matrix table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:613,Modifiability,variab,variable-length,613,"""""""Select existing global fields or create new fields by name, dropping the rest. Examples; --------; Select one existing field and compute a new one:. >>> dataset_result = dataset.select_globals(dataset.global_field_1,; ... another_global=['AFR', 'EUR', 'EAS', 'AMR', 'SAS']). Notes; -----; This method creates new global fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified global fields.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:921,Modifiability,variab,variable-length,921,"""""""Select existing row fields or create new fields by name, dropping all; other non-key fields. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_rows(; ... dataset.variant_qc.gq_stats.mean,; ... high_quality_cases = hl.agg.count_where((dataset.GQ > 20) &; ... dataset.is_case)). Notes; -----; This method creates new row fields. If a created field shares its name; with a differently-indexed field of the table, or with a row key, the; method will fail. Row keys are preserved. To drop or change a row key field, use; :meth:`MatrixTable.key_rows_by`. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method supports aggregation over columns. For instance, the usage:. >>> dataset_result = dataset.select_rows(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per row. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified row fields.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:771,Modifiability,variab,variable-length,771,"""""""Select existing column fields or create new fields by name, dropping the rest. Examples; --------; Select existing fields and compute a new one:. >>> dataset_result = dataset.select_cols(; ... dataset.sample_qc,; ... dataset.pheno.age,; ... isCohort1 = dataset.pheno.cohort_name == 'Cohort1'). Notes; -----; This method creates new column fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method supports aggregation over rows. For instance, the usage:. >>> dataset_result = dataset.select_cols(mean_GQ = hl.agg.mean(dataset.GQ)). will compute the mean per column. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified column fields.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:532,Modifiability,variab,variable-length,532,"""""""Select existing entry fields or create new fields by name, dropping the rest. Examples; --------; Drop all entry fields aside from `GT`:. >>> dataset_result = dataset.select_entries(dataset.GT). Notes; -----; This method creates new entry fields. If a created field shares its name; with a differently-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.MatrixTable`; MatrixTable with specified entry fields.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:1040,Availability,down,downstream,1040,"ix. Parameters; ----------; expr : bool or :class:`.BooleanExpression`; Filter expression.; keep : bool; Keep entries where `expr` is true. Returns; -------; :class:`.MatrixTable`; Filtered matrix table. Examples; --------. Keep entries where the sum of `AD` is greater than 10 and `GQ` is greater than 20:. >>> dataset_result = dataset.filter_entries((hl.sum(dataset.AD) > 10) & (dataset.GQ > 20)). Warning; -------; When `expr` evaluates to missing, the entry will be removed regardless of; `keep`. Note; ----; This method does not support aggregation. Notes; -----; The expression `expr` will be evaluated for every entry of the table.; If `keep` is ``True``, then entries where `expr` evaluates to ``True``; will be kept (the filter removes the entries where the predicate; evaluates to ``False``). If `keep` is ``False``, then entries where; `expr` evaluates to ``True`` will be removed (the filter keeps the; entries where the predicate evaluates to ``False``). Filtered entries are removed entirely from downstream operations. This; means that the resulting matrix table has sparsity -- that is, that the; number of entries is **smaller** than the product of :meth:`count_rows`; and :meth:`count_cols`. To re-densify a filtered matrix table, use the; :meth:`unfilter_entries` method to restore filtered entries, populated; all fields with missing values. Below are some properties of an; entry-filtered matrix table. 1. Filtered entries are not included in the :meth:`entries` table. >>> mt_range = hl.utils.range_matrix_table(10, 10); >>> mt_range = mt_range.annotate_entries(x = mt_range.row_idx + mt_range.col_idx); >>> mt_range.count(); (10, 10). >>> mt_range.entries().count(); 100. >>> mt_filt = mt_range.filter_entries(mt_range.x % 2 == 0); >>> mt_filt.count(); (10, 10). >>> mt_filt.count_rows() * mt_filt.count_cols(); 100. >>> mt_filt.entries().count(); 50. 2. Filtered entries are not included in aggregation. >>> mt_filt.aggregate_entries(hl.agg.count()); 50. >>> mt_filt = mt_filt",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:172,Availability,down,downstream,172,"""""""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:163,Deployability,pipeline,pipeline,163,"""""""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:312,Deployability,pipeline,pipeline,312,"""""""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:335,Deployability,pipeline,pipeline,335,"""""""Unfilters filtered entries, populating fields with missing values. Returns; -------; :class:`MatrixTable`. Notes; -----; This method is used in the case that a pipeline downstream of :meth:`filter_entries`; requires a fully dense (no filtered entries) matrix table. Generally, if this method is required in a pipeline, the upstream pipeline can; be rewritten to use annotation instead of entry filtering. See Also; --------; :meth:`filter_entries`, :meth:`compute_entry_filter_stats`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:878,Integrability,depend,dependent,878,"""""""Aggregate over rows to a local value. Examples; --------; Aggregate over rows:. >>> dataset.aggregate_rows(hl.struct(n_high_quality=hl.agg.count_where(dataset.qual > 40),; ... mean_qual=hl.agg.mean(dataset.qual))); Struct(n_high_quality=9, mean_qual=140054.73333333334). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful references to fields that are not global or indexed by row. This method should be thought of as a more convenient alternative to; the following:. >>> rows_table = dataset.rows(); >>> rows_table.aggregate(hl.struct(n_high_quality=hl.agg.count_where(rows_table.qual > 40),; ... mean_qual=hl.agg.mean(rows_table.qual))). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:956,Integrability,depend,dependent,956,"""""""Aggregate over columns to a local value. Examples; --------; Aggregate over columns:. >>> dataset.aggregate_cols(; ... hl.struct(fraction_female=hl.agg.fraction(dataset.pheno.is_female),; ... case_ratio=hl.agg.count_where(dataset.is_case) / hl.agg.count())); Struct(fraction_female=0.44, case_ratio=1.0). Notes; -----; Unlike most :class:`.MatrixTable` methods, this method does not support; meaningful references to fields that are not global or indexed by column. This method should be thought of as a more convenient alternative to; the following:. >>> cols_table = dataset.cols(); >>> cols_table.aggregate(; ... hl.struct(fraction_female=hl.agg.fraction(cols_table.pheno.is_female),; ... case_ratio=hl.agg.count_where(cols_table.is_case) / hl.agg.count())). Note; ----; This method supports (and expects!) aggregation over columns. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:784,Integrability,depend,dependent,784,"""""""Aggregate over entries to a local value. Examples; --------; Aggregate over entries:. >>> dataset.aggregate_entries(hl.struct(global_gq_mean=hl.agg.mean(dataset.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(dataset.GT)))); Struct(global_gq_mean=69.60514541387025, call_rate=0.9933333333333333). Notes; -----; This method should be thought of as a more convenient alternative to; the following:. >>> entries_table = dataset.entries(); >>> entries_table.aggregate(hl.struct(global_gq_mean=hl.agg.mean(entries_table.GQ),; ... call_rate=hl.agg.fraction(hl.is_defined(entries_table.GT)))). Note; ----; This method supports (and expects!) aggregation over entries. Parameters; ----------; expr : :class:`.Expression`; Aggregation expressions. Returns; -------; any; Aggregated value dependent on `expr`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:792,Availability,checkpoint,checkpoint,792,"""""""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:91,Energy Efficiency,efficient,efficient,91,"""""""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:644,Energy Efficiency,efficient,efficient,644,"""""""Checkpoint the matrix table to disk by writing and reading using a fast, but less space-efficient codec. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`MatrixTable`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_matrix_table`. It is; possible to read the file at this path later with; :func:`.read_matrix_table`. A faster, but less efficient, codec is used; or writing the data so the file will be larger than if one used; :meth:`write`. Examples; --------; >>> dataset = dataset.checkpoint('output/dataset_checkpoint.mt'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:237,Testability,log,logging,237,"""""""Print the first few rows of the matrix table to the console. .. include:: _templates/experimental.rst. Notes; -----; The output can be passed piped to another output source using the `handler` argument:. >>> mt.show(handler=lambda x: logging.info(x)) # doctest: +SKIP. Parameters; ----------; n_rows : :obj:`int`; Maximum number of rows to show.; n_cols : :obj:`int`; Maximum number of columns to show.; width : :obj:`int`; Horizontal width at which to break fields.; truncate : :obj:`int`, optional; Truncate each field to the given number of characters. If; ``None``, truncate fields to the given `width`.; types : :obj:`bool`; Print an extra header line with the type of each field.; handler : Callable[[str], Any]; Handler function for data string.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:1976,Deployability,pipeline,pipeline,1976,"et.entries(). Notes; -----; The coordinate table representation of the source matrix table contains; one row for each **non-filtered** entry of the matrix -- if a matrix table; has no filtered entries and contains N rows and M columns, the table will contain; ``M * N`` rows, which can be **a very large number**. This representation can be useful for aggregating over both axes of a matrix table; at the same time -- it is not possible to aggregate over a matrix table using; :meth:`group_rows_by` and :meth:`group_cols_by` at the same time (aggregating; by population and chromosome from a variant-by-sample genetics representation,; for instance). After moving to the coordinate representation with :meth:`entries`,; it is possible to group and aggregate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; -------; :class:`.Table`; Table with all non-global fields from the matrix, with **one row per entry of the matrix**.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:1958,Integrability,depend,depending,1958,"et.entries(). Notes; -----; The coordinate table representation of the source matrix table contains; one row for each **non-filtered** entry of the matrix -- if a matrix table; has no filtered entries and contains N rows and M columns, the table will contain; ``M * N`` rows, which can be **a very large number**. This representation can be useful for aggregating over both axes of a matrix table; at the same time -- it is not possible to aggregate over a matrix table using; :meth:`group_rows_by` and :meth:`group_cols_by` at the same time (aggregating; by population and chromosome from a variant-by-sample genetics representation,; for instance). After moving to the coordinate representation with :meth:`entries`,; it is possible to group and aggregate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; -------; :class:`.Table`; Table with all non-global fields from the matrix, with **one row per entry of the matrix**.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:961,Performance,perform,performance,961,"""""""Returns a matrix in coordinate table form. Examples; --------; Extract the entry table:. >>> entries_table = dataset.entries(). Notes; -----; The coordinate table representation of the source matrix table contains; one row for each **non-filtered** entry of the matrix -- if a matrix table; has no filtered entries and contains N rows and M columns, the table will contain; ``M * N`` rows, which can be **a very large number**. This representation can be useful for aggregating over both axes of a matrix table; at the same time -- it is not possible to aggregate over a matrix table using; :meth:`group_rows_by` and :meth:`group_cols_by` at the same time (aggregating; by population and chromosome from a variant-by-sample genetics representation,; for instance). After moving to the coordinate representation with :meth:`entries`,; it is possible to group and aggregate the resulting table much more flexibly,; albeit with potentially poorer computational performance. Warning; -------; The table returned by this method should be used for aggregation or queries,; but never exported or written to disk without extensive filtering and field; selection -- the disk footprint of an entries_table could be 100x (or more!); larger than its parent matrix. This means that if you try to export the entries; table of a 10 terabyte matrix, you could write a petabyte of data!. Warning; -------; Matrix table columns are typically sorted by the order at import, and; not necessarily by column key. Since tables are always sorted by key,; the table which results from this command will have its rows sorted by; the compound (row key, column key) which becomes the table key.; To preserve the original row-major entry order as the table row order,; first unkey the columns using :meth:`key_cols_by` with no arguments. Warning; -------; If the matrix table has no row key, but has a column key, this operation; may require a full shuffle to sort by the column key, depending on the; pipeline. Returns; ------",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:37,Modifiability,variab,variables,37,"""""""Return this matrix table's global variables for use in another; expression context. Examples; --------; >>> dataset1 = dataset.annotate_globals(pli={'SCN1A': 0.999, 'SONIC': 0.014}); >>> pli_dict = dataset1.index_globals().pli; >>> dataset_result = dataset2.annotate_rows(gene_pli = dataset2.gene.map(lambda x: pli_dict.get(x))). Returns; -------; :class:`.StructExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:355,Modifiability,variab,variable-length,355,"""""""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.locus, dataset.alleles).qual). Or equivalently:. >>> dataset_result = dataset.annotate_rows(qual = dataset2.index_rows(dataset.row_key).qual). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Notes; -----; ``index_rows(exprs)`` is equivalent to ``rows().index(exprs)``; or ``rows()[exprs]``. The type of the resulting struct is the same as the type of; :meth:`.row_value`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:341,Modifiability,variab,variable-length,341,"""""""Expose the column values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.s).pheno). Or equivalently:. >>> dataset_result = dataset.annotate_cols(pheno = dataset2.index_cols(dataset.col_key).pheno). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Notes; -----; ``index_cols(cols)`` is equivalent to ``cols().index(exprs)``; or ``cols()[exprs]``. The type of the resulting struct is the same as the type of; :meth:`.col_value`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:219,Availability,avail,available,219,"""""""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:375,Availability,resilien,resilient-distributed-datasets-rdds,375,"""""""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:364,Usability,guid,guide,364,"""""""Number of partitions. Notes; -----. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. Partitions are a; core concept of distributed computation in Spark, see `here; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. Returns; -------; int; Number of partitions.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:396,Availability,avail,available,396,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:996,Availability,resilien,resilient-distributed-datasets-rdds,996,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:649,Performance,perform,performance,649,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:1209,Safety,avoid,avoid,1209,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:985,Usability,guid,guide,985,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> dataset_result = dataset.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a matrix with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full; shuffle. These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n_partitions : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.MatrixTable`; Repartitioned dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:2,Availability,checkpoint,checkpoint,2,"# checkpoint rather than write to use fast codec",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:194,Usability,simpl,simply,194,"""""""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> dataset_result = dataset.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.MatrixTable`; Matrix table with at most `max_partitions` partitions.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:108,Performance,cache,cache,108,"""""""Persist the dataset in memory. Examples; --------; Persist the dataset in memory:. >>> dataset = dataset.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.MatrixTable.persist>`. Returns; -------; :class:`.MatrixTable`; Cached dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:316,Availability,redundant,redundant,316,"""""""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:375,Deployability,pipeline,pipelines,375,"""""""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:231,Performance,cache,cache,231,"""""""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:354,Performance,perform,performance,354,"""""""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:310,Safety,avoid,avoid,310,"""""""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:316,Safety,redund,redundant,316,"""""""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:617,Usability,guid,guide,617,"""""""Persist this table in memory or on disk. Examples; --------; Persist the dataset to both memory and disk:. >>> dataset = dataset.persist() # doctest: +SKIP. Notes; -----. The :meth:`.MatrixTable.persist` and :meth:`.MatrixTable.cache`; methods store the current dataset on disk or in memory temporarily to; avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for :meth:`.Table.write`,; which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.MatrixTable`; Persisted dataset.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:58,Testability,test,testsetup,58,"""""""Take the union of dataset rows. Examples; --------. .. testsetup::. dataset_to_union_1 = dataset; dataset_to_union_2 = dataset. Union the rows of two datasets:. >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2). Given a list of datasets, take the union of all rows:. >>> all_datasets = [dataset_to_union_1, dataset_to_union_2]. The following three syntaxes are equivalent:. >>> dataset_result = dataset_to_union_1.union_rows(dataset_to_union_2); >>> dataset_result = all_datasets[0].union_rows(*all_datasets[1:]); >>> dataset_result = hl.MatrixTable.union_rows(*all_datasets). Notes; -----. In order to combine two datasets, three requirements must be met:. - The column keys must be identical, both in type, value, and ordering.; - The row key schemas and row schemas must match.; - The entry schemas must match. The column fields in the resulting dataset are the column fields from; the first dataset; the column schemas do not need to match. This method does not deduplicate; if a row exists identically in two; datasets, then it will be duplicated in the result. Warning; -------; This method can trigger a shuffle, if partitions from two datasets; overlap. Parameters; ----------; datasets : varargs of :class:`.MatrixTable`; Datasets to combine. Returns; -------; :class:`.MatrixTable`; Dataset with rows from each member of `datasets`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:825,Performance,perform,performed,825,"""""""Take the union of dataset columns. Warning; -------. This method does not preserve the global fields from the other matrix table. Examples; --------. Union the columns of two datasets:. >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; -----. In order to combine two datasets, three requirements must be met:. - The row keys must match.; - The column key schemas and column schemas must match.; - The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match. This method creates a :class:`.MatrixTable` which contains all columns; from both input datasets. The set of rows included in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns; -------;",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:1678,Performance,perform,perform,1678,"es not preserve the global fields from the other matrix table. Examples; --------. Union the columns of two datasets:. >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; -----. In order to combine two datasets, three requirements must be met:. - The row keys must match.; - The column key schemas and column schemas must match.; - The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match. This method creates a :class:`.MatrixTable` which contains all columns; from both input datasets. The set of rows included in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns; -------; :class:`.MatrixTable`; Dataset with columns from both datasets.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:1721,Performance,perform,perform,1721,"es not preserve the global fields from the other matrix table. Examples; --------. Union the columns of two datasets:. >>> dataset_result = dataset_to_union_1.union_cols(dataset_to_union_2). Notes; -----. In order to combine two datasets, three requirements must be met:. - The row keys must match.; - The column key schemas and column schemas must match.; - The entry schemas must match. The row fields in the resulting dataset are the row fields from the; first dataset; the row schemas do not need to match. This method creates a :class:`.MatrixTable` which contains all columns; from both input datasets. The set of rows included in the result is; determined by the `row_join_type` parameter. - With the default value of ``'inner'``, an inner join is performed; on rows, so that only rows whose row key exists in both input datasets; are included. In this case, the entries for each row are the; concatenation of all entries of the corresponding rows in the input; datasets.; - With `row_join_type` set to ``'outer'``, an outer join is perfomed on; rows, so that row keys which exist in only one input dataset are also; included. For those rows, the entry fields for the columns coming; from the other dataset will be missing. Only distinct row keys from each dataset are included (equivalent to; calling :meth:`.distinct_by_row` on each dataset first). This method does not deduplicate; if a column key exists identically in; two datasets, then it will be duplicated in the result. Parameters; ----------; other : :class:`.MatrixTable`; Dataset to concatenate.; row_join_type : :obj:`.str`; If `outer`, perform an outer join on rows; if 'inner', perform an; inner join. Default `inner`.; drop_right_row_fields : :obj:`.bool`; If true, non-key row fields of `other` are dropped. Otherwise,; non-key row fields in the two datasets must have distinct names,; and the result contains the union of the row fields. Returns; -------; :class:`.MatrixTable`; Dataset with columns from both datasets.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py:607,Modifiability,inherit,inherits,607,"""""""Make a table from a matrix table with one field per sample. .. deprecated:: 0.2.129; use :meth:`.localize_entries` instead because it supports more; columns. Parameters; ----------; separator : :class:`str`; Separator between sample IDs and entry field names. Returns; -------; :class:`.Table`. See Also; --------; :meth:`.localize_entries`. Notes; -----; The table has one row for each row of the input matrix. The; per sample and entry fields are formed by concatenating the; sample ID with the entry field name using `separator`. If the; entry field name is empty, the separator is omitted. The table inherits the globals from the matrix table. Examples; --------; Consider a matrix table with the following schema:. .. code-block:: text. Global fields:; 'batch': str; Column fields:; 's': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; Entry fields:; 'GT': call; 'GQ': int32; Column key:; 's': str; Row key:; 'locus': locus<GRCh37>; 'alleles': array<str>. and three sample IDs: `A`, `B` and `C`. Then the result of; :meth:`.make_table`:. >>> ht = mt.make_table() # doctest: +SKIP. has the original row fields along with 6 additional fields,; one for each sample and entry field:. .. code-block:: text. Global fields:; 'batch': str; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'A.GT': call; 'A.GQ': int32; 'B.GT': call; 'B.GQ': int32; 'C.GT': call; 'C.GQ': int32; Key:; 'locus': locus<GRCh37>; 'alleles': array<str>; """"""",MatchSource.CODE_COMMENT,hail/python/hail/matrixtable.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/matrixtable.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:751,Availability,down,downstream,751,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:161,Deployability,pipeline,pipeline,161,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:424,Deployability,pipeline,pipeline,424,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:651,Energy Efficiency,reduce,reduces,651,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:354,Performance,optimiz,optimizer,354,"""""""Set the target number of partitions for aggregation. Examples; --------. Use `partition_hint` in a :meth:`.Table.group_by` / :meth:`.GroupedTable.aggregate`; pipeline:. >>> table_result = (table1.group_by(table1.ID); ... .partition_hint(5); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Notes; -----; Until Hail's query optimizer is intelligent enough to sample records at all; stages of a pipeline, it can be necessary in some places to provide some; explicit hints. The default number of partitions for :meth:`.GroupedTable.aggregate` is the; number of partitions in the upstream table. If the aggregation greatly; reduces the size of the table, providing a hint for the target number of; partitions can accelerate downstream operations. Parameters; ----------; n : int; Number of partitions. Returns; -------; :class:`.GroupedTable`; Same grouped table with a partition hint.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:296,Usability,guid,guide,296,"""""""Hail's distributed implementation of a dataframe or SQL table. Use :func:`.read_table` to read a table that was written with; :meth:`.Table.write`. Use :meth:`.to_spark` and :meth:`.Table.from_spark`; to inter-operate with PySpark's; `SQL <https://spark.apache.org/docs/latest/sql-programming-guide.html>`__ and; `machine learning <https://spark.apache.org/docs/latest/ml-guide.html>`__; functionality. Examples; --------. The examples below use ``table1`` and ``table2``, which are imported; from text files using :func:`.import_table`. >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). .. code-block:: text. +-------+-------+--------+; | ID | A | B |; +-------+-------+--------+; | int32 | int32 | str |; +-------+-------+--------+; | 1 | 65 | cat |; | 2 | 72 | dog |; | 3 | 70 | mouse |; | 4 | 60 | rabbit |; +-------+-------+--------+. Define new annotations:. >>> height_mean_m = 68; >>> height_sd_m = 3; >>> height_mean_f = 65; >>> height_sd_f = 2.5; >>>; >>> def get_z(height, sex):; ... return hl.if_else(sex == 'M',; ... (height - height_mean_m) / height_sd_m,; ... (height - height_mean_f) / height_sd_f); >>>; >>> table1 = table1.annotate(height_z = get_z(table1.HT, table1.SEX)); >>> table1 = table1.annotate_globals(global_field_1 = [1, 2, 3]). Filter rows of the table:. >>> table2 = table2.filter(ta",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:325,Usability,learn,learning,325,"""""""Hail's distributed implementation of a dataframe or SQL table. Use :func:`.read_table` to read a table that was written with; :meth:`.Table.write`. Use :meth:`.to_spark` and :meth:`.Table.from_spark`; to inter-operate with PySpark's; `SQL <https://spark.apache.org/docs/latest/sql-programming-guide.html>`__ and; `machine learning <https://spark.apache.org/docs/latest/ml-guide.html>`__; functionality. Examples; --------. The examples below use ``table1`` and ``table2``, which are imported; from text files using :func:`.import_table`. >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). .. code-block:: text. +-------+-------+--------+; | ID | A | B |; +-------+-------+--------+; | int32 | int32 | str |; +-------+-------+--------+; | 1 | 65 | cat |; | 2 | 72 | dog |; | 3 | 70 | mouse |; | 4 | 60 | rabbit |; +-------+-------+--------+. Define new annotations:. >>> height_mean_m = 68; >>> height_sd_m = 3; >>> height_mean_f = 65; >>> height_sd_f = 2.5; >>>; >>> def get_z(height, sex):; ... return hl.if_else(sex == 'M',; ... (height - height_mean_m) / height_sd_m,; ... (height - height_mean_f) / height_sd_f); >>>; >>> table1 = table1.annotate(height_z = get_z(table1.HT, table1.SEX)); >>> table1 = table1.annotate_globals(global_field_1 = [1, 2, 3]). Filter rows of the table:. >>> table2 = table2.filter(ta",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:375,Usability,guid,guide,375,"""""""Hail's distributed implementation of a dataframe or SQL table. Use :func:`.read_table` to read a table that was written with; :meth:`.Table.write`. Use :meth:`.to_spark` and :meth:`.Table.from_spark`; to inter-operate with PySpark's; `SQL <https://spark.apache.org/docs/latest/sql-programming-guide.html>`__ and; `machine learning <https://spark.apache.org/docs/latest/ml-guide.html>`__; functionality. Examples; --------. The examples below use ``table1`` and ``table2``, which are imported; from text files using :func:`.import_table`. >>> table1 = hl.import_table('data/kt_example1.tsv', impute=True, key='ID'); >>> table1.show(). .. code-block:: text. +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. >>> table2 = hl.import_table('data/kt_example2.tsv', impute=True, key='ID'); >>> table2.show(). .. code-block:: text. +-------+-------+--------+; | ID | A | B |; +-------+-------+--------+; | int32 | int32 | str |; +-------+-------+--------+; | 1 | 65 | cat |; | 2 | 72 | dog |; | 3 | 70 | mouse |; | 4 | 60 | rabbit |; +-------+-------+--------+. Define new annotations:. >>> height_mean_m = 68; >>> height_sd_m = 3; >>> height_mean_f = 65; >>> height_sd_f = 2.5; >>>; >>> def get_z(height, sex):; ... return hl.if_else(sex == 'M',; ... (height - height_mean_m) / height_sd_m,; ... (height - height_mean_f) / height_sd_f); >>>; >>> table1 = table1.annotate(height_z = get_z(table1.HT, table1.SEX)); >>> table1 = table1.annotate_globals(global_field_1 = [1, 2, 3]). Filter rows of the table:. >>> table2 = table2.filter(ta",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:587,Availability,down,down,587,"""""""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:596,Deployability,pipeline,pipeline,596,"""""""Returns the number of partitions in the table. Examples; --------. Range tables can be constructed with an explicit number of partitions:. >>> ht = hl.utils.range_table(100, n_partitions=10); >>> ht.n_partitions(); 10. Small files are often imported with one partition:. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True); >>> ht2.n_partitions(); 1. The `min_partitions` argument to :func:`.import_table` forces more partitions, but it can; produce empty partitions. Empty partitions do not affect correctness but introduce; unnecessary extra bookkeeping that slows down the pipeline. >>> ht2 = hl.import_table('data/coordinate_matrix.tsv', impute=True, min_partitions=10); >>> ht2.n_partitions(); 10. Returns; -------; :obj:`int`; Number of partitions. """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:98,Performance,load,loaded,98,"""""""Count the number of rows in the table. Examples; --------. Count the number of rows in a table loaded from 'data/kt_example1.tsv'. Each line of the TSV; becomes one row in the Hail Table. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.count(); 4. Returns; -------; :obj:`int`; The number of rows in the table. """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1766,Testability,log,login,1766,"5, 'b': 10}, {'a': 0, 'b': 200}],; ... schema=hl.tstruct(a=hl.tint, b=hl.tint),; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail :class:`.Struct` or :class:`.dict` s. >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+------------",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1895,Testability,log,login,1895," | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also elide schema entirely and let Hail guess the type. The list elements must; either be Hail :class:`.Struct` or :class:`.dict` s. >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | ",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:2031,Testability,log,login,2031,"e schema entirely and let Hail guess the type. The list elements must; either be Hail :class:`.Struct` or :class:`.dict` s. >>> t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:2156,Testability,log,login,2156," t = hl.Table.parallelize(; ... [{'a': 5, 'b': 10}, {'a': 0, 'b': 200}],; ... key='a'; ... ); >>> t.show(); +-------+-------+; | a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:2284,Testability,log,login,2284," a | b |; +-------+-------+; | int32 | int32 |; +-------+-------+; | 0 | 200 |; | 5 | 10 |; +-------+-------+. You may also specify only a handful of types in `partial_type`. Hail will automatically; deduce the types of the other fields. Hail _cannot_ deduce the type of a field which only; contains empty arrays (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+------------+; | milestone | labels |; +-----------+------------+; | str | array<str> |; +-----------+------------+; | NA | [] |",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:2612,Testability,log,login,2612," (the element type is unspecified), so we specify the type of labels; explicitly. >>> dictionaries = [; ... {""number"":10038,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794}, ""milestone"":None,""labels"":[]},; ... {""number"":10037,""state"":""open"",""user"":{""login"":""daniel-goldstein"",""site_admin"":False,""id"":24440116},""milestone"":None,""labels"":[]},; ... {""number"":10036,""state"":""open"",""user"":{""login"":""jigold"",""site_admin"":False,""id"":1693348},""milestone"":None,""labels"":[]},; ... {""number"":10035,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... {""number"":10033,""state"":""open"",""user"":{""login"":""tpoterba"",""site_admin"":False,""id"":10562794},""milestone"":None,""labels"":[]},; ... ]; >>> t = hl.Table.parallelize(; ... dictionaries,; ... partial_type={""milestone"": hl.tstr, ""labels"": hl.tarray(hl.tstr)}; ... ); >>> t.show(); +--------+--------+--------------------+-----------------+----------+; | number | state | user.login | user.site_admin | user.id |; +--------+--------+--------------------+-----------------+----------+; | int32 | str | str | bool | int32 |; +--------+--------+--------------------+-----------------+----------+; | 10038 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10037 | ""open"" | ""daniel-goldstein"" | False | 24440116 |; | 10036 | ""open"" | ""jigold"" | False | 1693348 |; | 10035 | ""open"" | ""tpoterba"" | False | 10562794 |; | 10033 | ""open"" | ""tpoterba"" | False | 10562794 |; +--------+--------+--------------------+-----------------+----------+; +-----------+------------+; | milestone | labels |; +-----------+------------+; | str | array<str> |; +-----------+------------+; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; | NA | [] |; +-----------+------------+. Parallelizing with a specified number of partitions:. >>> rows = [ {'a': i} for i in range(100) ]; >>> ht = hl.Table.parallelize(rows, n_partitions=10); >>> ht.n_partitions(); 10; >>> ht.count(); 100. Parallelizing with some global",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:213,Usability,simpl,simple,213,"""""""Key table by a new set of fields. Table keys control both the order of the rows in the table and the ability to join or; annotate one table with the information in another table. Examples; --------. Consider a simple unkeyed table. Its rows appear are guaranteed to appear in the same order; as they were in the source text file. >>> ht = hl.import_table('data/kt_example1.tsv', impute=True); >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Changing the key forces the rows to appear in ascending order. For this reason,; :meth:`.key_by` is a relatively expensive operation. It must sort the entire dataset. >>> ht = ht.key_by('HT'); >>> ht.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Suppose that `ht` represents some human subjects in an experiment. We might need to combine; sample metadata from `ht` with sample metadata from another source. For example:. >>> ht2 = hl.import_table('data/kt_example2.tsv', impute=True); >>> ht2 = ht2.key_by('ID'); >>> ht2.show(); +-------+-------+----------+; | ID | A | B ",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1581,Modifiability,variab,variable-length,1581,"""""""Select existing global fields or create new fields by name, dropping the rest. Examples; --------. Selecting two global fields, one by name and one new one, replacing any previously annotated; global fields. >>> ht = hl.utils.range_table(1); >>> ht = ht.annotate_globals(pops = ['EUR', 'AFR', 'EAS', 'SAS']); >>> ht = ht.annotate_globals(study_name = 'HGDP+1kg'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'study_name': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------; >>> ht = ht.select_globals(ht.pops, target_date='2025-01-01'); >>> ht.describe(); ----------------------------------------; Global fields:; 'pops': array<str>; 'target_date': str; ----------------------------------------; Row fields:; 'idx': int32; ----------------------------------------; Key: ['idx']; ----------------------------------------. Fields may also be selected by their name:. >>> ht = ht.select_globals('target_date'); >>> ht.globals.show(); +--------------------+; | <expr>.target_date |; +--------------------+; | str |; +--------------------+; | ""2025-01-01"" |; +--------------------+. Notes; -----; This method creates new global fields. If a created field shares its name; with a row-indexed field of the table, the method will fail. Note; ----. See :meth:`.Table.select` for more information about using ``select`` methods. Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.Table`; Table with specified global fields. """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:326,Deployability,pipeline,pipeline,326,"""""""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where ``Z`` is 3:. >>> filtered_ht = ht.filter(ht.Z == 3); >>> filtered_ht.show(). +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; +-------+-------+-----+-------+-------+. Remove rows where ``Z`` is 3:. >>> filtered_ht = ht.filter(ht.Z == 3, keep=False); >>> filtered_ht.show(); +-------+-------+-----+-----",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:712,Deployability,pipeline,pipeline,712,"""""""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where ``Z`` is 3:. >>> filtered_ht = ht.filter(ht.Z == 3); >>> filtered_ht.show(). +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; +-------+-------+-----+-------+-------+. Remove rows where ``Z`` is 3:. >>> filtered_ht = ht.filter(ht.Z == 3, keep=False); >>> filtered_ht.show(); +-------+-------+-----+-----",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:371,Performance,optimiz,optimization,371,"""""""Filter rows conditional on the value of each row's fields. Note; ----. Hail will can read much less data if a Table filter condition references the key field and; the Table is stored in Hail native format (i.e. read using :func:`.read_table`, _not_; :func:`.import_table`). In other words: filtering on the key will make a pipeline faster by; reading fewer rows. This optimization is prevented by certain operations appearing between a; :func:`.read_table` and a :meth:`.filter`. For example, a `key_by` and `group_by`, both; force reading all the data. Suppose we previously :meth:`.write` a Hail Table with one million rows keyed by a field; called `idx`. If we filter this table to one value of `idx`, the pipeline will be fast; because we read only the rows that have that value of `idx`:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx == 5) # doctest: +SKIP. This also works with inequality conditions:. >>> ht = hl.read_table('large-table.ht') # doctest: +SKIP; >>> ht = ht.filter(ht.idx <= 5) # doctest: +SKIP. Examples; --------. Consider this table:. >>> ht = ht.drop('C1', 'C2', 'C3'); >>> ht.show(); +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 |; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; | 4 | 60 | ""F"" | 8 | 2 |; +-------+-------+-----+-------+-------+. Keep rows where ``Z`` is 3:. >>> filtered_ht = ht.filter(ht.Z == 3); >>> filtered_ht.show(). +-------+-------+-----+-------+-------+; | ID | HT | SEX | X | Z |; +-------+-------+-----+-------+-------+; | int32 | int32 | str | int32 | int32 |; +-------+-------+-----+-------+-------+; | 2 | 72 | ""M"" | 6 | 3 |; | 3 | 70 | ""F"" | 7 | 3 |; +-------+-------+-----+-------+-------+. Remove rows where ``Z`` is 3:. >>> filtered_ht = ht.filter(ht.Z == 3, keep=False); >>> filtered_ht.show(); +-------+-------+-----+-----",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:615,Modifiability,variab,variable-length,615,"""""""Select existing fields or create new fields by name, dropping the rest. Examples; --------; Select a few old fields and compute a new one:. >>> table_result = table1.select(table1.C1, Y=table1.Z - table1.X). Notes; -----; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, w",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1176,Modifiability,variab,variable-length,1176,"table1.Z - table1.X). Notes; -----; This method creates new row-indexed fields. If a created field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Not",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1281,Modifiability,variab,variable-length,1281,"field shares its name; with a global field of the table, the method will fail. Note; ----. **Using select**. Select and its sibling methods (:meth:`.Table.select_globals`,; :meth:`.MatrixTable.select_globals`, :meth:`.MatrixTable.select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-l",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1486,Modifiability,variab,variable-length,1486,".select_rows`,; :meth:`.MatrixTable.select_cols`, and :meth:`.MatrixTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Re",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:2272,Modifiability,variab,variable-length,2272,"xTable.select_entries`) accept; both variable-length (``f(x, y, z)``) and keyword (``f(a=x, b=y, c=z)``); arguments. Select methods will always preserve the key along that axis; e.g. for; :meth:`.Table.select`, the table key will aways be kept. To modify the; key, use :meth:`.key_by`. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a new table with; fields `C1` and `C2` of `table1`, and the table key `ID`. First, variable-length string arguments:. >>> table_result = table1.select('C1', 'C2'). Second, field reference variable-length arguments:. >>> table_result = table1.select(table1.C1, table1.C2). Last, expression keyword arguments:. >>> table_result = table1.select(C1 = table1.C1, C2 = table1.C2). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, producing a table with one field, `x`.:. >>> table3_result = table3.select(table3.s.x). >>> table3_result = table3.select(x = table3.s.x). The keyword argument syntax permits arbitrary expressions:. >>> table_result = table1.select(foo=table1.X ** 2 + 1). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = table1.select(table1.X, 'Z', bar = [table1.C1, table1.C2]). Note; ----; This method does not support aggregation. Parameters; ----------; exprs : variable-length args of :class:`str` or :class:`.Expression`; Arguments that specify field names or nested field reference expressions.; named_exprs : keyword args of :class:`.Expression`; Field names and the expressions to compute them. Returns; -------; :class:`.Table`; Table with specified fields.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:613,Deployability,pipeline,pipeline,613,"""""""Export to a text file. Examples; --------; Export to a tab-separated file:. >>> table1.export('output/table1.tsv.bgz'). Note; ----; It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with any Hail method, as well as with Python's ``gzip.open``; and R's ``read.table``. Nested structures will be exported as JSON. In order to export nested struct; fields as separate fields in the resulting table, use :meth:`flatten` first. Warning; -------; Do not export to a path that is being read from in the same pipeline. See Also; --------; :meth:`flatten`, :meth:`write`. Parameters; ----------; output : :class:`str`; URI at which to write exported file.; types_file : :class:`str`, optional; URI at which to write file containing field type information.; header : :obj:`bool`; Include a header in the file.; parallel : :class:`str`, optional; If None, a single file is produced, otherwise a; folder of file shards is produced. If 'separate_header',; the header file is output separately from the file shards. If; 'header_per_shard', each file shard has a header. If set to None; the export will be slower.; delimiter : :class:`str`; Field delimiter.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:773,Modifiability,variab,variable-length,773,"""""""Group by a new key for use with :meth:`.GroupedTable.aggregate`. Examples; --------; Compute the mean value of `X` and the sum of `Z` per unique `ID`:. >>> table_result = (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1160,Modifiability,variab,variable-length,1160," (table1.group_by(table1.ID); ... .aggregate(meanX = hl.agg.mean(table1.X), sumZ = hl.agg.sum(table1.Z))). Group by a height bin and compute sex ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:. >>> table_result = (table1.group_by(foo=tab",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1316,Modifiability,variab,variable-length,1316," ratio per bin:. >>> table_result = (table1.group_by(height_bin = table1.HT // 20); ... .aggregate(fraction_female = hl.agg.fraction(table1.SEX == 'F'))). Notes; -----; This function is always followed by :meth:`.GroupedTable.aggregate`. Follow the; link for documentation on the aggregation step. Note; ----; **Using group_by**. **group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:. >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arg",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1623,Modifiability,variab,variable-length,1623,"*group_by** and its sibling methods (:meth:`.MatrixTable.group_rows_by` and; :meth:`.MatrixTable.group_cols_by`) accept both variable-length (``f(x, y, z)``); and keyword (``f(a=x, b=y, c=z)``) arguments. Variable-length arguments can be either strings or expressions that reference a; (possibly nested) field of the table. Keyword arguments can be arbitrary; expressions. **The following three usages are all equivalent**, producing a; :class:`.GroupedTable` grouped by fields `C1` and `C2` of `table1`. First, variable-length string arguments:. >>> table_result = (table1.group_by('C1', 'C2'); ... .aggregate(meanX = hl.agg.mean(table1.X))). Second, field reference variable-length arguments:. >>> table_result = (table1.group_by(table1.C1, table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Last, expression keyword arguments:. >>> table_result = (table1.group_by(C1 = table1.C1, C2 = table1.C2); ... .aggregate(meanX = hl.agg.mean(table1.X))). Additionally, the variable-length argument syntax also permits nested field; references. Given the following struct field `s`:. >>> table3 = table1.annotate(s = hl.struct(x=table1.X, z=table1.Z)). The following two usages are equivalent, grouping by one field, `x`:. >>> table_result = (table3.group_by(table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). >>> table_result = (table3.group_by(x = table3.s.x); ... .aggregate(meanX = hl.agg.mean(table3.X))). The keyword argument syntax permits arbitrary expressions:. >>> table_result = (table1.group_by(foo=table1.X ** 2 + 1); ... .aggregate(meanZ = hl.agg.mean(table1.Z))). These syntaxes can be mixed together, with the stipulation that all keyword arguments; must come at the end due to Python language restrictions. >>> table_result = (table1.group_by(table1.C1, 'C2', height_bin = table1.HT // 20); ... .aggregate(meanX = hl.agg.mean(table1.X))). Note; ----; This method does not support aggregation in key expressions. Arguments; ---------; exprs : varargs of type str or :class:`",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:430,Integrability,depend,dependent,430,"""""""Aggregate over rows into a local value. Examples; --------; Aggregate over rows:. >>> table1.aggregate(hl.struct(fraction_male=hl.agg.fraction(table1.SEX == 'M'),; ... mean_x=hl.agg.mean(table1.X))); Struct(fraction_male=0.5, mean_x=6.5). Note; ----; This method supports (and expects!) aggregation over rows. Parameters; ----------; expr : :class:`.Expression`; Aggregation expression. Returns; -------; any; Aggregated value dependent on `expr`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:592,Availability,checkpoint,checkpoint,592,"""""""Checkpoint the table to disk by writing and reading. Parameters; ----------; output : str; Path at which to write.; stage_locally: bool; If ``True``, major output will be written to temporary local storage; before being copied to ``output``; overwrite : bool; If ``True``, overwrite an existing file at the destination. Returns; -------; :class:`Table`. .. include:: _templates/write_warning.rst. Notes; -----; An alias for :meth:`write` followed by :func:`.read_table`. It is; possible to read the file at this path later with :func:`.read_table`. Examples; --------; >>> table1 = table1.checkpoint('output/table_checkpoint.ht', overwrite=True). """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:791,Testability,log,logging,791,"""""""Print the first few rows of the table to the console. Examples; --------; Show the first lines of the table:. >>> table1.show(); +-------+-------+-----+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 |; +-------+-------+-----+-------+-------+-------+-------+-------+; | 1 | 65 | ""M"" | 5 | 4 | 2 | 50 | 5 |; | 2 | 72 | ""M"" | 6 | 3 | 2 | 61 | 1 |; | 3 | 70 | ""F"" | 7 | 3 | 10 | 81 | -5 |; | 4 | 60 | ""F"" | 8 | 2 | 11 | 90 | -10 |; +-------+-------+-----+-------+-------+-------+-------+-------+. Notes; -----; The output can be passed piped to another output source using the `handler` argument:. >>> ht.show(handler=lambda x: logging.info(x)) # doctest: +SKIP. Parameters; ----------; n or n_rows : :obj:`int`; Maximum number of rows to show, or negative to show all rows.; width : :obj:`int`; Horizontal width at which to break fields.; truncate : :obj:`int`, optional; Truncate each field to the given number of characters. If; ``None``, truncate fields to the given `width`.; types : :obj:`bool`; Print an extra header line with the type of each field.; handler : Callable[[str], Any]; Handler function for data string.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1446,Availability,avail,available,1446,"""""""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; In the example below, both `table1` and `table2` are keyed by one; field `ID` of type ``int``. >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1738,Modifiability,variab,variable-length,1738,"""""""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; In the example below, both `table1` and `table2` are keyed by one; field `ID` of type ``int``. >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1034,Security,expose,expose,1034,"""""""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; In the example below, both `table1` and `table2` are keyed by one; field `ID` of type ``int``. >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1200,Usability,usab,usable,1200,"""""""Expose the row values as if looked up in a dictionary, indexing; with `exprs`. Examples; --------; In the example below, both `table1` and `table2` are keyed by one; field `ID` of type ``int``. >>> table_result = table1.select(B = table2.index(table1.ID).B); >>> table_result.B.show(); +-------+----------+; | ID | B |; +-------+----------+; | int32 | str |; +-------+----------+; | 1 | ""cat"" |; | 2 | ""dog"" |; | 3 | ""mouse"" |; | 4 | ""rabbit"" |; +-------+----------+. Using `key` as the sole index expression is equivalent to passing all; key fields individually:. >>> table_result = table1.select(B = table2.index(table1.key).B). It is also possible to use non-key fields or expressions as the index; expressions:. >>> table_result = table1.select(B = table2.index(table1.C1 % 4).B); >>> table_result.show(); +-------+---------+; | ID | B |; +-------+---------+; | int32 | str |; +-------+---------+; | 1 | ""dog"" |; | 2 | ""dog"" |; | 3 | ""dog"" |; | 4 | ""mouse"" |; +-------+---------+. Notes; -----; :meth:`.Table.index` is used to expose one table's fields for use in; expressions involving the another table or matrix table's fields. The; result of the method call is a struct expression that is usable in the; same scope as `exprs`, just as if `exprs` were used to look up values of; the table in a dictionary. The type of the struct expression is the same as the indexed table's; :meth:`.row_value` (the key fields are removed, as they are available; in the form of the index expressions). Note; ----; There is a shorthand syntax for :meth:`.Table.index` using square; brackets (the Python ``__getitem__`` syntax). This syntax is preferred. >>> table_result = table1.select(B = table2[table1.ID].B). Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Index expressions.; all_matches : bool; Experimental. If ``True``, value of expression is array of all matches. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:30,Modifiability,variab,variables,30,"""""""Return this table's global variables for use in another; expression context. Examples; --------; >>> table_result = table2.annotate(C = table2.A * table1.index_globals().global_field_1). Returns; -------; :class:`.StructExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:101,Performance,cache,cache,101,"""""""Persist this table in memory. Examples; --------; Persist the table in memory:. >>> table = table.cache() # doctest: +SKIP. Notes; -----. This method is an alias for :func:`persist(""MEMORY_ONLY"") <hail.Table.persist>`. Returns; -------; :class:`.Table`; Cached table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:295,Availability,redundant,redundant,295,"""""""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:354,Deployability,pipeline,pipelines,354,"""""""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:213,Performance,cache,cache,213,"""""""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:334,Performance,perform,performance,334,"""""""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:289,Safety,avoid,avoid,289,"""""""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:295,Safety,redund,redundant,295,"""""""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:596,Usability,guid,guide,596,"""""""Persist this table in memory or on disk. Examples; --------; Persist the table to both memory and disk:. >>> table = table.persist() # doctest: +SKIP. Notes; -----. The :meth:`.Table.persist` and :meth:`.Table.cache` methods store the; current table on disk or in memory temporarily to avoid redundant computation; and improve the performance of Hail pipelines. This method is not a substitution; for :meth:`.Table.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.Table`; Persisted table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:473,Availability,error,errors,473,"""""""Collect the rows of the table into a local list. Examples; --------; Collect a list of all `X` records:. >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; -----; This method returns a list whose elements are of type :class:`.Struct`. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (``struct.foo``) or string indexing (``struct['foo']``). Warning; -------; Using this method can cause out of memory errors. Only collect small tables. Returns; -------; :obj:`list` of :class:`.Struct`; List of rows.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:296,Security,access,accessed,296,"""""""Collect the rows of the table into a local list. Examples; --------; Collect a list of all `X` records:. >>> all_xs = [row['X'] for row in table1.select(table1.X).collect()]. Notes; -----; This method returns a list whose elements are of type :class:`.Struct`. Fields; of these structs can be accessed similarly to fields on a table, using dot; methods (``struct.foo``) or string indexing (``struct['foo']``). Warning; -------; Using this method can cause out of memory errors. Only collect small tables. Returns; -------; :obj:`list` of :class:`.Struct`; List of rows.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1226,Performance,perform,performance,1226,"""""""Add the integer index of each row as a new row field. Examples; --------. >>> table_result = table1.add_index(); >>> table_result.show() # doctest: +SKIP_OUTPUT_CHECK; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | idx |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int64 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 | 0 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 2 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 3 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; -----. This method returns a table with a new field whose name is given by; the `name` parameter, with type :py:data:`.tint64`. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like :meth:`.Table.take` or :meth:`.Table.export`) will; return rows in order. This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters; ----------; name : str; Name of index field. Returns; -------; :class:`.Table`; Table with a new index field.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1207,Usability,simpl,simple,1207,"""""""Add the integer index of each row as a new row field. Examples; --------. >>> table_result = table1.add_index(); >>> table_result.show() # doctest: +SKIP_OUTPUT_CHECK; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | ID | HT | SEX | X | Z | C1 | C2 | C3 | idx |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | int32 | int32 | str | int32 | int32 | int32 | int32 | int32 | int64 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+; | 1 | 65 | M | 5 | 4 | 2 | 50 | 5 | 0 |; | 2 | 72 | M | 6 | 3 | 2 | 61 | 1 | 1 |; | 3 | 70 | F | 7 | 3 | 10 | 81 | -5 | 2 |; | 4 | 60 | F | 8 | 2 | 11 | 90 | -10 | 3 |; +-------+-------+-----+-------+-------+-------+-------+-------+-------+. Notes; -----. This method returns a table with a new field whose name is given by; the `name` parameter, with type :py:data:`.tint64`. The value of this field; is the integer index of each row, starting from 0. Methods that respect; ordering (like :meth:`.Table.take` or :meth:`.Table.export`) will; return rows in order. This method is also helpful for creating a unique integer index for; rows of a table so that more complex types can be encoded as a simple; number for performance reasons. Parameters; ----------; name : str; Name of index field. Returns; -------; :class:`.Table`; Table with a new index field.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:617,Availability,error,error,617,"""""""Union the rows of multiple tables. Examples; --------. Take the union of rows from two tables:. >>> union_table = table1.union(other_table). Notes; -----; If a row appears in more than one table identically, it is duplicated; in the result. All tables must have the same key names and types. They; must also have the same row types, unless the `unify` parameter is; ``True``, in which case a field appearing in any table will be included; in the result, with missing values for tables that do not contain the; field. If a field appears in multiple tables with incompatible types,; like arrays and strings, then an error will be raised. Parameters; ----------; tables : varargs of :class:`.Table`; Tables to union.; unify : :obj:`bool`; Attempt to unify table field. Returns; -------; :class:`.Table`; Table with all rows from each component table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:393,Availability,avail,available,393,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:992,Availability,resilien,resilient-distributed-datasets-rdds,992,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:645,Performance,perform,performance,645,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:1205,Safety,avoid,avoid,1205,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:981,Usability,guid,guide,981,"""""""Change the number of partitions. Examples; --------. Repartition to 500 partitions:. >>> table_result = table1.repartition(500). Notes; -----. Check the current number of partitions with :meth:`.n_partitions`. The data in a dataset is divided into chunks called partitions, which; may be stored together or across a network, so that each partition may; be read and processed in parallel by available cores. When a table with; :math:`M` rows is first imported, each of the :math:`k` partitions will; contain about :math:`M/k` of the rows. Since each partition has some; computational overhead, decreasing the number of partitions can improve; performance after significant filtering. Since it's recommended to have; at least 2 - 4 partitions per core, increasing the number of partitions; can allow one to take advantage of more cores. Partitions are a core; concept of distributed computation in Spark, see `their documentation; <http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds>`__; for details. When ``shuffle=True``, Hail does a full shuffle of the data; and creates equal sized partitions. When ``shuffle=False``,; Hail combines existing partitions to avoid a full shuffle.; These algorithms correspond to the `repartition` and; `coalesce` commands in Spark, respectively. In particular,; when ``shuffle=False``, ``n_partitions`` cannot exceed current; number of partitions. Parameters; ----------; n : int; Desired number of partitions.; shuffle : bool; If ``True``, use full shuffle to repartition. Returns; -------; :class:`.Table`; Repartitioned table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:2,Availability,checkpoint,checkpoint,2,"# checkpoint rather than write to use fast codec",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:191,Usability,simpl,simply,191,"""""""Naively decrease the number of partitions. Example; -------; Naively repartition to 10 partitions:. >>> table_result = table1.naive_coalesce(10). Warning; -------; :meth:`.naive_coalesce` simply combines adjacent partitions to achieve; the desired number. It does not attempt to rebalance, unlike; :meth:`.repartition`, so it can produce a heavily unbalanced dataset. An; unbalanced dataset can be inefficient to operate on because the work is; not evenly distributed across partitions. Parameters; ----------; max_partitions : int; Desired number of partitions. If the current number of partitions is; less than or equal to `max_partitions`, do nothing. Returns; -------; :class:`.Table`; Table with at most `max_partitions` partitions.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:333,Integrability,depend,depends,333,"""""""Join two tables together. Examples; --------; Join `table1` to `table2` to produce `table_joined`:. >>> table_joined = table1.key_by('ID').join(table2.key_by('ID')). Notes; -----; Tables are joined at rows whose key fields have equal values. Missing values never match.; The inclusion of a row with no match in the opposite table depends on the; join type:. - **inner** -- Only rows with a matching key in the opposite table are included; in the resulting table.; - **left** -- All rows from the left table are included in the resulting table.; If a row in the left table has no match in the right table, then the fields; derived from the right table will be missing.; - **right** -- All rows from the right table are included in the resulting table.; If a row in the right table has no match in the left table, then the fields; derived from the left table will be missing.; - **outer** -- All rows are included in the resulting table. If a row in the right; table has no match in the left table, then the fields derived from the left; table will be missing. If a row in the right table has no match in the left table,; then the fields derived from the left table will be missing. Both tables must have the same number of keys and the corresponding; types of each key must be the same (order matters), but the key names; can be different. For example, if `table1` is keyed by fields ``['a',; 'b']``, both of type ``int32``, and `table2` is keyed by fields ``['c',; 'd']``, both of type ``int32``, then the two tables can be joined (their; rows will be joined where ``table1.a == table2.c`` and ``table1.b ==; table2.d``). The key fields and order from the left table are preserved,; while the key fields from the right table are not present in; the result. Note; ----; These join methods implement a traditional `Cartesian product; <https://en.wikipedia.org/wiki/Cartesian_product>`__ join, and; the number of records in the resulting table can be larger than; the number of records on the left or ",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py:296,Testability,test,test,296,"""""""Evaluate whether a boolean expression is true for all rows. Examples; --------; Test whether `C1` is greater than 5 in all rows of the table:. >>> if table1.all(table1.C1 == 5):; ... print(""All rows have C1 equal 5.""). Parameters; ----------; expr : :class:`.BooleanExpression`; Expression to test. Returns; -------; :obj:`bool`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/__init__.py:45,Safety,detect,detect,45,"# F403 'from .expr import *' used; unable to detect undefined names; # F401 '.expr.*' imported but unused; # E402 module level import not at top of file; # ruff: noqa: E402",MatchSource.CODE_COMMENT,hail/python/hail/__init__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/__init__.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py:41,Deployability,patch,patched,41,"# The original `get_return_value` is not patched, it's idempotent.",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py:7,Deployability,patch,patch,7,"# only patch the one used in py4j.java_gateway (call Java API)",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py:107,Integrability,message,messages,107,"""""""; This method starts a simple server which listens on a port for a; client to connect and start writing messages. Whenever a message; is received, it is written to sys.stderr. The server is run in; a daemon thread from the caller, which is killed when the caller; thread dies. If the socket is in use, then the server tries to listen on the; next port (port + 1). After 25 tries, it gives up. :param str host: Hostname for server.; :param int port: Port to listen on.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py:128,Integrability,message,message,128,"""""""; This method starts a simple server which listens on a port for a; client to connect and start writing messages. Whenever a message; is received, it is written to sys.stderr. The server is run in; a daemon thread from the caller, which is killed when the caller; thread dies. If the socket is in use, then the server tries to listen on the; next port (port + 1). After 25 tries, it gives up. :param str host: Hostname for server.; :param int port: Port to listen on.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py:26,Usability,simpl,simple,26,"""""""; This method starts a simple server which listens on a port for a; client to connect and start writing messages. Whenever a message; is received, it is written to sys.stderr. The server is run in; a daemon thread from the caller, which is killed when the caller; thread dies. If the socket is in use, then the server tries to listen on the; next port (port + 1). After 25 tries, it gives up. :param str host: Hostname for server.; :param int port: Port to listen on.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py:49,Availability,down,down,49,"# The thread should be a daemon so that it shuts down when the parent thread is killed",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py:88,Deployability,patch,patch,88,"# This has to go after creating the SparkSession. Unclear why.; # Maybe it does its own patch?",MatchSource.CODE_COMMENT,hail/python/hail/backend/py4j_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/py4j_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/service_backend.py:31,Integrability,protocol,protocol,31,"# is.hail.backend.service.Main protocol",MatchSource.CODE_COMMENT,hail/python/hail/backend/service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/service_backend.py:51,Integrability,protocol,protocol,51,"# is.hail.backend.service.ServiceBackendSocketAPI2 protocol",MatchSource.CODE_COMMENT,hail/python/hail/backend/service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/spark_backend.py:87,Performance,load,load,87,"# Setting extraClassPath in HDInsight overrides the classpath entirely so you can't; # load the Scala standard library. Interestingly, setting extraClassPath is not; # necessary in HDInsight.",MatchSource.CODE_COMMENT,hail/python/hail/backend/spark_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/backend/spark_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:55,Deployability,configurat,configuration,55,"# -*- coding: utf-8 -*-; #; # Hail documentation build configuration file, created by; # sphinx-quickstart on Fri Nov 4 10:55:10 2016.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:256,Deployability,configurat,configuration,256,"# -*- coding: utf-8 -*-; #; # Hail documentation build configuration file, created by; # sphinx-quickstart on Fri Nov 4 10:55:10 2016.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:330,Deployability,configurat,configuration,330,"# -*- coding: utf-8 -*-; #; # Hail documentation build configuration file, created by; # sphinx-quickstart on Fri Nov 4 10:55:10 2016.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:55,Modifiability,config,configuration,55,"# -*- coding: utf-8 -*-; #; # Hail documentation build configuration file, created by; # sphinx-quickstart on Fri Nov 4 10:55:10 2016.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:256,Modifiability,config,configuration,256,"# -*- coding: utf-8 -*-; #; # Hail documentation build configuration file, created by; # sphinx-quickstart on Fri Nov 4 10:55:10 2016.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:330,Modifiability,config,configuration,330,"# -*- coding: utf-8 -*-; #; # Hail documentation build configuration file, created by; # sphinx-quickstart on Fri Nov 4 10:55:10 2016.; #; # This file is execfile()d with the current directory set to its; # containing dir.; #; # Note that not all possible configuration values are present in this; # autogenerated file.; #; # All configuration values have a default; values that are commented out; # serve to show the default.; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:84,Deployability,configurat,configuration,84,"# sys.path.insert(0, os.path.abspath('.')); # import sphinx_rtd_theme; # -- General configuration ------------------------------------------------",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:84,Modifiability,config,configuration,84,"# sys.path.insert(0, os.path.abspath('.')); # import sphinx_rtd_theme; # -- General configuration ------------------------------------------------",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:97,Deployability,release,release,97,"# The version info for the project you're documenting, acts as replacement for; # |version| and |release|, also used in various other places throughout the; # built documents.; #; # The full version, including alpha/beta/rc tags.",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:123,Integrability,message,message,123,"# A list of ignored prefixes for module index sorting.; # modindex_common_prefix = []; # If true, keep warnings as ""system message"" paragraphs in the built documents.; # keep_warnings = False; # If true, `todo` and `todoList` produce output, else they produce nothing.",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:112,Availability,avail,available,112,"# Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; #",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:61,Deployability,release,release,61,"# The name for this set of Sphinx documents.; # ""<project> v<release> documentation"" by default.; #",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:23,Deployability,update,updated,23,"# If not None, a 'Last updated on:' timestamp is inserted at every page; # bottom, using the given strftime format.; # The empty string is equivalent to '%b %d, %Y'.; #",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:850,Deployability,configurat,configuration,850,"# If true, an OpenSearch description file will be output, and all pages will; # contain a <link> tag referring to it. The value of this option must be the; # base URL from which the finished HTML is served.; #; # html_use_opensearch = ''; # This is the file name suffix for HTML files (e.g. "".xhtml"").; # html_file_suffix = None; # Language to be used for generating the HTML full-text search index.; # Sphinx supports the following languages:; # 'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'; # 'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr', 'zh'; #; # html_search_language = 'en'; # A dictionary with options for the search language support, empty by default.; # 'ja' uses this config value.; # 'zh' user can custom change `jieba` dictionary path.; #; # html_search_options = {'type': 'default'}; # The name of a javascript file (relative to the configuration directory) that; # implements a search results scorer. If empty, the default will be used.; #; # html_search_scorer = 'scorer.js'; # Output file base name for HTML help builder.",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:682,Modifiability,config,config,682,"# If true, an OpenSearch description file will be output, and all pages will; # contain a <link> tag referring to it. The value of this option must be the; # base URL from which the finished HTML is served.; #; # html_use_opensearch = ''; # This is the file name suffix for HTML files (e.g. "".xhtml"").; # html_file_suffix = None; # Language to be used for generating the HTML full-text search index.; # Sphinx supports the following languages:; # 'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'; # 'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr', 'zh'; #; # html_search_language = 'en'; # A dictionary with options for the search language support, empty by default.; # 'ja' uses this config value.; # 'zh' user can custom change `jieba` dictionary path.; #; # html_search_options = {'type': 'default'}; # The name of a javascript file (relative to the configuration directory) that; # implements a search results scorer. If empty, the default will be used.; #; # html_search_scorer = 'scorer.js'; # Output file base name for HTML help builder.",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:850,Modifiability,config,configuration,850,"# If true, an OpenSearch description file will be output, and all pages will; # contain a <link> tag referring to it. The value of this option must be the; # base URL from which the finished HTML is served.; #; # html_use_opensearch = ''; # This is the file name suffix for HTML files (e.g. "".xhtml"").; # html_file_suffix = None; # Language to be used for generating the HTML full-text search index.; # Sphinx supports the following languages:; # 'da', 'de', 'en', 'es', 'fi', 'fr', 'hu', 'it', 'ja'; # 'nl', 'no', 'pt', 'ro', 'ru', 'sv', 'tr', 'zh'; #; # html_search_language = 'en'; # A dictionary with options for the search language support, empty by default.; # 'ja' uses this config value.; # 'zh' user can custom change `jieba` dictionary path.; #; # html_search_options = {'type': 'default'}; # The name of a javascript file (relative to the configuration directory) that; # implements a search results scorer. If empty, the default will be used.; #; # html_search_scorer = 'scorer.js'; # Output file base name for HTML help builder.",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py:632,Safety,avoid,avoid,632,"# The name of an image file (relative to this directory) to place at the top of; # the title page.; #; # latex_logo = None; # For ""manual"" documents, if this is true, then toplevel headings are parts,; # not chapters.; #; # latex_use_parts = False; # If true, show page references after internal links.; #; # latex_show_pagerefs = False; # If true, show URL addresses after external links.; #; # latex_show_urls = False; # Documents to append as an appendix to all manuals.; #; # latex_appendices = []; # It false, will not define \strong, \code, 	itleref, \crossref ... but only; # \sphinxstrong, ..., \sphinxtitleref, ... To help avoid clash with user added; # packages.; #; # latex_keep_old_macro_names = True; # If false, no module index is generated.; #; # latex_domain_indices = True; # -- Options for manual page output ---------------------------------------; # One entry per manual page. List of tuples; # (source start file, name, description, authors, manual section).",MatchSource.CODE_COMMENT,hail/python/hail/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py:482,Availability,avail,available,482,"""""""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/datasets.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py:1025,Availability,avail,available,1025,"""""""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/datasets.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py:399,Performance,load,load,399,"""""""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/datasets.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py:472,Performance,load,load,472,"""""""Load a genetic dataset from Hail's repository. Example; -------; >>> # Load the gnomAD ""HGDP + 1000 Genomes"" dense MatrixTable with GRCh38 coordinates.; >>> mt = hl.experimental.load_dataset(name='gnomad_hgdp_1kg_subset_dense',; ... version='3.1.2',; ... reference_genome='GRCh38',; ... region='us-central1',; ... cloud='gcp'). Parameters; ----------; name : :class:`str`; Name of the dataset to load.; version : :class:`str`, optional; Version of the named dataset to load (see available versions in; documentation). Possibly ``None`` for some datasets.; reference_genome : :class:`str`, optional; Reference genome build, ``'GRCh37'`` or ``'GRCh38'``. Possibly ``None``; for some datasets.; region : :class:`str`; Specify region for bucket, ``'us'``, ``'us-central1'``, or ``'europe-west1'``, (default is; ``'us-central1'``).; cloud : :class:`str`; Specify if using Google Cloud Platform or Amazon Web Services,; ``'gcp'`` or ``'aws'`` (default is ``'gcp'``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Returns; -------; :class:`.Table`, :class:`.MatrixTable`, or :class:`.BlockMatrix`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/datasets.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/datasets.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:371,Availability,avail,available,371,"""""""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:278,Security,access,access,278,"""""""Create :class:`.DatasetVersion` object from dictionary. Parameters; ----------; doc : :obj:`dict`; Dictionary containing url and version keys.; Value for url is a :obj:`dict` containing key: value pairs, like; ``cloud: {region: url}``.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`.DatasetVersion` if available on cloud platform, else ``None``.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:384,Availability,avail,available,384,"""""""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:547,Availability,avail,available,547,"""""""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:371,Security,access,access,371,"""""""Get versions of a :class:`.Dataset` in the specified region, if they; exist. Parameters; ----------; name : :obj:`str`; Name of dataset.; versions : :class:`list` of :class:`.DatasetVersion`; List of DatasetVersion objects where the value for :attr:`.url`; is a :obj:`dict` containing key: value pairs, like ``region: url``.; region : :obj:`str`; Region from which to access data, available regions given in; :attr:`hail.experimental.DB._valid_regions`. Returns; -------; available_versions : :class:`list` of :class:`.DatasetVersion`; List of available versions of a class:`.Dataset` for region.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:200,Availability,avail,available,200,"""""""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:49,Security,access,accessible,49,"""""""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:187,Security,access,access,187,"""""""Check if a :class:`.DatasetVersion` object is accessible in the; desired region. Parameters; ----------; name : :obj:`str`; Name of dataset.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`. Returns; -------; valid_region : :obj:`bool`; Whether or not the dataset exists in the specified region.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:107,Deployability,configurat,configuration,107,"""""""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:178,Deployability,configurat,configurations,178,"""""""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:107,Modifiability,config,configuration,107,"""""""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:178,Modifiability,config,configurations,178,"""""""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:343,Security,access,access,343,"""""""Dataset object constructed from name, description, url, key_properties,; and versions specified in JSON configuration file or a provided :obj:`dict`; mapping dataset names to configurations. Parameters; ----------; name : :obj:`str`; Name of dataset.; description : :obj:`str`; Brief description of dataset.; url : :obj:`str`; Cloud URL to access dataset.; key_properties : :class:`set` of :obj:`str`; Set containing key property strings, if present. Valid properties; include ``'gene'`` and ``'unique'``.; versions : :class:`list` of :class:`.DatasetVersion`; List of :class:`.DatasetVersion` objects.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:267,Availability,avail,available,267,"""""""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`Dataset`, optional; If versions exist for region returns a :class:`.Dataset` object,; else ``None``.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:254,Security,access,access,254,"""""""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`Dataset`, optional; If versions exist for region returns a :class:`.Dataset` object,; else ``None``.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:379,Security,access,access,379,"""""""Create :class:`.Dataset` object from dictionary. Parameters; ----------; name : :obj:`str`; Name of dataset.; doc : :obj:`dict`; Dictionary containing dataset description, url, key_properties, and; versions.; region : :obj:`str`; Region from which to access data, available regions given in; :func:`hail.experimental.DB._valid_regions`.; cloud : :obj:`str`; Cloud platform to access dataset, either ``'gcp'`` or ``'aws'``. Returns; -------; :class:`Dataset`, optional; If versions exist for region returns a :class:`.Dataset` object,; else ``None``.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:1119,Availability,avail,available,1119,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:174,Deployability,configurat,configuration,174,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:241,Deployability,configurat,configuration,241,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:839,Deployability,configurat,configuration,839,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:870,Deployability,configurat,configuration,870,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:989,Deployability,configurat,configuration,989,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:1021,Deployability,configurat,configuration,1021,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:174,Modifiability,config,configuration,174,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:241,Modifiability,config,configuration,241,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:839,Modifiability,config,configuration,839,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:870,Modifiability,config,configuration,870,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:909,Modifiability,config,config,909,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:989,Modifiability,config,configuration,989,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:1021,Modifiability,config,configuration,1021,"""""""An annotation database instance. This class facilitates the annotation of genetic datasets with variant annotations. It accepts; either an HTTP(S) URL to an Annotation DB configuration or a Python :obj:`dict` describing an; Annotation DB configuration. User must specify the `region` (aws: ``'us'``, gcp:; ``'us-central1'`` or ``'europe-west1'``) in which the cluster is running if connecting to the; default Hail Annotation DB. User must also specify the `cloud` platform that they are using; (``'gcp'`` or ``'aws'``). Parameters; ----------; region : :obj:`str`; Region cluster is running in, either ``'us'``, ``'us-central1'``, or ``'europe-west1'``; (default is ``'us-central1'``).; cloud : :obj:`str`; Cloud platform, either ``'gcp'`` or ``'aws'`` (default is ``'gcp'``).; url : :obj:`str`, optional; Optional URL to annotation DB configuration, if using custom configuration; (default is ``None``).; config : :obj:`str`, optional; Optional :obj:`dict` describing an annotation DB configuration, if using; custom configuration (default is ``None``). Note; ----; The ``'aws'`` `cloud` platform is currently only available for the ``'us'``; `region`. Examples; --------; Create an annotation database connecting to the default Hail Annotation DB:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:20,Availability,avail,available,20,"""""""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:90,Availability,avail,available,90,"""""""List of names of available annotation datasets. Returns; -------; :obj:`list`; List of available annotation datasets.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:42,Availability,avail,available,42,"""""""Check if datasets given in `names` are available in the annotation; database instance. Parameters; ----------; names : :obj:`iterable`; Names to check.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py:157,Availability,avail,available,157,"""""""Add annotations from datasets specified by name to a relational; object. List datasets with :attr:`~.available_datasets`. An interactive query builder is available in the; `Hail Annotation Database documentation; </docs/0.2/annotation_database_ui.html>`_. Examples; --------; Annotate a :class:`.MatrixTable` with ``gnomad_lof_metrics``:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> mt = db.annotate_rows_db(mt, 'gnomad_lof_metrics') # doctest: +SKIP. Annotate a :class:`.Table` with ``clinvar_gene_summary``, ``CADD``,; and ``DANN``:. >>> db = hl.experimental.DB(region='us-central1', cloud='gcp'); >>> ht = db.annotate_rows_db(ht, 'clinvar_gene_summary', 'CADD', 'DANN') # doctest: +SKIP. Notes; -----. If a dataset is gene-keyed, the annotation will be a dictionary mapping; from gene name to the annotation value. There will be one entry for each; gene overlapping the given locus. If a dataset does not have unique rows for each key (consider the; ``gencode`` genes, which may overlap; and ``clinvar_variant_summary``,; which contains many overlapping multiple nucleotide variants), then the; result will be an array of annotation values, one for each row. Parameters; ----------; rel : :class:`.MatrixTable` or :class:`.Table`; The relational object to which to add annotations.; names : varargs of :class:`str`; The names of the datasets with which to annotate `rel`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; The relational object `rel`, with the annotations from `names`; added.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/db.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/db.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py:2694,Performance,load,load,2694,"tint32`). Furthermore, if the ``reference_genome`` parameter is specified and; ``skip_invalid_contigs`` is ``True``, this import function will skip; lines in the GTF where ``seqname`` is not consistent with the reference; genome specified. Example; -------. >>> ht = hl.experimental.import_gtf('data/test.gtf',; ... reference_genome='GRCh37',; ... skip_invalid_contigs=True). >>> ht.describe() # doctest: +SKIP_OUTPUT_CHECK; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqname`` is not consistent with the reference genome.; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions (passed to import_table).; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py:2994,Performance,load,load,2994,"tint32`). Furthermore, if the ``reference_genome`` parameter is specified and; ``skip_invalid_contigs`` is ``True``, this import function will skip; lines in the GTF where ``seqname`` is not consistent with the reference; genome specified. Example; -------. >>> ht = hl.experimental.import_gtf('data/test.gtf',; ... reference_genome='GRCh37',; ... skip_invalid_contigs=True). >>> ht.describe() # doctest: +SKIP_OUTPUT_CHECK; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines where; ``seqname`` is not consistent with the reference genome.; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions (passed to import_table).; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism. Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py:1492,Testability,test,test,1492,"; 'frame': int32; 'interval': interval<>. There will also be corresponding fields for every tag found in the; attribute field of the GTF file. Note; ----. This function will return an ``interval`` field of type :class:`.tinterval`; constructed from the ``seqname``, ``start``, and ``end`` fields in the; GTF file. This interval is inclusive of both the start and end positions; in the GTF file. If the ``reference_genome`` parameter is specified, the start and end; points of the ``interval`` field will be of type :class:`.tlocus`.; Otherwise, the start and end points of the ``interval`` field will be of; type :class:`.tstruct` with fields ``seqname`` (type :class:`str`) and; ``position`` (type :obj:`.tint32`). Furthermore, if the ``reference_genome`` parameter is specified and; ``skip_invalid_contigs`` is ``True``, this import function will skip; lines in the GTF where ``seqname`` is not consistent with the reference; genome specified. Example; -------. >>> ht = hl.experimental.import_gtf('data/test.gtf',; ... reference_genome='GRCh37',; ... skip_invalid_contigs=True). >>> ht.describe() # doctest: +SKIP_OUTPUT_CHECK; ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'source': str; 'feature': str; 'score': float64; 'strand': str; 'frame': int32; 'gene_type': str; 'exon_id': str; 'havana_transcript': str; 'level': str; 'transcript_name': str; 'gene_status': str; 'gene_id': str; 'transcript_type': str; 'tag': str; 'transcript_status': str; 'gene_name': str; 'transcript_id': str; 'exon_number': str; 'havana_gene': str; 'interval': interval<locus<GRCh37>>; ----------------------------------------; Key: ['interval']; ----------------------------------------. Parameters; ----------. path : :class:`str`; File to import.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_contigs : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip l",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py:201,Availability,avail,available,201,"""""""Get intervals of genes or transcripts. Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable. On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz. Example; -------; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) # doctest: +SKIP. Parameters; ----------. gene_symbols : :obj:`list` of :class:`str`, optional; Gene symbols (e.g. PCSK9).; gene_ids : :obj:`list` of :class:`str`, optional; Gene IDs (e.g. ENSG00000223972).; transcript_ids : :obj:`list` of :class:`str`, optional; Transcript IDs (e.g. ENSG00000223972).; verbose : :obj:`bool`; If ``True``, print which genes and transcripts were matched in the GTF file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :obj:`list` of :class:`.Interval`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py:308,Availability,avail,available,308,"""""""Get intervals of genes or transcripts. Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable. On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz. Example; -------; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) # doctest: +SKIP. Parameters; ----------. gene_symbols : :obj:`list` of :class:`str`, optional; Gene symbols (e.g. PCSK9).; gene_ids : :obj:`list` of :class:`str`, optional; Gene IDs (e.g. ENSG00000223972).; transcript_ids : :obj:`list` of :class:`str`, optional; Transcript IDs (e.g. ENSG00000223972).; verbose : :obj:`bool`; If ``True``, print which genes and transcripts were matched in the GTF file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :obj:`list` of :class:`.Interval`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py:1076,Performance,load,load,1076,"""""""Get intervals of genes or transcripts. Get the boundaries of genes or transcripts from a GTF file, for quick filtering of a Table or MatrixTable. On Google Cloud platform:; Gencode v19 (GRCh37) GTF available at: gs://hail-common/references/gencode/gencode.v19.annotation.gtf.bgz; Gencode v29 (GRCh38) GTF available at: gs://hail-common/references/gencode/gencode.v29.annotation.gtf.bgz. Example; -------; >>> hl.filter_intervals(ht, get_gene_intervals(gene_symbols=['PCSK9'], reference_genome='GRCh37')) # doctest: +SKIP. Parameters; ----------. gene_symbols : :obj:`list` of :class:`str`, optional; Gene symbols (e.g. PCSK9).; gene_ids : :obj:`list` of :class:`str`, optional; Gene IDs (e.g. ENSG00000223972).; transcript_ids : :obj:`list` of :class:`str`, optional; Transcript IDs (e.g. ENSG00000223972).; verbose : :obj:`bool`; If ``True``, print which genes and transcripts were matched in the GTF file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :obj:`list` of :class:`.Interval`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py:226,Performance,load,load,226,"""""""; Get Gencode GTF (from file or reference genome). Parameters; ----------; reference_genome : :class:`.ReferenceGenome`, optional; Reference genome to use (passed along to import_gtf).; gtf_file : :class:`str`; GTF file to load. If none is provided, but `reference_genome` is one of; `GRCh37` or `GRCh38`, a default will be used (on Google Cloud Platform). Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/import_gtf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/import_gtf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscore.py:351,Deployability,continuous,continuous,351,"""""""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). .. code-block:: text. +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+00 |; | 20:103517 | 2.04604e+00 | 2.75392e+02 | 4.69239e+00 |; | 20:108286 | 2.06585e+00 | 2.86453e+02 | 5.00124e+00 |; +---------------+-------------------+-----------------------+-------------+. Warning; -------; :func:`.ld_score` will fail if ``entry_expr`` results in any missing; values. The special float value ``nan`` is not considered a; missing value. **Further reading**. For more in-depth discussion of LD scores, see:. - `LD Score regression distinguishes confounding from polygenicity in genome-wide association studies (Bulik-Sullivan et al, 2015) <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4495769/>`__; ",MatchSource.CODE_COMMENT,hail/python/hail/experimental/ldscore.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscore.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscore.py:618,Deployability,continuous,continuous,618,"""""""Calculate LD scores. Example; -------. >>> # Load genetic data into MatrixTable; >>> mt = hl.import_plink(bed='data/ldsc.bed',; ... bim='data/ldsc.bim',; ... fam='data/ldsc.fam'). >>> # Create locus-keyed Table with numeric variant annotations; >>> ht = hl.import_table('data/ldsc.annot',; ... types={'BP': hl.tint,; ... 'binary': hl.tfloat,; ... 'continuous': hl.tfloat}); >>> ht = ht.annotate(locus=hl.locus(ht.CHR, ht.BP)); >>> ht = ht.key_by('locus'). >>> # Annotate MatrixTable with external annotations; >>> mt = mt.annotate_rows(binary_annotation=ht[mt.locus].binary,; ... continuous_annotation=ht[mt.locus].continuous). >>> # Calculate LD scores using centimorgan coordinates; >>> ht_scores = hl.experimental.ld_score(entry_expr=mt.GT.n_alt_alleles(),; ... locus_expr=mt.locus,; ... radius=1.0,; ... coord_expr=mt.cm_position,; ... annotation_exprs=[mt.binary_annotation,; ... mt.continuous_annotation]). >>> # Show results; >>> ht_scores.show(3). .. code-block:: text. +---------------+-------------------+-----------------------+-------------+; | locus | binary_annotation | continuous_annotation | univariate |; +---------------+-------------------+-----------------------+-------------+; | locus<GRCh37> | float64 | float64 | float64 |; +---------------+-------------------+-----------------------+-------------+; | 20:82079 | 1.15183e+00 | 7.30145e+01 | 1.60117e+00 |; | 20:103517 | 2.04604e+00 | 2.75392e+02 | 4.69239e+00 |; | 20:108286 | 2.06585e+00 | 2.86453e+02 | 5.00124e+00 |; +---------------+-------------------+-----------------------+-------------+. Warning; -------; :func:`.ld_score` will fail if ``entry_expr`` results in any missing; values. The special float value ``nan`` is not considered a; missing value. **Further reading**. For more in-depth discussion of LD scores, see:. - `LD Score regression distinguishes confounding from polygenicity in genome-wide association studies (Bulik-Sullivan et al, 2015) <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4495769/>`__; ",MatchSource.CODE_COMMENT,hail/python/hail/experimental/ldscore.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscore.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscsim.py:30,Testability,test,testing,30,"""""""; Simulation framework for testing LDSC. Models for SNP effects:; - Infinitesimal (can simulate n correlated traits); - Spike & slab (can simulate up to 2 correlated traits); - Annotation-informed. Features:; - Field aggregation tools for annotation-informed model and; population stratification with many covariates.; - Automatic adjustment of genetic correlation parameters; to allow for the joint simulation of up to 100 randomly; correlated phenotypes.; - Methods for binarizing phenotypes to have a certain prevalence; and for adding ascertainment bias to binarized phenotypes. @author: nbaya; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/ldscsim.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscsim.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscsim.py:32,Availability,error,error,32,"# if intersect is empty: return error",MatchSource.CODE_COMMENT,hail/python/hail/experimental/ldscsim.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/ldscsim.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py:54,Performance,load,loadings,54,"""""""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/pca.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py:290,Performance,load,loadings,290,"""""""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/pca.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py:654,Performance,load,loadings,654,"""""""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/pca.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py:714,Performance,load,loadings,714,"""""""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/pca.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py:881,Performance,load,loadings,881,"""""""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/pca.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py:969,Performance,load,loadings,969,"""""""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/pca.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py:1132,Performance,load,loadings,1132,"""""""Projects genotypes onto pre-computed PCs. Requires loadings and; allele-frequency from a reference dataset (see example). Note that; `loadings_expr` must have no missing data and reflect the rows; from the original PCA run for this method to be accurate. Example; -------; >>> # Compute loadings and allele frequency for reference dataset; >>> _, _, loadings_ht = hl.hwe_normalized_pca(mt.GT, k=10, compute_loadings=True) # doctest: +SKIP; >>> mt = mt.annotate_rows(af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2) # doctest: +SKIP; >>> loadings_ht = loadings_ht.annotate(af=mt.rows()[loadings_ht.key].af) # doctest: +SKIP; >>> # Project new genotypes onto loadings; >>> ht = pc_project(mt_to_project.GT, loadings_ht.loadings, loadings_ht.af) # doctest: +SKIP. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression for genotypes; to project onto loadings.; loadings_expr : :class:`.ArrayNumericExpression`; Location of expression for loadings; af_expr : :class:`.Float64Expression`; Location of expression for allele frequency. Returns; -------; :class:`.Table`; Table with scores calculated from loadings in column `scores`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/pca.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/pca.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/phase_by_transmission.py:1270,Integrability,wrap,wrapper,1270,"""""""Phases genotype calls in a trio based allele transmission. Notes; -----; In the phased calls returned, the order is as follows:; - Proband: father_allele | mother_allele; - Parents: transmitted_allele | untransmitted_allele. Phasing of sex chromosomes:; - Sex chromosomes of male individuals should be haploid to be phased correctly.; - If `proband_call` is diploid on non-par regions of the sex chromosomes, it is assumed to be female. Returns `NA` when genotype calls cannot be phased.; The following genotype calls combinations cannot be phased by transmission:; 1. One of the calls in the trio is missing; 2. The proband genotype cannot be obtained from the parents alleles (Mendelian violation); 3. All individuals of the trio are heterozygous for the same two alleles; 4. Father is diploid on non-PAR region of X or Y; 5. Proband is diploid on non-PAR region of Y. In addition, individual phased genotype calls are returned as missing in the following situations:; 1. All mother genotype calls non-PAR region of Y; 2. Diploid father genotype calls on non-PAR region of X for a male proband (proband and mother are still phased as father doesn't participate in allele transmission). Note; ----; :func:`~.phase_trio_matrix_by_transmission` provides a convenience wrapper for phasing a trio matrix. Parameters; ----------; locus : :class:`.LocusExpression`; Expression for the locus in the trio matrix; alleles : :class:`.ArrayExpression`; Expression for the alleles in the trio matrix; proband_call : :class:`.CallExpression`; Expression for the proband call in the trio matrix; father_call : :class:`.CallExpression`; Expression for the father call in the trio matrix; mother_call : :class:`.CallExpression`; Expression for the mother call in the trio matrix. Returns; -------; :class:`.ArrayExpression`; Array containing: [phased proband call, phased father call, phased mother call]""""""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/phase_by_transmission.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/phase_by_transmission.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/tidyr.py:423,Modifiability,variab,variable-length,423,"""""""Collapse fields into key-value pairs. :func:`.gather` mimics the functionality of the `gather()` function found in R's; ``tidyr`` package. This is a way to turn ""wide"" format data into ""long""; format data. Parameters; ----------; ht : :class:`.Table`; A Hail table.; key : :class:`str`; The name of the key field in the gathered table.; value : :class:`str`; The name of the value field in the gathered table.; fields : variable-length args of obj:`str`; Names of fields to gather in ``ht``. Returns; -------; :class:`.Table`; Table with original ``fields`` gathered into ``key`` and ``value`` fields.""""""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/tidyr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/tidyr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/densify.py:489,Availability,down,downstream,489,"""""""Convert sparse matrix table to a dense VCF-like representation by expanding reference blocks. Parameters; ----------; sparse_mt : :class:`.MatrixTable`; Sparse MatrixTable to densify. The first row key field must; be named ``locus`` and have type ``locus``. Must have an; ``END`` entry field of type ``int32``. Returns; -------; :class:`.MatrixTable`; The densified MatrixTable. The ``END`` entry field is dropped. While computationally expensive, this; operation is necessary for many downstream analyses, and should be thought of as; roughly costing as much as reading a matrix table created by importing a dense; project VCF.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/densify.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/densify.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py:1312,Availability,down,downcoded,1312," This function drops the `LA` (local alleles) field, as it re-computes; entry fields based on the new, split globals alleles. Variants are split thus:. - A row with only one (reference) or two (reference and alternate) alleles; is unchanged, as local and global alleles are the same. - A row with multiple alternate alleles will be split, with one row for; each alternate allele, and each row will contain two alleles: ref and alt.; The reference and alternate allele will be minrepped using; :func:`.min_rep`. The split multi logic handles the following entry fields:. .. code-block:: text. struct {; LGT: call; LAD: array<int32>; DP: int32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with ei",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py:1390,Availability,down,downcoded,1390,"e new, split globals alleles. Variants are split thus:. - A row with only one (reference) or two (reference and alternate) alleles; is unchanged, as local and global alleles are the same. - A row with multiple alternate alleles will be split, with one row for; each alternate allele, and each row will contain two alleles: ref and alt.; The reference and alternate allele will be minrepped using; :func:`.min_rep`. The split multi logic handles the following entry fields:. .. code-block:: text. struct {; LGT: call; LAD: array<int32>; DP: int32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with either duplicate loci (in; which case the explode could possibly result in out-of-order rows, alt",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py:1888,Availability,down,downcode,1888,"lowing entry fields:. .. code-block:: text. struct {; LGT: call; LAD: array<int32>; DP: int32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with either duplicate loci (in; which case the explode could possibly result in out-of-order rows, although; the actual split_multi function also doesn't handle that case). It also checks that min-repping will not change the locus and will error if; it does. Unlike the normal split_multi function. Sparse split multi will not filter; ``*`` alleles. This is because a row with a bi-allelic spanning deletion; may contain reference blocks that start at this position for other samples. Parameters; ----------; sparse_mt : :class:`.MatrixTable`; Sparse Mat",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py:2541,Availability,error,error,2541,"exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with either duplicate loci (in; which case the explode could possibly result in out-of-order rows, although; the actual split_multi function also doesn't handle that case). It also checks that min-repping will not change the locus and will error if; it does. Unlike the normal split_multi function. Sparse split multi will not filter; ``*`` alleles. This is because a row with a bi-allelic spanning deletion; may contain reference blocks that start at this position for other samples. Parameters; ----------; sparse_mt : :class:`.MatrixTable`; Sparse MatrixTable to split.; filter_changed_loci : :obj:`.bool`; Rather than erroring if any REF/ALT pair changes locus under :func:`.min_rep`; filter that variant instead. Returns; -------; :class:`.MatrixTable`; The split MatrixTable in sparse format. """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py:2923,Availability,error,erroring,2923,"exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with either duplicate loci (in; which case the explode could possibly result in out-of-order rows, although; the actual split_multi function also doesn't handle that case). It also checks that min-repping will not change the locus and will error if; it does. Unlike the normal split_multi function. Sparse split multi will not filter; ``*`` alleles. This is because a row with a bi-allelic spanning deletion; may contain reference blocks that start at this position for other samples. Parameters; ----------; sparse_mt : :class:`.MatrixTable`; Sparse MatrixTable to split.; filter_changed_loci : :obj:`.bool`; Rather than erroring if any REF/ALT pair changes locus under :func:`.min_rep`; filter that variant instead. Returns; -------; :class:`.MatrixTable`; The split MatrixTable in sparse format. """"""",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py:1731,Deployability,update,updated,1731,"ference and alternate allele will be minrepped using; :func:`.min_rep`. The split multi logic handles the following entry fields:. .. code-block:: text. struct {; LGT: call; LAD: array<int32>; DP: int32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with either duplicate loci (in; which case the explode could possibly result in out-of-order rows, although; the actual split_multi function also doesn't handle that case). It also checks that min-repping will not change the locus and will error if; it does. Unlike the normal split_multi function. Sparse split multi will not filter; ``*`` alleles. This is because a row with a bi-allelic spanning deletion; may contain reference blocks that st",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py:834,Testability,log,logic,834,"""""""Splits multiallelic variants on a sparse matrix table. Analogous to :func:`.split_multi_hts` (splits entry fields) for sparse; representations. Takes a dataset formatted like the output of :func:`.run_combiner`. The; splitting will add `was_split` and `a_index` fields, as :func:`.vds.split_multi`; does. This function drops the `LA` (local alleles) field, as it re-computes; entry fields based on the new, split globals alleles. Variants are split thus:. - A row with only one (reference) or two (reference and alternate) alleles; is unchanged, as local and global alleles are the same. - A row with multiple alternate alleles will be split, with one row for; each alternate allele, and each row will contain two alleles: ref and alt.; The reference and alternate allele will be minrepped using; :func:`.min_rep`. The split multi logic handles the following entry fields:. .. code-block:: text. struct {; LGT: call; LAD: array<int32>; DP: int32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row ",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py:1923,Testability,log,logic,1923,"nt32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with either duplicate loci (in; which case the explode could possibly result in out-of-order rows, although; the actual split_multi function also doesn't handle that case). It also checks that min-repping will not change the locus and will error if; it does. Unlike the normal split_multi function. Sparse split multi will not filter; ``*`` alleles. This is because a row with a bi-allelic spanning deletion; may contain reference blocks that start at this position for other samples. Parameters; ----------; sparse_mt : :class:`.MatrixTable`; Sparse MatrixTable to split.; filter_changed_loci : :obj:`.bool`; Rather than erroring if any REF/AL",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py:1955,Testability,log,logic,1955,"nt32; GQ: int32; LPL: array<int32>; RGQ: int32; LPGT: call; LA: array<int32>; END: int32; }. All fields except for `LA` are optional, and only handled if they exist. - `LA` is used to find the corresponding local allele index for the desired; global `a_index`, and then dropped from the resulting dataset. If `LA`; does not contain the global `a_index`, calls will be downcoded to hom ref; and `PL` will be set to missing. - `LGT` and `LPGT` are downcoded using the corresponding local `a_index`.; They are renamed to `GT` and `PGT` respectively, as the resulting call is; no longer local. - `LAD` is used to create an `AD` field consisting of the allele depths; corresponding to the reference and global `a_index` alleles. - `DP` is preserved unchanged. - `GQ` is recalculated from the updated `PL`, if it exists, but otherwise; preserved unchanged. - `PL` array elements are calculated from the minimum `LPL` value for all; allele pairs that downcode to the desired one. (This logic is identical to; the `PL` logic in :func:`~.split_multi_hts`.) If a row has an alternate; allele but it is not present in `LA`, the `PL` field is set to missing.; The `PL` for `ref/<NON_REF>` in that case can be drawn from `RGQ`. - `RGQ` (the reference genotype quality) is preserved unchanged. - `END` is untouched. Notes; -----; This version of split-multi doesn't deal with either duplicate loci (in; which case the explode could possibly result in out-of-order rows, although; the actual split_multi function also doesn't handle that case). It also checks that min-repping will not change the locus and will error if; it does. Unlike the normal split_multi function. Sparse split multi will not filter; ``*`` alleles. This is because a row with a bi-allelic spanning deletion; may contain reference blocks that start at this position for other samples. Parameters; ----------; sparse_mt : :class:`.MatrixTable`; Sparse MatrixTable to split.; filter_changed_loci : :obj:`.bool`; Rather than erroring if any REF/AL",MatchSource.CODE_COMMENT,hail/python/hail/experimental/sparse_mt/sparse_split_multi.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/experimental/sparse_mt/sparse_split_multi.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/blockmatrix_type.py:9,Deployability,patch,patch,9,"# monkey-patch pprint",MatchSource.CODE_COMMENT,hail/python/hail/expr/blockmatrix_type.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/blockmatrix_type.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:15,Testability,test,test,15,"""""""Add a value test. If the `base` expression is equal to `value`, then; returns `then`. Warning; -------; Missingness always compares to missing. Both ``NA == NA`` and; ``NA != NA`` return ``NA``. Use :meth:`~SwitchBuilder.when_missing`; to test missingness. Parameters; ----------; value : :class:`.Expression`; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates and returns `self`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:242,Testability,test,test,242,"""""""Add a value test. If the `base` expression is equal to `value`, then; returns `then`. Warning; -------; Missingness always compares to missing. Both ``NA == NA`` and; ``NA != NA`` return ``NA``. Use :meth:`~SwitchBuilder.when_missing`; to test missingness. Parameters; ----------; value : :class:`.Expression`; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates and returns `self`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:9,Testability,test,test,9,"""""""Add a test for missingness. If the `base` expression is missing,; returns `then`. Parameters; ----------; then : :class:`.Expression`. Returns; -------; :class:`.SwitchBuilder`; Mutates and returns `self`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:46,Availability,error,error,46,"""""""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:163,Availability,error,error,163,"""""""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:67,Integrability,message,message,67,"""""""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:204,Integrability,message,message,204,"""""""Finish the switch statement by throwing an error with the given message. Notes; -----; If no value from a :meth:`.SwitchBuilder.when` call is matched, then an; error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:290,Testability,test,test,290,"""""""Add a branch. If `condition` is ``True``, then returns `then`. Warning; -------; Missingness is treated similarly to :func:`.cond`. Missingness is; **not** treated as ``False``. A `condition` that evaluates to missing; will return a missing result, not proceed to the next case. Always; test missingness first in a :class:`.CaseBuilder`. Parameters; ----------; condition: :class:`.BooleanExpression`; then : :class:`.Expression`. Returns; -------; :class:`.CaseBuilder`; Mutates and returns `self`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:44,Availability,error,error,44,"""""""Finish the case statement by throwing an error with the given message. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; an error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:164,Availability,error,error,164,"""""""Finish the case statement by throwing an error with the given message. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; an error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:65,Integrability,message,message,65,"""""""Finish the case statement by throwing an error with the given message. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; an error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py:205,Integrability,message,message,205,"""""""Finish the case statement by throwing an error with the given message. Notes; -----; If no condition from a :meth:`.CaseBuilder.when` call is ``True``, then; an error is thrown. Parameters; ----------; message : :class:`.Expression` of type :obj:`.tstr`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/builders.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/builders.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:13,Availability,error,error,13,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:253,Availability,error,error,253,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:288,Availability,error,error,288,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:374,Availability,error,error,374,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:493,Availability,error,error,493,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :class:`.StructExpression`; Result of :func:`.approx_cdf` aggregator; failure_prob: :class:`.NumericExpression`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :class:`.NumericExpression`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:13,Availability,error,error,13,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:251,Availability,error,error,251,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:286,Availability,error,error,286,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:372,Availability,error,error,372,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:476,Availability,error,error,476,"""""""Estimates error of approx_cdf aggregator, using Hoeffding's inequality. Parameters; ----------; cdf : :obj:`dict`; Result of :func:`.approx_cdf` aggregator, evaluated to a python dict; failure_prob: :obj:`float`; Upper bound on probability of true error being greater than estimated error.; all_quantiles: :obj:`bool`; If ``True``, with probability 1 - `failure_prob`, error estimate applies; to all quantiles simultaneously. Returns; -------; :obj:`float`; Upper bound on error of quantile estimates.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:25,Availability,error,error,25,"# no compactions ergo no error",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:36,Modifiability,variab,variable,36,"""""""Captures and broadcasts a Python variable or object as an expression. Examples; --------. >>> table = hl.utils.range_table(8); >>> greetings = hl.literal({1: 'Good morning', 4: 'Good afternoon', 6 : 'Good evening'}); >>> table.annotate(greeting = greetings.get(table.idx)).show(); +-------+------------------+; | idx | greeting |; +-------+------------------+; | int32 | str |; +-------+------------------+; | 0 | NA |; | 1 | ""Good morning"" |; | 2 | NA |; | 3 | NA |; | 4 | ""Good afternoon"" |; | 5 | NA |; | 6 | ""Good evening"" |; | 7 | NA |; +-------+------------------+. Notes; -----; Use this function to capture large Python objects for use in expressions. This; function provides an alternative to adding an object as a global annotation on a; :class:`.Table` or :class:`.MatrixTable`. Parameters; ----------; x; Object to capture and broadcast as an expression. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:81,Testability,test,tests,81,"""""""Deprecated in favor of :func:`.if_else`. Expression for an if/else statement; tests a condition and returns one of two options based on the result. Examples; --------. >>> x = 5; >>> hl.eval(hl.cond(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.cond(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; -----. If `condition` evaluates to ``True``, returns `consequent`. If `condition`; evaluates to ``False``, returns `alternate`. If `predicate` is missing, returns; missing. Note; ----; The type of `consequent` and `alternate` must be the same. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test.; consequent : :class:`.Expression`; Branch to return if the condition is ``True``.; alternate : :class:`.Expression`; Branch to return if the condition is ``False``.; missing_false : :obj:`.bool`; If ``True``, treat missing `condition` as ``False``. See Also; --------; :func:`.case`, :func:`.switch`, :func:`.if_else`. Returns; -------; :class:`.Expression`; One of `consequent`, `alternate`, or missing, based on `condition`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:671,Testability,test,test,671,"""""""Deprecated in favor of :func:`.if_else`. Expression for an if/else statement; tests a condition and returns one of two options based on the result. Examples; --------. >>> x = 5; >>> hl.eval(hl.cond(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.cond(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; -----. If `condition` evaluates to ``True``, returns `consequent`. If `condition`; evaluates to ``False``, returns `alternate`. If `predicate` is missing, returns; missing. Note; ----; The type of `consequent` and `alternate` must be the same. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test.; consequent : :class:`.Expression`; Branch to return if the condition is ``True``.; alternate : :class:`.Expression`; Branch to return if the condition is ``False``.; missing_false : :obj:`.bool`; If ``True``, treat missing `condition` as ``False``. See Also; --------; :func:`.case`, :func:`.switch`, :func:`.if_else`. Returns; -------; :class:`.Expression`; One of `consequent`, `alternate`, or missing, based on `condition`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:40,Testability,test,tests,40,"""""""Expression for an if/else statement; tests a condition and returns one of two options based on the result. Examples; --------. >>> x = 5; >>> hl.eval(hl.if_else(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.if_else(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; -----. If `condition` evaluates to ``True``, returns `consequent`. If `condition`; evaluates to ``False``, returns `alternate`. If `predicate` is missing, returns; missing. Note; ----; The type of `consequent` and `alternate` must be the same. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test.; consequent : :class:`.Expression`; Branch to return if the condition is ``True``.; alternate : :class:`.Expression`; Branch to return if the condition is ``False``.; missing_false : :obj:`.bool`; If ``True``, treat missing `condition` as ``False``. See Also; --------; :func:`.case`, :func:`.switch`. Returns; -------; :class:`.Expression`; One of `consequent`, `alternate`, or missing, based on `condition`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:636,Testability,test,test,636,"""""""Expression for an if/else statement; tests a condition and returns one of two options based on the result. Examples; --------. >>> x = 5; >>> hl.eval(hl.if_else(x < 2, 'Hi', 'Bye')); 'Bye'. >>> a = hl.literal([1, 2, 3, 4]); >>> hl.eval(hl.if_else(hl.len(a) > 0, 2.0 * a, a / 2.0)); [2.0, 4.0, 6.0, 8.0]. Notes; -----. If `condition` evaluates to ``True``, returns `consequent`. If `condition`; evaluates to ``False``, returns `alternate`. If `predicate` is missing, returns; missing. Note; ----; The type of `consequent` and `alternate` must be the same. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test.; consequent : :class:`.Expression`; Branch to return if the condition is ``True``.; alternate : :class:`.Expression`; Branch to return if the condition is ``False``.; missing_false : :obj:`.bool`; If ``True``, treat missing `condition` as ``False``. See Also; --------; :func:`.case`, :func:`.switch`. Returns; -------; :class:`.Expression`; One of `consequent`, `alternate`, or missing, based on `condition`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:20,Modifiability,variab,variable,20,"""""""Bind a temporary variable and use it in a function. Examples; --------. >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. :func:`.bind` also can take multiple arguments:. >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters; ----------; f : function ( (args) -> :class:`.Expression`); Function of `exprs`.; exprs : variable-length args of :class:`.Expression`; Expressions to bind. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:324,Modifiability,variab,variable-length,324,"""""""Bind a temporary variable and use it in a function. Examples; --------. >>> hl.eval(hl.bind(lambda x: x + 1, 1)); 2. :func:`.bind` also can take multiple arguments:. >>> hl.eval(hl.bind(lambda x, y: x / y, x, x)); 1.0. Parameters; ----------; f : function ( (args) -> :class:`.Expression`); Function of `exprs`.; exprs : variable-length args of :class:`.Expression`; Expressions to bind. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:41,Availability,error,errors,41,"# FIXME: hacky. May drop field refs from errors?",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:20,Modifiability,variab,variable,20,"""""""Bind a temporary variable and use it in a function. This is :func:`.bind` with flipped argument order. Examples; --------. >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. :func:`.rbind` also can take multiple arguments:. >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Expressions to bind.; f : function ( (args) -> :class:`.Expression`); Function of `exprs`. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:312,Modifiability,variab,variable-length,312,"""""""Bind a temporary variable and use it in a function. This is :func:`.bind` with flipped argument order. Examples; --------. >>> hl.eval(hl.rbind(1, lambda x: x + 1)); 2. :func:`.rbind` also can take multiple arguments:. >>> hl.eval(hl.rbind(4.0, 2.0, lambda x, y: x / y)); 2.0. Parameters; ----------; exprs : variable-length args of :class:`.Expression`; Expressions to bind.; f : function ( (args) -> :class:`.Expression`); Function of `exprs`. Returns; -------; :class:`.Expression`; Result of evaluating `f` with `exprs` as arguments.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:24,Testability,test,test,24,"""""""Performs chi-squared test of independence on a 2x2 contingency table. Examples; --------. >>> hl.eval(hl.chi_squared_test(10, 10, 10, 10)); Struct(p_value=1.0, odds_ratio=1.0). >>> hl.eval(hl.chi_squared_test(51, 43, 22, 92)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). Notes; -----; The odds ratio is given by ``(c1 / c2) / (c3 / c4)``. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:42,Testability,test,test,42,"""""""Performs chi-squared or Fisher's exact test of independence on a 2x2; contingency table. Examples; --------. >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; -----; If all cell counts are at least `min_cell_count`, the chi-squared test is; used. Otherwise, Fisher's exact test is used. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4.; min_cell_count : int or :class:`.Expression` of type :py:data:`.tint32`; Minimum count in every cell to use the chi-squared test. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:482,Testability,test,test,482,"""""""Performs chi-squared or Fisher's exact test of independence on a 2x2; contingency table. Examples; --------. >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; -----; If all cell counts are at least `min_cell_count`, the chi-squared test is; used. Otherwise, Fisher's exact test is used. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4.; min_cell_count : int or :class:`.Expression` of type :py:data:`.tint32`; Minimum count in every cell to use the chi-squared test. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:523,Testability,test,test,523,"""""""Performs chi-squared or Fisher's exact test of independence on a 2x2; contingency table. Examples; --------. >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; -----; If all cell counts are at least `min_cell_count`, the chi-squared test is; used. Otherwise, Fisher's exact test is used. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4.; min_cell_count : int or :class:`.Expression` of type :py:data:`.tint32`; Minimum count in every cell to use the chi-squared test. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:1048,Testability,test,test,1048,"""""""Performs chi-squared or Fisher's exact test of independence on a 2x2; contingency table. Examples; --------. >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=22)); Struct(p_value=1.4626257805267089e-07, odds_ratio=4.959830866807611). >>> hl.eval(hl.contingency_table_test(51, 43, 22, 92, min_cell_count=23)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967). Notes; -----; If all cell counts are at least `min_cell_count`, the chi-squared test is; used. Otherwise, Fisher's exact test is used. Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4.; min_cell_count : int or :class:`.Expression` of type :py:data:`.tint32`; Minimum count in every cell to use the chi-squared test. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `p_value`; (:py:data:`.tfloat64`) and `odds_ratio` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:39,Testability,test,test,39,"""""""Perform the Cochran-Mantel-Haenszel test for association. Examples; --------; >>> a = [56, 61, 73, 71]; >>> b = [69, 257, 65, 48]; >>> c = [40, 57, 71, 55]; >>> d = [77, 301, 79, 48]; >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); Struct(test_statistic=5.0496881823306765, p_value=0.024630370456863417). >>> mt = ds.filter_rows(mt.locus == hl.Locus(20, 10633237)); >>> mt.count_rows(); 1; >>> a, b, c, d = mt.aggregate_entries(; ... hl.tuple([; ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(mt.GT.is_non_ref() & mt.pheno.is_case & ~mt.pheno.is_female)]),; ... hl.array([hl.agg.count_where(mt.GT.is_non_ref() & ~mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(mt.GT.is_non_ref() & ~mt.pheno.is_case & ~mt.pheno.is_female)]),; ... hl.array([hl.agg.count_where(~mt.GT.is_non_ref() & mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(~mt.GT.is_non_ref() & mt.pheno.is_case & ~mt.pheno.is_female)]),; ... hl.array([hl.agg.count_where(~mt.GT.is_non_ref() & ~mt.pheno.is_case & mt.pheno.is_female), hl.agg.count_where(~mt.GT.is_non_ref() & ~mt.pheno.is_case & ~mt.pheno.is_female)]); ... ]); ... ); >>> hl.eval(hl.cochran_mantel_haenszel_test(a, b, c, d)); Struct(test_statistic=0.2188830334629822, p_value=0.6398923118508772). Notes; -----; See the `Wikipedia article <https://en.m.wikipedia.org/wiki/Cochran%E2%80%93Mantel%E2%80%93Haenszel_statistics>`_; for more details. Parameters; ----------; a : :class:`.ArrayExpression` of type :py:data:`.tint64`; Values for the upper-left cell in the contingency tables.; b : :class:`.ArrayExpression` of type :py:data:`.tint64`; Values for the upper-right cell in the contingency tables.; c : :class:`.ArrayExpression` of type :py:data:`.tint64`; Values for the lower-left cell in the contingency tables.; d : :class:`.ArrayExpression` of type :py:data:`.tint64`; Values for the lower-right cell in the contingency tables. Returns; -------; :class:`.StructExpres",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:6,Modifiability,variab,variable,6,"# The variable names below correspond to the notation used in the Wikipedia article.; # https://en.m.wikipedia.org/wiki/Cochran%E2%80%93Mantel%E2%80%93Haenszel_statistics",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:725,Testability,log,logarithm,725,"""""""Compute the probability density at `x` of a chi-squared distribution with `df`; degrees of freedom. Examples; --------. >>> hl.eval(hl.dchisq(1, 2)); 0.3032653298563167. >>> hl.eval(hl.dchisq(1, 2, ncp=2)); 0.17472016746112667. >>> hl.eval(hl.dchisq(1, 2, log_p=True)); -1.1931471805599454. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; df : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom.; ncp: float or :class:`.Expression` of type :py:data:`.tfloat64`; Noncentrality parameter, defaults to 0 if unspecified.; log_p : bool or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The probability density.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:782,Testability,log,logarithm,782,"""""""Compute the probability density at `x` of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns density of standard normal; distribution by default. Examples; --------. >>> hl.eval(hl.dnorm(1)); 0.24197072451914337. >>> hl.eval(hl.dnorm(1, mu=1, sigma=2)); 0.19947114020071635. >>> hl.eval(hl.dnorm(1, log_p=True)); -1.4189385332046727. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Real number at which to compute the probability density.; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The probability density.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:16,Testability,log,log,16,"""""""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability density.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:529,Testability,log,logarithm,529,"""""""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability density.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:653,Testability,log,log,653,"""""""Compute the (log) probability density at x of a Poisson distribution with rate parameter `lamb`. Examples; --------. >>> hl.eval(hl.dpois(5, 3)); 0.10081881344492458. Parameters; ----------; x : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Non-negative number at which to compute the probability density.; lamb : :obj:`float` or :class:`.Expression` of type :py:data:`.tfloat64`; Poisson rate parameter. Must be non-negative.; log_p : :obj:`bool` or :class:`.BooleanExpression`; If ``True``, the natural logarithm of the probability density is returned. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; The (log) probability density.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:30,Energy Efficiency,power,power,30,"""""""Computes `e` raised to the power `x`. Examples; --------. >>> hl.eval(hl.exp(2)); 7.38905609893065. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:89,Testability,test,test,89,"""""""Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher's exact test for a 2x2 table. Examples; --------. >>> hl.eval(hl.fisher_exact_test(10, 10, 10, 10)); Struct(p_value=1.0000000000000002, odds_ratio=1.0,; ci_95_lower=0.24385796914260355, ci_95_upper=4.100747675033819). >>> hl.eval(hl.fisher_exact_test(51, 43, 22, 92)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967,; ci_95_lower=2.5659373368248444, ci_95_upper=9.677929632035475). Notes; -----; This method is identical to the version implemented in; `R <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/fisher.test.html>`_ with default; parameters (two-sided, alpha = 0.05, null hypothesis that the odds ratio equals 1). Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with four fields, `p_value`; (:py:data:`.tfloat64`), `odds_ratio` (:py:data:`.tfloat64`),; `ci_95_lower (:py:data:`.tfloat64`), and `ci_95_upper`; (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:622,Testability,test,test,622,"""""""Calculates the p-value, odds ratio, and 95% confidence interval using; Fisher's exact test for a 2x2 table. Examples; --------. >>> hl.eval(hl.fisher_exact_test(10, 10, 10, 10)); Struct(p_value=1.0000000000000002, odds_ratio=1.0,; ci_95_lower=0.24385796914260355, ci_95_upper=4.100747675033819). >>> hl.eval(hl.fisher_exact_test(51, 43, 22, 92)); Struct(p_value=2.1564999740157304e-07, odds_ratio=4.918058171469967,; ci_95_lower=2.5659373368248444, ci_95_upper=9.677929632035475). Notes; -----; This method is identical to the version implemented in; `R <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/fisher.test.html>`_ with default; parameters (two-sided, alpha = 0.05, null hypothesis that the odds ratio equals 1). Returned fields may be ``nan`` or ``inf``. Parameters; ----------; c1 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 1.; c2 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 2.; c3 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 3.; c4 : int or :class:`.Expression` of type :py:data:`.tint32`; Value for cell 4. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with four fields, `p_value`; (:py:data:`.tfloat64`), `odds_ratio` (:py:data:`.tfloat64`),; `ci_95_lower (:py:data:`.tfloat64`), and `ci_95_upper`; (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:519,Energy Efficiency,efficient,efficient,519,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:351,Performance,perform,performs,351,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:1082,Performance,perform,perform,1082,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:1665,Performance,perform,perform,1665,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:12,Testability,test,test,12,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:378,Testability,test,test,378,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:1106,Testability,test,test,1106,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:1254,Testability,test,test,1254,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:1683,Testability,test,test,1683,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------. >>> hl.eval(hl.hardy_weinberg_test(250, 500, 250)); Struct(het_freq_hwe=0.5002501250625313, p_value=0.9747844394217698). >>> hl.eval(hl.hardy_weinberg_test(37, 200, 85)); Struct(het_freq_hwe=0.48964964307448583, p_value=1.1337210383168987e-06). Notes; -----; By default, this method performs a two-sided exact test with mid-p-value correction of; `Hardy-Weinberg equilibrium <https://en.wikipedia.org/wiki/Hardy%E2%80%93Weinberg_principle>`__; via an efficient implementation of the; `Levene-Haldane distribution <../_static/LeveneHaldane.pdf>`__,; which models the number of heterozygous individuals under equilibrium. The mean of this distribution is ``(n_ref * n_var) / (2n - 1)``, where; ``n_ref = 2*n_hom_ref + n_het`` is the number of reference alleles,; ``n_var = 2*n_hom_var + n_het`` is the number of variant alleles,; and ``n = n_hom_ref + n_het + n_hom_var`` is the number of individuals.; So the expected frequency of heterozygotes under equilibrium,; `het_freq_hwe`, is this mean divided by ``n``. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Parameters; ----------; n_hom_ref : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous reference genotypes.; n_het : int or :class:`.Expression` of type :py:data:`.tint32`; Number of heterozygous genotypes.; n_hom_var : int or :class:`.Expression` of type :py:data:`.tint32`; Number of homozygous variant genotypes.; one_sided : :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; A struct expression with two fields, `het_freq_hwe`; (:py:data:`.tfloat64`) and `p_value` (:py:data:`.tfloat64`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:150,Modifiability,variab,variable-length,150,"""""""Construct a call expression. Examples; --------. >>> hl.eval(hl.call(1, 0)); Call(alleles=[0, 1], phased=False). Parameters; ----------; alleles : variable-length args of :obj:`int` or :class:`.Expression` of type :py:data:`.tint32`; List of allele indices.; phased : :obj:`bool`; If ``True``, preserve the order of `alleles`. Returns; -------; :class:`.CallExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:279,Testability,test,test,279,"""""""Returns ``True`` if the argument is not missing. Examples; --------. >>> hl.eval(hl.is_defined(5)); True. >>> hl.eval(hl.is_defined(hl.missing(hl.tstr))); False. >>> hl.eval(hl.is_defined(hl.missing(hl.tbool) & True)); False. Parameters; ----------; expression; Expression to test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `expression` is not missing, ``False`` otherwise.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:274,Testability,test,test,274,"""""""Returns ``True`` if the argument is missing. Examples; --------. >>> hl.eval(hl.is_missing(5)); False. >>> hl.eval(hl.is_missing(hl.missing(hl.tstr))); True. >>> hl.eval(hl.is_missing(hl.missing(hl.tbool) & True)); True. Parameters; ----------; expression; Expression to test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `expression` is missing, ``False`` otherwise.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:518,Testability,test,test,518,"""""""Returns ``True`` if the argument is ``nan`` (not a number). Examples; --------. >>> hl.eval(hl.is_nan(0)); False. >>> hl.eval(hl.is_nan(hl.literal(0) / 0)); True. >>> hl.eval(hl.is_nan(hl.literal(0) / hl.missing(hl.tfloat64))); None. Notes; -----; Note that :func:`~.is_missing` will return ``False`` on ``nan`` since ``nan``; is a defined value. Additionally, this method will return missing if `x` is; missing. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; Expression to test or or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.BooleanExpression`; ``True`` if `x` is ``nan``, ``False`` otherwise or; :class:`.NDArrayNumericExpression` filled with such values; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:12,Testability,log,logarithm,12,"""""""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:86,Testability,log,log,86,"""""""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:130,Testability,log,log,130,"""""""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:164,Testability,log,log,164,"""""""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:256,Testability,log,logarithm,256,"""""""Take the logarithm of the `x` with base `base`. Examples; --------. >>> hl.eval(hl.log(10)); 2.302585092994046. >>> hl.eval(hl.log(10, 10)); 1.0. >>> hl.eval(hl.log(1024, 2)); 10.0. Notes; -----; If the `base` argument is not supplied, then the natural logarithm is used. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; base : float or :class:`.Expression` of type :py:data:`.tfloat64`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:12,Testability,log,logarithm,12,"""""""Take the logarithm of the `x` with base 10. Examples; --------. >>> hl.eval(hl.log10(1000)); 3.0. >>> hl.eval(hl.log10(0.0001123)); -3.949620243738542. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:7,Testability,log,logistic,7,"""""""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:61,Testability,log,logit,61,"""""""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:108,Testability,log,logit,108,"""""""The logistic function. Examples; --------; >>> hl.eval(hl.logit(.01)); -4.59511985013459; >>> hl.eval(hl.logit(.5)); 0.0. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:7,Testability,log,logistic,7,"""""""The logistic sigmoid function. .. math::. \textrm{expit}(x) = \frac{1}{1 + e^{-x}}. Examples; --------; >>> hl.eval(hl.expit(.01)); 0.5024999791668749; >>> hl.eval(hl.expit(0.0)); 0.5. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64` or :class:`.NDArrayNumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:339,Modifiability,variab,variable-length,339,"""""""Returns the first non-missing value of `args`. Examples; --------. >>> x1 = hl.missing('int'); >>> x2 = 2; >>> hl.eval(hl.coalesce(x1, x2)); 2. Notes; -----; All arguments must have the same type, or must be convertible to a common; type (all numeric, for instance). See Also; --------; :func:`.or_else`. Parameters; ----------; args : variable-length args of :class:`.Expression`. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:23,Testability,test,test,23,"""""""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:115,Testability,test,test,115,"""""""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:336,Testability,test,test,336,"""""""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:419,Testability,test,test,419,"""""""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:507,Testability,test,test,507,"""""""Performs a binomial test on `p` given `x` successes in `n` trials. Returns the p-value from the `exact binomial test; <https://en.wikipedia.org/wiki/Binomial_test>`__ of the null hypothesis that; success has probability `p`, given `x` successes in `n` trials. The alternatives are interpreted as follows:; - ``'less'``: a one-tailed test of the significance of `x` or fewer successes,; - ``'greater'``: a one-tailed test of the significance of `x` or more successes, and; - ``'two-sided'``: a two-tailed test of the significance of `x` or any equivalent or more unlikely outcome. Examples; --------. All the examples below use a fair coin as the null hypothesis. Zero is; interpreted as tail and one as heads. Test if a coin is biased towards heads or tails after observing two heads; out of ten flips:. >>> hl.eval(hl.binom_test(2, 10, 0.5, 'two-sided')); 0.10937499999999994. Test if a coin is biased towards tails after observing four heads out of ten; flips:. >>> hl.eval(hl.binom_test(4, 10, 0.5, 'less')); 0.3769531250000001. Test if a coin is biased towards heads after observing thirty-two heads out; of fifty flips:. >>> hl.eval(hl.binom_test(32, 50, 0.5, 'greater')); 0.03245432353613613. Parameters; ----------; x : int or :class:`.Expression` of type :py:data:`.tint32`; Number of successes.; n : int or :class:`.Expression` of type :py:data:`.tint32`; Number of trials.; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability of success, between 0 and 1.; alternative; : One of, ""two-sided"", ""greater"", ""less"", (deprecated: ""two.sided""). Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; p-value.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:946,Testability,log,logarithm,946,"""""""Returns the probability under the right-tail starting at x for a chi-squared; distribution with df degrees of freedom. Examples; --------. >>> hl.eval(hl.pchisqtail(5, 1)); 0.025347318677468304. >>> hl.eval(hl.pchisqtail(5, 1, ncp=2)); 0.20571085634347097. >>> hl.eval(hl.pchisqtail(5, 1, lower_tail=True)); 0.9746526813225317. >>> hl.eval(hl.pchisqtail(5, 1, log_p=True)); -3.6750823266311876. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; The value at which to evaluate the CDF.; df : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom.; ncp: float or :class:`.Expression` of type :py:data:`.tfloat64`; Noncentrality parameter, defaults to 0 if unspecified.; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:534,Modifiability,variab,variable,534,"""""""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; -----; Returns the left-tail probability `p` = Prob(:math:`Z < x`) with :math:`Z`; a normal random variable. Defaults to a standard normal random variable. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:581,Modifiability,variab,variable,581,"""""""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; -----; Returns the left-tail probability `p` = Prob(:math:`Z < x`) with :math:`Z`; a normal random variable. Defaults to a standard normal random variable. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:1079,Testability,log,logarithm,1079,"""""""The cumulative probability function of a normal distribution with mean; `mu` and standard deviation `sigma`. Returns cumulative probability of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.pnorm(0)); 0.5. >>> hl.eval(hl.pnorm(1, mu=2, sigma=2)); 0.30853753872598694. >>> hl.eval(hl.pnorm(2, lower_tail=False)); 0.022750131948179212. >>> hl.eval(hl.pnorm(2, log_p=True)); -0.023012909328963493. Notes; -----; Returns the left-tail probability `p` = Prob(:math:`Z < x`) with :math:`Z`; a normal random variable. Defaults to a standard normal random variable. Parameters; ----------; x : float or :class:`.Expression` of type :py:data:`.tfloat64`; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; If ``True``, compute the probability of an outcome at or below `x`,; otherwise greater than `x`.; log_p : bool or :class:`.BooleanExpression`; Return the natural logarithm of the probability. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:525,Modifiability,variab,variable,525,"""""""The quantile function of a chi-squared distribution with `df` degrees of; freedom, inverts :func:`~.pchisqtail`. Examples; --------. >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; -----; Returns right-quantile `x` for which `p` = Prob(:math:`Z^2` > x) with; :math:`Z^2` a chi-squared random variable with degrees of freedom specified; by `df`. The probability `p` must satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; df : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom.; ncp: float or :class:`.Expression` of type :py:data:`.tfloat64`; Corresponds to `ncp` parameter in :func:`.pchisqtail`.; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pchisqtail`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pchisqtail`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:360,Testability,log,log,360,"""""""The quantile function of a chi-squared distribution with `df` degrees of; freedom, inverts :func:`~.pchisqtail`. Examples; --------. >>> hl.eval(hl.qchisqtail(0.05, 2)); 5.991464547107979. >>> hl.eval(hl.qchisqtail(0.05, 2, ncp=2)); 10.838131614372958. >>> hl.eval(hl.qchisqtail(0.05, 2, lower_tail=True)); 0.10258658877510107. >>> hl.eval(hl.qchisqtail(hl.log(0.05), 2, log_p=True)); 5.991464547107979. Notes; -----; Returns right-quantile `x` for which `p` = Prob(:math:`Z^2` > x) with; :math:`Z^2` a chi-squared random variable with degrees of freedom specified; by `df`. The probability `p` must satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; df : float or :class:`.Expression` of type :py:data:`.tfloat64`; Degrees of freedom.; ncp: float or :class:`.Expression` of type :py:data:`.tfloat64`; Corresponds to `ncp` parameter in :func:`.pchisqtail`.; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pchisqtail`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pchisqtail`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:561,Modifiability,variab,variable,561,"""""""The quantile function of a normal distribution with mean `mu` and; standard deviation `sigma`, inverts :func:`~.pnorm`. Returns quantile of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; -----; Returns left-quantile `x` for which p = Prob(:math:`Z` < x) with :math:`Z`; a normal random variable with mean `mu` and standard deviation `sigma`.; Defaults to a standard normal random variable, and the probability `p` must; satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pnorm`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pnorm`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:655,Modifiability,variab,variable,655,"""""""The quantile function of a normal distribution with mean `mu` and; standard deviation `sigma`, inverts :func:`~.pnorm`. Returns quantile of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; -----; Returns left-quantile `x` for which p = Prob(:math:`Z` < x) with :math:`Z`; a normal random variable with mean `mu` and standard deviation `sigma`.; Defaults to a standard normal random variable, and the probability `p` must; satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pnorm`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pnorm`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:410,Testability,log,log,410,"""""""The quantile function of a normal distribution with mean `mu` and; standard deviation `sigma`, inverts :func:`~.pnorm`. Returns quantile of; standard normal distribution by default. Examples; --------. >>> hl.eval(hl.qnorm(0.90)); 1.2815515655446008. >>> hl.eval(hl.qnorm(0.90, mu=1, sigma=2)); 3.5631031310892016. >>> hl.eval(hl.qnorm(0.90, lower_tail=False)); -1.2815515655446008. >>> hl.eval(hl.qnorm(hl.log(0.90), log_p=True)); 1.2815515655446008. Notes; -----; Returns left-quantile `x` for which p = Prob(:math:`Z` < x) with :math:`Z`; a normal random variable with mean `mu` and standard deviation `sigma`.; Defaults to a standard normal random variable, and the probability `p` must; satisfy 0 < `p` < 1. Parameters; ----------; p : float or :class:`.Expression` of type :py:data:`.tfloat64`; Probability.; mu : float or :class:`.Expression` of type :py:data:`.tfloat64`; Mean (default = 0).; sigma: float or :class:`.Expression` of type :py:data:`.tfloat64`; Standard deviation (default = 1).; lower_tail : bool or :class:`.BooleanExpression`; Corresponds to `lower_tail` parameter in :func:`.pnorm`.; log_p : bool or :class:`.BooleanExpression`; Exponentiate `p`, corresponds to `log_p` parameter in :func:`.pnorm`. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:27,Modifiability,polymorphi,polymorphism,27,"""""""Returns the type of the polymorphism as an integer. The value returned; is the integer value of :class:`.AlleleType` representing that kind of; polymorphism. Examples; --------. >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; -----; The values of :class:`.AlleleType` are not stable and thus should not be; relied upon across hail versions.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:147,Modifiability,polymorphi,polymorphism,147,"""""""Returns the type of the polymorphism as an integer. The value returned; is the integer value of :class:`.AlleleType` representing that kind of; polymorphism. Examples; --------. >>> hl.eval(hl.numeric_allele_type('A', 'T')) == AlleleType.SNP; True. Notes; -----; The values of :class:`.AlleleType` are not stable and thus should not be; relied upon across hail versions.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:66,Modifiability,polymorphi,polymorphism,66,"""""""Returns ``True`` if the alleles constitute a single nucleotide polymorphism. Examples; --------. >>> hl.eval(hl.is_snp('A', 'T')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:68,Modifiability,polymorphi,polymorphism,68,"""""""Returns ``True`` if the alleles constitute a multiple nucleotide polymorphism. Examples; --------. >>> hl.eval(hl.is_mnp('AA', 'GT')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:56,Modifiability,polymorphi,polymorphism,56,"""""""Returns ``True`` if the alleles constitute a complex polymorphism. Examples; --------. >>> hl.eval(hl.is_complex('ATT', 'GCAC')); True. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.BooleanExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:27,Modifiability,polymorphi,polymorphism,27,"""""""Returns the type of the polymorphism as a string. Examples; --------. >>> hl.eval(hl.allele_type('A', 'T')); 'SNP'. >>> hl.eval(hl.allele_type('ATT', 'A')); 'Deletion'. Notes; -----; The possible return values are:; - ``""SNP""``; - ``""MNP""``; - ``""Insertion""``; - ``""Deletion""``; - ``""Complex""``; - ``""Star""``; - ``""Symbolic""``; - ``""Unknown""``. Parameters; ----------; ref : :class:`.StringExpression`; Reference allele.; alt : :class:`.StringExpression`; Alternate allele. Returns; -------; :class:`.StringExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:115,Availability,down,downcode,115,"""""""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:171,Availability,down,downcode,171,"""""""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:251,Availability,down,downcode,251,"""""""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:331,Availability,down,downcode,331,"""""""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:606,Availability,down,downcoded,606,"""""""Create a new call by setting all alleles other than i to ref. Examples; --------; Preserve the third allele and downcode all other alleles to reference. >>> hl.eval(hl.downcode(hl.call(1, 2), 2)); Call(alleles=[0, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(2, 2), 2)); Call(alleles=[1, 1], phased=False). >>> hl.eval(hl.downcode(hl.call(0, 1), 2)); Call(alleles=[0, 0], phased=False). Parameters; ----------; c : :class:`.CallExpression`; A call.; i : :class:`.Expression` of type :py:data:`.tint32`; The index of the allele that will be sent to the alternate allele. All; other alleles will be downcoded to reference. Returns; -------; :class:`.CallExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:560,Modifiability,variab,variable-length,560,"""""""Zip together arrays into a single array. Examples; --------. >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the `fill_missing` parameter. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; -----; The element type of the resulting array is a :class:`.ttuple` with a field; for each array. Parameters; ----------; arrays: : variable-length args of :class:`.ArrayExpression`; Array expressions.; fill_missing : :obj:`bool`; If ``False``, return an array with length equal to the shortest length; of the `arrays`. If ``True``, return an array equal to the longest; length of the `arrays`, by extending the shorter arrays with missing; values. Returns; -------; :class:`.ArrayExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:826,Modifiability,extend,extending,826,"""""""Zip together arrays into a single array. Examples; --------. >>> hl.eval(hl.zip([1, 2, 3], [4, 5, 6])); [(1, 4), (2, 5), (3, 6)]. If the arrays are different lengths, the behavior is decided by the `fill_missing` parameter. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300])); [(1, 10, 100)]. >>> hl.eval(hl.zip([1], [10, 20], [100, 200, 300], fill_missing=True)); [(1, 10, 100), (None, 20, 200), (None, None, 300)]. Notes; -----; The element type of the resulting array is a :class:`.ttuple` with a field; for each array. Parameters; ----------; arrays: : variable-length args of :class:`.ArrayExpression`; Array expressions.; fill_missing : :obj:`bool`; If ``False``, return an array with length equal to the shortest length; of the `arrays`. If ``True``, return an array equal to the longest; length of the `arrays`, by extending the shorter arrays with missing; values. Returns; -------; :class:`.ArrayExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:452,Modifiability,variab,variable-length,452,"""""""Returns the maximum value of a collection or of given arguments, excluding NaN. Examples; --------. Compute the maximum value of an array:. >>> hl.eval(hl.nanmax([1.1, 50.1, float('nan')])); 50.1. Take the maximum value of arguments:. >>> hl.eval(hl.nanmax(1.1, 50.1, float('nan'))); 50.1. Notes; -----; Like the Python builtin ``max`` function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; ----; If `filter_missing` is ``True``, then the result is the maximum of; non-missing arguments or elements. If `filter_missing` is ``False``, then; any missing argument or element causes the result to be missing. NaN arguments / array elements are ignored; the maximum value of `NaN` and; any non-`NaN` value `x` is `x`. See Also; --------; :func:`max`, :func:`min`, :func:`nanmin`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing maximum. Returns; -------; :class:`.NumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:410,Modifiability,variab,variable-length,410,"""""""Returns the maximum element of a collection or of given numeric expressions. Examples; --------. Take the maximum value of an array:. >>> hl.eval(hl.max([1, 3, 5, 6, 7, 9])); 9. Take the maximum value of values:. >>> hl.eval(hl.max(1, 50, 2)); 50. Notes; -----; Like the Python builtin ``max`` function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; ----; If `filter_missing` is ``True``, then the result is the maximum of; non-missing arguments or elements. If `filter_missing` is ``False``, then; any missing argument or element causes the result to be missing. If any element or argument is `NaN`, then the result is `NaN`. See Also; --------; :func:`nanmax`, :func:`min`, :func:`nanmin`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing maximum. Returns; -------; :class:`.NumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:450,Modifiability,variab,variable-length,450,"""""""Returns the minimum value of a collection or of given arguments, excluding NaN. Examples; --------. Compute the minimum value of an array:. >>> hl.eval(hl.nanmin([1.1, 50.1, float('nan')])); 1.1. Take the minimum value of arguments:. >>> hl.eval(hl.nanmin(1.1, 50.1, float('nan'))); 1.1. Notes; -----; Like the Python builtin ``min`` function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; ----; If `filter_missing` is ``True``, then the result is the minimum of; non-missing arguments or elements. If `filter_missing` is ``False``, then; any missing argument or element causes the result to be missing. NaN arguments / array elements are ignored; the minimum value of `NaN` and; any non-`NaN` value `x` is `x`. See Also; --------; :func:`min`, :func:`max`, :func:`nanmax`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing minimum. Returns; -------; :class:`.NumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:412,Modifiability,variab,variable-length,412,"""""""Returns the minimum element of a collection or of given numeric expressions. Examples; --------. Take the minimum value of an array:. >>> hl.eval(hl.min([1, 3, 5, 6, 7, 9])); 1. Take the minimum value of arguments:. >>> hl.eval(hl.min(1, 50, 2)); 1. Notes; -----; Like the Python builtin ``min`` function, this function can either take a; single iterable expression (an array or set of numeric elements), or; variable-length arguments of numeric expressions. Note; ----; If `filter_missing` is ``True``, then the result is the minimum of; non-missing arguments or elements. If `filter_missing` is ``False``, then; any missing argument or element causes the result to be missing. If any element or argument is `NaN`, then the result is `NaN`. See Also; --------; :func:`nanmin`, :func:`max`, :func:`nanmax`. Parameters; ----------; exprs : :class:`.ArrayExpression` or :class:`.SetExpression` or varargs of :class:`.NumericExpression`; Single numeric array or set, or multiple numeric values.; filter_missing : :obj:`bool`; Remove missing arguments/elements before computing minimum. Returns; -------; :class:`.NumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:153,Availability,down,downstream,153,"""""""Compute the intersection of sorted arrays on a given key. Requires sorted arrays with distinct keys. Warning; -------; Experimental. Does not support downstream randomness. Parameters; ----------; arrays; key. Returns; -------; :class:`.ArrayExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:155,Availability,down,downstream,155,"""""""Compute the distinct union of sorted arrays on a given key. Requires sorted arrays with distinct keys. Warning; -------; Experimental. Does not support downstream randomness. Parameters; ----------; exprs; key. Returns; -------; :class:`.ArrayExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:1131,Availability,avail,available,1131,"""""""Return the reference sequence at a given locus. Examples; --------. Return the reference allele for ``'GRCh37'`` at the locus ``'1:45323'``:. >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) # doctest: +SKIP; ""T"". Notes; -----; This function requires `reference genome` has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Returns ``None`` if `contig` and `position` are not valid coordinates in; `reference_genome`. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; Locus contig.; position : :class:`.Expression` of type :py:data:`.tint32`; Locus position.; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus of interest. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Must have a reference sequence available. Returns; -------; :class:`.StringExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:375,Performance,load,load,375,"""""""Return the reference sequence at a given locus. Examples; --------. Return the reference allele for ``'GRCh37'`` at the locus ``'1:45323'``:. >>> hl.eval(hl.get_sequence('1', 45323, reference_genome='GRCh37')) # doctest: +SKIP; ""T"". Notes; -----; This function requires `reference genome` has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Returns ``None`` if `contig` and `position` are not valid coordinates in; `reference_genome`. Parameters; ----------; contig : :class:`.Expression` of type :py:data:`.tstr`; Locus contig.; position : :class:`.Expression` of type :py:data:`.tint32`; Locus position.; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus of interest. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus of interest. Truncates at; contig boundary.; reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to use. Must have a reference sequence available. Returns; -------; :class:`.StringExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:61,Modifiability,polymorphi,polymorphism,61,"""""""Computes the minimal representation of a (locus, alleles) polymorphism. Examples; --------. >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['TAA', 'TA'])); Struct(locus=Locus(contig=1, position=100000, reference_genome=GRCh37), alleles=['TA', 'T']). >>> hl.eval(hl.min_rep(hl.locus('1', 100000), ['AATAA', 'AACAA'])); Struct(locus=Locus(contig=1, position=100002, reference_genome=GRCh37), alleles=['T', 'C']). Notes; -----; Computing the minimal representation can cause the locus shift right (the; position can increase). Parameters; ----------; locus : :class:`.LocusExpression`; alleles : :class:`.ArrayExpression` of type :py:data:`.tstr`. Returns; -------; :class:`.StructExpression`; A :class:`.tstruct` expression with two fields, `locus`; (:class:`.LocusExpression`) and `alleles`; (:class:`.ArrayExpression` of type :py:data:`.tstr`).; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:856,Performance,load,loaded,856,"""""""Lift over coordinates to a different reference genome. Examples; --------. Lift over the locus coordinates from reference genome ``'GRCh37'`` to; ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome ``'GRCh37'``; to ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See :ref:`liftover_howto` for more instructions on lifting over a Table; or MatrixTable. Notes; -----; This function requires the reference genome of `x` has a chain file loaded; for `dest_reference_genome`. Use :meth:`.ReferenceGenome.add_liftover` to; load and attach a chain file to a reference genome. Returns ``None`` if `x` could not be converted. Warning; -------; Before using the result of :func:`.liftover` as a new row key or column; key, be sure to filter out missing values. Parameters; ----------; x : :class:`.Expression` of type :class:`.tlocus` or :class:`.tinterval` of :class:`.tlocus`; Locus or locus interval to lift over.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; min_match : :obj:`float`; Minimum ratio of bases that must remap.; include_strand : :obj:`bool`; If True, output the result as a :class:`.StructExpression` with the first field `result` being; the locus or locus interval and the second field `is_negative_strand` is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns; -------; :class:`.Expression`; A locus or locus interval converted to `dest_reference_genome`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:939,Performance,load,load,939,"""""""Lift over coordinates to a different reference genome. Examples; --------. Lift over the locus coordinates from reference genome ``'GRCh37'`` to; ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus('1', 1034245, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Locus(contig='chr1', position=1098865, reference_genome='GRCh38'). Lift over the locus interval coordinates from reference genome ``'GRCh37'``; to ``'GRCh38'``:. >>> hl.eval(hl.liftover(hl.locus_interval('20', 60001, 82456, True, True, 'GRCh37'), 'GRCh38')) # doctest: +SKIP; Interval(Locus(contig='chr20', position=79360, reference_genome='GRCh38'),; Locus(contig='chr20', position=101815, reference_genome='GRCh38'),; True,; True). See :ref:`liftover_howto` for more instructions on lifting over a Table; or MatrixTable. Notes; -----; This function requires the reference genome of `x` has a chain file loaded; for `dest_reference_genome`. Use :meth:`.ReferenceGenome.add_liftover` to; load and attach a chain file to a reference genome. Returns ``None`` if `x` could not be converted. Warning; -------; Before using the result of :func:`.liftover` as a new row key or column; key, be sure to filter out missing values. Parameters; ----------; x : :class:`.Expression` of type :class:`.tlocus` or :class:`.tinterval` of :class:`.tlocus`; Locus or locus interval to lift over.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; min_match : :obj:`float`; Minimum ratio of bases that must remap.; include_strand : :obj:`bool`; If True, output the result as a :class:`.StructExpression` with the first field `result` being; the locus or locus interval and the second field `is_negative_strand` is a boolean indicating; whether the locus or locus interval has been mapped to the negative strand of the destination; reference genome. Otherwise, output the converted locus or locus interval. Returns; -------; :class:`.Expression`; A locus or locus interval converted to `dest_reference_genome`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:80,Performance,load,load,80,"""""""Reference genome '{}' does not have liftover to '{}'.; Use 'add_liftover' to load a liftover chain file.""""""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:369,Availability,toler,tolerance,369,"""""""Finds a root of the function `f` within the interval `[min, max]`. Examples; --------. >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; -----; `f(min)` and `f(max)` must not have the same sign. If no root can be found, the result of this call will be `NA` (missing). :func:`.uniroot` returns an estimate for a root with accuracy; `4 * epsilon * abs(x) + tolerance`. 4*EPSILON*abs(x) + tol. Parameters; ----------; f : function ( (arg) -> :class:`.Float64Expression`); Must return a :class:`.Float64Expression`.; min : :class:`.Float64Expression`; max : :class:`.Float64Expression`; max_iter : `int`; The maximum number of iterations before giving up.; epsilon : `float`; The scaling factor in the accuracy of the root found.; tolerance : `float`; The constant factor in approximate accuracy of the root found. Returns; -------; :class:`.Float64Expression`; The root of the function `f`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:741,Availability,toler,tolerance,741,"""""""Finds a root of the function `f` within the interval `[min, max]`. Examples; --------. >>> hl.eval(hl.uniroot(lambda x: x - 1, -5, 5)); 1.0. Notes; -----; `f(min)` and `f(max)` must not have the same sign. If no root can be found, the result of this call will be `NA` (missing). :func:`.uniroot` returns an estimate for a root with accuracy; `4 * epsilon * abs(x) + tolerance`. 4*EPSILON*abs(x) + tol. Parameters; ----------; f : function ( (arg) -> :class:`.Float64Expression`); Must return a :class:`.Float64Expression`.; min : :class:`.Float64Expression`; max : :class:`.Float64Expression`; max_iter : `int`; The maximum number of iterations before giving up.; epsilon : `float`; The scaling factor in the accuracy of the root found.; tolerance : `float`; The constant factor in approximate accuracy of the root found. Returns; -------; :class:`.Float64Expression`; The root of the function `f`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:826,Modifiability,variab,variable-length,826,"""""""Returns a formatted string using a specified format string and arguments. Examples; --------. >>> hl.eval(hl.format('%.3e', 0.09345332)); '9.345e-02'. >>> hl.eval(hl.format('%.4f', hl.missing(hl.tfloat64))); 'null'. >>> hl.eval(hl.format('%s %s %s', 'hello', hl.tuple([3, hl.locus('1', 2453)]), True)); 'hello (3, 1:2453) true'. Notes; -----; See the `Java documentation <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__; for valid format specifiers and arguments. Missing values are printed as ``'null'`` except when using the; format flags `'b'` and `'B'` (printed as ``'false'`` instead). Parameters; ----------; f : :class:`.StringExpression`; Java `format string <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/Formatter.html#syntax>`__.; args : variable-length arguments of :class:`.Expression`; Arguments to format. Returns; -------; :class:`.StringExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:168,Availability,toler,tolerance,168,"""""""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:298,Availability,toler,tolerance,298,"""""""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:412,Availability,toler,tolerance,412,"""""""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:465,Availability,toler,tolerance,465,"""""""Tests whether two numbers are approximately equal. Examples; --------; >>> hl.eval(hl.approx_equal(0.25, 0.2500001)); True. >>> hl.eval(hl.approx_equal(0.25, 0.251, tolerance=1e-3, absolute=True)); False. Parameters; ----------; x : :class:`.NumericExpression`; y : :class:`.NumericExpression`; tolerance : :class:`.NumericExpression`; absolute : :class:`.BooleanExpression`; If True, compute ``abs(x - y) <= tolerance``. Otherwise, compute; ``abs(x - y) <= max(tolerance * max(abs(x), abs(y)), 2 ** -1022)``.; nan_same : :class:`.BooleanExpression`; If True, then ``NaN == NaN`` will evaluate to True. Otherwise,; it will return False. Returns; -------; :class:`.BooleanExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:202,Modifiability,extend,extended,202,"""""""Bitwise left-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_lshift(5, 3)); 40. >>> hl.eval(hl.bit_lshift(1, 8)); 256. Unlike Python, Hail integers are fixed-size (32 or 64 bits),; and bits extended beyond will be ignored:. >>> hl.eval(hl.bit_lshift(1, 31)); -2147483648. >>> hl.eval(hl.bit_lshift(1, 32)); 0. >>> hl.eval(hl.bit_lshift(hl.int64(1), 32)); 4294967296. >>> hl.eval(hl.bit_lshift(hl.int64(1), 64)); 0. Notes; -----; See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:102,Testability,log,logical,102,"""""""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:199,Testability,log,logical,199,"""""""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:287,Testability,log,logical,287,"""""""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:333,Testability,log,logical,333,"""""""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:410,Testability,log,logical,410,"""""""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:450,Testability,log,logical,450,"""""""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:763,Testability,log,logical,763,"""""""Bitwise right-shift `x` by `y`. Examples; --------; >>> hl.eval(hl.bit_rshift(256, 3)); 32. With ``logical=False`` (default), the sign is preserved:. >>> hl.eval(hl.bit_rshift(-1, 1)); -1. With ``logical=True``, the sign bit is treated as any other:. >>> hl.eval(hl.bit_rshift(-1, 1, logical=True)); 2147483647. Notes; -----; If `logical` is ``False``, then the shift is a sign-preserving right shift.; If `logical` is ``True``, then the shift is logical, with the sign bit; treated as any other bit. See `the Python wiki <https://wiki.python.org/moin/BitwiseOperators>`__; for more information about bit operators. Parameters; ----------; x : :class:`.Int32Expression` or :class:`.Int64Expression`; y : :class:`.Int32Expression` or :class:`.Int64Expression`; logical : :obj:`bool`. Returns; -------; :class:`.Int32Expression` or :class:`.Int64Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:306,Performance,perform,perform,306,"""""""Binary search `array` for the insertion point of `elem`. Parameters; ----------; array : :class:`.Expression` of type :class:`.tarray`; elem : :class:`.Expression`. Returns; -------; :class:`.Int32Expression`. Notes; -----; This function assumes that `array` is sorted in ascending order, and does; not perform any sortedness check. Missing values sort last. The returned index is the lower bound on the insertion point of `elem` into; the ordered array, or the index of the first element in `array` not smaller; than `elem`. This is a value between 0 and the length of `array`, inclusive; (if all elements in `array` are smaller than `elem`, the returned value is; the length of `array` or the index of the first missing value, if one; exists). If either `elem` or `array` is missing, the result is missing. Examples; --------. >>> a = hl.array([0, 2, 4, 8]). >>> hl.eval(hl.binary_search(a, -1)); 0. >>> hl.eval(hl.binary_search(a, 1)); 1. >>> hl.eval(hl.binary_search(a, 10)); 4. """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py:282,Safety,safe,safeguards,282,"""""""Query records from a table corresponding to a given point or range of keys. Notes; -----; This function does not dispatch to a distributed runtime; it can be used inside; already-distributed queries such as in :meth:`.Table.annotate`. Warning; -------; This function contains no safeguards against reading large amounts of data; using a single thread. Parameters; ----------; path : :class:`str`; Table path.; point_or_interval; Point or interval to query. Returns; -------; :class:`.ArrayExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/matrix_type.py:9,Deployability,patch,patch,9,"# monkey-patch pprint",MatchSource.CODE_COMMENT,hail/python/hail/expr/matrix_type.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/matrix_type.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/table_type.py:9,Deployability,patch,patch,9,"# monkey-patch pprint",MatchSource.CODE_COMMENT,hail/python/hail/expr/table_type.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/table_type.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py:212,Modifiability,parameteriz,parameterized,212,"""""""Hail type for n-dimensional arrays. .. include:: _templates/experimental.rst. In Python, these are represented as NumPy :obj:`numpy.ndarray`. Notes; -----. NDArrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array.; ndim : int32; Number of dimensions. See Also; --------; :class:`.NDArrayExpression`, :obj:`.nd.array`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py:17,Modifiability,variab,variable-length,17,"""""""Hail type for variable-length arrays of elements. In Python, these are represented as :obj:`list`. Notes; -----; Arrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array. See Also; --------; :class:`.ArrayExpression`, :class:`.CollectionExpression`,; :func:`~hail.expr.functions.array`, :ref:`sec-collection-functions`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py:167,Modifiability,parameteriz,parameterized,167,"""""""Hail type for variable-length arrays of elements. In Python, these are represented as :obj:`list`. Notes; -----; Arrays contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of array. See Also; --------; :class:`.ArrayExpression`, :class:`.CollectionExpression`,; :func:`~hail.expr.functions.array`, :ref:`sec-collection-functions`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py:162,Modifiability,parameteriz,parameterized,162,"""""""Hail type for collections of distinct elements. In Python, these are represented as :obj:`set`. Notes; -----; Sets contain elements of only one type, which is parameterized by; `element_type`. Parameters; ----------; element_type : :class:`.HailType`; Element type of set. See Also; --------; :class:`.SetExpression`, :class:`.CollectionExpression`,; :func:`.set`, :ref:`sec-collection-functions`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py:102,Modifiability,parameteriz,parameterize,102,"""""""Hail type for key-value maps. In Python, these are represented as :obj:`dict`. Notes; -----; Dicts parameterize the type of both their keys and values with; `key_type` and `value_type`. Parameters; ----------; key_type: :class:`.HailType`; Key type.; value_type: :class:`.HailType`; Value type. See Also; --------; :class:`.DictExpression`, :func:`.dict`, :ref:`sec-collection-functions`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py:48,Integrability,wrap,wrapper,48,"# NB: We ensure the key is always frozen with a wrapper on the key_type in the _array_repr.",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py:15,Testability,assert,assert,15,"# TODO another assert",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py:9,Deployability,patch,patch,9,"# monkey-patch pprint",MatchSource.CODE_COMMENT,hail/python/hail/expr/types.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/types.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:63,Security,access,accessing,63,"""""""Compute a summary of an array using aggregators. Useful for accessing; functionality that exists in `hl.agg` but not elsewhere, like `hl.agg.call_stats`. Parameters; ----------; array; f. Returns; -------; Aggregation result.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1137,Availability,down,downstream,1137,"""""""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y ≤ values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1148,Availability,error,error,1148,"""""""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y ≤ values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:730,Usability,intuit,intuition,730,"""""""Produce a summary of the distribution of values. Notes; -----; This method returns a struct containing two arrays: `values` and `ranks`.; The `values` array contains an ordered sample of values seen. The `ranks`; array is one longer, and contains the approximate ranks for the; corresponding values. These represent a summary of the CDF of the distribution of values. In; particular, for any value `x = values(i)` in the summary, we estimate that; there are `ranks(i)` values strictly less than `x`, and that there are; `ranks(i+1)` values less than or equal to `x`. For any value `y` (not; necessarily in the summary), we estimate CDF(y) to be `ranks(i)`, where `i`; is such that `values(i-1) < y ≤ values(i)`. An alternative intuition is that the summary encodes a compressed; approximation to the sorted list of values. For example, values=[0,2,5,6,9]; and ranks=[0,3,4,5,8,10] represents the approximation [0,0,0,2,5,6,6,6,9,9],; with the value `values(i)` occupying indices `ranks(i)` (inclusive) to; `ranks(i+1)` (exclusive). The returned struct also contains an array `_compaction_counts`, which is; used internally to support downstream error estimation. Warning; -------; This is an approximate and nondeterministic method. Parameters; ----------; expr : :class:`.Expression`; Expression to collect.; k : :obj:`int`; Parameter controlling the accuracy vs. memory usage tradeoff. Returns; -------; :class:`.StructExpression`; Struct containing `values` and `ranks` arrays.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:603,Testability,test,test,603,"""""""Returns ``True`` if `condition` is ``True`` for any record. Examples; --------. >>> (table1.group_by(table1.SEX); ... .aggregate(any_over_70 = hl.agg.any(table1.HT > 70)); ... .show()); +-----+-------------+; | SEX | any_over_70 |; +-----+-------------+; | str | bool |; +-----+-------------+; | ""F"" | False |; | ""M"" | True |; +-----+-------------+. Notes; -----; If there are no records to aggregate, the result is ``False``. Missing records are not considered. If every record is missing,; the result is also ``False``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:610,Testability,test,test,610,"""""""Returns ``True`` if `condition` is ``True`` for every record. Examples; --------. >>> (table1.group_by(table1.SEX); ... .aggregate(all_under_70 = hl.agg.all(table1.HT < 70)); ... .show()); +-----+--------------+; | SEX | all_under_70 |; +-----+--------------+; | str | bool |; +-----+--------------+; | ""F"" | False |; | ""M"" | False |; +-----+--------------+. Notes; -----; If there are no records to aggregate, the result is ``True``. Missing records are not considered. If every record is missing,; the result is also ``True``. Parameters; ----------; condition : :class:`.BooleanExpression`; Condition to test. Returns; -------; :class:`.BooleanExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:391,Performance,perform,performs,391,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1110,Performance,perform,perform,1110,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1740,Performance,perform,perform,1740,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:12,Testability,test,test,12,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:404,Testability,test,test,404,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:775,Testability,test,test,775,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1134,Testability,test,test,1134,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1282,Testability,test,test,1282,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1431,Testability,test,test,1431,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1641,Testability,test,test,1641,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1758,Testability,test,test,1758,"""""""Performs test of Hardy-Weinberg equilibrium. Examples; --------; Test each row of a dataset:. >>> dataset_result = dataset.annotate_rows(hwe = hl.agg.hardy_weinberg_test(dataset.GT)). Test each row on a sub-population:. >>> dataset_result = dataset.annotate_rows(; ... hwe_eas = hl.agg.filter(dataset.pop == 'EAS',; ... hl.agg.hardy_weinberg_test(dataset.GT))). Notes; -----; This method performs the test described in :func:`.functions.hardy_weinberg_test` based solely on; the counts of homozygous reference, heterozygous, and homozygous variant calls. The resulting struct expression has two fields:. - `het_freq_hwe` (:py:data:`.tfloat64`) - Expected frequency; of heterozygous calls under Hardy-Weinberg equilibrium. - `p_value` (:py:data:`.tfloat64`) - p-value from test of Hardy-Weinberg; equilibrium. By default, Hail computes the exact p-value with mid-p-value correction, i.e. the; probability of a less-likely outcome plus one-half the probability of an; equally-likely outcome. See this `document <_static/LeveneHaldane.pdf>`__ for; details on the Levene-Haldane distribution and references. To perform one-sided exact test of excess heterozygosity with mid-p-value; correction instead, set `one_sided=True` and the p-value returned will be; from the one-sided exact test. Warning; -------; Non-diploid calls (``ploidy != 2``) are ignored in the counts. While the; counts are defined for multiallelic variants, this test is only statistically; rigorous in the biallelic setting; use :func:`~hail.methods.split_multi`; to split multiallelic variants beforehand. Parameters; ----------; expr : :class:`.CallExpression`; Call to test for Hardy-Weinberg equilibrium.; one_sided: :obj:`bool`; ``False`` by default. When ``True``, perform one-sided test for excess heterozygosity. Returns; -------; :class:`.StructExpression`; Struct expression with fields `het_freq_hwe` and `p_value`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:116,Availability,down,downsampled,116,"""""""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:178,Availability,down,downsampled,178,"""""""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:408,Availability,down,downsample,408,"""""""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:552,Availability,down,downsampled,552,"""""""Downsample (x, y) coordinate datapoints. Parameters; ----------; x : :class:`.NumericExpression`; X-values to be downsampled.; y : :class:`.NumericExpression`; Y-values to be downsampled.; label : :class:`.StringExpression` or :class:`.ArrayExpression`; Additional data for each (x, y) coordinate. Can pass in multiple fields in an :class:`.ArrayExpression`.; n_divisions : :obj:`int`; Factor by which to downsample (default value = 500). A lower input results in fewer output datapoints. Returns; -------; :class:`.ArrayExpression`; Expression for downsampled coordinate points (x, y). The element type of the array is; :class:`.ttuple` of :py:data:`.tfloat64`, :py:data:`.tfloat64`, and :class:`.tarray` of :py:data:`.tstr`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:1813,Availability,error,error,1813," of genotype and age:. >>> ds_ann = ds.annotate_rows(linreg =; ... hl.agg.linreg(ds.pheno.blood_pressure,; ... [1,; ... ds.GT.n_alt_alleles(),; ... ds.pheno.age,; ... ds.GT.n_alt_alleles() * ds.pheno.age])). Warning; -------; As in the example, the intercept covariate ``1`` must be included; **explicitly** if desired. Notes; -----; In relation to; `lm.summary <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/summary.lm.html>`__; in R, ``linreg(y, x = [1, mt.x1, mt.x2])`` computes; ``summary(lm(y ~ x1 + x2))`` and; ``linreg(y, x = [mt.x1, mt.x2], nested_dim=0)`` computes; ``summary(lm(y ~ x1 + x2 - 1))``. More generally, `nested_dim` defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; numb",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:2111,Availability,error,error,2111,"st be included; **explicitly** if desired. Notes; -----; In relation to; `lm.summary <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/summary.lm.html>`__; in R, ``linreg(y, x = [1, mt.x1, mt.x2])`` computes; ``summary(lm(y ~ x1 + x2))`` and; ``linreg(y, x = [mt.x1, mt.x2], nested_dim=0)`` computes; ``summary(lm(y ~ x1 + x2 - 1))``. More generally, `nested_dim` defines the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning;",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:2861,Integrability,depend,dependent,2861,"imated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:3232,Integrability,depend,dependent,3232,"imated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:3242,Modifiability,variab,variable,3242,"imated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:3357,Modifiability,variab,variables,3357,"imated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float64Expression`, optional; Non-negative weight for weighted least squares. Returns; -------; :class:`.StructExpression`; Struct of regression results.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:2470,Testability,test,test,2470,"nes the number of effects to fit in the; nested (null) model, with the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:2508,Testability,test,test,2508,"th the effects on the remaining covariates fixed; to zero. The returned struct has ten fields:; - `beta` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated regression coefficient for each covariate.; - `standard_error` (:class:`.tarray` of :py:data:`.tfloat64`):; Estimated standard error for each covariate.; - `t_stat` (:class:`.tarray` of :py:data:`.tfloat64`):; t-statistic for each covariate.; - `p_value` (:class:`.tarray` of :py:data:`.tfloat64`):; p-value for each covariate.; - `multiple_standard_error` (:py:data:`.tfloat64`):; Estimated standard deviation of the random error.; - `multiple_r_squared` (:py:data:`.tfloat64`):; Coefficient of determination for nested models.; - `adjusted_r_squared` (:py:data:`.tfloat64`):; Adjusted `multiple_r_squared` taking into account degrees of; freedom.; - `f_stat` (:py:data:`.tfloat64`):; F-statistic for nested models.; - `multiple_p_value` (:py:data:`.tfloat64`):; p-value for the; `F-test <https://en.wikipedia.org/wiki/F-test#Regression_problems>`__ of; nested models.; - `n` (:py:data:`.tint64`):; Number of samples included in the regression. A sample is included if and; only if `y`, all elements of `x`, and `weight` (if set) are non-missing. All but the last field are missing if `n` is less than or equal to the; number of covariates or if the covariates are linearly dependent. If set, the `weight` parameter generalizes the model to `weighted least; squares <https://en.wikipedia.org/wiki/Weighted_least_squares>`__, useful; for heteroscedastic (diagonal but non-constant) variance. Warning; -------; If any weight is negative, the resulting statistics will be ``nan``. Parameters; ----------; y : :class:`.Float64Expression`; Response (dependent variable).; x : :class:`.Float64Expression` or :obj:`list` of :class:`.Float64Expression`; Covariates (independent variables).; nested_dim : :obj:`int`; The null model includes the first `nested_dim` covariates.; Must be between 0 and `k` (the length of `x`).; weight : :class:`.Float",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py:20,Availability,error,error,20,"# residual standard error squared",MatchSource.CODE_COMMENT,hail/python/hail/expr/aggregators/aggregators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/aggregators/aggregators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py:2,Testability,assert,assert,2,"# assert there are at least 2 numeric types",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/base_expression.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py:44,Integrability,depend,dependencies,44,"""""""Print information about type, index, and dependencies.""""""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/base_expression.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py:242,Availability,error,error,242,"""""""Returns ``True`` if the two expressions are equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x == y); True. >>> hl.eval(x == z); False. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for equality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are equal.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/base_expression.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py:246,Availability,error,error,246,"""""""Returns ``True`` if the two expressions are not equal. Examples; --------. >>> x = hl.literal(5); >>> y = hl.literal(5); >>> z = hl.literal(1). >>> hl.eval(x != y); False. >>> hl.eval(x != z); True. Notes; -----; This method will fail with an error if the two expressions are not; of comparable types. Parameters; ----------; other : :class:`.Expression`; Expression for inequality comparison. Returns; -------; :class:`.BooleanExpression`; ``True`` if the two expressions are not equal.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/base_expression.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py:648,Testability,log,logging,648,"""""""Print the first few records of the expression to the console. If the expression refers to a value on a keyed axis of a table or matrix; table, then the accompanying keys will be shown along with the records. Examples; --------. >>> table1.SEX.show(); +-------+-----+; | ID | SEX |; +-------+-----+; | int32 | str |; +-------+-----+; | 1 | ""M"" |; | 2 | ""M"" |; | 3 | ""F"" |; | 4 | ""F"" |; +-------+-----+. >>> hl.literal(123).show(); +--------+; | <expr> |; +--------+; | int32 |; +--------+; | 123 |; +--------+. Notes; -----; The output can be passed piped to another output source using the `handler` argument:. >>> ht.foo.show(handler=lambda x: logging.info(x)) # doctest: +SKIP. Parameters; ----------; n : :obj:`int`; Maximum number of rows to show.; width : :obj:`int`; Horizontal width at which to break columns.; truncate : :obj:`int`, optional; Truncate each field to the given number of characters. If; ``None``, truncate fields to the given `width`.; types : :obj:`bool`; Print an extra header line with the type of each field.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/base_expression.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/base_expression.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/expression_utils.py:89,Usability,learn,learning,89,"""""""Evaluate a Hail expression, returning the result. This method is extremely useful for learning about Hail expressions and; understanding how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.Table` or :class:`.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval(hl.if_else(x % 2 == 0, 'Even', 'Odd')); 'Even'. Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; Any; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/expression_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/expression_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/expression_utils.py:116,Usability,learn,learning,116,"""""""Evaluate a Hail expression, returning the result and the type of the result. This method is extremely useful for learning about Hail expressions and understanding; how to compose them. The expression must have no indices, but can refer to the globals; of a :class:`.hail.Table` or :class:`.hail.MatrixTable`. Examples; --------; Evaluate a conditional:. >>> x = 6; >>> hl.eval_typed(hl.if_else(x % 2 == 0, 'Even', 'Odd')); ('Even', dtype('str')). Parameters; ----------; expression : :class:`.Expression`; Any expression, or a Python value that can be implicitly interpreted as an expression. Returns; -------; (any, :class:`.HailType`); Result of evaluating `expression`, and its type. """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/expression_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/expression_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:93,Security,access,accessing,93,"""""""Uses the aggregator library to compute a summary from an array. This method is useful for accessing functionality that exists in the aggregator library; but not the basic expression library, for instance, :func:`.call_stats`. Parameters; ----------; f; Aggregation function. Returns; -------; :class:`.Expression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:347,Deployability,pipeline,pipeline,347,"""""""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:422,Energy Efficiency,efficient,efficient,422,"""""""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:253,Testability,test,test,253,"""""""Returns a boolean indicating whether `item` is found in the array. Examples; --------. >>> hl.eval(names.contains('Charlie')); True. >>> hl.eval(names.contains('Helen')); False. Parameters; ----------; item : :class:`.Expression`; Item for inclusion test. Warning; -------; This method takes time proportional to the length of the array. If a; pipeline uses this method on the same array several times, it may be; more efficient to convert the array to a set first early in the script; (:func:`~hail.expr.functions.set`). Returns; -------; :class:`.BooleanExpression`; ``True`` if the element is found in the array, ``False`` otherwise.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:87,Modifiability,extend,extend,87,"""""""Concatenate two arrays and return the result. Examples; --------. >>> hl.eval(names.extend(['Dan', 'Edith'])); ['Alice', 'Bob', 'Charlie', 'Dan', 'Edith']. Parameters; ----------; a : :class:`.ArrayExpression`; Array to concatenate, same type as the callee. Returns; -------; :class:`.ArrayExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:29,Energy Efficiency,power,power,29,"""""""Positionally raise to the power of an array or a scalar. Examples; --------. >>> hl.eval(a1 ** 2); [0.0, 1.0, 4.0, 9.0, 16.0, 25.0]. >>> hl.eval(a1 ** a2); [0.0, 1.0, 2.0, 0.3333333333333333, 4.0, 0.2]. Parameters; ----------; other : :class:`.NumericExpression` or :class:`.ArrayNumericExpression`. Returns; -------; :class:`.ArrayNumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:210,Testability,test,test,210,"""""""Returns ``True`` if `item` is in the set. Examples; --------. >>> hl.eval(s1.contains(1)); True. >>> hl.eval(s1.contains(10)); False. Parameters; ----------; item : :class:`.Expression`; Value for inclusion test. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is in the set.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:118,Availability,error,error,118,"""""""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:231,Availability,error,error,231,"""""""Get the value associated with key `item`. Examples; --------. >>> hl.eval(d['Alice']); 43. Notes; -----; Raises an error if `item` is not a key of the dictionary. Use; :meth:`.DictExpression.get` to return missing instead of an error. Parameters; ----------; item : :class:`.Expression`; Key expression. Returns; -------; :class:`.Expression`; Value associated with key `item`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:221,Testability,test,test,221,"""""""Returns whether a given key is present in the dictionary. Examples; --------. >>> hl.eval(d.contains('Alice')); True. >>> hl.eval(d.contains('Anne')); False. Parameters; ----------; item : :class:`.Expression`; Key to test for inclusion. Returns; -------; :class:`.BooleanExpression`; ``True`` if `item` is a key of the dictionary, ``False`` otherwise.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:355,Safety,safe,safer,355,"""""""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:97,Security,access,accessible,97,"""""""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:161,Security,access,access,161,"""""""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:516,Security,access,accessible,516,"""""""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:614,Security,access,access,614,"""""""Expression of type :class:`.tstruct`. >>> struct = hl.struct(a=5, b='Foo'). Struct fields are accessible as attributes and keys. It is therefore; possible to access field `a` of struct `s` with dot syntax:. >>> hl.eval(struct.a); 5. However, it is recommended to use square brackets to select fields:. >>> hl.eval(struct['a']); 5. The latter syntax is safer, because fields that share their name with; an existing attribute of :class:`.StructExpression` (`keys`, `values`,; `annotate`, `drop`, etc.) will only be accessible using the; :meth:`.StructExpression.__getitem__` syntax. This is also the only way; to access fields that are not valid Python identifiers, like fields with; spaces or symbols.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:115,Availability,error,error,115,"# Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:121,Integrability,message,message,121,"# Avoid using hasattr on self. Each new field added will fall through to __getattr__,; # which has to build a nice error message.",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:513,Deployability,update,updated,513,"""""""Add new fields or recompute existing fields. Examples; --------. >>> hl.eval(struct.annotate(a=10, c=2*2*2)); Struct(a=10, b='Foo', c=8). Notes; -----; If an expression in `named_exprs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; named_exprs : keyword args of :class:`.Expression`; Fields to add. Returns; -------; :class:`.StructExpression`; Struct with new or updated fields.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:31,Energy Efficiency,power,power,31,"""""""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:192,Energy Efficiency,power,power,192,"""""""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:364,Energy Efficiency,power,power,364,"""""""Raise the left to the right power. Examples; --------. >>> hl.eval(x ** 2); 9.0. >>> hl.eval(x ** -2); 0.1111111111111111. >>> hl.eval(y ** 1.5); 9.545941546018392. Parameters; ----------; power : :class:`.NumericExpression`; modulo; Unsupported argument. Returns; -------; :class:`.Expression` of type :py:data:`.tfloat64`; Result of raising left to the right power.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:472,Performance,load,load,472,"""""""Return the reference genome sequence at the locus. Examples; --------. Get the reference allele at a locus:. >>> hl.eval(locus.sequence_context()) # doctest: +SKIP; ""G"". Get the reference sequence at a locus including the previous 5 bases:. >>> hl.eval(locus.sequence_context(before=5)) # doctest: +SKIP; ""ACTCGG"". Notes; -----; This function requires that this locus' reference genome has an attached; reference sequence. Use :meth:`.ReferenceGenome.add_sequence` to; load and attach a reference sequence to a reference genome. Parameters; ----------; before : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include before the locus. Truncates at; contig boundary.; after : :class:`.Expression` of type :py:data:`.tint32`, optional; Number of bases to include after the locus. Truncates at; contig boundary. Returns; -------; :class:`.StringExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:133,Performance,perform,performed,133,"""""""Matrix multiplication: `a @ b`, semantically equivalent to `NumPy` matmul. If `a` and `b` are vectors,; the vector dot product is performed, returning a `NumericExpression`. If `a` and `b` are both 2-dimensional; matrices, this performs normal matrix multiplication. If `a` and `b` have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if `a` has shape `(3, 4, 5)` and `b` has shape `(3, 5, 6)`, `a` is treated; as a stack of three matrices of shape `(4, 5)` and `b` as a stack of three matrices of shape `(5, 6)`. `a @ b`; would then have shape `(3, 4, 6)`. Notes; -----; The last dimension of `a` and the second to last dimension of `b` (or only dimension if `b` is a vector); must have the same length. The dimensions to the left of the last two dimensions of `a` and `b` (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters; ----------; other : :class:`numpy.ndarray` :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression` or :class:`.NumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py:231,Performance,perform,performs,231,"""""""Matrix multiplication: `a @ b`, semantically equivalent to `NumPy` matmul. If `a` and `b` are vectors,; the vector dot product is performed, returning a `NumericExpression`. If `a` and `b` are both 2-dimensional; matrices, this performs normal matrix multiplication. If `a` and `b` have more than 2 dimensions, they are; treated as multi-dimensional stacks of 2-dimensional matrices. Matrix multiplication is applied element-wise; across the higher dimensions. E.g. if `a` has shape `(3, 4, 5)` and `b` has shape `(3, 5, 6)`, `a` is treated; as a stack of three matrices of shape `(4, 5)` and `b` as a stack of three matrices of shape `(5, 6)`. `a @ b`; would then have shape `(3, 4, 6)`. Notes; -----; The last dimension of `a` and the second to last dimension of `b` (or only dimension if `b` is a vector); must have the same length. The dimensions to the left of the last two dimensions of `a` and `b` (for NDArrays; of dimensionality > 2) must be equal or be compatible for broadcasting.; Number of dimensions of both NDArrays must be at least 1. Parameters; ----------; other : :class:`numpy.ndarray` :class:`.NDArrayNumericExpression`. Returns; -------; :class:`.NDArrayNumericExpression` or :class:`.NumericExpression`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/expr/expressions/typed_expressions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/expr/expressions/typed_expressions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/fs/hadoop_fs.py:171,Usability,simpl,simply,171,"""""""Get information about a path other than its file/directory status. In the cloud, determining if a given path is a file, a directory, or both is expensive. This; method simply returns file metadata if there is a file at this path. If there is no file at; this path, this operation will fail. The presence or absence of a directory at this path; does not affect the behaviors of this method. """"""",MatchSource.CODE_COMMENT,hail/python/hail/fs/hadoop_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/fs/hadoop_fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/pedigree.py:103,Testability,test,test,103,"""""""Read a PLINK .fam file and return a pedigree object. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'). Notes; -------. See `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_ for; the required format. :param str fam_path: path to .fam file. :param str delimiter: Field delimiter. :rtype: :class:`.Pedigree`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/pedigree.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/pedigree.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/pedigree.py:87,Testability,test,test,87,"""""""Write a .fam file to the given path. **Examples**. >>> ped = hl.Pedigree.read('data/test.fam'); >>> ped.write('output/out.fam'). **Notes**. This method writes a `PLINK .fam file <https://www.cog-genomics.org/plink2/formats#fam>`_. .. caution::. Phenotype information is not preserved in the Pedigree data; structure in Hail. Reading and writing a PLINK .fam file will; result in loss of this information. Use :func:`~.import_fam` to; manipulate this information. :param path: output path; :type path: str; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/pedigree.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/pedigree.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:642,Security,access,access,642,"""""""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference genome using; :func:`~hail.get_reference` anytime afterwards. Note; ----; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; ----; Hail allows setting a default reference so that the ``reference_genome``; argument of :func:`~hail.methods.import_vcf` does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the ``default_reference`` argument of; :func:`~hail.init`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:1052,Security,access,access,1052,"""""""An object that represents a `reference genome <https://en.wikipedia.org/wiki/Reference_genome>`__. Examples; --------. >>> contigs = [""1"", ""X"", ""Y"", ""MT""]; >>> lengths = {""1"": 249250621, ""X"": 155270560, ""Y"": 59373566, ""MT"": 16569}; >>> par = [(""X"", 60001, 2699521)]; >>> my_ref = hl.ReferenceGenome(""my_ref"", contigs, lengths, ""X"", ""Y"", ""MT"", par). Notes; -----; Hail comes with predefined reference genomes (case sensitive!):. - GRCh37, Genome Reference Consortium Human Build 37; - GRCh38, Genome Reference Consortium Human Build 38; - GRCm38, Genome Reference Consortium Mouse Build 38; - CanFam3, Canis lupus familiaris (dog). You can access these reference genome objects using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37'); >>> rg = hl.get_reference('GRCh38'); >>> rg = hl.get_reference('GRCm38'); >>> rg = hl.get_reference('CanFam3'). Note that constructing a new reference genome, either by using the class; constructor or by using `read` will add the reference genome to the list of; known references; it is possible to access the reference genome using; :func:`~hail.get_reference` anytime afterwards. Note; ----; Reference genome names must be unique. It is not possible to overwrite the; built-in reference genomes. Note; ----; Hail allows setting a default reference so that the ``reference_genome``; argument of :func:`~hail.methods.import_vcf` does not need to be used; constantly. It is a current limitation of Hail that a custom reference; genome cannot be used as the ``default_reference`` argument of; :func:`~hail.init`. In order to set a custom reference genome as default,; pass the reference as an argument to :func:`~hail.default_reference` after; initializing Hail. Parameters; ----------; name : :class:`str`; Name of reference. Must be unique and NOT one of Hail's; predefined references: ``'GRCh37'``, ``'GRCh38'``, ``'GRCm38'``,; ``'CanFam3'`` and ``'default'``.; contigs : :obj:`list` of :class:`str`; Contig names.; lengths : :obj:`dict` of :class",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:1073,Availability,down,download,1073,"""""""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:1092,Availability,avail,available,1092,"""""""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:641,Performance,load,loaded,641,"""""""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:614,Testability,test,test,614,"""""""Load the reference sequence from a FASTA file. Examples; --------; Access the GRCh37 reference genome using :func:`~hail.get_reference`:. >>> rg = hl.get_reference('GRCh37') # doctest: +SKIP. Add a sequence file:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz',; ... 'gs://hail-common/references/human_g1k_v37.fasta.fai') # doctest: +SKIP. Add a sequence file with the default index location:. >>> rg.add_sequence('gs://hail-common/references/human_g1k_v37.fasta.gz') # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_sequence` to test whether a sequence is loaded. FASTA and index files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37**. - FASTA file: ``gs://hail-common/references/human_g1k_v37.fasta.gz``; - Index file: ``gs://hail-common/references/human_g1k_v37.fasta.fai``. **GRCh38**. - FASTA file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.gz``; - Index file: ``gs://hail-common/references/Homo_sapiens_assembly38.fasta.fai``. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; fasta_file : :class:`str`; Path to FASTA file. Can be compressed (GZIP) or uncompressed.; index_file : :obj:`None` or :class:`str`; Path to FASTA index file. Must be uncompressed. If `None`, replace; the fasta_file's extension with `fai`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:43,Performance,load,loaded,43,"""""""True if the reference sequence has been loaded. Returns; -------; :obj:`bool`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:40,Availability,avail,available,40,"""""""``True`` if a liftover chain file is available from this reference; genome to the destination reference. Parameters; ----------; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`. Returns; -------; :obj:`bool`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:870,Availability,down,download,870,"""""""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP. Add a chain file from 37 to 38:. >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_liftover` to test whether a chain file has been registered. The chain file format is described; `here <https://genome.ucsc.edu/goldenpath/help/chain.html>`__. Chain files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37 to GRCh38**; gs://hail-common/references/grch37_to_grch38.over.chain.gz. **GRCh38 to GRCh37**; gs://hail-common/references/grch38_to_grch37.over.chain.gz. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; chain_file : :class:`str`; Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:889,Availability,avail,available,889,"""""""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP. Add a chain file from 37 to 38:. >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_liftover` to test whether a chain file has been registered. The chain file format is described; `here <https://genome.ucsc.edu/goldenpath/help/chain.html>`__. Chain files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37 to GRCh38**; gs://hail-common/references/grch37_to_grch38.over.chain.gz. **GRCh38 to GRCh37**; gs://hail-common/references/grch38_to_grch37.over.chain.gz. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; chain_file : :class:`str`; Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py:472,Testability,test,test,472,"""""""Register a chain file for liftover. Examples; --------; Access GRCh37 and GRCh38 using :func:`~hail.get_reference`:. >>> rg37 = hl.get_reference('GRCh37') # doctest: +SKIP; >>> rg38 = hl.get_reference('GRCh38') # doctest: +SKIP. Add a chain file from 37 to 38:. >>> rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38) # doctest: +SKIP. Notes; -----; This method can only be run once per reference genome. Use; :meth:`~has_liftover` to test whether a chain file has been registered. The chain file format is described; `here <https://genome.ucsc.edu/goldenpath/help/chain.html>`__. Chain files are hosted on google cloud for some of Hail's built-in; references:. **GRCh37 to GRCh38**; gs://hail-common/references/grch37_to_grch38.over.chain.gz. **GRCh38 to GRCh37**; gs://hail-common/references/grch38_to_grch37.over.chain.gz. Public download links are available; `here <https://console.cloud.google.com/storage/browser/hail-common/references/>`__. Parameters; ----------; chain_file : :class:`str`; Path to chain file. Can be compressed (GZIP) or uncompressed.; dest_reference_genome : :class:`str` or :class:`.ReferenceGenome`; Reference genome to convert to.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/genetics/reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/genetics/reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py:76,Integrability,interface,interface,76,"""""""Creates a histogram. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; min_val: `int` or `float`; Minimum value to include in histogram; max_val: `int` or `float`; Maximum value to include in histogram; bins: `int`; Number of bins to plot. 30 by default.; fill:; A single fill color for all bars of histogram, overrides ``fill`` aesthetic.; color:; A single outline color for all bars of histogram, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; position: :class:`str`; Tells how to deal with different groups of data at same point. Options are ""stack"" and ""dodge"". Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py:614,Availability,down,down,614,"# Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py:1248,Availability,down,down,1248,"# Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py:979,Integrability,contract,contract,979,"# Computes the maximum entropy distribution whose cdf is within +- e of the; # staircase-shaped cdf encoded by min_x, max_x, x, y.; #; # x is an array of n x-coordinates between min_x and max_x, and y is an array; # of (n+1) y-coordinates between 0 and 1, both sorted. Together they encode a; # staircase-shaped cdf.; # For example, if min_x = 1, max_x=4, x=[2], y=[.2, .6], then the cdf is the; # staircase tracing the points; # (1, 0) - (1, .2) - (2, .2) - (2, .6) - (4, .6) - (4, 1); #; # Now consider the set of all possible cdfs within +-e of the one above. In; # other words, shift the staircase both up and down by e, capping above and; # below at 1 and 0, and consider all possible cdfs that lie in between. The; # distribution with maximum entropy whose cdf is between the two staircases; # is the one whose cdf is the graph constructed as follows: tie a rubber band; # to the points (min_x, 0) and (max_x, 1), place the middle between the two; # staircases, and let it contract. In other words, it will be the shortest; # path between the staircases.; #; # It's easy to see this path must be piecewise linear, and the points where the; # slopes change will be either; # * bending up at a point of the form (x[i], y[i]+e), or; # * bending down at a point of the form (x[i], y[i+1]-e); #; # Returns (new_y, keep).; # keep is the array of indices i at which the piecewise linear max-ent cdf; # changes slope, as described in the previous paragraph.; # new_y is an array the same length as x. For each i in keep, new_y[i] is the; # y coordinate of the point on the max-ent cdf.",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py:9,Modifiability,variab,variables,9,"# Result variables:",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py:8,Modifiability,variab,variables,8,"# State variables:; # (fx, fy) is most recently fixed point on max-ent cdf",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py:17,Availability,down,down,17,"# Line must bend down at x[li]. We know the max-entropy cdf passes; # through this point, so record it in new_y, keep.; # This becomes the new fixed point, and we must restart the scan; # from there.",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py:287,Integrability,interface,interface,287,"""""""Creates a smoothed density plot. This method uses the `hl.agg.approx_cdf` aggregator to compute a sketch; of the distribution of the values of `x`. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; k: `int`; Passed to the `approx_cdf` aggregator. The size of the aggregator scales; linearly with `k`. The default value of `1000` is likely sufficient for; most uses.; smoothing: `float`; Controls the amount of smoothing applied.; fill:; A single fill color for all density plots, overrides ``fill`` aesthetic.; color:; A single line color for all density plots, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; smoothed: `boolean`; If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py:1037,Modifiability,variab,variable,1037,"""""""Creates a smoothed density plot. This method uses the `hl.agg.approx_cdf` aggregator to compute a sketch; of the distribution of the values of `x`. It then uses an ad hoc method to; estimate a smoothed pdf consistent with that cdf. Note: this function currently does not support same interface as R's ggplot. Supported aesthetics: ``x``, ``color``, ``fill``. Parameters; ----------; mapping: :class:`Aesthetic`; Any aesthetics specific to this geom.; k: `int`; Passed to the `approx_cdf` aggregator. The size of the aggregator scales; linearly with `k`. The default value of `1000` is likely sufficient for; most uses.; smoothing: `float`; Controls the amount of smoothing applied.; fill:; A single fill color for all density plots, overrides ``fill`` aesthetic.; color:; A single line color for all density plots, overrides ``color`` aesthetic.; alpha: `float`; A measure of transparency between 0 and 1.; smoothed: `boolean`; If true, attempts to fit a smooth kernel density estimator.; If false, uses a custom method do generate a variable width histogram; directly from the approx_cdf results. Returns; -------; :class:`FigureAttribute`; The geom to be applied.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/geoms.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/geoms.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py:88,Deployability,update,updated,88,"""""""Turn the hail plot into a Plotly plot. Returns; -------; A Plotly figure that can be updated with plotly methods.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/ggplot.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py:15,Deployability,update,update,15,"# Important to update axes after labels, axes names take precedence.",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/ggplot.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py:62,Deployability,install,installed,62,"""""""Write out this plot as an image. This requires you to have installed the python package kaleido from pypi. Parameters; ----------; path: :class:`str`; The path to write the file to.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/ggplot.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py:105,Integrability,interface,interface,105,"""""""Create the initial plot object. This function is the beginning of all plots using the ``hail.ggplot`` interface. Plots are constructed; by calling this function, then adding attributes to the plot to get the desired result. Examples; --------. Create a y = x^2 scatter plot. >>> ht = hl.utils.range_table(10); >>> ht = ht.annotate(squared = ht.idx**2); >>> my_plot = hl.ggplot.ggplot(ht, hl.ggplot.aes(x=ht.idx, y=ht.squared)) + hl.ggplot.geom_point(). Parameters; ----------; table; The table containing the data to plot.; mapping; Default list of aesthetic mappings from table data to plot attributes. Returns; -------; :class:`.GGPlot`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/ggplot.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/ggplot.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py:28,Deployability,continuous,continuous,28,"# What else do discrete and continuous scales have in common?",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/scale.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py:27,Testability,log,log,27,"""""""Transforms x axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on x-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/scale.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py:27,Testability,log,log,27,"""""""Transforms y-axis to be log base 10 scaled. Parameters; ----------; name: :class:`str`; The label to show on y-axis. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/scale.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py:15,Deployability,continuous,continuous,15,"""""""The default continuous x scale. Parameters; ----------; name: :class:`str`; The label to show on x-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the x-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the x-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/scale.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py:15,Deployability,continuous,continuous,15,"""""""The default continuous y scale. Parameters; ----------; name: :class:`str`; The label to show on y-axis; breaks: :class:`list` of :class:`float`; The locations to draw ticks on the y-axis.; labels: :class:`list` of :class:`str`; The labels of the ticks on the axis.; trans: :class:`str`; The transformation to apply to the y-axis. Supports ""identity"", ""reverse"", ""log10"". Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/scale.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py:15,Deployability,continuous,continuous,15,"""""""The default continuous color scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/scale.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py:15,Deployability,continuous,continuous,15,"""""""The default continuous fill scale. This linearly interpolates colors between the min and max observed values. Returns; -------; :class:`.FigureAttribute`; The scale to be applied.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ggplot/scale.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ggplot/scale.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py:11,Modifiability,variab,variables,11,"""""""Compute variables bound in child 'i'. Returns; -------; dict; mapping from bound variables to 'default_value', if provided,; otherwise to their types; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ir/base_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py:84,Modifiability,variab,variables,84,"""""""Compute variables bound in child 'i'. Returns; -------; dict; mapping from bound variables to 'default_value', if provided,; otherwise to their types; """"""",MatchSource.CODE_COMMENT,hail/python/hail/ir/base_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py:12,Modifiability,variab,variable,12,"# Used as a variable, bound by any node which defines the meaning of; # aggregations (e.g. MatrixMapRows, AggFilter, etc.), and ""referenced"" by; # any node which performs aggregations (e.g. AggFilter, ApplyAggOp, etc.).",MatchSource.CODE_COMMENT,hail/python/hail/ir/base_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py:162,Performance,perform,performs,162,"# Used as a variable, bound by any node which defines the meaning of; # aggregations (e.g. MatrixMapRows, AggFilter, etc.), and ""referenced"" by; # any node which performs aggregations (e.g. AggFilter, ApplyAggOp, etc.).",MatchSource.CODE_COMMENT,hail/python/hail/ir/base_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/base_ir.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/ir.py:61,Modifiability,variab,variable,61,"# FIXME: If body uses randomness, create a new uid induction variable",MatchSource.CODE_COMMENT,hail/python/hail/ir/ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/ir.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/matrix_ir.py:34,Integrability,depend,dependent,34,"# FIXME: might cause issues being dependent on col order",MatchSource.CODE_COMMENT,hail/python/hail/ir/matrix_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/matrix_ir.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py:2,Safety,avoid,avoid,2,"# avoid lifting refs",MatchSource.CODE_COMMENT,hail/python/hail/ir/renderer.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py:38,Modifiability,variab,variables,38,"# The binding context of 'node'. Maps variables bound above to the; # depth at which they were bound (more precisely, if; # 'context[var] == depth', then 'stack[depth-1].node' binds 'var' in; # the subtree rooted at 'stack[depth].node').",MatchSource.CODE_COMMENT,hail/python/hail/ir/renderer.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py:38,Modifiability,variab,variables,38,"# The binding context of 'node'. Maps variables bound above to the; # depth at which they were bound (more precisely, if; # 'context[var] == depth', then 'stack[depth-1].node' binds 'var' in; # the subtree rooted at 'stack[depth].node').",MatchSource.CODE_COMMENT,hail/python/hail/ir/renderer.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py:340,Safety,safe,safe,340,"# The array of strings building 'node's IR.; # * If 'insert_lets', all lets will be added to the parent's; # 'builder' before appending this 'builder'.; # * If 'lift_to_frame', 'builder' will be added to 'lift_to_frame's; # list of lifted lets, while only ""(Ref ...)"" will be added to; # the parent's 'builder'.; # * If neither, then it is safe for 'builder' to be an alias of the; # parent's 'builder', to save copying.",MatchSource.CODE_COMMENT,hail/python/hail/ir/renderer.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/ir/renderer.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:3044,Availability,resilien,resilience,3044,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:5370,Availability,down,downstream,5370,"2, :]`` is a block matrix with 1 row, 10 columns,; and elements from row 2 of ``bm``. - ``bm[:3, -1]`` is a block matrix with 3 rows, 1 column,; and the first 3 elements of the last column of ``bm``. - ``bm[::2, ::2]`` is a block matrix with 5 rows, 5 columns,; and all evenly-indexed elements of ``bm``. Use :meth:`filter`, :meth:`filter_rows`, and :meth:`filter_cols` to; subset to non-slice subsets of rows and columns, e.g. to rows ``[0, 2, 5]``. **Block-sparse representation**. By default, block matrices compute and store all blocks explicitly.; However, some applications involve block matrices in which:. - some blocks consist entirely of zeroes. - some blocks are not of interest. For example, statistical geneticists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`spa",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:1002,Energy Efficiency,power,power,1002,"""""""Hail's block-distributed matrix of :py:data:`.tfloat64` elements. .. include:: ../_templates/experimental.rst. A block matrix is a distributed analogue of a two-dimensional; `NumPy ndarray; <https://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html>`__ with; shape ``(n_rows, n_cols)`` and NumPy dtype ``float64``.; Import the class with:. >>> from hail.linalg import BlockMatrix. Under the hood, block matrices are partitioned like a checkerboard into; square blocks with side length a common block size. Blocks in the final row; or column of blocks may be truncated, so block size need not evenly divide; the matrix dimensions. Block size defaults to the value given by; :meth:`default_block_size`. **Operations and broadcasting**. The core operations are consistent with NumPy: ``+``, ``-``, ``*``, and; ``/`` for element-wise addition, subtraction, multiplication, and division;; ``@`` for matrix multiplication; ``T`` for transpose; and ``**`` for; element-wise exponentiation to a scalar power. For element-wise binary operations, each operand may be a block matrix, an; ndarray, or a scalar (:obj:`int` or :obj:`float`). For matrix; multiplication, each operand may be a block matrix or an ndarray. If either; operand is a block matrix, the result is a block matrix. Binary operations; between block matrices require that both operands have the same block size. To interoperate with block matrices, ndarray operands must be one or two; dimensional with dtype convertible to ``float64``. One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exception: block matrices do not currently support element-wise; ""outer product"" of a single row and a single column, although the sam",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:2525,Integrability,depend,dependency,2525,". One-dimensional ndarrays; of shape ``(n)`` are promoted to two-dimensional ndarrays of shape ``(1,; n)``, i.e. a single row. Block matrices support broadcasting of ``+``, ``-``, ``*``, and ``/``; between matrices of different shapes, consistent with the NumPy; `broadcasting rules; <https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__.; There is one exception: block matrices do not currently support element-wise; ""outer product"" of a single row and a single column, although the same; effect can be achieved for ``*`` by using ``@``. Warning; -------. For binary operations, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read(",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:6021,Integrability,depend,depend,6021,"cists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following metho",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:3028,Performance,perform,performance,3028,"s, if the first operand is an ndarray and the; second operand is a block matrix, the result will be a ndarray of block; matrices. To achieve the desired behavior for ``+`` and ``*``, place the; block matrix operand first; for ``-``, ``/``, and ``@``, first convert; the ndarray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 r",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:3335,Performance,cache,cache,3335,"darray to a block matrix using :meth:`.from_numpy`. Warning; -------. Block matrix multiplication requires special care due to each block; of each operand being a dependency of multiple blocks in the product. The :math:`(i, j)`-block in the product ``a @ b`` is computed by summing; the products of corresponding blocks in block row :math:`i` of ``a`` and; block column :math:`j` of ``b``. So overall, in addition to this; multiplication and addition, the evaluation of ``a @ b`` realizes each; block of ``a`` as many times as the number of block columns of ``b``; and realizes each block of ``b`` as many times as the number of; block rows of ``a``. This becomes a performance and resilience issue whenever ``a`` or ``b``; is defined in terms of pending transformations (such as linear; algebra operations). For example, evaluating ``a @ (c @ d)`` will; effectively evaluate ``c @ d`` as many times as the number of block rows; in ``a``. To limit re-computation, write or cache transformed block matrix; operands before feeding them into matrix multiplication:. >>> c = BlockMatrix.read('c.bm') # doctest: +SKIP; >>> d = BlockMatrix.read('d.bm') # doctest: +SKIP; >>> (c @ d).write('cd.bm') # doctest: +SKIP; >>> a = BlockMatrix.read('a.bm') # doctest: +SKIP; >>> e = a @ BlockMatrix.read('cd.bm') # doctest: +SKIP. **Indexing and slicing**. Block matrices also support NumPy-style 2-dimensional; `indexing and slicing <https://docs.scipy.org/doc/numpy/user/basics.indexing.html>`__,; with two differences.; First, slices ``start:stop:step`` must be non-empty with positive ``step``.; Second, even if only one index is a slice, the resulting block matrix is still; 2-dimensional. For example, for a block matrix ``bm`` with 10 rows and 10 columns:. - ``bm[0, 0]`` is the element in row 0 and column 0 of ``bm``. - ``bm[0:1, 0]`` is a block matrix with 1 row, 1 column,; and element ``bm[0, 0]``. - ``bm[2, :]`` is a block matrix with 1 row, 10 columns,; and elements from row 2 of ``bm``. - ``bm[:3,",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:6076,Safety,avoid,avoiding,6076,"cists often want to compute and manipulate a; banded correlation matrix capturing ""linkage disequilibrium"" between nearby; variants along the genome. In this case, working with the full correlation; matrix for tens of millions of variants would be prohibitively expensive,; and in any case, entries far from the diagonal are either not of interest or; ought to be zeroed out before downstream linear algebra. To enable such computations, block matrices do not require that all blocks; be realized explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following metho",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:7455,Testability,log,logarithm,7455,"d explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following methods fail if any operand is block-sparse, but can be forced; by first applying :meth:`densify`. - Element-wise division between two block matrices. - Multiplication by a scalar or broadcasted vector which includes an; infinite or ``nan`` value. - Division by a scalar or broadcasted vector which includes a zero, infinite; or ``nan`` value. - Division of a scalar or broadcasted vector by a block matrix. - Element-wise exponentiation by a negative exponent. - Natural logarithm, :meth:`log`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:7473,Testability,log,log,7473,"d explicitly. Implicit (dropped) blocks behave as blocks of; zeroes, so we refer to a block matrix in which at least one block is; implicitly zero as a **block-sparse matrix**. Otherwise, we say the matrix; is block-dense. The property :meth:`is_sparse` encodes this state. Dropped blocks are not stored in memory or on :meth:`write`. In fact,; blocks that are dropped prior to an action like :meth:`export` or; :meth:`to_numpy` are never computed in the first place, nor are any blocks; of upstream operands on which only dropped blocks depend! In addition,; linear algebra is accelerated by avoiding, for example, explicit addition of; or multiplication by blocks of zeroes. Block-sparse matrices may be created with; :meth:`sparsify_band`,; :meth:`sparsify_rectangles`,; :meth:`sparsify_row_intervals`,; and :meth:`sparsify_triangle`. The following methods naturally propagate block-sparsity:. - Addition and subtraction ""union"" realized blocks. - Element-wise multiplication ""intersects"" realized blocks. - Transpose ""transposes"" realized blocks. - :meth:`abs` and :meth:`sqrt` preserve the realized blocks. - :meth:`sum` along an axis realizes those blocks for which at least one; block summand is realized. - Matrix slicing, and more generally :meth:`filter`, :meth:`filter_rows`,; and :meth:`filter_cols`. These following methods always result in a block-dense matrix:. - :meth:`fill`. - Addition or subtraction of a scalar or broadcasted vector. - Matrix multiplication, ``@``. The following methods fail if any operand is block-sparse, but can be forced; by first applying :meth:`densify`. - Element-wise division between two block matrices. - Multiplication by a scalar or broadcasted vector which includes an; infinite or ``nan`` value. - Division by a scalar or broadcasted vector which includes a zero, infinite; or ``nan`` value. - Division of a scalar or broadcasted vector by a block matrix. - Element-wise exponentiation by a negative exponent. - Natural logarithm, :meth:`log`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:896,Performance,load,load,896,"""""""Creates a block matrix from a binary file. Examples; --------; >>> import numpy as np; >>> a = np.random.rand(10, 20); >>> a.tofile('/local/file') # doctest: +SKIP. To create a block matrix of the same dimensions:. >>> bm = BlockMatrix.fromfile('file:///local/file', 10, 20) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.fromfile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html>`__,; reads a binary file of float64 values in row-major order, such as that; produced by `numpy.tofile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html>`__; or :meth:`BlockMatrix.tofile`. Binary files produced and consumed by :meth:`.tofile` and; :meth:`.fromfile` are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; :meth:`BlockMatrix.write` and :meth:`BlockMatrix.read` to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent. A NumPy ndarray must have type float64 for the output of; func:`numpy.tofile` to be a valid binary input to :meth:`.fromfile`.; This is not checked. The number of entries must be less than :math:`2^{31}`. Parameters; ----------; uri: :class:`str`, optional; URI of binary input file.; n_rows: :obj:`int`; Number of rows.; n_cols: :obj:`int`; Number of columns.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`default_block_size`. See Also; --------; :meth:`.from_numpy`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:727,Availability,error,error,727,"""""""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:1446,Availability,error,error,1446,"""""""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:670,Performance,perform,performance,670,"""""""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:933,Performance,concurren,concurrently,933,"""""""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:416,Safety,avoid,avoid,416,"""""""Creates a block matrix using a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> bm = BlockMatrix.from_entry_expr(mt.GT.n_alt_alleles()). Notes; -----; This convenience method writes the block matrix to a temporary file on; persistent disk and then reads the file. If you want to store the; resulting block matrix, use :meth:`write_from_entry_expr` directly to; avoid writing the result twice. See :meth:`write_from_entry_expr` for; further documentation. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. If you encounter a Hadoop write/replication error, increase the; number of persistent workers or the disk size per persistent worker,; or use :meth:`write_from_entry_expr` to write to external storage. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:37,Testability,test,test,37,"""""""Private method for creating small test matrices.""""""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:342,Availability,checkpoint,checkpointing,342,"""""""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:372,Availability,checkpoint,checkpoint,372,"""""""Checkpoint the block matrix. .. include:: ../_templates/write_warning.rst. Parameters; ----------; path: :class:`str`; Path for output file.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; force_row_major: :obj:`bool`; If ``True``, transform blocks in column-major format; to row-major format before checkpointing.; If ``False``, checkpoint blocks in their current format.; stage_locally: :obj:`bool`; If ``True``, major output will be written to temporary local storage; before being copied to ``output``.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:367,Availability,down,downsamples,367,"""""""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; cente",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:1988,Availability,error,error,1988,"table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:328,Deployability,pipeline,pipelined,328,"""""""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; cente",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:255,Performance,load,loaded,255,"""""""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; cente",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:505,Performance,perform,performance,505,"""""""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; cente",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:1290,Performance,perform,performance,1290,"BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:1351,Performance,concurren,concurrently,1351," transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; center: :obj:`bool`; If true, subtract the row mean.; normalize: :obj:`bool`; If true and ``center=False``, divide by the row magnitude.; If true and ``center=True``, divide the centered value by the; centered row magnitude.; axis: :class:`str`; One of ""rows"" or ""cols"": axis by which to normalize or center.; block_size: :obj:`int`, optional",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:585,Usability,clear,clear,585,"""""""Writes a block matrix from a matrix table entry expression. Examples; --------; >>> mt = hl.balding_nichols_model(3, 25, 50); >>> BlockMatrix.write_from_entry_expr(mt.GT.n_alt_alleles(),; ... 'output/model.bm'). Notes; -----; The resulting file can be loaded with :meth:`BlockMatrix.read`.; Blocks are stored row-major. If a pipelined transformation significantly downsamples the rows of the; underlying matrix table, then repartitioning the matrix table ahead of; this method will greatly improve its performance. By default, this method will fail if any values are missing (to be clear,; special float values like ``nan`` are not missing values). - Set `mean_impute` to replace missing values with the row mean before; possibly centering or normalizing. If all values are missing, the row; mean is ``nan``. - Set `center` to shift each row to have mean zero before possibly; normalizing. - Set `normalize` to normalize each row to have unit length. To standardize each row, regarded as an empirical distribution, to have; mean 0 and variance 1, set `center` and `normalize` and then multiply; the result by ``sqrt(n_cols)``. Warning; -------; If the rows of the matrix table have been filtered to a small fraction,; then :meth:`.MatrixTable.repartition` before this method to improve; performance. This method opens ``n_cols / block_size`` files concurrently per task.; To not blow out memory when the number of columns is very large,; limit the Hadoop write buffer size; e.g. on GCP, set this property on; cluster startup (the default is 64MB):; ``--properties 'core:fs.gs.io.buffersize.write=1048576``. Parameters; ----------; entry_expr: :class:`.Float64Expression`; Entry expression for numeric matrix entries.; path: :class:`str`; Path for output.; overwrite : :obj:`bool`; If ``True``, overwrite an existing file at the destination.; mean_impute: :obj:`bool`; If true, set missing values to the row mean before centering or; normalizing. If false, missing values will raise an error.; cente",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:244,Energy Efficiency,efficient,efficient,244,"""""""Filters matrix rows and columns. Notes; -----; This method has the same effect as :meth:`BlockMatrix.filter_cols`; followed by :meth:`BlockMatrix.filter_rows` (or vice versa), but; filters the block matrix in a single pass which may be more efficient. Parameters; ----------; rows_to_keep: :obj:`list` of :obj:`int`; Indices of rows to keep. Must be non-empty and increasing.; cols_to_keep: :obj:`list` of :obj:`int`; Indices of columns to keep. Must be non-empty and increasing. Returns; -------; :class:`.BlockMatrix`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:1343,Energy Efficiency,efficient,efficient,1343,"""""""Filter to the upper or lower triangle. Examples; --------; Consider the following block matrix:. >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Filter to the upper triangle and collect to NumPy:. >>> bm.sparsify_triangle().to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 0., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 0., 16.]]). Set all blocks fully outside the upper triangle to zero; and collect to NumPy:. >>> bm.sparsify_triangle(blocks_only=True).to_numpy() # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 3., 4.],; [ 5., 6., 7., 8.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; -----; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from the (non-strict) upper or lower triangle. By; default, all elements outside the triangle but inside blocks that; overlap the triangle are set to zero as well. Parameters; ----------; lower: :obj:`bool`; If ``False``, keep the upper triangle.; If ``True``, keep the lower triangle.; blocks_only: :obj:`bool`; If ``False``, set all elements outside the triangle to zero.; If ``True``, only set all blocks outside the triangle to; blocks of zeros; this is more efficient. Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:1953,Energy Efficiency,efficient,efficient,1953,"x by filtering to an interval for each row. Examples; --------; Consider the following block matrix:. >>> import numpy as np; >>> nd = np.array([[ 1.0, 2.0, 3.0, 4.0],; ... [ 5.0, 6.0, 7.0, 8.0],; ... [ 9.0, 10.0, 11.0, 12.0],; ... [13.0, 14.0, 15.0, 16.0]]); >>> bm = BlockMatrix.from_numpy(nd, block_size=2). Set all elements outside the given row intervals to zero; and collect to NumPy:. >>> (bm.sparsify_row_intervals(starts=[1, 0, 2, 2],; ... stops= [2, 0, 3, 4]); ... .to_numpy()) # doctest: +SKIP_OUTPUT_CHECK; array([[ 0., 2., 0., 0.],; [ 0., 0., 0., 0.],; [ 0., 0., 11., 0.],; [ 0., 0., 15., 16.]]). Set all blocks fully outside the given row intervals to; blocks of zeros and collect to NumPy:. >>> (bm.sparsify_row_intervals(starts=[1, 0, 2, 2],; ... stops= [2, 0, 3, 4],; ... blocks_only=True); ... .to_numpy()) # doctest: +SKIP_OUTPUT_CHECK; array([[ 1., 2., 0., 0.],; [ 5., 6., 0., 0.],; [ 0., 0., 11., 12.],; [ 0., 0., 15., 16.]]). Notes; -----; This method creates a block-sparse matrix by zeroing out all blocks; which are disjoint from all row intervals. By default, all elements; outside the row intervals but inside blocks that overlap the row; intervals are set to zero as well. `starts` and `stops` must both have length equal to the number of; rows. The interval for row ``i`` is ``[starts[i], stops[i])``. In; particular, ``0 <= starts[i] <= stops[i] <= n_cols`` is required; for all ``i``. This method requires the number of rows to be less than :math:`2^{31}`. Parameters; ----------; starts: :obj:`list` of :obj:`int`, or :class:`numpy.ndarray` of :obj:`int`; Start indices for each row (inclusive).; stops: :obj:`list` of :obj:`int`, or :class:`numpy.ndarray` of :obj:`int`; Stop indices for each row (exclusive).; blocks_only: :obj:`bool`; If ``False``, set all elements outside row intervals to zero.; If ``True``, only set all blocks outside row intervals to blocks; of zeros; this is more efficient.; Returns; -------; :class:`.BlockMatrix`; Sparse block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:950,Performance,load,load,950,"""""""Collects and writes data to a binary file. Examples; --------; >>> import numpy as np; >>> bm = BlockMatrix.random(10, 20); >>> bm.tofile('file:///local/file') # doctest: +SKIP. To create a :class:`numpy.ndarray` of the same dimensions:. >>> a = np.fromfile('/local/file').reshape((10, 20)) # doctest: +SKIP. Notes; -----; This method, analogous to `numpy.tofile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.tofile.html>`__,; produces a binary file of float64 values in row-major order, which can; be read by functions such as `numpy.fromfile; <https://docs.scipy.org/doc/numpy/reference/generated/numpy.fromfile.html>`__; (if a local file) and :meth:`BlockMatrix.fromfile`. Binary files produced and consumed by :meth:`.tofile` and; :meth:`.fromfile` are not platform independent, so should only be used; for inter-operating with NumPy, not storage. Use; :meth:`BlockMatrix.write` and :meth:`BlockMatrix.read` to save and load; block matrices, since these methods write and read blocks in parallel; and are platform independent. The number of entries must be less than :math:`2^{31}`. Parameters; ----------; uri: :class:`str`, optional; URI of binary output file. See Also; --------; :meth:`.to_numpy`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:213,Availability,redundant,redundant,213,"""""""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:272,Deployability,pipeline,pipelines,272,"""""""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:123,Performance,cache,cache,123,"""""""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:251,Performance,perform,performance,251,"""""""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:207,Safety,avoid,avoid,207,"""""""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:213,Safety,redund,redundant,213,"""""""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:520,Usability,guid,guide,520,"""""""Persists this block matrix in memory or on disk. Notes; -----; The :meth:`.BlockMatrix.persist` and :meth:`.BlockMatrix.cache`; methods store the current block matrix on disk or in memory temporarily; to avoid redundant computation and improve the performance of Hail; pipelines. This method is not a substitution for; :meth:`.BlockMatrix.write`, which stores a permanent file. Most users should use the ""MEMORY_AND_DISK"" storage level. See the `Spark; documentation; <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>`__; for a more in-depth discussion of persisting data. Parameters; ----------; storage_level : str; Storage level. One of: NONE, DISK_ONLY,; DISK_ONLY_2, MEMORY_ONLY, MEMORY_ONLY_2, MEMORY_ONLY_SER,; MEMORY_ONLY_SER_2, MEMORY_AND_DISK, MEMORY_AND_DISK_2,; MEMORY_AND_DISK_SER, MEMORY_AND_DISK_SER_2, OFF_HEAP. Returns; -------; :class:`.BlockMatrix`; Persisted block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:208,Availability,checkpoint,checkpoints,208,"""""""Matrix multiplication in situations with large inner dimension. This function splits a single matrix multiplication into `split_on_inner` smaller matrix multiplications,; does the smaller multiplications, checkpoints them with names defined by `file_name_prefix`, and adds them; together. This is useful in cases when the multiplication of two large matrices results in a much smaller matrix. Parameters; ----------; b: :class:`numpy.ndarray` or :class:`BlockMatrix`; splits: :obj:`int` (keyword only argument); The number of smaller multiplications to do.; path_prefix: :class:`str` (keyword only argument); The prefix of the path to write the block matrices to. If unspecified, writes to a tmpdir. Returns; -------; :class:`.BlockMatrix`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:24,Testability,log,logarithm,24,"""""""Element-wise natural logarithm. Returns; -------; :class:`.BlockMatrix`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:1160,Energy Efficiency,reduce,reduce,1160,"""""""Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; - **row_idx** (:py:data.`tint64`, key field) -- Row index; - **entries** (:py:class:`.tarray` of :py:data:`.tfloat64`) -- Entries for the row. Examples; --------; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters; ----------; n_partitions : int or None; Number of partitions of the table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:907,Performance,cache,cache,907,"""""""Returns a table where each row represents a row in the block matrix. The resulting table has the following fields:; - **row_idx** (:py:data.`tint64`, key field) -- Row index; - **entries** (:py:class:`.tarray` of :py:data:`.tfloat64`) -- Entries for the row. Examples; --------; >>> import numpy as np; >>> block_matrix = BlockMatrix.from_numpy(np.array([[1, 2], [3, 4], [5, 6]]), 2); >>> t = block_matrix.to_table_row_major(); >>> t.show(); +---------+---------------------+; | row_idx | entries |; +---------+---------------------+; | int64 | array<float64> |; +---------+---------------------+; | 0 | [1.00e+00,2.00e+00] |; | 1 | [3.00e+00,4.00e+00] |; | 2 | [5.00e+00,6.00e+00] |; +---------+---------------------+. Parameters; ----------; n_partitions : int or None; Number of partitions of the table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.Table`; Table where each row corresponds to a row in the block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:575,Energy Efficiency,reduce,reduce,575,"""""""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; entries are structs of a single field `element`. Parameters; ----------; n_partitions : int or None; Number of partitions of the matrix table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.MatrixTable`; Matrix table where each entry corresponds to an entry in the block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:322,Performance,cache,cache,322,"""""""Returns a matrix table with row key of `row_idx` and col key `col_idx`, whose; entries are structs of a single field `element`. Parameters; ----------; n_partitions : int or None; Number of partitions of the matrix table.; maximum_cache_memory_in_bytes : int or None; The amount of memory to reserve, per partition, to cache rows of the; matrix in memory. This value must be at least large enough to hold; one row of the matrix in memory. If this value is exactly the size of; one row, then a partition makes a network request for every row of; every block. Larger values reduce the number of network requests. If; memory permits, setting this value to the size of one output; partition permits one network request per block per partition. Notes; -----; Does not support block-sparse matrices. Returns; -------; :class:`.MatrixTable`; Matrix table where each entry corresponds to an entry in the block matrix.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:2128,Energy Efficiency,efficient,efficient,2128,"k:: text. idx A B C; 0 1.0 0.8 0.7; 1 0.8 1.0 0.3. .. code-block:: text. idx A B C; 2 0.7 0.3 1.0. Warning; -------; The block matrix must be stored in row-major format, as results; from :meth:`.BlockMatrix.write` with ``force_row_major=True`` and from; :meth:`.BlockMatrix.write_from_entry_expr`. Otherwise,; :meth:`export` will fail. Notes; -----; The five options for `entries` are illustrated below. Full:. .. code-block:: text. 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:. .. code-block:: text. 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:. .. code-block:: text. 0.8; 0.7 0.3. Upper triangle:. .. code-block:: text. 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:. .. code-block:: text. 0.8 0.7; 0.3. The number of columns must be less than :math:`2^{31}`. The number of partitions (file shards) exported equals the ceiling; of ``n_rows / partition_size``. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data. If `parallel` is ``None``, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below. It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with Python's ``gzip.open`` and R's ``read.table``. Parameters; ----------; path_in: :class:`str`; Path to input block matrix, stored row-major on disk.; path_out: :class:`str`; Path for export.; Use extension ``.gz`` for gzip or ``.bgz`` for block gzip.; delimiter: :class:`str`; Column delimiter.; header: :class:`str`, optional; If provided, `header` is prepended before the first row of data.; add_index: :obj:`bool`; If ``True``, add an initial column with the absolute row index.; parallel: :class",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py:2256,Energy Efficiency,reduce,reduces,2256,"he block matrix must be stored in row-major format, as results; from :meth:`.BlockMatrix.write` with ``force_row_major=True`` and from; :meth:`.BlockMatrix.write_from_entry_expr`. Otherwise,; :meth:`export` will fail. Notes; -----; The five options for `entries` are illustrated below. Full:. .. code-block:: text. 1.0 0.8 0.7; 0.8 1.0 0.3; 0.7 0.3 1.0. Lower triangle:. .. code-block:: text. 1.0; 0.8 1.0; 0.7 0.3 1.0. Strict lower triangle:. .. code-block:: text. 0.8; 0.7 0.3. Upper triangle:. .. code-block:: text. 1.0 0.8 0.7; 1.0 0.3; 1.0. Strict upper triangle:. .. code-block:: text. 0.8 0.7; 0.3. The number of columns must be less than :math:`2^{31}`. The number of partitions (file shards) exported equals the ceiling; of ``n_rows / partition_size``. By default, there is one partition; per row of blocks in the block matrix. The number of partitions; should be at least the number of cores for efficient parallelism.; Setting the partition size to an exact (rather than approximate); divisor or multiple of the block size reduces superfluous shuffling; of data. If `parallel` is ``None``, these file shards are then serially; concatenated by one core into one file, a slow process. See; other options below. It is highly recommended to export large files with a ``.bgz`` extension,; which will use a block gzipped compression codec. These files can be; read natively with Python's ``gzip.open`` and R's ``read.table``. Parameters; ----------; path_in: :class:`str`; Path to input block matrix, stored row-major on disk.; path_out: :class:`str`; Path for export.; Use extension ``.gz`` for gzip or ``.bgz`` for block gzip.; delimiter: :class:`str`; Column delimiter.; header: :class:`str`, optional; If provided, `header` is prepended before the first row of data.; add_index: :obj:`bool`; If ``True``, add an initial column with the absolute row index.; parallel: :class:`str`, optional; If ``'header_per_shard'``, create a folder with one file per; partition, each with a header if provid",MatchSource.CODE_COMMENT,hail/python/hail/linalg/blockmatrix.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/linalg/blockmatrix.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/family_methods.py:29,Availability,error,error,29,"# this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het",MatchSource.CODE_COMMENT,hail/python/hail/methods/family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/family_methods.py:104,Modifiability,config,config,104,"# this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het",MatchSource.CODE_COMMENT,hail/python/hail/methods/family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/family_methods.py:70,Safety,avoid,avoids,70,"# this filter removes mendel error of het father in x_nonpar. It also avoids; # building and looking up config in common case that neither parent is het",MatchSource.CODE_COMMENT,hail/python/hail/methods/family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:3231,Availability,down,downstream,3231,"om their corresponding Hail types. To output a desired; Description, Number, and/or Type value in a FORMAT or INFO field or to; specify FILTER lines, use the `metadata` parameter to supply a dictionary; with the relevant information. See; :func:`get_vcf_metadata` for how to obtain the; dictionary corresponding to the original VCF, and for info on how this; dictionary should be structured. The output VCF header will also contain CONTIG lines; with ID, length, and assembly fields derived from the reference genome of; the dataset. The output VCF header will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, re",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:3815,Deployability,pipeline,pipeline,3815,"will `not` contain lines added by external tools; (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface ",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:4770,Integrability,interface,interface,4770," (such as bcftools and GATK) unless they are explicitly inserted using the; `append_to_header` parameter. Warning; -------. INFO fields stored at VCF import are `not` automatically modified to; reflect filtering of samples or genotypes, which can affect the value of; AC (allele count), AF (allele frequency), AN (allele number), etc. If a; filtered dataset is exported to VCF without updating `info`, downstream; tools which may produce erroneous results. The solution is to create new; fields in `info` or overwrite existing fields. For example, in order to; produce an accurate `AC` field, one can run :func:`.variant_qc` and copy; the `variant_qc.AC` field to `info.AC` as shown below. >>> ds = dataset.filter_entries(dataset.GQ >= 20); >>> ds = hl.variant_qc(ds); >>> ds = ds.annotate_rows(info = ds.info.annotate(AC=ds.variant_qc.AC)) # doctest: +SKIP; >>> hl.export_vcf(ds, 'output/example.vcf.bgz'). Warning; -------; Do not export to a path that is being read from in the same pipeline. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; output : :class:`str`; Path of .vcf or .vcf.bgz file to write.; append_to_header : :class:`str`, optional; Path of file to append to VCF header.; parallel : :class:`str`, optional; If ``'header_per_shard'``, return a set of VCF files (one per; partition) rather than serially concatenating these files. If; ``'separate_header'``, return a separate VCF header file and a set of; VCF files (one per partition) without the header. If ``None``,; concatenate the header and all partitions into one VCF file.; metadata : :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`dict` [:obj:`str`, :obj:`str`]]], optional; Dictionary with information to fill in the VCF header. See; :func:`get_vcf_metadata` for how this; dictionary should be structured.; tabix : :obj:`bool`, optional; If true, writes a tabix index for the output VCF.; **Note**: This feature is experimental, and the interface and defaults; may change in future versions.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:868,Modifiability,parameteriz,parameterized,868,"""""""Import a locus interval list as a :class:`.Table`. Examples; --------. Add the row field `capture_region` indicating inclusion in; at least one locus interval from `capture_intervals.txt`:. >>> intervals = hl.import_locus_intervals('data/capture_intervals.txt', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(capture_region = hl.is_defined(intervals[dataset.locus])). Notes; -----. Hail expects an interval file to contain either one, three or five fields; per line in the following formats:. - ``contig:start-end``; - ``contig start end`` (tab-separated); - ``contig start end direction target`` (tab-separated). A file in either of the first two formats produces a table with one; field:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :obj:`.tstr` and `position` with type :py:data:`.tint32`. A file in the third format (with a ""target"" column) produces a table with two; fields:. - **interval** (:class:`.tinterval`) - Row key. Same schema as above.; - **target** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_i",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:2267,Performance,load,loaded,2267,"es a table with one; field:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :obj:`.tstr` and `position` with type :py:data:`.tint32`. A file in the third format (with a ""target"" column) produces a table with two; fields:. - **interval** (:class:`.tinterval`) - Row key. Same schema as above.; - **target** (:py:data:`.tstr`). If `reference_genome` is defined **AND** the file has one field, intervals; are parsed with :func:`.parse_locus_interval`. See the documentation for; valid inputs. If `reference_genome` is **NOT** defined and the file has one field,; intervals are parsed with the regex ```""([^:]*):(\\d+)\\-(\\d+)""``; where contig, start, and end match each of the three capture groups.; ``start`` and ``end`` match positions inclusively, e.g.; ``start <= position <= end``. For files with three or five fields, ``start`` and ``end`` match positions; inclusively, e.g. ``start <= position <= end``. Parameters; ----------; path : :class:`str`; Path to file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in file to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid; arguments here except: `no_header`, `comment`, `impute`, and; `types`, as these are used by :func:`import_locus_intervals`. Returns; -------; :class:`.Table`; Interval-keyed table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:1119,Modifiability,parameteriz,parameterized,1119,"rack name=""BedTest""; 20 1 14000000; 20 17000000 18000000; ... $ cat file2.bed; track name=""BedTest""; 20 1 14000000 cnv1; 20 17000000 18000000 cnv2; ... Add the row field `cnv_region` indicating inclusion in; at least one interval of the three-column BED file:. >>> bed = hl.import_bed('data/file1.bed', reference_genome='GRCh37'); >>> result = dataset.annotate_rows(cnv_region = hl.is_defined(bed[dataset.locus])). Add a row field `cnv_id` with the value given by the; fourth column of a BED file:. >>> bed = hl.import_bed('data/file2.bed'); >>> result = dataset.annotate_rows(cnv_id = bed[dataset.locus].target). Notes; -----. The table produced by this method has one of two possible structures. If; the .bed file has only three fields (`chrom`, `chromStart`, and; `chromEnd`), then the produced table has only one column:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :py:data:`.tstr` and `position` with type :py:data:`.tint32`. If the .bed file has four or more columns, then Hail will store the fourth; column as a row field in the table:. - *interval* (:class:`.tinterval`) - Row key. Genomic interval. Same schema as above.; - *target* (:py:data:`.tstr`) - Fourth column of .bed file. `UCSC bed files <https://genome.ucsc.edu/FAQ/FAQformat.html#format1>`__ can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; -------; Intervals in UCSC BED files are 0-indexed and half open.; The line ""5 100 105"" correpsonds to the interval ``[5:101-5:106)`` in Hail's; 1-indexed notation. Details; `here <http://genome.ucsc.edu/blog/the-ucsc-genome-browser-coordinate-counting-systems/>`__. Parameters; ----------; path : :class:`str`; Path to .bed file.; reference_genome : :class:`str` or :c",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:2475,Performance,load,loaded,2475," `chromEnd`), then the produced table has only one column:. - **interval** (:class:`.tinterval`) - Row key. Genomic interval. If; `reference_genome` is defined, the point type of the interval will be; :class:`.tlocus` parameterized by the `reference_genome`. Otherwise,; the point type is a :class:`.tstruct` with two fields: `contig` with; type :py:data:`.tstr` and `position` with type :py:data:`.tint32`. If the .bed file has four or more columns, then Hail will store the fourth; column as a row field in the table:. - *interval* (:class:`.tinterval`) - Row key. Genomic interval. Same schema as above.; - *target* (:py:data:`.tstr`) - Fourth column of .bed file. `UCSC bed files <https://genome.ucsc.edu/FAQ/FAQformat.html#format1>`__ can; have up to 12 fields, but Hail will only ever look at the first four. Hail; ignores header lines in BED files. Warning; -------; Intervals in UCSC BED files are 0-indexed and half open.; The line ""5 100 105"" correpsonds to the interval ``[5:101-5:106)`` in Hail's; 1-indexed notation. Details; `here <http://genome.ucsc.edu/blog/the-ucsc-genome-browser-coordinate-counting-systems/>`__. Parameters; ----------; path : :class:`str`; Path to .bed file.; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; skip_invalid_intervals : :obj:`bool`; If ``True`` and `reference_genome` is not ``None``, skip lines with; intervals that are not consistent with the reference genome.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`); Mapping from contig name in BED to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; **kwargs; Additional optional arguments to :func:`import_table` are valid arguments here except:; `no_header`, `delimiter`, `impute`, `skip_blank_lines`, `types`, and `comment` as these; are used by import_bed. Returns; -------; :class:`.Table`; Interval-keyed table.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:582,Availability,error,error,582,"""""""Import a PLINK FAM file into a :class:`.Table`. Examples; --------. Import a tab-separated; `FAM file <https://www.cog-genomics.org/plink2/formats#fam>`__; with a case-control phenotype:. >>> fam_kt = hl.import_fam('data/case_control_study.fam'). Import a FAM file with a quantitative phenotype:. >>> fam_kt = hl.import_fam('data/quantitative_study.fam', quant_pheno=True). Notes; -----. In Hail, unlike PLINK, the user must *explicitly* distinguish between; case-control and quantitative phenotypes. Importing a quantitative; phenotype with ``quant_pheno=False`` will return an error; (unless all values happen to be `0`, `1`, `2`, or `-9`):. The resulting :class:`.Table` will have fields, types, and values that are interpreted as missing. - *fam_id* (:py:data:`.tstr`) -- Family ID (missing = ""0""); - *id* (:py:data:`.tstr`) -- Sample ID (key column); - *pat_id* (:py:data:`.tstr`) -- Paternal ID (missing = ""0""); - *mat_id* (:py:data:`.tstr`) -- Maternal ID (missing = ""0""); - *is_female* (:py:data:`.tstr`) -- Sex (missing = ""NA"", ""-9"", ""0""). One of:. - *is_case* (:py:data:`.tbool`) -- Case-control phenotype (missing = ""0"", ""-9"",; non-numeric or the ``missing`` argument, if given.; - *quant_pheno* (:py:data:`.tfloat64`) -- Quantitative phenotype (missing = ""NA"" or; the ``missing`` argument, if given. Warning; -------; Hail will interpret the value ""-9"" as a valid quantitative phenotype, which; differs from default PLINK behavior. Use ``missing='-9'`` to interpret this; value as missing. Parameters; ----------; path : :class:`str`; Path to FAM file.; quant_pheno : :obj:`bool`; If ``True``, phenotype is interpreted as quantitative.; delimiter : :class:`str`; Field delimiter regex.; missing : :class:`str`; The string used to denote missing values. For case-control, 0, -9, and; non-numeric are also treated as missing. Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:2914,Modifiability,parameteriz,parameterized,2914,"op Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for `entry_fields`, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the greatest probab",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:1875,Performance,load,load,1875,"y('v'); >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants.v). Load a set of variants specified by a table keyed by 'locus' and 'alleles' from a BGEN file:. >>> ds_result = hl.import_bgen(""data/example.8bits.bgen"",; ... entry_fields=['dosage'],; ... sample_file=""data/example.8bits.sample"",; ... variants=variants_table). Notes; -----. Hail supports importing data from v1.2 of the `BGEN file format; <http://www.well.ox.ac.uk/~gav/bgen_format/bgen_format.html>`__.; Genotypes must be **unphased** and **diploid**, genotype; probabilities must be stored with 8 bits, and genotype probability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index file, which can be generated; with :func:`.index_bgen`. All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` para",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:2581,Performance,perform,performance,2581,"ability; blocks must be compressed with zlib or uncompressed. All variants; must be bi-allelic. Each BGEN file must have a corresponding index file, which can be generated; with :func:`.index_bgen`. All files must have been indexed with the same; reference genome. To load multiple files at the same time,; use :ref:`Hadoop Glob Patterns <sec-hadoop-glob>`. If n_partitions and block_size are both specified, block_size is; used. If neither are specified, the default is a 128MB block; size. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file if given. Otherwise, the sample; ID is taken from the sample identifying block in the first BGEN file if it; exists; else IDs are assigned from `_0`, `_1`, to `_N`. **Row Fields**. Between two and four row fields are created. The `locus` and `alleles` are; always included. `_row_fields` determines if `varid` and `rsid` are also; included. For best performance, only include fields necessary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:3587,Performance,perform,performance,3587,"ssary for your; analysis. NOTE: the `_row_fields` parameter is considered an experimental; feature and may be removed without warning. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The chromosome; and position. If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference; allele is the first element in the array.; - `varid` (:py:data:`.tstr`) -- The variant identifier. The third field in; each variant identifying block.; - `rsid` (:py:data:`.tstr`) -- The rsID for the variant. The fifth field in; each variant identifying block. **Entry Fields**. Up to three entry fields are created, as determined by; `entry_fields`. For best performance, include precisely those; fields required for your analysis. It is also possible to pass an; empty tuple or list for `entry_fields`, which can greatly; accelerate processing speed if your workflow does not use the; genotype data. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the greatest probability. If there is not a unique maximum probability, the; hard call is set to missing.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the BGEN file spec. For bi-allelic variants, the array has; three elements giving the probabilities of homozygous reference,; heterozygous, and homozygous alternate genotype, in that order.; - `dosage` (:py:data:`.tfloat64`) -- The expected value of the number of; alternate alleles, given by the probability of heterozygous genotype plus; twice the probability of homozygous alternate genotype. All variants must; be bi-allelic. See Also; --------; :func:`.index_bgen`. Parameters; ----------; path : :class:`s",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:2128,Availability,toler,tolerance,2128,"ence_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the `tolerance`; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input ``[0.98, 0.0, 0.0]`` will be normalized to; ``[1.0, 0.0, 0.0]``. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; GEN files to import.; sample_file : :class:`str`; Sample file to import.; tolerance : :obj:`float`; If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions : :obj:`int`, optional; Number of partitions.; chromosome : :class:`str`, optional; Chromosome if not included in the GEN file; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:2456,Availability,toler,tolerance,2456,"ta:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the `tolerance`; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input ``[0.98, 0.0, 0.0]`` will be normalized to; ``[1.0, 0.0, 0.0]``. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; GEN files to import.; sample_file : :class:`str`; Sample file to import.; tolerance : :obj:`float`; If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions : :obj:`int`, optional; Number of partitions.; chromosome : :class:`str`, optional; Chromosome if not included in the GEN file; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:2572,Availability,toler,tolerance,2572,"ta:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the probabilities is a distance greater than the `tolerance`; parameter from 1.0. Otherwise, the probabilities are normalized to sum to; 1.0. For example, the input ``[0.98, 0.0, 0.0]`` will be normalized to; ``[1.0, 0.0, 0.0]``. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; GEN files to import.; sample_file : :class:`str`; Sample file to import.; tolerance : :obj:`float`; If the sum of the genotype probabilities for a genotype differ from 1.0; by more than the tolerance, set the genotype to missing.; min_partitions : :obj:`int`, optional; Number of partitions.; chromosome : :class:`str`, optional; Chromosome if not included in the GEN file; reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by `reference_genome`.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:1072,Modifiability,parameteriz,parameterized,1072,"port_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by `chromosome`) and position (4th column if `chromosome` is not; defined). If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities; as defined by the GEN file spec. The array is set to missing if the; sum of the prob",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:534,Performance,load,load,534,"""""""; Import GEN file(s) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_gen('data/example.gen',; ... sample_file='data/example.sample',; ... reference_genome='GRCh37'). Notes; -----. For more information on the GEN file format, see `here; <http://www.stats.ox.ac.uk/%7Emarchini/software/gwas/file_format.html#mozTocId40300>`__. If the GEN file has only 5 columns before the start of the genotype; probability data (chromosome field is missing), you must specify the; chromosome using the `chromosome` parameter. To load multiple files at the same time, use :ref:`Hadoop Glob Patterns; <sec-hadoop-glob>`. **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID imported; from the first column of the sample file. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The genomic; location consisting of the chromosome (1st column if present, otherwise; given by `chromosome`) and position (4th column if `chromosome` is not; defined). If `reference_genome` is defined, the type will be; :class:`.tlocus` parameterized by `reference_genome`. Otherwise, the type; will be a :class:`.tstruct` with two fields: `contig` with type; :py:data:`.tstr` and `position` with type :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (4th column if; `chromosome` is not defined) is the first element of the array and the; alternate allele (5th column if `chromosome` is not defined) is the second; element.; - `varid` (:py:data:`.tstr`) -- The variant identifier. 2nd column of GEN; file if chromosome present, otherwise 1st column.; - `rsid` (:py:data:`.tstr`) -- The rsID. 3rd column of GEN file if; chromosome present, otherwise 2nd column. **Entry Fields**. - `GT` (:py:data:`.tcall`) -- The hard call corresponding to the genotype with; the highest probability.; - `GP` (:class:`.tarray` of :py:data:`.tfloat64`) -- Genotype probabilities;",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:5526,Availability,error,error,5526,"Files to import.; key : :class:`str` or :obj:`list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter : :class:`str`; Field delimiter regex.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_fi",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:5599,Performance,load,load,5599,"s : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter : :class:`str`; Field delimiter regex.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:6362,Performance,load,load,6362,"me the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; delimiter : :class:`str`; Field delimiter regex.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:628,Performance,load,load,628,"""""""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:928,Performance,load,load,928,"""""""Import lines of file(s) as a :class:`.Table` of strings. Examples; --------. To import a file as a table of strings:. >>> ht = hl.import_lines('data/matrix2.tsv'); >>> ht.describe(); ----------------------------------------; Global fields:; None; ----------------------------------------; Row fields:; 'file': str; 'text': str; ----------------------------------------; Key: []; ----------------------------------------. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; file_per_partition : :obj:`bool`; If ``True``, each file will be in a seperate partition. Not recommended; for most uses. Error thrown if ``True`` and `min_partitions` is less than; the number of files. Returns; -------; :class:`.Table`; Table constructed from imported data.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:4536,Performance,load,load,4536,"types must be specified; for all columns that should be imported as row fields. (The other columns are; imported as entries in the matrix.). The header information for row fields is allowed to be missing, if the; column IDs are present, but the header must then consist only of tab-delimited; column IDs (no row field names). The column IDs will never be missing, even if the `missing` string appears; in the column IDs. Parameters; ----------; paths: :class:`str` or :obj:`list` of :obj:`str`; Files to import.; row_fields: :obj:`dict` of :class:`str` to :class:`.HailType`; Columns to take as row fields in the MatrixTable. They must be located; before all entry columns.; row_key: :class:`str` or :obj:`list` of :obj:`str`; Key fields(s). If empty, creates an index `row_id` to use as key.; entry_type: :class:`.HailType`; Type of entries in matrix table. Must be one of: :py:data:`.tint32`,; :py:data:`.tint64`, :py:data:`.tfloat32`, :py:data:`.tfloat64`, or; :py:data:`.tstr`. Default: :py:data:`.tint32`.; missing: :class:`str`; Identifier to be treated as missing. Default: NA; min_partitions: :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header: :obj:`bool`; If ``True``, assume the file has no header and name the row fields `f0`,; `f1`, ... `fK` (0-indexed) and the column keys 0, 1, ... N.; force_bgz : :obj:`bool`; If ``True``, load **.gz** files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec.; sep : :class:`str`; This parameter is a deprecated name for `delimiter`, please use that; instead.; delimiter : :class:`str`; A single character string which separates values in the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list. Returns; -------; :class:`.MatrixTable`; MatrixTable constructed from imported data.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:3577,Integrability,depend,dependent,3577," the FAM file. Only; present if `quant_pheno` equals True. Set to missing if value equals; `missing`. * Entry fields:. * `GT` (:py:data:`.tcall`) -- Genotype call (diploid, unphased). Warning; -------; Hail will interpret the value ""-9"" as a valid quantitative phenotype, which; differs from default PLINK behavior. Use ``missing='-9'`` to interpret this; value as missing. Parameters; ----------; bed : :class:`str`; PLINK BED file. bim : :class:`str`; PLINK BIM file. fam : :class:`str`; PLINK FAM file. min_partitions : :obj:`int`, optional; Minimum number of partitions. Useful in conjunction with `block_size`. missing : :class:`str`; String used to denote missing values **only** for the phenotype field.; This is in addition to ""-9"", ""0"", and ""N/A"" for case-control; phenotypes. delimiter : :class:`str`; FAM file field delimiter regex. quant_pheno : :obj:`bool`; If ``True``, FAM phenotype is interpreted as quantitative. a2_reference : :obj:`bool`; If ``True``, A2 is treated as the reference allele. If False, A1 is treated; as the reference allele. reference_genome : :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use. contig_recoding : :obj:`dict` of :class:`str` to :obj:`str`, optional; Dict of old contig name to new contig name. The new contig name must be; in the reference genome given by ``reference_genome``. If ``None``, the; default is dependent on the ``reference_genome``. For ""GRCh37"", the default; is ``{'23': 'X', '24': 'Y', '25': 'X', '26': 'MT'}``. For ""GRCh38"", the; default is ``{'1': 'chr1', ..., '22': 'chr22', '23': 'chrX', '24': 'chrY', '25': 'chrX', '26': 'chrM'}``. skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`. n_partitions : :obj:`int`, optional; Number of partitions. If both `n_partitions` and `block_size`; are specified, `n_partitions` will be used. block_size : :obj:`int`, optional; Block size, in MB. Default: 128MB blocks. Returns; -------; :class:`.MatrixTable`. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:779,Modifiability,parameteriz,parameterized,779,"""""""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_reference` is ``True``) is the first element in the array.; * `rsid` (:py:data:`.tstr`) -- Column 2 in the BIM file.; * `cm_position` (:py:data:`.tfloat64`) -- Column 3 in the BIM file,; the position in centimorgans. * Column fields:. * `s` (:py:data:`.tstr`) -- Column 2 in the Fam file (key field).; * `fam_id` (:py:data:`.tstr`) -- Column 1 in the FAM file. Set to; missing if ID equals ""0"".; * `pat_id` (:py:data:`.tstr`) -- Column 3 in the FAM file. Set to; missing if ID equals ""0"".; * `mat_id` (:py:data:`.tstr`) -- Column 4 in the FAM file. Set to; missing if ID equals ""0"".; * `is_female` (:py:data:`.tstr`) -- Column 5 in the FAM file. Set to; missing if value equals ""-9"", ""0"", or ""N/A"". Set to true if value; equals ""2"". Set to false if value equals ""1"".; * `is_case` (:py:data:`.tbool`) -- Column 6 in the FAM file. Only; present if `quant_pheno` equals False. Set to missing i",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:125,Testability,test,test,125,"""""""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_reference` is ``True``) is the first element in the array.; * `rsid` (:py:data:`.tstr`) -- Column 2 in the BIM file.; * `cm_position` (:py:data:`.tfloat64`) -- Column 3 in the BIM file,; the position in centimorgans. * Column fields:. * `s` (:py:data:`.tstr`) -- Column 2 in the Fam file (key field).; * `fam_id` (:py:data:`.tstr`) -- Column 1 in the FAM file. Set to; missing if ID equals ""0"".; * `pat_id` (:py:data:`.tstr`) -- Column 3 in the FAM file. Set to; missing if ID equals ""0"".; * `mat_id` (:py:data:`.tstr`) -- Column 4 in the FAM file. Set to; missing if ID equals ""0"".; * `is_female` (:py:data:`.tstr`) -- Column 5 in the FAM file. Set to; missing if value equals ""-9"", ""0"", or ""N/A"". Set to true if value; equals ""2"". Set to false if value equals ""1"".; * `is_case` (:py:data:`.tbool`) -- Column 6 in the FAM file. Only; present if `quant_pheno` equals False. Set to missing i",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:151,Testability,test,test,151,"""""""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_reference` is ``True``) is the first element in the array.; * `rsid` (:py:data:`.tstr`) -- Column 2 in the BIM file.; * `cm_position` (:py:data:`.tfloat64`) -- Column 3 in the BIM file,; the position in centimorgans. * Column fields:. * `s` (:py:data:`.tstr`) -- Column 2 in the Fam file (key field).; * `fam_id` (:py:data:`.tstr`) -- Column 1 in the FAM file. Set to; missing if ID equals ""0"".; * `pat_id` (:py:data:`.tstr`) -- Column 3 in the FAM file. Set to; missing if ID equals ""0"".; * `mat_id` (:py:data:`.tstr`) -- Column 4 in the FAM file. Set to; missing if ID equals ""0"".; * `is_female` (:py:data:`.tstr`) -- Column 5 in the FAM file. Set to; missing if value equals ""-9"", ""0"", or ""N/A"". Set to true if value; equals ""2"". Set to false if value equals ""1"".; * `is_case` (:py:data:`.tbool`) -- Column 6 in the FAM file. Only; present if `quant_pheno` equals False. Set to missing i",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:177,Testability,test,test,177,"""""""Import a PLINK dataset (BED, BIM, FAM) as a :class:`.MatrixTable`. Examples; --------. >>> ds = hl.import_plink(bed='data/test.bed',; ... bim='data/test.bim',; ... fam='data/test.fam',; ... reference_genome='GRCh37'). Notes; -----. Only binary SNP-major mode files can be read into Hail. To convert your; file from individual-major mode to SNP-major mode, use PLINK to read in; your fileset and use the ``--make-bed`` option. Hail uses the individual ID (column 2 in FAM file) as the sample id (`s`).; The individual IDs must be unique. The resulting :class:`.MatrixTable` has the following fields:. * Row fields:. * `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome and position. If `reference_genome` is defined, the type; will be :class:`.tlocus` parameterized by `reference_genome`.; Otherwise, the type will be a :class:`.tstruct` with two fields:; `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; * `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An; array containing the alleles of the variant. The reference allele (A2; if `a2_reference` is ``True``) is the first element in the array.; * `rsid` (:py:data:`.tstr`) -- Column 2 in the BIM file.; * `cm_position` (:py:data:`.tfloat64`) -- Column 3 in the BIM file,; the position in centimorgans. * Column fields:. * `s` (:py:data:`.tstr`) -- Column 2 in the Fam file (key field).; * `fam_id` (:py:data:`.tstr`) -- Column 1 in the FAM file. Set to; missing if ID equals ""0"".; * `pat_id` (:py:data:`.tstr`) -- Column 3 in the FAM file. Set to; missing if ID equals ""0"".; * `mat_id` (:py:data:`.tstr`) -- Column 4 in the FAM file. Set to; missing if ID equals ""0"".; * `is_female` (:py:data:`.tstr`) -- Column 5 in the FAM file. Set to; missing if value equals ""-9"", ""0"", or ""N/A"". Set to true if value; equals ""2"". Set to false if value equals ""1"".; * `is_case` (:py:data:`.tbool`) -- Column 6 in the FAM file. Only; present if `quant_pheno` equals False. Set to missing i",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:7025,Availability,down,downstream,7025,"ant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for ",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:5530,Modifiability,parameteriz,parameterized,5530," the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. .. note::. Using the **FILTER** field:. The information in the FILTER field of a VCF is contained in the; ``filters`` row field. This annotation is a ``set<str>`` and can be; queried for filter membership with expressions like; ``ds.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged; as ""PASS"" will have no filters applied; for these variants,; ``hl.len(ds.filters)`` is ``0``. Thus, filtering to PASS variants; can be done with :meth:`.MatrixTable.filter_rows` as follows:. >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome (CHROM field) and position (POS field). If `reference_genome`; is defined, the type will be :class:`.tlocus` parameterized by; `reference_genome`. Otherwise, the type will be a :class:`.tstruct` with; two fields: `contig` with type :py:data:`.tstr` and `position` with type; :py:data:`.tint32`.; - `alleles` (:class:`.tarray` of :py:data:`.tstr`) -- Row key. An array; containing the alleles of the variant. The reference allele (REF field) is; the first element in the array and the alternate alleles (ALT field) are; the subsequent elements.; - `filters` (:class:`.tset` of :py:data:`.tstr`) -- Set containing all filters applied to a; variant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in t",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:3537,Performance,load,load,3537,"+------------+------+---------+----------+--------------+; <BLANKLINE>; +------------------+----------------+----------------+--------------+; | info.B | info.C | info.D | 'SAMPLE1'.GT |; +------------------+----------------+----------------+--------------+; | array<float64> | array<float64> | array<float64> | call |; +------------------+----------------+----------------+--------------+; | [NA,2.00e+00,NA] | NA | NA | 0/0 |; +------------------+----------------+----------------+--------------+; <BLANKLINE>; +--------------+--------------+--------------+; | 'SAMPLE1'.X | 'SAMPLE1'.Y | 'SAMPLE1'.Z |; +--------------+--------------+--------------+; | array<int32> | array<int32> | array<int32> |; +--------------+--------------+--------------+; | [1,NA,1] | NA | NA |; +--------------+--------------+--------------+. Notes; -----. Hail is designed to be maximally compatible with files in the `VCF v4.2; spec <https://samtools.github.io/hts-specs/VCFv4.2.pdf>`__. :func:`.import_vcf` takes a list of VCF files to load. All files must have; the same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as :ref:`Hadoop glob; patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (**.vcf**) or block compressed (**.vcf.bgz**). If you; have a large compressed VCF that ends in **.vcf.gz**, it is likely that the; file is actually block-compressed, and you should rename the file to; **.vcf.bgz** accordingly. If you are unable to rename this file, please use; `force_bgz=True` to ignore the extension and treat this file as; block-gzipped. If you have a **non-block** (aka standard) gzipped file, you may use; `force=True`; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset. :func:`.import_vcf` does not perform deduplication - if the provided VCF(s); cont",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:4467,Performance,perform,perform,4467,"e same header and the same set of samples in the same order (e.g., a; dataset split by chromosome). Files can be specified as :ref:`Hadoop glob; patterns <sec-hadoop-glob>`. Ensure that the VCF file is correctly prepared for import: VCFs should; either be uncompressed (**.vcf**) or block compressed (**.vcf.bgz**). If you; have a large compressed VCF that ends in **.vcf.gz**, it is likely that the; file is actually block-compressed, and you should rename the file to; **.vcf.bgz** accordingly. If you are unable to rename this file, please use; `force_bgz=True` to ignore the extension and treat this file as; block-gzipped. If you have a **non-block** (aka standard) gzipped file, you may use; `force=True`; however, we strongly discourage this because each file will be; processed by a single core. Import will take significantly longer for any; non-trivial dataset. :func:`.import_vcf` does not perform deduplication - if the provided VCF(s); contain multiple records with the same chrom, pos, ref, alt, all these; records will be imported as-is (in multiple rows) and will not be collapsed; into a single variant. .. note::. Using the **FILTER** field:. The information in the FILTER field of a VCF is contained in the; ``filters`` row field. This annotation is a ``set<str>`` and can be; queried for filter membership with expressions like; ``ds.filters.contains(""VQSRTranche99.5..."")``. Variants that are flagged; as ""PASS"" will have no filters applied; for these variants,; ``hl.len(ds.filters)`` is ``0``. Thus, filtering to PASS variants; can be done with :meth:`.MatrixTable.filter_rows` as follows:. >>> pass_ds = dataset.filter_rows(hl.len(dataset.filters) == 0). **Column Fields**. - `s` (:py:data:`.tstr`) -- Column key. This is the sample ID. **Row Fields**. - `locus` (:class:`.tlocus` or :class:`.tstruct`) -- Row key. The; chromosome (CHROM field) and position (POS field). If `reference_genome`; is defined, the type will be :class:`.tlocus` parameterized by; `reference_genome`",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:6989,Performance,load,load,6989,"s` (:class:`.tset` of :py:data:`.tstr`) -- Set containing all filters applied to a; variant.; - `rsid` (:py:data:`.tstr`) -- rsID of the variant.; - `qual` (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:7146,Performance,load,load,7146," (:py:data:`.tfloat64`) -- Floating-point number in the QUAL field.; - `info` (:class:`.tstruct`) -- All INFO fields defined in the VCF header; can be found in the struct `info`. Data types match the type specified; in the VCF header, and if the declared ``Number`` is not 1, the result; will be stored as an array. **Entry Fields**. :func:`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_e",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:7495,Performance,load,load,7495,"`.import_vcf` generates an entry field for each FORMAT field declared; in the VCF header. The types of these fields are generated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:7585,Performance,load,load,7585,"ated according to the; same rules as INFO fields, with one difference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:7683,Performance,load,load,7683,"ference -- ""GT"" and other fields; specified in `call_fields` will be read as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:7719,Performance,load,loaded,7719,"d as :py:data:`.tcall`. Parameters; ----------; path : :class:`str` or :obj:`list` of :obj:`str`; One or more paths to VCF files to read. Each path may or may not include glob expressions; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType`; Type of floating point entries in matrix table. Must be one of:; :py:",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:7969,Performance,load,loaded,7969,"s; like ``*``, ``?``, or ``[abc123]``.; force : :obj:`bool`; If ``True``, load **.vcf.gz** files serially. No downstream operations; can be parallelized, so this mode is strongly discouraged.; force_bgz : :obj:`bool`; If ``True``, load **.vcf.gz** files as blocked gzip files, assuming that they were actually; compressed using the BGZ codec.; header_file : :class:`str`, optional; Optional header override file. If not specified, the first file in; `path` is used. Glob patterns are not allowed in the `header_file`.; min_partitions : :obj:`int`, optional; Minimum partitions to load per file.; drop_samples : :obj:`bool`; If ``True``, create sites-only dataset. Don't load sample IDs or; entries.; call_fields : :obj:`list` of :class:`str`; List of FORMAT fields to load as :py:data:`.tcall`. ""GT"" is; loaded as a call automatically.; reference_genome: :class:`str` or :class:`.ReferenceGenome`, optional; Reference genome to use.; contig_recoding: :obj:`dict` of (:class:`str`, :obj:`str`), optional; Mapping from contig name in VCF to contig name in loaded dataset.; All contigs must be present in the `reference_genome`, so this is; useful for mapping differently-formatted data onto known references.; array_elements_required : :obj:`bool`; If ``True``, all elements in an array field must be present. Set this; parameter to ``False`` for Hail to allow array fields with missing; values such as ``1,.,5``. In this case, the second element will be; missing. However, in the case of a single missing element ``.``, the; entire field will be missing and **not** an array with one missing; element.; skip_invalid_loci : :obj:`bool`; If ``True``, skip loci that are not consistent with `reference_genome`.; entry_float_type: :class:`.HailType`; Type of floating point entries in matrix table. Must be one of:; :py:data:`.tfloat32` or :py:data:`.tfloat64`. Default:; :py:data:`.tfloat64`.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:548,Deployability,configurat,configuration,548,"""""""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:453,Modifiability,config,config,453,"""""""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:548,Modifiability,config,configuration,548,"""""""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:402,Safety,avoid,avoid,402,"""""""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:510,Usability,guid,guide,510,"""""""Export a :class:`.Table` to Elasticsearch. By default, this method supports Elasticsearch versions 6.8.x - 7.x.x. Older versions of elasticsearch will require; recompiling hail. .. warning::; :func:`.export_elasticsearch` is EXPERIMENTAL. .. note::; Table rows may be exported more than once. For example, if a task has to be retried after being preempted; midway through processing a partition. To avoid duplicate documents in Elasticsearch, use a `config` with the; `es.mapping.id <https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-mapping>`__; option set to a field that contains a unique value for each row.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:9,Deployability,patch,patch,9,"# monkey patch DataFileReader.determine_file_length to account for bug in Google HadoopFS",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:3635,Availability,error,error,3635,"aths : :class:`str` or :obj:`list` of :obj:`str`; Files to import.; key : :class:`str` or :obj:`list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_fi",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:3708,Performance,load,load,3708,"list` of :obj:`str`; Key fields(s).; min_partitions : :obj:`int` or :obj:`None`; Minimum number of partitions.; no_header : :obj:`bool`; If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py:4471,Performance,load,load,4471,"If ``True```, assume the file has no header and name the N fields `f0`,; `f1`, ... `fN` (0-indexed).; impute : :obj:`bool`; If ``True``, Impute field types from the file.; comment : :class:`str` or :obj:`list` of :obj:`str`; Skip lines beginning with the given string if the string is a single; character. Otherwise, skip lines that match the regex specified. Multiple; comment characters or patterns should be passed as a list.; missing : :class:`str` or :obj:`list` [:obj:`str`]; Identifier(s) to be treated as missing.; types : :obj:`dict` mapping :class:`str` to :class:`.HailType`; Dictionary defining field types.; quote : :class:`str` or :obj:`None`; Quote character.; skip_blank_lines : :obj:`bool`; If ``True``, ignore empty lines. Otherwise, throw an error if an empty; line is found.; force_bgz : :obj:`bool`; If ``True``, load files as blocked gzip files, assuming; that they were actually compressed using the BGZ codec. This option is; useful when the file extension is not ``'.bgz'``, but the file is; blocked gzip, so that the file can be read in parallel and not on a; single node.; filter : :class:`str`, optional; Line filter regex. A partial match results in the line being removed; from the file. Applies before `find_replace`, if both are defined.; find_replace : (:class:`str`, :obj:`str`); Line substitution regex. Functions like ``re.sub``, but obeys the exact; semantics of Java's; `String.replaceAll <https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/lang/String.html#replaceAll(java.lang.String,java.lang.String)>`__.; force : :obj:`bool`; If ``True``, load gzipped files serially on one core. This should; be used only when absolutely necessary, as processing time will be; increased due to lack of parallelism.; source_file_field : :class:`str`, optional; If defined, the source file name for each line will be a field of the table; with this name. Can be useful when importing multiple tables using glob patterns.; Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/misc.py:607,Modifiability,parameteriz,parameterized,607,"""""""Rename duplicate column keys. .. include:: ../_templates/req_tstring.rst. Examples; --------. >>> renamed = hl.rename_duplicates(dataset).cols(); >>> duplicate_samples = (renamed.filter(renamed.s != renamed.unique_id); ... .select(); ... .collect()). Notes; -----. This method produces a new column field from the string column key by; appending a unique suffix ``_N`` as necessary. For example, if the column; key ""NA12878"" appears three times in the dataset, the first will produce; ""NA12878"", the second will produce ""NA12878_1"", and the third will produce; ""NA12878_2"". The name of this new field is parameterized by `name`. Parameters; ----------; dataset : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name of new field. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/misc.py:652,Performance,load,loaded,652,"""""""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/misc.py:737,Performance,latency,latency,737,"""""""Filter rows with a list of intervals. Examples; --------. Filter to loci falling within one interval:. >>> ds_result = hl.filter_intervals(dataset, [hl.parse_locus_interval('17:38449840-38530994')]). Remove all loci within list of intervals:. >>> intervals = [hl.parse_locus_interval(x) for x in ['1:50M-75M', '2:START-400000', '3-22']]; >>> ds_result = hl.filter_intervals(dataset, intervals, keep=False). Notes; -----; Based on the `keep` argument, this method will either restrict to points; in the supplied interval ranges, or remove all rows in those ranges. When ``keep=True``, partitions that don't overlap any supplied interval; will not be loaded at all. This enables :func:`.filter_intervals` to be; used for reasonably low-latency queries of small ranges of the dataset, even; on large datasets. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; Dataset to filter.; intervals : :class:`.ArrayExpression` of type :class:`.tinterval`; Intervals to filter on. The point type of the interval must; be a prefix of the key or equal to the first field of the key.; keep : :obj:`bool`; If ``True``, keep only rows that fall within any interval in `intervals`.; If ``False``, keep only rows that fall outside all intervals in; `intervals`. Returns; -------; :class:`.MatrixTable` or :class:`.Table`. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1114,Availability,error,error,1114,"-------. Compute sample QC metrics and remove low-quality samples:. >>> dataset = hl.sample_qc(dataset, name='sample_qc'); >>> filtered_dataset = dataset.filter_cols((dataset.sample_qc.dp_stats.mean > 20) & (dataset.sample_qc.r_ti_tv > 1.5)). Notes; -----. This method computes summary statistics per sample from a genetic matrix and stores; the results as a new column-indexed struct field in the matrix, named based on the; `name` parameter. If `mt` contains an entry field `DP` of type :py:data:`.tint32`, then the; field `dp_stats` is computed. If `mt` contains an entry field `GQ` of type; :py:data:`.tint32`, then the field `gq_stats` is computed. Both `dp_stats`; and `gq_stats` are structs with with four fields:. - `mean` (``float64``) -- Mean value.; - `stdev` (``float64``) -- Standard deviation (zero degrees of freedom).; - `min` (``int32``) -- Minimum value.; - `max` (``int32``) -- Maximum value. If the dataset does not contain an entry field `GT` of type; :py:data:`.tcall`, then an error is raised. The following fields are always; computed from `GT`:. - `call_rate` (``float64``) -- Fraction of calls not missing or filtered.; Equivalent to `n_called` divided by :meth:`.count_rows`.; - `n_called` (``int64``) -- Number of non-missing calls.; - `n_not_called` (``int64``) -- Number of missing calls.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_hom_ref` (``int64``) -- Number of homozygous reference calls.; - `n_het` (``int64``) -- Number of heterozygous calls.; - `n_hom_var` (``int64``) -- Number of homozygous alternate calls.; - `n_non_ref` (``int64``) -- Sum of `n_het` and `n_hom_var`.; - `n_snp` (``int64``) -- Number of SNP alternate alleles.; - `n_insertion` (``int64``) -- Number of insertion alternate alleles.; - `n_deletion` (``int64``) -- Number of deletion alternate alleles.; - `n_singleton` (``int64``) -- Number of private alleles. Reference alleles are never counted as singletons, even if; every other allele at a site is non-reference.; - ",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:922,Availability,error,error,922,"""""""Compute common variant statistics (quality control metrics). .. include:: ../_templates/req_tvariant.rst. Examples; --------. >>> dataset_result = hl.variant_qc(dataset). Notes; -----; This method computes variant statistics from the genotype data, returning; a new struct field `name` with the following metrics based on the fields; present in the entry schema. If `mt` contains an entry field `DP` of type :py:data:`.tint32`, then the; field `dp_stats` is computed. If `mt` contains an entry field `GQ` of type; :py:data:`.tint32`, then the field `gq_stats` is computed. Both `dp_stats`; and `gq_stats` are structs with with four fields:. - `mean` (``float64``) -- Mean value.; - `stdev` (``float64``) -- Standard deviation (zero degrees of freedom).; - `min` (``int32``) -- Minimum value.; - `max` (``int32``) -- Maximum value. If the dataset does not contain an entry field `GT` of type; :py:data:`.tcall`, then an error is raised. The following fields are always; computed from `GT`:. - `AF` (``array<float64>``) -- Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; `AC` / `AN`.; - `AC` (``array<int32>``) -- Calculated allele count, one element per; allele, including the reference. Sums to `AN`.; - `AN` (``int32``) -- Total number of called alleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference allele.; - `het_freq_hwe` (``float64``) -- Expected frequency of heterozygous; sa",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:2155,Testability,test,test,2155,"rray<float64>``) -- Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; `AC` / `AN`.; - `AC` (``array<int32>``) -- Calculated allele count, one element per; allele, including the reference. Sums to `AN`.; - `AN` (``int32``) -- Total number of called alleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference allele.; - `het_freq_hwe` (``float64``) -- Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; :func:`.functions.hardy_weinberg_test` for details.; - `p_value_hwe` (``float64``) -- p-value from two-sided test of Hardy-Weinberg; equilibrium. See :func:`.functions.hardy_weinberg_test` for details.; - `p_value_excess_het` (``float64``) -- p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See :func:`.functions.hardy_weinberg_test` for details. Warning; -------; `het_freq_hwe` and `p_value_hwe` are calculated as in; :func:`.functions.hardy_weinberg_test`, with non-diploid calls; (``ploidy != 2``) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, :func:`.variant_qc`; sets both fields to missing for multiallelic variants. Consider using; :func:`~hail.methods.split_multi` to split multi-allelic variants beforehand. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name for resulting field. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:2312,Testability,test,test,2312,"rray<float64>``) -- Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; `AC` / `AN`.; - `AC` (``array<int32>``) -- Calculated allele count, one element per; allele, including the reference. Sums to `AN`.; - `AN` (``int32``) -- Total number of called alleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference allele.; - `het_freq_hwe` (``float64``) -- Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; :func:`.functions.hardy_weinberg_test` for details.; - `p_value_hwe` (``float64``) -- p-value from two-sided test of Hardy-Weinberg; equilibrium. See :func:`.functions.hardy_weinberg_test` for details.; - `p_value_excess_het` (``float64``) -- p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See :func:`.functions.hardy_weinberg_test` for details. Warning; -------; `het_freq_hwe` and `p_value_hwe` are calculated as in; :func:`.functions.hardy_weinberg_test`, with non-diploid calls; (``ploidy != 2``) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, :func:`.variant_qc`; sets both fields to missing for multiallelic variants. Consider using; :func:`~hail.methods.split_multi` to split multi-allelic variants beforehand. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name for resulting field. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:2618,Testability,test,test,2618,"rray<float64>``) -- Calculated allele frequency, one element; per allele, including the reference. Sums to one. Equivalent to; `AC` / `AN`.; - `AC` (``array<int32>``) -- Calculated allele count, one element per; allele, including the reference. Sums to `AN`.; - `AN` (``int32``) -- Total number of called alleles.; - `homozygote_count` (``array<int32>``) -- Number of homozygotes per; allele. One element per allele, including the reference.; - `call_rate` (``float64``) -- Fraction of calls neither missing nor filtered.; Equivalent to `n_called` / :meth:`.count_cols`.; - `n_called` (``int64``) -- Number of samples with a defined `GT`.; - `n_not_called` (``int64``) -- Number of samples with a missing `GT`.; - `n_filtered` (``int64``) -- Number of filtered entries.; - `n_het` (``int64``) -- Number of heterozygous samples.; - `n_non_ref` (``int64``) -- Number of samples with at least one called; non-reference allele.; - `het_freq_hwe` (``float64``) -- Expected frequency of heterozygous; samples under Hardy-Weinberg equilibrium. See; :func:`.functions.hardy_weinberg_test` for details.; - `p_value_hwe` (``float64``) -- p-value from two-sided test of Hardy-Weinberg; equilibrium. See :func:`.functions.hardy_weinberg_test` for details.; - `p_value_excess_het` (``float64``) -- p-value from one-sided test of; Hardy-Weinberg equilibrium for excess heterozygosity.; See :func:`.functions.hardy_weinberg_test` for details. Warning; -------; `het_freq_hwe` and `p_value_hwe` are calculated as in; :func:`.functions.hardy_weinberg_test`, with non-diploid calls; (``ploidy != 2``) ignored in the counts. As this test is only; statistically rigorous in the biallelic setting, :func:`.variant_qc`; sets both fields to missing for multiallelic variants. Consider using; :func:`~hail.methods.split_multi` to split multi-allelic variants beforehand. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; name : :class:`str`; Name for resulting field. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:671,Performance,perform,performs,671,"""""""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or filtered entry); 1. No Call (missing genotype call); 2. Hom Ref; 3. Heterozygous; 4. Hom Var. The first index is the state in the left dataset and the second index is; the state in the right dataset. Typical uses of the summary list are shown; below. >>> summary, samples, variants = hl.concordance(datas",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:832,Performance,perform,performs,832,"""""""Calculate call concordance with another dataset. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. .. include:: ../_templates/req_unphased_diploid_gt.rst. Examples; --------. Compute concordance between two datasets and output the global concordance; statistics and two tables with concordance computed per column key and per; row key:. >>> global_conc, cols_conc, rows_conc = hl.concordance(dataset, dataset2). Notes; -----. This method computes the genotype call concordance (from the entry; field **GT**) between two biallelic variant datasets. It requires; unique sample IDs and performs an inner join on samples (only; samples in both datasets will be considered). In addition, all genotype; calls must be **diploid** and **unphased**. It performs an ordered zip join of the variants. That means the; variants of each dataset are sorted, with duplicate variants; appearing in some random relative order, and then zipped together.; When a variant appears a different number of times between the two; datasets, the dataset with the fewer number of instances is padded; with ""no data"". For example, if a variant is only in one dataset,; then each genotype is treated as ""no data"" in the other. This method returns a tuple of three objects: a nested list of; list of int with global concordance summary statistics, a table; with concordance statistics per column key, and a table with; concordance statistics per row key. **Using the global summary result**. The global summary is a list of list of int (conceptually a 5 by 5 matrix),; where the indices have special meaning:. 0. No Data (missing variant or filtered entry); 1. No Call (missing genotype call); 2. Hom Ref; 3. Heterozygous; 4. Hom Var. The first index is the state in the left dataset and the second index is; the state in the right dataset. Typical uses of the summary list are shown; below. >>> summary, samples, variants = hl.concordance(datas",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:3228,Availability,avail,available,3228," run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:58,Deployability,configurat,configuration,58,"""""""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {in",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:18,Modifiability,config,configuring,18,"""""""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {in",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:58,Modifiability,config,configuration,58,"""""""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {in",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:122,Modifiability,inherit,inherits,122,"""""""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {in",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:814,Modifiability,variab,variables,814,"""""""Base class for configuring VEP. To define a custom VEP configuration to for Query on Batch, construct a new class that inherits from :class:`.VEPConfig`; and has the following parameters defined:. - `json_type` (:class:`.HailType`): The type of the VEP JSON schema (as produced by VEP when invoked with the `--json` option).; - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `batch_run_command` (:obj:`.list` of :obj:`.str`) -- The command line to run for a VEP job for a partition.; - `batch_run_csq_header_command` (:obj:`.list` of :obj:`.str`) -- The command line to run when generating the consequence header.; - `env` (dict of :obj:`.str` to :obj:`.str`) -- A map of environment variables to values to add to the environment when invoking the command.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {in",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:2183,Modifiability,plugin,plugin,2183,"an run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPU",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:2486,Modifiability,variab,variables,2486," run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:2753,Modifiability,config,config,2753," run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:3212,Modifiability,variab,variable,3212," run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is False or True.; - `VEP_OUTPUT_FILE` - String specifying the local path where the output TSV file with the VEP result should be located.; - `VEP_INPUT_FILE` - String specifying the local path where the input VCF shard is located for all jobs. The `VEP_INPUT_FILE` environment variable is not available for the single job that computes the consequence header when; ``csq=True``; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:2098,Performance,cache,cache,2098,"Service is located.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. In addition, the method `command` must be defined with the following signature. The output is the exact command to run the; VEP executable. The inputs are `consequence` and `tolerate_parse_error` which are user-defined parameters to :func:`.vep`,; `part_id` which is the partition ID, `input_file` which is the path to the input file where the input data can be found, and; `output_file` is the path to the output file where the VEP annotations are written to. An example is shown below:. .. code-block:: python3. def command(self,; consequence: bool,; tolerate_parse_error: bool,; part_id: int,; input_file: Optional[str],; output_file: str) -> List[str]:; vcf_or_json = '--vcf' if consequence else '--json'; input_file = f'--input_file {input_file}' if input_file else ''; return f'''/vep/vep {input_file} \; --format vcf \; {vcf_or_json} \; --everything \; --allele_number \; --no_stats \; --cache \; --offline \; --minimal \; --assembly GRCh37 \; --dir={self.data_mount} \; --plugin LoF,human_ancestor_fa:{self.data_mount}/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:{self.data_mount}/loftee_data/phylocsf_gerp.sql,gerp_file:{self.data_mount}/loftee_data/GERP_scores.final.sorted.txt.gz \; -o STDOUT; '''. The following environment variables are added to the job's environment:. - `VEP_BLOCK_SIZE` - The maximum number of variants provided as input to each invocation of VEP.; - `VEP_PART_ID` - Partition ID.; - `VEP_DATA_MOUNT` - Location where the vep data is mounted (same as `data_mount` in the config).; - `VEP_CONSEQUENCE` - Integer equal to 0 or 1 on whether `csq` is False or True.; - `VEP_TOLERATE_PARSE_ERROR` - Integer equal to 0 or 1 on whether `tolerate_parse_error` is Fals",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:29,Deployability,configurat,configuration,29,"""""""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:29,Modifiability,config,configuration,29,"""""""; The Hail-maintained VEP configuration for GRCh37 for VEP version 85. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:29,Deployability,configurat,configuration,29,"""""""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is set to requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:29,Modifiability,config,configuration,29,"""""""; The Hail-maintained VEP configuration for GRCh38 for VEP version 95. This class takes the following constructor arguments:. - `data_bucket` (:obj:`.str`) -- The location where the VEP data is stored.; - `data_mount` (:obj:`.str`) -- The location in the container where the data should be mounted.; - `image` (:obj:`.str`) -- The docker image to run VEP.; - `cloud` (:obj:`.str`) -- The cloud where the Batch Service is located.; - `data_bucket_is_requester_pays` (:obj:`.bool`) -- True if the data bucket is set to requester pays.; - `regions` (:obj:`.list` of :obj:`.str`) -- A list of regions the VEP jobs can run in. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:336,Deployability,configurat,configuration,336,"""""""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:455,Deployability,install,installed,455,"""""""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:556,Deployability,install,installing,556,"""""""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:777,Deployability,configurat,configuration,777,"""""""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1127,Deployability,configurat,configuration,1127,"mbl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ances",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1765,Deployability,configurat,configuration,1765,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1801,Deployability,release,release,1801,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1813,Deployability,install,installed,1813,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:4860,Deployability,configurat,configuration,4860,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixT",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5332,Deployability,configurat,configuration,5332,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5381,Deployability,configurat,configuration,5381,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:6006,Deployability,configurat,configuration,6006,"imised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1358,Integrability,depend,depending,1358,"""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOU",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:336,Modifiability,config,configuration,336,"""""""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:777,Modifiability,config,configuration,777,"""""""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:837,Modifiability,config,config,837,"""""""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:978,Modifiability,config,config,978,"""""""Annotate variants with VEP. .. include:: ../_templates/req_tvariant.rst. :func:`.vep` runs `Variant Effect Predictor; <http://www.ensembl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1127,Modifiability,config,configuration,1127,"mbl.org/info/docs/tools/vep/index.html>`__ on the; current dataset and adds the result as a row field. Examples; --------. Add VEP annotations to the dataset:. >>> result = hl.vep(dataset, ""data/vep-configuration.json"") # doctest: +SKIP. Notes; -----. **Installation**. This VEP command only works if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ances",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1420,Modifiability,variab,variables,1420," if you have already installed VEP on your; computing environment. If you use `hailctl dataproc` to start Hail clusters,; installing VEP is achieved by specifying the `--vep` flag. For more detailed instructions,; see :ref:`vep_dataproc`. If you use `hailctl hdinsight`, see :ref:`vep_hdinsight`. **Spark Configuration**. :func:`.vep` needs a configuration file to tell it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1765,Modifiability,config,configuration,1765,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1849,Modifiability,plugin,plugin,1849,"ll it how to run VEP. This is the ``config`` argument; to the VEP function. If you are using `hailctl dataproc` as mentioned above, you can just use the; default argument for ``config`` and everything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:2072,Modifiability,plugin,plugin,2072,"ything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:Float64,exac_fin_allele:String,exac_fin_maf:Float64,exa",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:4860,Modifiability,config,configuration,4860,"ring],impact:String,minimised:Int32,regulatory_feature_id:String,variant_allele:String}],seq_region_name:String,start:Int32,strand:Int32,transcript_consequences:Array[Struct{allele_num:Int32,amino_acids:String,biotype:String,canonical:Int32,ccds:String,cdna_start:Int32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixT",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5103,Modifiability,config,config,5103,"nt32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row fi",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5176,Modifiability,variab,variable,5176,"nt32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row fi",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5226,Modifiability,config,config,5226,"nt32,cdna_end:Int32,cds_end:Int32,cds_start:Int32,codons:String,consequence_terms:Array[String],distance:Int32,domains:Array[Struct{db:String,name:String}],exon:String,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row fi",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5278,Modifiability,config,config,5278,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5332,Modifiability,config,configuration,5332,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5381,Modifiability,config,configuration,5381,"g,gene_id:String,gene_pheno:Int32,gene_symbol:String,gene_symbol_source:String,hgnc_id:String,hgvsc:String,hgvsp:String,hgvs_offset:Int32,impact:String,intron:String,lof:String,lof_flags:String,lof_filter:String,lof_info:String,minimised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5938,Modifiability,config,config,5938,"imised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:6006,Modifiability,config,configuration,6006,"imised:Int32,polyphen_prediction:String,polyphen_score:Float64,protein_end:Int32,protein_start:Int32,protein_id:String,sift_prediction:String,sift_score:Float64,strand:Int32,swissprot:String,transcript_id:String,trembl:String,uniparc:String,variant_allele:String}],variant_class:String}""; }. The configuration files used by``hailctl dataproc`` can be found at the following locations:. - ``GRCh37``: ``gs://hail-us-central1-vep/vep85-loftee-gcloud.json``; - ``GRCh38``: ``gs://hail-us-central1-vep/vep95-GRCh38-loftee-gcloud.json``. If no config file is specified, this function will check to see if environment variable `VEP_CONFIG_URI` is set with a path to a config file. **Batch Service Configuration**. If no config is specified, Hail will use the user's Service configuration parameters to find a supported VEP configuration.; However, if you wish to use your own implementation of VEP, then see the documentation for :class:`.VEPConfig`. **Annotations**. A new row field is added in the location specified by `name` with type given; by the type given by the `json_vep_schema` (if `csq` is ``False``) or; :class:`.tarray` of :py:data:`.tstr` (if `csq` is ``True``). If csq is ``True``, then the CSQ header string is also added as a global; field with name ``name + '_csq_header'``. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str` or :class:`.VEPConfig`, optional; Path to VEP configuration file or a VEPConfig object.; block_size : :obj:`int`; Number of rows to process per VEP invocation.; name : :class:`str`; Name for resulting row field.; csq : :obj:`bool`; If ``True``, annotates with the VCF CSQ field as a :py:data:`.tstr`.; If ``False``, annotates as the `vep_json_schema`.; tolerate_parse_error : :obj:`bool`; If ``True``, ignore invalid JSON produced by VEP and return a missing annotation. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing VEP annotations. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:2008,Performance,cache,cache,2008,"ything will work. If you need to run VEP with Hail in other environments,; there are detailed instructions below. The format of the configuration file is JSON, and :func:`.vep`; expects a JSON object with three fields:. - `command` (array of string) -- The VEP command line to run. The string literal `__OUTPUT_FORMAT_FLAG__` is replaced with `--json` or `--vcf` depending on `csq`.; - `env` (object) -- A map of environment variables to values to add to the environment when invoking the command. The value of each object member must be a string.; - `vep_json_schema` (string): The type of the VEP JSON schema (as produced by the VEP when invoked with the `--json` option). Note: This is the old-style 'parseable' Hail type syntax. This will change. Here is an example configuration file for invoking VEP release 85; installed in `/vep` with the Loftee plugin:. .. code-block:: text. {; ""command"": [; ""/vep"",; ""--format"", ""vcf"",; ""__OUTPUT_FORMAT_FLAG__"",; ""--everything"",; ""--allele_number"",; ""--no_stats"",; ""--cache"", ""--offline"",; ""--minimal"",; ""--assembly"", ""GRCh37"",; ""--plugin"", ""LoF,human_ancestor_fa:/root/.vep/loftee_data/human_ancestor.fa.gz,filter_position:0.05,min_intron_size:15,conservation_file:/root/.vep/loftee_data/phylocsf_gerp.sql,gerp_file:/root/.vep/loftee_data/GERP_scores.final.sorted.txt.gz"",; ""-o"", ""STDOUT""; ],; ""env"": {; ""PERL5LIB"": ""/vep_data/loftee""; },; ""vep_json_schema"": ""Struct{assembly_name:String,allele_string:String,ancestral:String,colocated_variants:Array[Struct{aa_allele:String,aa_maf:Float64,afr_allele:String,afr_maf:Float64,allele_string:String,amr_allele:String,amr_maf:Float64,clin_sig:Array[String],end:Int32,eas_allele:String,eas_maf:Float64,ea_allele:String,ea_maf:Float64,eur_allele:String,eur_maf:Float64,exac_adj_allele:String,exac_adj_maf:Float64,exac_allele:String,exac_afr_allele:String,exac_afr_maf:Float64,exac_amr_allele:String,exac_amr_maf:Float64,exac_eas_allele:String,exac_eas_maf:Float64,exac_fin_allele:String,exac_fin_maf:Float64,exa",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:511,Deployability,configurat,configuration,511,"""""""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1339,Deployability,configurat,configuration,1339,"-. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryR",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:6204,Deployability,configurat,configuration,6204,"; amrAc: int32,; amrAn: int32,; easAf: float64,; easAc: int32,; easAn: int32,; eurAf: float64,; eurAc: int32,; eurAn: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32; },; mitomap: array<struct {; refAllele: str,; altAllele: str,; diseases : array<str>,; hasHomoplasmy: bool,; hasHeteroplasmy: bool,; status: str,; clinicalSignificance: str,; scorePercentile: float64,; isAlleleSpecific: bool,; chromosome: str,; begin: int32,; end: int32,; variantType: str; }; transcripts: struct {; refSeq: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:511,Modifiability,config,configuration,511,"""""""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:887,Modifiability,variab,variable,887,"""""""Annotate variants using `Nirvana <https://github.com/Illumina/Nirvana>`_. .. include:: ../_templates/experimental.rst. .. include:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1339,Modifiability,config,configuration,1339,"-. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryR",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:5957,Modifiability,inherit,inheritance,5957,"Hc: int32,; asjAf: float64,; asjAc: int32,; asjAn: int32,; asjHc: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32,; sasHc: int32,; failedFilter: bool; },; topmed: struct {; failedFilter: bool,; allAc: int32,; allAn: int32,; allAf: float64,; allHc: int32; },; oneKg: struct {; ancestralAllele: str,; allAf: float64,; allAc: int32,; allAn: int32,; afrAf: float64,; afrAc: int32,; afrAn: int32,; amrAf: float64,; amrAc: int32,; amrAn: int32,; easAf: float64,; easAc: int32,; easAn: int32,; eurAf: float64,; eurAc: int32,; eurAn: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32; },; mitomap: array<struct {; refAllele: str,; altAllele: str,; diseases : array<str>,; hasHomoplasmy: bool,; hasHeteroplasmy: bool,; status: str,; clinicalSignificance: str,; scorePercentile: float64,; isAlleleSpecific: bool,; chromosome: str,; begin: int32,; end: int32,; variantType: str; }; transcripts: struct {; refSeq: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }.",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:6165,Modifiability,config,config,6165,"; amrAc: int32,; amrAn: int32,; easAf: float64,; easAc: int32,; easAn: int32,; eurAf: float64,; eurAc: int32,; eurAn: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32; },; mitomap: array<struct {; refAllele: str,; altAllele: str,; diseases : array<str>,; hasHomoplasmy: bool,; hasHeteroplasmy: bool,; status: str,; clinicalSignificance: str,; scorePercentile: float64,; isAlleleSpecific: bool,; chromosome: str,; begin: int32,; end: int32,; variantType: str; }; transcripts: struct {; refSeq: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:6204,Modifiability,config,configuration,6204,"; amrAc: int32,; amrAn: int32,; easAf: float64,; easAc: int32,; easAn: int32,; eurAf: float64,; eurAc: int32,; eurAn: int32,; sasAf: float64,; sasAc: int32,; sasAn: int32; },; mitomap: array<struct {; refAllele: str,; altAllele: str,; diseases : array<str>,; hasHomoplasmy: bool,; hasHeteroplasmy: bool,; status: str,; clinicalSignificance: str,; scorePercentile: float64,; isAlleleSpecific: bool,; chromosome: str,; begin: int32,; end: int32,; variantType: str; }; transcripts: struct {; refSeq: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>,; ensembl: array<struct {; transcript: str,; bioType: str,; aminoAcids: str,; cdnaPos: str,; codons: str,; cdsPos: str,; exons: str,; introns: str,; geneId: str,; hgnc: str,; consequence: array<str>,; hgvsc: str,; hgvsp: str,; isCanonical: bool,; polyPhenScore: float64,; polyPhenPrediction: str,; proteinId: str,; proteinPos: str,; siftScore: float64,; siftPrediction: str; }>; },; overlappingGenes: array<str>; }>; genes: array<struct {; name: str,; omim: array<struct {; mimNumber: int32,; hgnc: str,; description: str,; phenotypes: array<struct {; mimNumber: int32,; phenotype: str,; mapping: str,; inheritance: array<str>,; comments: str; }>; }>; exac: struct {; pLi: float64,; pRec: float64,; pNull: float64; }; }>; }. Parameters; ----------; dataset : :class:`.MatrixTable` or :class:`.Table`; Dataset.; config : :class:`str`; Path to Nirvana configuration file.; block_size : :obj:`int`; Number of rows to process per Nirvana invocation.; name : :class:`str`; Name for resulting row field. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; Dataset with new row-indexed field `name` containing Nirvana annotations.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1116,Performance,cache,cache,1116,"de:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1139,Performance,cache,cache,1139,"de:: ../_templates/req_tvariant.rst. :func:`.nirvana` runs `Nirvana; <https://github.com/Illumina/Nirvana>`_ on the current dataset and adds a; new row field in the location specified by `name`. Examples; --------. Add Nirvana annotations to the dataset:. >>> result = hl.nirvana(dataset, ""data/nirvana.properties"") # doctest: +SKIP. **Configuration**. :func:`.nirvana` requires a configuration file. The format is a; `.properties file <https://en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1548,Performance,cache,cache,1548,"/en.wikipedia.org/wiki/.properties>`__, where each; line defines a property as a key-value pair of the form ``key = value``.; :func:`.nirvana` supports the following properties:. - **hail.nirvana.dotnet** -- Location of dotnet. Optional, default: dotnet.; - **hail.nirvana.path** -- Value of the PATH environment variable when; invoking Nirvana. Optional, by default PATH is not set.; - **hail.nirvana.location** -- Location of Nirvana.dll. Required.; - **hail.nirvana.reference** -- Location of reference genome. Required.; - **hail.nirvana.cache** -- Location of cache. Required.; - **hail.nirvana.supplementaryAnnotationDirectory** -- Location of; Supplementary Database. Optional, no supplementary database by default. Here is an example ``nirvana.properties`` configuration file:. .. code-block:: text. hail.nirvana.location = /path/to/dotnet/netcoreapp2.0/Nirvana.dll; hail.nirvana.reference = /path/to/nirvana/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.cache = /path/to/nirvana/Cache/GRCh37/Ensembl; hail.nirvana.supplementaryAnnotationDirectory = /path/to/nirvana/SupplementaryDatabase/GRCh37. **Annotations**. A new row field is added in the location specified by `name` with the; following schema:. .. code-block:: text. struct {; chromosome: str,; refAllele: str,; position: int32,; altAlleles: array<str>,; cytogeneticBand: str,; quality: float64,; filters: array<str>,; jointSomaticNormalQuality: int32,; copyNumber: int32,; strandBias: float64,; recalibratedQuality: float64,; variants: array<struct {; altAllele: str,; refAllele: str,; chromosome: str,; begin: int32,; end: int32,; phylopScore: float64,; isReferenceMinor: bool,; variantType: str,; vid: str,; hgvsg: str,; isRecomposedVariant: bool,; isDecomposedVariant: bool,; regulatoryRegions: array<struct {; id: str,; type: str,; consequence: set<str>; }>,; clinvar: array<struct {; id: str,; reviewStatus: str,; isAlleleSpecific: bool,; alleleOrigins: array<str>,; refAllele: str,; altAllele: str,; phenotypes: arr",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:1249,Availability,down,down,1249,"""""""Summarize the variants present in a dataset and print the results. Examples; --------; >>> hl.summarize_variants(dataset) # doctest: +SKIP; ==============================; Number of variants: 346; ==============================; Alleles per variant; -------------------; 2 alleles: 346 variants; ==============================; Variants per contig; -------------------; 20: 346 variants; ==============================; Allele type distribution; ------------------------; SNP: 301 alleles; Deletion: 27 alleles; Insertion: 18 alleles; ==============================. Parameters; ----------; mt : :class:`.MatrixTable` or :class:`.Table`; Matrix table with a variant (locus / alleles) row key.; show : :obj:`bool`; If ``True``, print results instead of returning them.; handler. Notes; -----; The result returned if `show` is ``False`` is a :class:`.Struct` with; five fields:. - `n_variants` (:obj:`int`): Number of variants present in the matrix table.; - `allele_types` (:obj:`dict` [:obj:`str`, :obj:`int`]): Number of alternate alleles in; each allele allele category.; - `contigs` (:obj:`dict` [:obj:`str`, :obj:`int`]): Number of variants on each contig.; - `allele_counts` (:obj:`dict` [:obj:`int`, :obj:`int`]): Number of variants broken down; by number of alleles (biallelic is 2, for example).; - `r_ti_tv` (:obj:`float`): Ratio of transition alternate alleles to; transversion alternate alleles. Returns; -------; :obj:`None` or :class:`.Struct`; Returns ``None`` if `show` is ``True``, or returns results as a struct.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:555,Performance,load,loaded,555,"""""""Compute CHARR, the DNA sample contamination estimator. .. include:: ../_templates/experimental.rst. Notes; -----. The returned table has the sample ID field, plus the field:. - `charr` (float64): CHARR contamination estimation. Note; -----; It is possible to use gnomAD reference allele frequencies with the following:. >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') # doctest: +SKIP; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) # doctest: +SKIP. If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:. >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.VariantDataset`; Dataset.; min_af; Minimum reference allele frequency to filter variants.; max_af; Maximum reference allele frequency to filter variants.; min_dp; Minimum sequencing depth to filter variants.; max_dp; Maximum sequencing depth to filter variants.; min_gq; Minimum genotype quality to filter variants; ref_AF; Reference AF expression. Necessary when the sample size is below 10,000. Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py:642,Performance,load,load,642,"""""""Compute CHARR, the DNA sample contamination estimator. .. include:: ../_templates/experimental.rst. Notes; -----. The returned table has the sample ID field, plus the field:. - `charr` (float64): CHARR contamination estimation. Note; -----; It is possible to use gnomAD reference allele frequencies with the following:. >>> gnomad_sites = hl.experimental.load_dataset('gnomad_genome_sites', version='3.1.2') # doctest: +SKIP; >>> charr_result = hl.compute_charr(mt, ref_af=(1 - gnomad_sites[mt.row_key].freq[0].AF)) # doctest: +SKIP. If the dataset is loaded from a gvcf and has NON_REF alleles, drop the last allele with the following or load it with the hail vcf combiner:. >>> mt = mt.key_rows_by(locus=mt.locus, alleles=mt.alleles[:-1]). Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.VariantDataset`; Dataset.; min_af; Minimum reference allele frequency to filter variants.; max_af; Maximum reference allele frequency to filter variants.; min_dp; Minimum sequencing depth to filter variants.; max_dp; Maximum sequencing depth to filter variants.; min_gq; Minimum genotype quality to filter variants; ref_AF; Reference AF expression. Necessary when the sample size is below 10,000. Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/qc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/qc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:14,Testability,log,logreg,14,"# Helpers for logreg:",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:71,Testability,log,logit,71,"""""""Iteratively reweighted least squares to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:96,Testability,log,logit,96,"""""""Iteratively reweighted least squares using Firth's regression to fit the model y ~ Bernoulli(logit(X \beta)). When fitting the null model, K=n_covariates, otherwise K=n_covariates + 1.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:36,Integrability,depend,depending,36,"# n_covariates or n_covariates + 1, depending on improved null fit vs full fit",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:57,Integrability,depend,dependent,57,"# yvecs is a list of sample-length vectors, one for each dependent variable.",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:67,Modifiability,variab,variable,67,"# yvecs is a list of sample-length vectors, one for each dependent variable.",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:39,Testability,log,logreg,39,"# Fit null models, which means doing a logreg fit with just the covariates for each phenotype.",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:19,Testability,test,test,19,"# FIXME: we should test a whole block of variants at a time not one-by-one",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:31,Modifiability,variab,variable,31,"""""""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:17,Testability,test,test,17,"""""""For each row, test an input variable for association using a linear; mixed model. .. warning::. This functionality is no longer implemented/supported as of Hail 0.2.94.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:96,Energy Efficiency,reduce,reduced,96,"# Instead of finding the best-fit beta, we go directly to the best-predicted value using the; # reduced QR decomposition:; #; # Q @ R = X; # y = X beta; # X^T y = X^T X beta; # (X^T X)^-1 X^T y = beta; # (R^T Q^T Q R)^-1 R^T Q^T y = beta; # (R^T R)^-1 R^T Q^T y = beta; # R^-1 R^T^-1 R^T Q^T y = beta; # R^-1 Q^T y = beta; #; # X beta = X R^-1 Q^T y; # = Q R R^-1 Q^T y; # = Q Q^T y; #",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:67,Safety,predict,predicted,67,"# Instead of finding the best-fit beta, we go directly to the best-predicted value using the; # reduced QR decomposition:; #; # Q @ R = X; # y = X beta; # X^T y = X^T X beta; # (X^T X)^-1 X^T y = beta; # (R^T Q^T Q R)^-1 R^T Q^T y = beta; # (R^T R)^-1 R^T Q^T y = beta; # R^-1 R^T^-1 R^T Q^T y = beta; # R^-1 Q^T y = beta; #; # X beta = X R^-1 Q^T y; # = Q R R^-1 Q^T y; # = Q Q^T y; #",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:160,Energy Efficiency,reduce,reduced,160,"# Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:957,Modifiability,variab,variables,957,"# Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1476,Modifiability,variab,variables,1476,"hat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1761,Modifiability,variab,variables,1761,"hat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1819,Modifiability,variab,variables,1819,"hat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1889,Modifiability,variab,variable,1889,"hat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:817,Testability,assert,asserted,817,"# Null model:; #; # y = X b + e, e ~ N(0, \sigma^2); #; # We can find a best-fit b, bhat, and a best-fit y, yhat:; #; # bhat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1440,Testability,assert,asserted,1440,"hat = (X.T X).inv X.T y; #; # Q R = X (reduced QR decomposition); # bhat = R.inv Q.T y; #; # yhat = X bhat; # = Q R R.inv Q.T y; # = Q Q.T y; #; # The residual phenotype not captured by the covariates alone is r:; #; # r = y - yhat; # = (I - Q Q.T) y; #; # We can factor the Q-statistic (note there are two Qs: the Q from the QR decomposition and the; # Q-statistic from the paper):; #; # Q = r.T G diag(w) G.T r; # Z = r.T G diag(sqrt(w)); # Q = Z Z.T; #; # Plugging in our expresion for r:; #; # Z = y.T (I - Q Q.T) G diag(sqrt(w)); #; # Notice that I - Q Q.T is symmetric (ergo X = X.T) because each summand is symmetric and sums; # of symmetric matrices are symmetric matrices.; #; # We have asserted that; #; # y ~ N(0, \sigma^2); #; # It will soon be apparent that the distribution of Q is easier to characterize if our random; # variables are standard normals:; #; # h ~ N(0, 1); # y = \sigma h; #; # We set \sigma^2 to the sample variance of the residual vectors.; #; # Returning to Z:; #; # Z = h.T \sigma (I - Q Q.T) G diag(sqrt(w)); # Q = Z Z.T; #; # Which we can factor into a symmetric matrix and a standard normal:; #; # A = \sigma (I - Q Q.T) G diag(sqrt(w)); # B = A A.T; # Q = h.T B h; #; # This is called a ""quadratic form"". It is a weighted sum of products of pairs of entries of h,; # which we have asserted are i.i.d. standard normal variables. The distribution of such sums is; # given by the generalized chi-squared distribution:; #; # U L U.T = B B is symmetric and thus has an eigendecomposition; # h.T B h = Q ~ GeneralizedChiSquare(L, 1, 0, 0, 0); #; # The orthogonal matrix U remixes the vector of i.i.d. normal variables into a new vector of; # different i.i.d. normal variables. The L matrix is diagonal and scales each squared normal; # variable.; #; # Since B = A A.T is symmetric, its eigenvalues are the square of the singular values of A or; # A.T:; #; # W S V = A; # U L U.T = B; # = A A.T; # = W S V V.T S W; # = W S S W V is orthogonal so V V.T = I; # = W S^2 W",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:53,Performance,optimiz,optimized,53,"# FIXME: remove this logic when annotation is better optimized",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:21,Testability,log,logic,21,"# FIXME: remove this logic when annotation is better optimized",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1357,Availability,error,errors,1357,"`.MatrixTable.select_entries`, :meth:`.MatrixTable.transmute_entries`. The resulting dataset will be keyed by the split locus and alleles. :func:`.split_multi` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. - `old_locus` (*locus*) -- The original, unsplit locus. - `old_alleles` (*array<str>*) -- The original, unsplit alleles. All other fields are left unchanged. Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2232,Availability,down,downcoding,2232,"t variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2453,Availability,down,downcode,2453,"not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2635,Availability,down,downcode,2635,"s one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:3212,Availability,error,error,3212,"s one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2204,Deployability,update,updates,2204,"t variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1252,Energy Efficiency,efficient,efficient,1252," yourself using; one of the entry modification methods: :meth:`.MatrixTable.annotate_entries`,; :meth:`.MatrixTable.select_entries`, :meth:`.MatrixTable.transmute_entries`. The resulting dataset will be keyed by the split locus and alleles. :func:`.split_multi` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. - `old_locus` (*locus*) -- The original, unsplit locus. - `old_alleles` (*array<str>*) -- The original, unsplit alleles. All other fields are left unchanged. Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:3160,Safety,avoid,avoids,3160,"s one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False, old_locus=bi.locus, old_alleles=bi.alleles); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi(multi); >>> mt = split.union_rows(bi). Example; -------. :func:`.split_multi_hts`, which splits multiallelic variants for the HTS; genotype schema and updates the entry fields by downcoding the genotype, is; implemented as:. >>> sm = hl.split_multi(ds); >>> pl = hl.or_missing(; ... hl.is_defined(sm.PL),; ... (hl.range(0, 3).map(lambda i: hl.min(hl.range(0, hl.len(sm.PL)); ... .filter(lambda j: hl.downcode(hl.unphased_diploid_gt_index_call(j), sm.a_index) == hl.unphased_diploid_gt_index_call(i)); ... .map(lambda j: sm.PL[j]))))); >>> split_ds = sm.annotate_entries(; ... GT=hl.downcode(sm.GT, sm.a_index),; ... AD=hl.or_missing(hl.is_defined(sm.AD),; ... [hl.sum(sm.AD) - sm.AD[sm.a_index], sm.AD[sm.a_index]]),; ... DP=sm.DP,; ... PL=pl,; ... GQ=hl.gq_from_pl(pl)).drop('old_locus', 'old_alleles'). See Also; --------; :func:`.split_multi_hts`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left aligned and have unique; loci. This avoids a shuffle. If the assumption is violated, an error; is generated.; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:679,Availability,error,errors,679,"""""""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1756,Availability,down,downcoded,1756,"this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2340,Availability,down,downcode,2340,"eles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for a genotype is; the minimum over multiallelic `PL` entries for genotypes that map to that; genotype. `GQ` is recomputed from `PL` if `PL` is provided and is not; missing. If not, it is copied from the original GQ. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split fields in the info field. This means that if a; multiallelic site with `info.AC` value `",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2512,Availability,down,downcoding,2512,"unc:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and 0/1. The biallelic alt `AD` entry is just the multiallelic `AD` entry; corresponding to the alternate allele. The ref AD entry is the sum of the; other multiallelic entries. The biallelic `DP` is the same as the multiallelic `DP`. The biallelic `PL` entry for a genotype g is the minimum over `PL` entries; for multiallelic genotypes that downcode to g. For example, the `PL` for (A,; T) at 0/1 is the minimum of the PLs for 0/1 (50) and 1/2 (45), and thus 45. Fixing an alternate allele and biallelic variant, downcoding gives a map; from multiallelic to biallelic alleles and genotypes. The biallelic `AD` entry; for an allele is just the sum of the multiallelic `AD` entries for alleles; that map to that allele. Similarly, the biallelic `PL` entry for a genotype is; the minimum over multiallelic `PL` entries for genotypes that map to that; genotype. `GQ` is recomputed from `PL` if `PL` is provided and is not; missing. If not, it is copied from the original GQ. Here is a second example for a het non-ref. .. code-block:: text. A C,T 1/2:2,8,6:16:45:99,50,99,45,0,99. splits as. .. code-block:: text. A C 0/1:8,8:16:45:45,0,99; A T 0/1:10,6:16:50:50,0,99. **VCF Info Fields**. Hail does not split fields in the info field. This means that if a; multiallelic site with `info.AC` value ``[10, 2]`` is split, each split; site will contain the same array ``[10, 2]``. The provided allele index; field `a_index` can be used to select the value corresponding to the split; allele's position:. >>> split_ds = hl.split_multi_",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:5211,Availability,error,error,5211," matches the structure on import (""A"" for 1 per; allele, for example), Hail will export these fields with; number ""."". If the desired output is one value per site, then it is; possible to use annotate_variants_expr to remap these; values. Here is an example:. >>> split_ds = hl.split_multi_hts(dataset); >>> split_ds = split_ds.annotate_rows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') # doctest: +SKIP. The info field AC in *data/export.vcf* will have ``Number=1``. **New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:570,Energy Efficiency,efficient,efficient,570,"""""""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:5293,Modifiability,variab,variable-length,5293," matches the structure on import (""A"" for 1 per; allele, for example), Hail will export these fields with; number ""."". If the desired output is one value per site, then it is; possible to use annotate_variants_expr to remap these; values. Here is an example:. >>> split_ds = hl.split_multi_hts(dataset); >>> split_ds = split_ds.annotate_rows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') # doctest: +SKIP. The info field AC in *data/export.vcf* will have ``Number=1``. **New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:98,Performance,throughput,throughput,98,"""""""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:5158,Safety,avoid,avoids,5158," matches the structure on import (""A"" for 1 per; allele, for example), Hail will export these fields with; number ""."". If the desired output is one value per site, then it is; possible to use annotate_variants_expr to remap these; values. Here is an example:. >>> split_ds = hl.split_multi_hts(dataset); >>> split_ds = split_ds.annotate_rows(info = split_ds.info.annotate(AC = split_ds.info.AC[split_ds.a_index - 1])); >>> hl.export_vcf(split_ds, 'output/export.vcf') # doctest: +SKIP. The info field AC in *data/export.vcf* will have ``Number=1``. **New Fields**. :func:`.split_multi_hts` adds the following fields:. - `was_split` (*bool*) -- ``True`` if this variant was originally; multiallelic, otherwise ``False``. - `a_index` (*int*) -- The original index of this alternate allele in the; multiallelic representation (NB: 1 is the first alternate allele or the; only alternate allele in a biallelic variant). For example, 1:100:A:T,C; splits into two variants: 1:100:A:T with ``a_index = 1`` and 1:100:A:C; with ``a_index = 2``. See Also; --------; :func:`.split_multi`. Parameters; ----------; ds : :class:`.MatrixTable` or :class:`.Table`; An unsplit dataset.; keep_star : :obj:`bool`; Do not filter out * alleles.; left_aligned : :obj:`bool`; If ``True``, variants are assumed to be left; aligned and have unique loci. This avoids a shuffle. If the assumption; is violated, an error is generated.; vep_root : :class:`str`; Top-level location of vep data. All variable-length VEP fields; (intergenic_consequences, motif_feature_consequences,; regulatory_feature_consequences, and transcript_consequences); will be split properly (i.e. a_index corresponding to the VEP allele_num).; permit_shuffle : :obj:`bool`; If ``True``, permit a data shuffle to sort out-of-order split results.; This will only be required if input data has duplicate loci, one of; which contains more than one alternate allele. Returns; -------; :class:`.MatrixTable` or :class:`.Table`; A biallelic variant dataset. """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:313,Testability,log,logic,313,"""""""Split multiallelic variants for datasets that contain one or more fields; from a standard high-throughput sequencing entry schema. .. code-block:: text. struct {; GT: call,; AD: array<int32>,; DP: int32,; GQ: int32,; PL: array<int32>,; PGT: call,; PID: str; }. For other entry fields, write your own splitting logic using; :meth:`.MatrixTable.annotate_entries`. Examples; --------. >>> hl.split_multi_hts(dataset).write('output/split.mt'). Warning; -------; This method assumes `ds` contains at most one non-split variant per locus. This assumption permits the; most efficient implementation of the splitting algorithm. If your queries involving `split_multi_hts`; crash with errors about out-of-order keys, this assumption may be violated. Otherwise, this; warning likely does not apply to your dataset. If each locus in `ds` contains one multiallelic variant and one or more biallelic variants, you; can filter to the multiallelic variants, split those, and then combine the split variants with; the original biallelic variants. For example, the following code splits a dataset `mt` which contains a mixture of split and; non-split variants. >>> bi = mt.filter_rows(hl.len(mt.alleles) == 2); >>> bi = bi.annotate_rows(a_index=1, was_split=False); >>> multi = mt.filter_rows(hl.len(mt.alleles) > 2); >>> split = hl.split_multi_hts(multi); >>> mt = split.union_rows(bi). Notes; -----. We will explain by example. Consider a hypothetical 3-allelic; variant:. .. code-block:: text. A C,T 0/2:7,2,6:15:45:99,50,99,0,45,99. :func:`.split_multi_hts` will create two biallelic variants (one for each; alternate allele) at the same position. .. code-block:: text. A C 0/0:13,2:15:45:0,45,99; A T 0/1:9,6:15:50:50,0,99. Each multiallelic `GT` or `PGT` field is downcoded once for each alternate allele. A; call for an alternate allele maps to 1 in the biallelic variant; corresponding to itself and 0 otherwise. For example, in the example above,; 0/2 maps to 0/0 and 0/1. The genotype 1/2 maps to 0/1 and ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:3175,Energy Efficiency,efficient,efficiently,3175,"r row.; The ``(i, j)`` element of the resulting block matrix is the correlation; between rows ``i`` and ``j`` (as 0-indexed by order in the matrix table;; see :meth:`~hail.MatrixTable.add_row_index`). The correlation of two vectors is defined as the; `Pearson correlation coeffecient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors. This method has two stages:. - writing the row-normalized block matrix to a temporary file on persistent; disk with :meth:`.BlockMatrix.from_entry_expr`. The parallelism is; ``n_rows / block_size``. - reading and multiplying this block matrix by its transpose. The; parallelism is ``(n_rows / block_size)^2`` if all blocks are computed. Warning; -------; See all warnings on :meth:`.BlockMatrix.from_entry_expr`. In particular,; for large matrices, it may be preferable to run the two stages separately,; saving the row-normalized block matrix to a file on external storage with; :meth:`.BlockMatrix.write_from_entry_expr`. The resulting number of matrix elements is the square of the number of rows; in the matrix table, so computing the full matrix may be infeasible. For; example, ten million rows would produce 800TB of float64 values. The; block-sparse representation on BlockMatrix may be used to work efficiently; with regions of such matrices, as in the second example above and; :meth:`ld_matrix`. To prevent excessive re-computation, be sure to write and read the (possibly; block-sparsified) result before multiplication by another matrix. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; block_size : :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.BlockMatrix`; Correlation matrix between row vectors. Row and column indices; correspond to matrix table row index.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1508,Safety,avoid,avoid,1508,"G:T', 's': 'd', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'a', 'GT': hl.Call([0, 1])},; ... {'v': '1:3:C:G', 's': 'b', 'GT': hl.Call([0, 0])},; ... {'v': '1:3:C:G', 's': 'c', 'GT': hl.Call([1, 1])},; ... {'v': '1:3:C:G', 's': 'd', 'GT': hl.missing(hl.tcall)}]; >>> ht = hl.Table.parallelize(data, hl.dtype('struct{v: str, s: str, GT: call}')); >>> mt = ht.to_matrix_table(row_key=['v'], col_key=['s']). Compute genotype correlation between all pairs of variants:. >>> ld = hl.row_correlation(mt.GT.n_alt_alleles()); >>> ld.to_numpy(); array([[ 1. , -0.85280287, 0.42640143],; [-0.85280287, 1. , -0.5 ],; [ 0.42640143, -0.5 , 1. ]]). Compute genotype correlation between consecutively-indexed variants:. >>> ld.sparsify_band(lower=0, upper=1).to_numpy(); array([[ 1. , -0.85280287, 0. ],; [ 0. , 1. , -0.5 ],; [ 0. , 0. , 1. ]]). Warning; -------; Rows with a constant value (i.e., zero variance) will result `nan`; correlation values. To avoid this, first check that all rows vary or filter; out constant rows (for example, with the help of :func:`.aggregators.stats`). Notes; -----; In this method, each row of entries is regarded as a vector with elements; defined by `entry_expr` and missing values mean-imputed per row.; The ``(i, j)`` element of the resulting block matrix is the correlation; between rows ``i`` and ``j`` (as 0-indexed by order in the matrix table;; see :meth:`~hail.MatrixTable.add_row_index`). The correlation of two vectors is defined as the; `Pearson correlation coeffecient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__; between the corresponding empirical distributions of elements,; or equivalently as the cosine of the angle between the vectors. This method has two stages:. - writing the row-normalized block matrix to a temporary file on persistent; disk with :meth:`.BlockMatrix.from_entry_expr`. The parallelism is; ``n_rows / block_size``. - reading and multiplying this block matrix by its transpose. The; parallelism is ``(n_rows / bl",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:4023,Availability,error,error,4023,"ghborhood of the diagonal. If variants :math:`i` and :math:`j` are on the; same contig and within `radius` base pairs (inclusive) then the; :math:`(i, j)` element is their; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__.; Otherwise, the :math:`(i, j)` element is ``0.0``. Rows with a constant value (i.e., zero variance) will result in ``nan``; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; locus_expr : :class:`.LocusExpression`; Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of `entry_expr`.; radius: :obj:`int` or :obj:`float`; Radius of window for row values.; coord_expr: :class:`.Float64Expression`, optional; Row-indexed numeric expression for the row value on the same table or; matrix table as ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:4342,Availability,error,error,4342,"constant value (i.e., zero variance) will result in ``nan``; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment may; silently produce unexpected results. Parameters; ----------; entry_expr : :class:`.Float64Expression`; Entry-indexed numeric expression on matrix table.; locus_expr : :class:`.LocusExpression`; Row-indexed locus expression on a table or matrix table that is; row-aligned with the matrix table of `entry_expr`.; radius: :obj:`int` or :obj:`float`; Radius of window for row values.; coord_expr: :class:`.Float64Expression`, optional; Row-indexed numeric expression for the row value on the same table or; matrix table as `locus_expr`.; By default, the row value is given by the locus position.; block_size : :obj:`int`, optional; Block size. Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.BlockMatrix`; Windowed correlation matrix between variants.; Row and column indices correspond to matrix table variant index.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:3343,Safety,avoid,avoid,3343,"ly compute linkage disequilibrium between nearby; variants. Use :meth:`row_correlation` directly to calculate correlation; without windowing. More precisely, variants are 0-indexed by their order in the matrix table; (see :meth:`~hail.MatrixTable.add_row_index`). Each variant is regarded as a vector of; elements defined by `entry_expr`, typically the number of alternate alleles; or genotype dosage. Missing values are mean-imputed within variant. The method produces a symmetric block-sparse matrix supported in a; neighborhood of the diagonal. If variants :math:`i` and :math:`j` are on the; same contig and within `radius` base pairs (inclusive) then the; :math:`(i, j)` element is their; `Pearson correlation coefficient <https://en.wikipedia.org/wiki/Pearson_correlation_coefficient>`__.; Otherwise, the :math:`(i, j)` element is ``0.0``. Rows with a constant value (i.e., zero variance) will result in ``nan``; correlation values. To avoid this, first check that all variants vary or; filter out constant variants (for example, with the help of; :func:`.aggregators.stats`). If the :meth:`.global_position` on `locus_expr` is not in ascending order,; this method will fail. Ascending order should hold for a matrix table keyed; by locus or variant (and the associated row table), or for a table that's; been ordered by `locus_expr`. Set `coord_expr` to use a value other than position to define the windows.; This row-indexed numeric expression must be non-missing, non-``nan``, on the; same source as `locus_expr`, and ascending with respect to locus; position for each contig; otherwise the method will raise an error. Warning; -------; See the warnings in :meth:`row_correlation`. In particular, for large; matrices it may be preferable to run its stages separately. `entry_expr` and `locus_expr` are implicitly aligned by row-index, though; they need not be on the same source. If their sources differ in the number; of rows, an error will be raised; otherwise, unintended misalignment ma",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2100,Deployability,update,update,2100,"e minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2289,Deployability,update,updated,2289,"e minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2306,Deployability,update,update,2306,"e minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. If all alternate alleles of a variant are filtered out, the variant itself; is filtered out. **Using** `f`. The `f` argument is a function or lambda evaluated per alternate allele to; determine whether that allele is kept. If `f` evaluates to ``True``, the; allele is kept. If `f` evaluates to ``False`` or missing, the allele is; removed. `f` is a function that takes two arguments: the allele string (of type; :class:`.StringExpression`) and the allele index (of type; :class:`.Int32Expression`), and returns a boolean expression. This can; be either a defined function or a lambda. For example, these two usages; are equivalent:. (with a lambda). >>> ds_result = hl.filter_alleles(ds, lambda allele, i: hl.is_snp(ds.alleles[0], allele)). (with a defined function). >>> def filter_f(allele, allele_index):; ... return hl.is_snp(ds.alleles[0], allele); >>> ds_result = hl.filter_alleles(ds, filter_f). Warning; -------; :func:`.filter_alleles` does not update any fields other than `locus` and; `alleles`. This means that row fields like allele count (AC) and entry; fields like allele depth (AD) can become meaningless unless they are also; updated. You can update them with :meth:`.annotate_rows` and; :meth:`.annotate_entries`. See Also; --------; :func:`.filter_alleles_hts`. Parameters; ----------; mt : :class:`.MatrixTable`; Dataset.; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1783,Availability,down,downcode,1783," rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, comb",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2017,Availability,down,downcoding,2017,"mputing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2391,Availability,down,downcode,2391,"gth is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:4875,Availability,down,downcode,4875,"r on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:4963,Availability,down,downcodes,4963,"r on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:32,Deployability,update,update,32,"""""""Filter alternate alleles and update standard GATK entry fields. Examples; --------; Filter to SNP alleles using the subset strategy:. >>> ds_result = hl.filter_alleles_hts(; ... ds,; ... lambda allele, _: hl.is_snp(ds.alleles[0], allele),; ... subset=True). Update the AC field of the resulting dataset:. >>> updated_info = ds_result.info.annotate(AC = ds_result.new_to_old.map(lambda i: ds_result.info.AC[i-1])); >>> ds_result = ds_result.annotate_rows(info = updated_info). Notes; -----; For usage of the `f` argument, see the :func:`.filter_alleles`; documentation. :func:`.filter_alleles_hts` requires the dataset have the GATK VCF schema,; namely the following entry fields in this order:. .. code-block:: text. GT: call; AD: array<int32>; DP: int32; GQ: int32; PL: array<int32>. Use :meth:`.MatrixTable.select_entries` to rearrange these fields if; necessary. The following new fields are generated:. - `old_locus` (``locus``) -- The old locus, before filtering and computing; the minimal representation.; - `old_alleles` (``array<str>``) -- The old alleles, before filtering and; computing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:4366,Deployability,update,update,4366,"r on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:4518,Deployability,update,updated,4518,"r on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:4535,Deployability,update,update,4535,"r on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:4929,Integrability,depend,depend,4929,"r on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The subset algorithm subsets the AD and PL arrays; (i.e. removes entries corresponding to filtered alleles) and; then sets GT to the genotype with the minimum PL. Note that; if the genotype changes (as in the example), the PLs are; re-normalized (shifted) so that the most likely genotype has a; PL of 0. Qualitatively, subsetting corresponds to the belief; that the filtered alleles are not real so we should discard; any probability mass associated with them. The subset algorithm would produce the following:. .. code-block:: text. GT: 1/1; GQ: 980; AD: 0,50. 0 | 980; 1 | 980 0; +-----------; 0 1. In summary:. - GT: Set to most likely genotype based on the PLs ignoring; the filtered allele(s).; - AD: The filtered alleles' columns are eliminated, e.g.,; filtering alleles 1 and 2 transforms ``25,5,10,20`` to; ``25,20``.; - DP: Unchanged.; - PL: Columns involving filtered alleles are eliminated and; the remaining columns' values are shifted so the minimum; value is 0.; - GQ: The second-lowest PL (after shifting). Warning; -------; :func:`.filter_alleles_hts` does not update any row fields other than; `locus` and `alleles`. This means that row fields like allele count (AC) can; become meaningless unless they are also updated. You can update them with; :meth:`.annotate_rows`. See Also; --------; :func:`.filter_alleles`. Parameters; ----------; mt : :class:`.MatrixTable`; f : callable; Function from (allele: :class:`.StringExpression`, allele_index:; :class:`.Int32Expression`) to :class:`.BooleanExpression`; subset : :obj:`.bool`; Subset PL field if ``True``, otherwise downcode PL field. The; calculation of GT and GQ also depend on whether one subsets or; downcodes the PL. Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2114,Testability,log,log,2114,"mputing the minimal representation.; - `old_to_new` (``array<int32>``) -- An array that maps old allele index to; new allele index. Its length is the same as `old_alleles`. Alleles that; are filtered are missing.; - `new_to_old` (``array<int32>``) -- An array that maps new allele index to; the old allele index. Its length is the same as the modified `alleles`; field. **Downcode algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate allele (allele 1) at a site; with 1 reference allele and 2 alternate alleles. .. code-block:: text. GT: 1/2; GQ: 10; AD: 0,50,35. 0 | 1000; 1 | 1000 10; 2 | 1000 0 20; +-----------------; 0 1 2. The downcode algorithm recodes occurances of filtered alleles; to occurances of the reference allele (e.g. 1 -> 0 in our; example). So the depths of filtered alleles in the AD field; are added to the depth of the reference allele. Where; downcoding filtered alleles merges distinct genotypes, the; minimum PL is used (since PL is on a log scale, this roughly; corresponds to adding probabilities). The PLs are then; re-normalized (shifted) so that the most likely genotype has a; PL of 0, and GT is set to this genotype. If an allele is; filtered, this algorithm acts similarly to; :func:`.split_multi_hts`. The downcode algorithm would produce the following:. .. code-block:: text. GT: 0/1; GQ: 10; AD: 35,50. 0 | 20; 1 | 0 10; +-----------; 0 1. In summary:. - GT: Downcode filtered alleles to reference.; - AD: Columns of filtered alleles are eliminated and their; values are added to the reference column, e.g., filtering; alleles 1 and 2 transforms ``25,5,10,20`` to ``40,20``.; - DP: No change.; - PL: Downcode filtered alleles to reference, combine PLs; using minimum for each overloaded genotype, and shift so; the overall minimum PL is 0.; - GQ: The second-lowest PL (after shifting). **Subset algorithm**. We will illustrate the behavior on the example genotype below; when filtering the first alternate ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:12,Availability,down,downcode,12,"# otherwise downcode",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2553,Availability,error,errors,2553,"a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.from_entry_expr` with; regard to memory and Hadoop replication errors. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 : :obj:`float`; Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size: :obj:`int`; Window size in base pairs (inclusive upper bound).; memory_per_core : :obj:`int`; Memory in MB per core for local pruning queue.; keep_higher_maf: :obj:`int`; If ``True``, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size: :obj:`int`, optional; Block size for block matrices in the second stage.; Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.Table`; Table of a maximal independent set of variants.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1404,Performance,queue,queue,1404,"contains multiallelic variants, the multiallelic variants; must be filtered out or split before being passed to :func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned ",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:1467,Performance,queue,queue,1467,"func:`.ld_prune`. >>> biallelic_dataset = dataset.filter_rows(hl.len(dataset.alleles) == 2); >>> pruned_variant_table = hl.ld_prune(biallelic_dataset.GT, r2=0.2, bp_window_size=500000); >>> filtered_ds = dataset.filter_rows(hl.is_defined(pruned_variant_table[dataset.row_key])). Notes; -----; This method finds a maximal subset of variants such that the squared Pearson; correlation coefficient :math:`r^2` of any pair at most `bp_window_size`; base pairs apart is strictly less than `r2`. Each variant is represented as; a vector over samples with elements given by the (mean-imputed) number of; alternate alleles. In particular, even if present, **phase information is; ignored**. Variants that do not vary across samples are dropped. The method prunes variants in linkage disequilibrium in three stages. - The first, ""local pruning"" stage prunes correlated variants within each; partition, using a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py:2986,Performance,queue,queue,2986,"a local variant queue whose size is determined by; `memory_per_core`. A larger queue may facilitate more local pruning in; this stage. Minor allele frequency is not taken into account. The; parallelism is the number of matrix table partitions. - The second, ""global correlation"" stage uses block-sparse matrix; multiplication to compute correlation between each pair of remaining; variants within `bp_window_size` base pairs, and then forms a graph of; correlated variants. The parallelism of writing the locally-pruned matrix; table as a block matrix is ``n_locally_pruned_variants / block_size``. - The third, ""global pruning"" stage applies :func:`.maximal_independent_set`; to prune variants from this graph until no edges remain. This algorithm; iteratively removes the variant with the highest vertex degree. If; `keep_higher_maf` is true, then in the case of a tie for highest degree,; the variant with lowest minor allele frequency is removed. Warning; -------; The locally-pruned matrix table and block matrix are stored as temporary files; on persistent disk. See the warnings on `BlockMatrix.from_entry_expr` with; regard to memory and Hadoop replication errors. Parameters; ----------; call_expr : :class:`.CallExpression`; Entry-indexed call expression on a matrix table with row-indexed; variants and column-indexed samples.; r2 : :obj:`float`; Squared correlation threshold (exclusive upper bound).; Must be in the range [0.0, 1.0].; bp_window_size: :obj:`int`; Window size in base pairs (inclusive upper bound).; memory_per_core : :obj:`int`; Memory in MB per core for local pruning queue.; keep_higher_maf: :obj:`int`; If ``True``, break ties at each step of the global pruning stage by; preferring to keep variants with higher minor allele frequency.; block_size: :obj:`int`, optional; Block size for block matrices in the second stage.; Default given by :meth:`.BlockMatrix.default_block_size`. Returns; -------; :class:`.Table`; Table of a maximal independent set of variants.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/methods/statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/relatedness/identity_by_descent.py:952,Performance,perform,perform,952,"""""""Compute matrix of identity-by-descent estimates. .. include:: ../_templates/req_tstring.rst. .. include:: ../_templates/req_tvariant.rst. .. include:: ../_templates/req_biallelic.rst. Examples; --------. To calculate a full IBD matrix, using minor allele frequencies computed; from the dataset itself:. >>> hl.identity_by_descent(dataset). To calculate an IBD matrix containing only pairs of samples with; ``PI_HAT`` in :math:`[0.2, 0.9]`, using minor allele frequencies stored in; the row field `panel_maf`:. >>> hl.identity_by_descent(dataset, maf=dataset['panel_maf'], min=0.2, max=0.9). Notes; -----. The dataset must have a column field named `s` which is a :class:`.StringExpression`; and which uniquely identifies a column. The implementation is based on the IBD algorithm described in the `PLINK; paper <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1950838>`__. :func:`.identity_by_descent` requires the dataset to be biallelic and does; not perform LD pruning. Linkage disequilibrium may bias the result so; consider filtering variants first. The resulting :class:`.Table` entries have the type: *{ i: String,; j: String, ibd: { Z0: Double, Z1: Double, Z2: Double, PI_HAT: Double },; ibs0: Long, ibs1: Long, ibs2: Long }*. The key list is: `*i: String, j:; String*`. Conceptually, the output is a symmetric, sample-by-sample matrix. The; output table has the following form. .. code-block:: text. i		j	ibd.Z0	ibd.Z1	ibd.Z2	ibd.PI_HAT ibs0	ibs1	ibs2; sample1	sample2	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample3	1.0000	0.0000	0.0000	0.0000 ...; sample1	sample4	0.6807	0.0000	0.3193	0.3193 ...; sample1	sample5	0.1966	0.0000	0.8034	0.8034 ... Parameters; ----------; dataset : :class:`.MatrixTable`; Variant-keyed and sample-keyed :class:`.MatrixTable` containing genotype information.; maf : :class:`.Float64Expression`, optional; Row-indexed expression for the minor allele frequency.; bounded : :obj:`bool`; Forces the estimations for ``Z0``, ``Z1``, ``Z2``, and ``PI_HAT`` to take; on",MatchSource.CODE_COMMENT,hail/python/hail/methods/relatedness/identity_by_descent.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/methods/relatedness/identity_by_descent.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/nd/nd.py:185,Modifiability,variab,variables,185,"""""""Solve a linear system. Parameters; ----------; a : :class:`.NDArrayNumericExpression`, (N, N); Coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the system Ax = B. Shape is same as shape of B. """"""",MatchSource.CODE_COMMENT,hail/python/hail/nd/nd.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/nd/nd.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/nd/nd.py:220,Modifiability,variab,variables,220,"""""""Solve a triangular linear system Ax = b for x. Parameters; ----------; A : :class:`.NDArrayNumericExpression`, (N, N); Triangular coefficient matrix.; b : :class:`.NDArrayNumericExpression`, (N,) or (N, K); Dependent variables.; lower : `bool`:; If true, A is interpreted as a lower triangular matrix; If false, A is interpreted as a upper triangular matrix. Returns; -------; :class:`.NDArrayNumericExpression`, (N,) or (N, K); Solution to the triangular system Ax = B. Shape is same as shape of B. """"""",MatchSource.CODE_COMMENT,hail/python/hail/nd/nd.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/nd/nd.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:364,Testability,log,log,364,"""""""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:419,Testability,log,log,419,"""""""Create a cumulative density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter (passed to :func:`~.approx_cdf`).; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:17,Availability,down,down,17,"# line must bend down at j",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:286,Testability,log,log,286,"""""""Create a density plot. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; k : int; Accuracy parameter.; smoothing : float; Degree of smoothing.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts.; interactive : bool; If `True`, return a handle to pass to :func:`bokeh.io.show`.; figure : :class:`bokeh.plotting.figure`; If not None, add density plot to figure. Otherwise, create a new figure. Returns; -------; :class:`bokeh.plotting.figure`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:474,Testability,log,log,474,"""""""Create a histogram. Notes; -----; `data` can be a :class:`.Float64Expression`, or the result of the :func:`~.aggregators.hist`; or :func:`~.aggregators.approx_cdf` aggregators. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:403,Testability,log,log,403,"""""""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:458,Testability,log,log,458,"""""""Create a cumulative histogram. Parameters; ----------; data : :class:`.Struct` or :class:`.Float64Expression`; Sequence of data to plot.; range : Tuple[float]; Range of x values in the histogram.; bins : int; Number of bins in the histogram.; legend : str; Label of data on the x-axis.; title : str; Title of the histogram.; normalize: bool; Whether or not the cumulative data should be normalized.; log: bool; Whether or not the y-axis should be of type log. Returns; -------; :class:`bokeh.plotting.figure`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:1670,Availability,avail,available,1670,"""""""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:254,Modifiability,variab,variable,254,"""""""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:1757,Testability,log,log,1757,"""""""Plot a two-dimensional histogram. ``x`` and ``y`` must both be a :class:`.NumericExpression` from the same :class:`.Table`. If ``x_range`` or ``y_range`` are not provided, the function will do a pass through the data to determine; min and max of each variable. Examples; --------. >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y). >>> ht = hail.utils.range_table(1000).annotate(x=hail.rand_norm(), y=hail.rand_norm()); >>> p_hist = hail.plot.histogram2d(ht.x, ht.y, bins=10, range=((0, 1), None)). Parameters; ----------; x : :class:`.NumericExpression`; Expression for x-axis (from a Hail table).; y : :class:`.NumericExpression`; Expression for y-axis (from the same Hail table as ``x``).; bins : int or [int, int]; The bin specification:; - If int, the number of bins for the two dimensions (nx = ny = bins).; - If [int, int], the number of bins in each dimension (nx, ny = bins).; The default value is 40.; range : None or ((float, float), (float, float)); The leftmost and rightmost edges of the bins along each dimension:; ((xmin, xmax), (ymin, ymax)). All values outside of this range will be considered outliers; and not tallied in the histogram. If this value is None, or either of the inner lists is None,; the range will be computed from the data.; width : int; Plot width (default 600px).; height : int; Plot height (default 600px).; title : str; Title of the plot.; colors : Sequence[str]; List of colors (hex codes, or strings as described; `here <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__). Compatible with one of the many; built-in palettes available `here <https://bokeh.pydata.org/en/latest/docs/reference/palettes.html>`__.; log : bool; Plot the log10 of the bin counts. Returns; -------; :class:`bokeh.plotting.figure`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:50,Availability,down,downsample,50,"# FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:70,Deployability,continuous,continuous,70,"# FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:36,Testability,log,logic,36,"# FIXME: remove the type conversion logic if/when downsample supports continuous values for labels; # Save all numeric types to cast in DataFrame",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:1306,Availability,down,down,1306,"element is used as the hover label. If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive scatter plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space unit",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:3059,Availability,down,downsample,3059,"tional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`bokeh.models.Plot` if no label or a single label was given, otherwise :class:`bokeh.models.layouts.Column`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:1022,Deployability,continuous,continuous,1022,"""""""Create an interactive scatter plot. ``x`` and ``y`` must both be either:; - a :class:`.NumericExpression` from the same :class:`.Table`.; - a tuple (str, :class:`.NumericExpression`) from the same :class:`.Table`. If passed as a tuple the first element is used as the hover label. If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive scatter plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the diffe",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:2048,Deployability,continuous,continuous,2048,"For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Fac",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:1344,Availability,down,down,1344,"ement is used as the hover label. This function returns a :class:`bokeh.models.layouts.Column` containing two :class:`figure.Row`:; - The first row contains the X-axis marginal density and a selection widget if multiple entries are specified in the ``label``; - The second row contains the scatter plot and the y-axis marginal density. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of ",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:3125,Availability,down,downsample,3125,"vering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`.GridPlot`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:1040,Deployability,continuous,continuous,1040,"""""""Create an interactive scatter plot with marginal densities on the side. ``x`` and ``y`` must both be either:; - a :class:`.NumericExpression` from the same :class:`.Table`.; - a tuple (str, :class:`.NumericExpression`) from the same :class:`.Table`. If passed as a tuple the first element is used as the hover label. This function returns a :class:`bokeh.models.layouts.Column` containing two :class:`figure.Row`:; - The first row contains the X-axis marginal density and a selection widget if multiple entries are specified in the ``label``; - The second row contains the scatter plot and the y-axis marginal density. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When mult",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:2114,Deployability,continuous,continuous,2114,"tems in the legend will hide/show all points with the corresponding label in the scatter plot.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points in the scatter plot displays their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; ----------; x : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of x-values to be plotted.; y : :class:`.NumericExpression` or (str, :class:`.NumericExpression`); List of y-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]], optional; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool, optional; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Fac",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:1095,Availability,down,down,1095,"https://en.wikipedia.org/wiki/Q-Q_plot). If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive qq plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:2690,Availability,down,downsample,2690,"those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; missing_label: str; Label to use when a point is missing data for a categorical label. Returns; -------; :class:`bokeh.plotting.figure` if no label or a single label was given, otherwise :class:`bokeh.models.layouts.Column`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:811,Deployability,continuous,continuous,811,"""""""Create a Quantile-Quantile plot. (https://en.wikipedia.org/wiki/Q-Q_plot). If no label or a single label is provided, then returns :class:`bokeh.plotting.figure`; Otherwise returns a :class:`bokeh.models.layouts.Column` containing:; - a :class:`bokeh.models.widgets.inputs.Select` dropdown selection widget for labels; - a :class:`bokeh.plotting.figure` containing the interactive qq plot. Points will be colored by one of the labels defined in the ``label`` using the color scheme defined in; the corresponding entry of ``colors`` if provided (otherwise a default scheme is used). To specify your color; mapper, check `the bokeh documentation <https://bokeh.pydata.org/en/latest/docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:1689,Deployability,continuous,continuous,1689,"docs/reference/colors.html>`__; for CategoricalMapper for categorical labels, and for LinearColorMapper and LogColorMapper; for continuous labels.; For categorical labels, clicking on one of the items in the legend will hide/show all points with the corresponding label.; Note that using many different labelling schemes in the same plots, particularly if those labels contain many; different classes could slow down the plot interactions. Hovering on points will display their coordinates, labels and any additional fields specified in ``hover_fields``. Parameters; ----------; pvals : :class:`.NumericExpression`; List of x-values to be plotted.; label : :class:`.Expression` or Dict[str, :class:`.Expression`]]; Either a single expression (if a single label is desired), or a; dictionary of label name -> label value for x and y values.; Used to color each point w.r.t its label.; When multiple labels are given, a dropdown will be displayed with the different options.; Can be used with categorical or continuous expressions.; title : str, optional; Title of the scatterplot.; xlabel : str, optional; X-axis label.; ylabel : str, optional; Y-axis label.; size : int; Size of markers in screen space units.; legend: bool; Whether or not to show the legend in the resulting figure.; hover_fields : Dict[str, :class:`.Expression`], optional; Extra fields to be displayed when hovering over a point on the plot.; colors : :class:`bokeh.models.mappers.ColorMapper` or Dict[str, :class:`bokeh.models.mappers.ColorMapper`], optional; If a single label is used, then this can be a color mapper, if multiple labels are used, then this should; be a Dict of label name -> color mapper.; Used to set colors for the labels defined using ``label``.; If not used at all, or label names not appearing in this dict will be colored using a default color scheme.; width: int; Plot width; height: int; Plot height; collect_all : bool; Deprecated. Use `n_divisions` instead.; n_divisions : int, optional; Factor by whi",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py:589,Availability,down,downsample,589,"""""""Create a Manhattan plot. (https://en.wikipedia.org/wiki/Manhattan_plot). Parameters; ----------; pvals : :class:`.Float64Expression`; P-values to be plotted.; locus : :class:`.LocusExpression`, optional; Locus values to be plotted.; title : str, optional; Title of the plot.; size : int; Size of markers in screen space units.; hover_fields : Dict[str, :class:`.Expression`], optional; Dictionary of field names and values to be shown in the HoverTool of the plot.; collect_all : bool, optional; Deprecated - use `n_divisions` instead.; n_divisions : int, optional.; Factor by which to downsample (default value = 500).; A lower input results in fewer output datapoints.; Use `None` to collect all points.; significance_line : float, optional; p-value at which to add a horizontal, dotted red line indicating; genome-wide significance. If ``None``, no line is added. Returns; -------; :class:`bokeh.models.Plot`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/plot/plots.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/plot/plots.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/typecheck/check.py:24,Availability,error,errors,24,"# reject str because of errors due to sequenceof(strlike) permitting str",MatchSource.CODE_COMMENT,hail/python/hail/typecheck/check.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/typecheck/check.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/typecheck/check.py:21,Performance,perform,performs,21,"""""""Type checker that performs argument transformations. The `fs` argument should be a varargs of 2-tuples that each contain a; TypeChecker and a lambda function, e.g.:. ((only(int), lambda x: x * 2),; sequenceof(int), lambda x: x[0])); """"""",MatchSource.CODE_COMMENT,hail/python/hail/typecheck/check.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/typecheck/check.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/typecheck/check.py:23,Modifiability,variab,variable,23,"# kwargs now holds all variable kwargs",MatchSource.CODE_COMMENT,hail/python/hail/typecheck/check.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/typecheck/check.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:1352,Availability,error,error,1352,"nything larger than 50 MB). Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:1615,Availability,error,error,1615,"nything larger than 50 MB). Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:2105,Security,access,access,2105,"nything larger than 50 MB). Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hadoop_open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hadoop_open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier). .. caution::. These file handles are slower than standard Python file handles. If you; are writing a large file (larger than ~50M), it will be faster to write; to a local file using standard Python I/O and use :func:`.hadoop_copy`; to move your file to a distributed file system. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:355,Usability,simpl,simpler,355,"""""""Copy a file through the Hadoop filesystem API.; Supports distributed file systems like hdfs, gs, and s3. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hadoop_copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. Try using :func:`.hadoop_open` first, it's simpler, but not great; for large data! For example:. >>> with hadoop_open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... pandas_df.to_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers). Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:92,Availability,error,error,92,"""""""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:70,Availability,error,error,70,"""""""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:31,Testability,log,log,31,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:166,Testability,log,log,166,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:204,Testability,log,log,204,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:245,Testability,log,log,245,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:342,Testability,log,log,342,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:403,Testability,log,log,403,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:453,Testability,log,log,453,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:550,Testability,log,logs,550,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:612,Testability,log,log,612,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:693,Testability,log,log,693,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py:741,Testability,log,log,741,"""""""Attempt to copy the session log to a hadoop-API-compatible location. Examples; --------; Specify a manual path:. >>> hl.copy_log('gs://my-bucket/analysis-10-jan19.log') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/analysis-10-jan19.log'... Copy to a directory:. >>> hl.copy_log('gs://my-bucket/') # doctest: +SKIP; INFO: copying log to 'gs://my-bucket/hail-20180924-2018-devel-46e5fad57524.log'... Notes; -----; Since Hail cannot currently log directly to distributed file systems, this; function is provided as a utility for offloading logs from ephemeral nodes. If `path` is a directory, then the log file will be copied using its; base name to the directory (e.g. ``/home/hail.log`` would be copied as; ``gs://my-bucket/hail.log`` if `path` is ``gs://my-bucket``. Parameters; ----------; path: :class:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/hadoop_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/hadoop_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/java.py:33,Availability,error,error,33,""""""":class:`.HailUserError` is an error thrown by Hail when the user makes an error.""""""",MatchSource.CODE_COMMENT,hail/python/hail/utils/java.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/java.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/java.py:77,Availability,error,error,77,""""""":class:`.HailUserError` is an error thrown by Hail when the user makes an error.""""""",MatchSource.CODE_COMMENT,hail/python/hail/utils/java.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/java.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/java.py:30,Availability,error,error,30,""""""":class:`.FatalError` is an error thrown by Hail method failures""""""",MatchSource.CODE_COMMENT,hail/python/hail/utils/java.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/java.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/java.py:58,Availability,failure,failures,58,""""""":class:`.FatalError` is an error thrown by Hail method failures""""""",MatchSource.CODE_COMMENT,hail/python/hail/utils/java.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/java.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:506,Performance,optimiz,optimized,506,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:532,Performance,perform,performance,532,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:473,Testability,test,testing,473,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:485,Usability,learn,learning,485,"""""""Construct a matrix table with row and column indices and no entry fields. Examples; --------. >>> range_ds = hl.utils.range_matrix_table(n_rows=100, n_cols=10). >>> range_ds.count_rows(); 100. >>> range_ds.count_cols(); 10. Notes; -----; The resulting matrix table contains the following fields:. - `row_idx` (:py:data:`.tint32`) - Row index (row key).; - `col_idx` (:py:data:`.tint32`) - Column index (column key). It contains no entry fields. This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n_rows : :obj:`int`; Number of rows.; n_cols : :obj:`int`; Number of columns.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:299,Performance,optimiz,optimized,299,"""""""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:325,Performance,perform,performance,325,"""""""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:266,Testability,test,testing,266,"""""""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:278,Usability,learn,learning,278,"""""""Construct a table with the row index and no other fields. Examples; --------. >>> df = hl.utils.range_table(100). >>> df.count(); 100. Notes; -----; The resulting table contains one field:. - `idx` (:py:data:`.tint32`) - Row index (key). This method is meant for testing and learning, and is not optimized for; production performance. Parameters; ----------; n : int; Number of rows.; n_partitions : int, optional; Number of partitions (uses Spark default parallelism if None). Returns; -------; :class:`.Table`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:35,Security,access,access,35,"# don't handle 'private' attribute access",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:25,Availability,redundant,redundant,25,"# don't clog the IR with redundant field names",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py:25,Safety,redund,redundant,25,"# don't clog the IR with redundant field names",MatchSource.CODE_COMMENT,hail/python/hail/utils/misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py:174,Security,access,accessing,174,"""""""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/struct.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py:367,Security,access,accessed,367,"""""""; Nested annotation structure. >>> bar = hl.Struct(**{'foo': 5, '1kg': 10}). Struct elements are treated as both 'items' and 'attributes', which; allows either syntax for accessing the element ""foo"" of struct ""bar"":. >>> bar.foo; >>> bar['foo']. Field names that are not valid Python identifiers, such as fields that; start with numbers or contain spaces, must be accessed with the latter; syntax:. >>> bar['1kg']. The ``pprint`` module can be used to print nested Structs in a more; human-readable fashion:. >>> from pprint import pprint; >>> pprint(bar). Parameters; ----------; attributes; Field names and values. Note; ----; This object refers to the Python value returned by taking or collecting; Hail expressions, e.g. ``mt.info.take(5)``. This is rare; it is much; more common to manipulate the :class:`.StructExpression` object, which is; constructed using the :func:`.struct` function.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/struct.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py:18,Safety,avoid,avoid,18,"# Set this way to avoid an infinite recursion in `__getattr__`.",MatchSource.CODE_COMMENT,hail/python/hail/utils/struct.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py:376,Deployability,update,updated,376,"""""""Add new fields or recompute existing fields. Notes; -----; If an expression in `kwargs` shares a name with a field of the; struct, then that field will be replaced but keep its position in; the struct. New fields will be appended to the end of the struct. Parameters; ----------; kwargs : keyword args; Fields to add. Returns; -------; :class:`.Struct`; Struct with new or updated fields. Examples; --------. Define a Struct `s`. >>> s = hl.Struct(food=8, fruit=5). Add a new field to `s`. >>> s.annotate(bar=2); Struct(food=8, fruit=5, bar=2). Add multiple fields to `s`. >>> s.annotate(banana=2, apple=3); Struct(food=8, fruit=5, banana=2, apple=3). Recompute an existing field in `s`. >>> s.annotate(bar=4, fruit=2); Struct(food=8, fruit=2, bar=4); """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/struct.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py:9,Deployability,patch,patch,9,"# monkey-patch pprint",MatchSource.CODE_COMMENT,hail/python/hail/utils/struct.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/struct.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/tutorial.py:132,Availability,down,download,132,"""""""Download subset of the `1000 Genomes <http://www.internationalgenome.org/>`__; dataset and sample annotations. Notes; -----; The download is about 15M. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/tutorial.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/tutorial.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/tutorial.py:183,Availability,down,download,183,"""""""Download subset of the `Human Genome Diversity Panel; <https://www.internationalgenome.org/data-portal/data-collection/hgdp/>`__; dataset and sample annotations. Notes; -----; The download is about 30MB. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite any existing files/directories at `output_dir`.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/tutorial.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/tutorial.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/tutorial.py:57,Availability,down,download,57,"""""""Download public Movie Lens dataset. Notes; -----; The download is about 6M. See the; `MovieLens website <https://grouplens.org/datasets/movielens/100k/>`__; for more information about this dataset. Parameters; ----------; output_dir; Directory in which to write data.; overwrite; If ``True``, overwrite existing files/directories at those locations.; """"""",MatchSource.CODE_COMMENT,hail/python/hail/utils/tutorial.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/utils/tutorial.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py:2,Availability,checkpoint,checkpoint,2,"# checkpoint for efficient multiple downstream usages",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py:36,Availability,down,downstream,36,"# checkpoint for efficient multiple downstream usages",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py:17,Energy Efficiency,efficient,efficient,17,"# checkpoint for efficient multiple downstream usages",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py:302,Availability,error,error,302,"""""""Split the multiallelic variants in a :class:`.VariantDataset`. Parameters; ----------; vds : :class:`.VariantDataset`; Dataset in VariantDataset representation.; filter_changed_loci : :obj:`bool`; If any REF/ALT pair changes locus under :func:`.min_rep`, filter that; variant instead of throwing an error. Returns; -------; :class:`.VariantDataset`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py:2,Modifiability,rewrite,rewrite,2,"# rewrite reference blocks to end at the first of (interval end, reference block end)",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py:883,Deployability,patch,patch,883,"""""""Cap reference blocks at a maximum length in order to permit faster interval filtering. Examples; --------; Truncate reference blocks to 5 kilobases:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, max_ref_block_base_pairs=5000) # doctest: +SKIP. Truncate the longest 1% of reference blocks to the length of the 99th percentile block:. >>> vds2 = hl.vds.truncate_reference_blocks(vds, ref_block_winsorize_fraction=0.01) # doctest: +SKIP. Notes; -----; After this function has been run, the reference blocks have a known maximum length `ref_block_max_length`,; stored in the global fields, which permits :func:`.vds.filter_intervals` to filter to intervals of the reference; data by reading `ref_block_max_length` bases ahead of each interval. This allows narrow interval queries; to run in roughly O(data kept) work rather than O(all reference data) work. It is also possible to patch an existing VDS to store the max reference block length with :func:`.vds.store_ref_block_max_length`. See Also; --------; :func:`.vds.store_ref_block_max_length`. Parameters; ----------; vds : :class:`.VariantDataset` or :class:`.MatrixTable`; max_ref_block_base_pairs; Maximum size of reference blocks, in base pairs.; ref_block_winsorize_fraction; Fraction of reference block length distribution to truncate / winsorize. Returns; -------; :class:`.VariantDataset` or :class:`.MatrixTable`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/vds/methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/variant_dataset.py:326,Availability,down,downstream,326,"""""""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/vds/variant_dataset.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/variant_dataset.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/variant_dataset.py:234,Deployability,patch,patch,234,"""""""Patches an existing VDS file to store the max reference block length for faster interval filters. This method permits :func:`.vds.filter_intervals` to remove reference data not overlapping a target interval. This method is able to patch an existing VDS file in-place, without copying all the data. However,; if significant downstream interval filtering is anticipated, it may be advantageous to run; :func:`.vds.truncate_reference_blocks` to truncate long reference blocks and make interval filters; even faster. However, truncation requires rewriting the entire VDS. Examples; --------; >>> hl.vds.store_ref_block_max_length('gs://path/to/my.vds') # doctest: +SKIP. See Also; --------; :func:`.vds.filter_intervals`, :func:`.vds.truncate_reference_blocks`. Parameters; ----------; vds_path : :obj:`str`; """"""",MatchSource.CODE_COMMENT,hail/python/hail/vds/variant_dataset.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/variant_dataset.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py:2,Security,hash,hashable,2,"# hashable stable value",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/combine.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py:2,Security,hash,hashable,2,"# hashable stable value",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/combine.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py:2,Security,hash,hashable,2,"# hashable stable value",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/combine.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py:2,Security,hash,hashable,2,"# hashable stable value",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/combine.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py:349,Availability,checkpoint,checkpointing,349,"""""""takes a table, keyed by ['locus', ...] and produces a list of intervals suitable; for repartitioning a combiner matrix table. Parameters; ----------; mt : :class:`.MatrixTable`; Sparse MT intermediate.; desired_average_partition_size : :obj:`int`; Average target number of rows for each partition.; tmp_path : :obj:`str`; Temporary path for scan checkpointing. Returns; -------; (:obj:`List[Interval]`, :obj:`.Type`); """"""",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/combine.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/combine.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:21,Availability,failure,failure-tolerant,21,"""""""A restartable and failure-tolerant method for combining one or more GVCFs and Variant Datasets. Examples; --------. A Variant Dataset comprises one or more sequences. A new Variant Dataset is constructed from; GVCF files and/or extant Variant Datasets. For example, the following produces a new Variant; Dataset from four GVCF files containing whole genome sequences ::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:1582,Availability,avail,available,1582,"ew_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :class:`int`; The number of Variant Datasets to combine at once.; target_records : :class:`int`; The target number of var",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:1884,Availability,failure,failure,1884,"Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :class:`int`; The number of Variant Datasets to combine at once.; target_records : :class:`int`; The target number of variants per partition.; gvcf_batch_size : :class:`int`; The number of GVCFs to combine into a Variant Dataset at once.; contig_recoding : :class:`dict` mapping :class:`str` to :class:`str` or :obj:`None`; This mapping is applied to GVCF contigs before importing them into Hail. This is u",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:1457,Integrability,depend,depends,1457,"10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; gvcf_paths=gvcfs,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The following combines four new samples from GVCFs with multiple extant Variant Datasets::. gvcfs = [; 'gs://bucket/sample_10123.g.vcf.bgz',; 'gs://bucket/sample_10124.g.vcf.bgz',; 'gs://bucket/sample_10125.g.vcf.bgz',; 'gs://bucket/sample_10126.g.vcf.bgz',; ]. vdses = [; 'gs://bucket/hgdp.vds',; 'gs://bucket/1kg.vds'; ]. combiner = hl.vds.new_combiner(; output_path='gs://bucket/dataset.vds',; temp_path='gs://1-day-temp-bucket/',; save_path='gs://1-day-temp-bucket/combiner-plan.json',; gvcf_paths=gvcfs,; vds_paths=vdses,; use_genome_default_intervals=True,; ). combiner.run(). vds = hl.read_vds('gs://bucket/dataset.vds'). The speed of the Variant Dataset Combiner critically depends on data partitioning. Although the; partitioning is fully customizable, two high-quality partitioning strategies are available by; default, one for exomes and one for genomes. These partitioning strategies can be enabled,; respectively, with the parameters: ``use_exome_default_intervals=True`` and; ``use_genome_default_intervals=True``. The combiner serializes itself to `save_path` so that it can be restarted after failure. Parameters; ----------; save_path : :class:`str`; The file path to store this VariantDatasetCombiner plan. A failed or interrupted; execution can be restarted using this plan.; output_path : :class:`str`; The location to store the new VariantDataset.; temp_path : :class:`str`; The location to store temporary intermediates. We recommend using a bucket with an automatic; deletion or lifecycle policy.; reference_genome : :class:`.ReferenceGenome`; The reference genome to which all inputs (GVCFs and Variant Datasets) are aligned.; branch_factor : :cl",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:8,Integrability,message,messages,8,"# these messages get printed, because there is absolutely no guarantee; # that the hail context is in a sane state if any of the above operations; # fail",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:52,Performance,load,load,52,"""""""Create a new :class:`.VariantDatasetCombiner` or load one from `save_path`.""""""",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:234,Availability,failure,failure,234,"# we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large)",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:75,Security,hash,hash,75,"# we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large)",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:209,Usability,resume,resume,209,"# we overwrite these values as they are serialized, but not part of the; # hash for an autogenerated name and we want users to be able to overwrite; # these when resuming a combine (a common reason to need to resume a combine; # is a failure due to branch factor being too large)",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py:44,Security,validat,validating,44,"# We do the first save_path check now after validating the arguments",MatchSource.CODE_COMMENT,hail/python/hail/vds/combiner/variant_dataset_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hail/vds/combiner/variant_dataset_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/tls.py:75,Security,certificate,certificates,75,"# setting cafile in `create_default_context` ignores the system default; # certificates. We must explicitly request them again with; # load_default_certs.",MatchSource.CODE_COMMENT,hail/python/hailtop/tls.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/tls.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py:104,Performance,load,loaded,104,"# pylint: disable=unused-argument; # Because the S3 upload_part API call requires the entire part; # be loaded into memory, use a smaller part size.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py:676,Availability,failure,failures,676,"# pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py:704,Availability,failure,failure,704,"# pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py:72,Energy Efficiency,efficient,efficient,72,"# pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py:1077,Integrability,interface,interface,1077,"# pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py:1611,Integrability,interface,interface,1611,"# pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py:513,Performance,perform,performance,513,"# pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py:718,Safety,safe,safe,718,"# pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py:983,Security,hash,hash,983,"# pylint: disable=unused-argument; # It may be possible to write a more efficient version of this; # that takes advantage of retry_writes=False. Here's the; # background information:; #; # There are essentially three options for implementing writes.; # The first two handle retries:; #; # 1. Use some form of multipart uploads (which, in the case; # of GCS, we implement by writing temporary objects and; # then calling compose).; #; # 2. Use resumable uploads. This is what the GCS backend; # does, although the performance is must worse than; # non-resumable uploads so in fact it may always be better; # to always use multipart uploads (1).; #; # The third does not handle failures:; #; # 3. Don't be failure/retry safe. Just write the object, and; # if the API call fails, fail. This is useful when you can; # retry at a higher level (this is what the copy code does).; #; # Unfortunately, I don't see how to do (3) with boto3, since; # AWS APIs require a header that includes a hash of the; # request body, and that needs to be computed up front. In; # terms of the boto3 interface, this contraint translates into; # calls like `put_object` require bytes or a seekable stream; # (so it can make two passes over the data, one to compute the; # checksome, and the other to send the data).; #; # Here, we use S3CreateManager, which in turn uses boto3; # `upload_fileobj` which is implemented in terms of multipart; # uploads.; #; # Another possibility is to make an alternate `create` call; # that takes bytes instead of returning a file-like object,; # and then using `put_object`, and make copy use that; # interface. This has the disadvantage that the read must; # complete before the write can begin (unlike the current; # code, that copies 128MB parts in 256KB chunks).",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioaws/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioaws/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py:6,Availability,error,errors,6,"# ABS errors if you attempt credentialed access for a public container,; # so we try once with credentials, if that fails use anonymous access for; # that container going forward.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py:41,Security,access,access,41,"# ABS errors if you attempt credentialed access for a public container,; # so we try once with credentials, if that fails use anonymous access for; # that container going forward.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py:136,Security,access,access,136,"# ABS errors if you attempt credentialed access for a public container,; # so we try once with credentials, if that fails use anonymous access for; # that container going forward.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py:117,Deployability,configurat,configuration,117,"# https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py:117,Modifiability,config,configuration,117,"# https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py:117,Deployability,configurat,configuration,117,"# https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py:117,Modifiability,config,configuration,117,"# https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/storage/azure-storage-blob#other-client--per-operation-configuration",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/client/arm_client.py:54,Deployability,deploy,deployments,54,"# https://docs.microsoft.com/en-us/rest/api/resources/deployments/list-by-resource-group",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aioazure/client/arm_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aioazure/client/arm_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py:2,Integrability,protocol,protocol,2,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/web-server#offline; # studying `gcloud --log-http print-access-token` was also useful",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py:70,Integrability,protocol,protocols,70,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/web-server#offline; # studying `gcloud --log-http print-access-token` was also useful",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py:143,Security,access,access-token,143,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/web-server#offline; # studying `gcloud --log-http print-access-token` was also useful",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py:128,Testability,log,log-http,128,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/web-server#offline; # studying `gcloud --log-http print-access-token` was also useful",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py:2,Integrability,protocol,protocol,2,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/service-account; # studying `gcloud --log-http print-access-token` was also useful",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py:70,Integrability,protocol,protocols,70,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/service-account; # studying `gcloud --log-http print-access-token` was also useful",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py:140,Security,access,access-token,140,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/service-account; # studying `gcloud --log-http print-access-token` was also useful",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py:125,Testability,log,log-http,125,"# protocol documented here:; # https://developers.google.com/identity/protocols/oauth2/service-account; # studying `gcloud --log-http print-access-token` was also useful",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py:40,Security,access,access,40,"# https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#applications",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/user_config.py:129,Deployability,configurat,configuration,129,"# https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md#cloud-storage-requester-pays-feature-configuration",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/user_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/user_config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/user_config.py:129,Modifiability,config,configuration,129,"# https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md#cloud-storage-requester-pays-feature-configuration",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/user_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/user_config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/logging_client.py:36,Testability,log,logging,36,"# docs:; # https://cloud.google.com/logging/docs/reference/v2/rest; # https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/list",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/logging_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/logging_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/logging_client.py:95,Testability,log,logging,95,"# docs:; # https://cloud.google.com/logging/docs/reference/v2/rest; # https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/list",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/logging_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/logging_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:205,Performance,perform,performing-resumable-uploads,205,"# If the last request was unsuccessful, we are out of sync; # with the server and we don't know what byte to send; # next. Perform a status check to find out. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads#status-check; # note: this retries",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:77,Availability,avail,available,77,"# status check can advance the offset, so there might not be a; # full chunk available to write",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:71,Performance,perform,performing-resumable-uploads,71,"# Upload a single chunk. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads#chunked-upload",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:68,Safety,timeout,timeout,68,"# Around May 2022, GCS started timing out a lot with our default 5s timeout",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:79,Performance,perform,performing-resumable-uploads,79,"# Write using resumable uploads. See:; # https://cloud.google.com/storage/docs/performing-resumable-uploads",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:250,Security,access,access,250,"""""""; See `the GCS API docs https://cloud.google.com/storage/docs/storage-classes`_ for a list of possible storage; classes. Raises; ------; :class:`aiohttp.ClientResponseError`; If the specified object does not exist, or if the account being used to access GCS does not have permission; to read the bucket's default storage policy and it is not a public access bucket.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:354,Security,access,access,354,"""""""; See `the GCS API docs https://cloud.google.com/storage/docs/storage-classes`_ for a list of possible storage; classes. Raises; ------; :class:`aiohttp.ClientResponseError`; If the specified object does not exist, or if the account being used to access GCS does not have permission; to read the bucket's default storage policy and it is not a public access bucket.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:421,Modifiability,rewrite,rewrite,421,"""""""Copy a google cloud object ``src`` to another google cloud object ``dest``. Parameters; ----------; src : :class:`str`; GCS object to copy, must be a valid ``gs://`` URL; dest : :class:`str`; Valid ``gs://`` URL for the destination of the copy; callback : function ( (response, first) -> None ); Optional callback to call after every request (for updating things like a progress bar).; The ``response`` argument is a `rewrite <https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite#response>`_; response dictionary, and ``first`` will be ``True`` if this is the first time this; callback has been called for this copy. Returns; -------; :obj:`NoneType`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:488,Modifiability,rewrite,rewrite,488,"""""""Copy a google cloud object ``src`` to another google cloud object ``dest``. Parameters; ----------; src : :class:`str`; GCS object to copy, must be a valid ``gs://`` URL; dest : :class:`str`; Valid ``gs://`` URL for the destination of the copy; callback : function ( (response, first) -> None ); Optional callback to call after every request (for updating things like a progress bar).; The ``response`` argument is a `rewrite <https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite#response>`_; response dictionary, and ``first`` will be ``True`` if this is the first time this; callback has been called for this copy. Returns; -------; :obj:`NoneType`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py:373,Usability,progress bar,progress bar,373,"""""""Copy a google cloud object ``src`` to another google cloud object ``dest``. Parameters; ----------; src : :class:`str`; GCS object to copy, must be a valid ``gs://`` URL; dest : :class:`str`; Valid ``gs://`` URL for the destination of the copy; callback : function ( (response, first) -> None ); Optional callback to call after every request (for updating things like a progress bar).; The ``response`` argument is a `rewrite <https://cloud.google.com/storage/docs/json_api/v1/objects/rewrite#response>`_; response dictionary, and ``first`` will be ``True`` if this is the first time this; callback has been called for this copy. Returns; -------; :obj:`NoneType`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/aiogoogle/client/storage_client.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/common/credentials.py:15,Security,authenticat,authentication,15,"""""""Return HTTP authentication headers and the time of expiration in seconds since the epoch (Unix time). None indicates a non-expiring credentials.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/common/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/common/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/common/credentials.py:13,Security,access,access,13,"""""""Return an access token and the time of expiration in seconds since the epoch (Unix time). None indicates a non-expiring credentials.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiocloud/common/credentials.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiocloud/common/credentials.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/delete.py:67,Availability,error,error,67,"# only advance if file or directory removal was successful, not on error",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/delete.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/delete.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/diff.py:6,Performance,concurren,concurrency,6,"# The concurrency limit is the number of worker corooutines, not the queue size. Queue size must; # be large because a single driver process is trying to feed max_simultaneous workers.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/diff.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/diff.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/diff.py:69,Performance,queue,queue,69,"# The concurrency limit is the number of worker corooutines, not the queue size. Queue size must; # be large because a single driver process is trying to feed max_simultaneous workers.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/diff.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/diff.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/validators.py:276,Security,validat,validation,276,"""""""; Validates a URI's scheme if the ``validate_scheme`` flag was provided, and its cloud location's default storage; policy if the URI points to a cloud with an ``AsyncFS`` implementation that supports checking that policy. Raises; ------; :class:`ValueError`; If one of the validation steps fails.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/validators.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/validators.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/copier.py:131,Performance,concurren,concurrently,131,"# This is essentially a limit on amount of memory in temporary; # buffers during copying. We allow ~10 full-sized copies to; # run concurrently.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/copier.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/copier.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/fs.py:132,Availability,error,error,132,"""""""The time the object was created in seconds since the epcoh, UTC. Some filesystems do not support creation time. In that case, an error is raised. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py:51,Safety,safe,safe,51,"# self.closed and self.close() must be multithread safe, because; # they can be accessed by both the stream reader and writer which; # are in different threads.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/stream.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py:80,Security,access,accessed,80,"# self.closed and self.close() must be multithread safe, because; # they can be accessed by both the stream reader and writer which; # are in different threads.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/stream.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py:124,Availability,error,error,124,"# If readinto only partially fills b without hitting the end; # of stream, then the upload_obj returns an EntityTooSmall; # error in some cases.",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/stream.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py:2,Energy Efficiency,drain,drain,2,"# drain the q so the writer doesn't deadlock",MatchSource.CODE_COMMENT,hail/python/hailtop/aiotools/fs/stream.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/aiotools/fs/stream.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py:176,Modifiability,config,configured,176,"# We prefer an extant hail token to an access token for the internal auth token; # during development of the idp access token feature because the production auth; # is not yet configured to accept access tokens. This can be changed to always prefer; # an idp access token when this change is in production.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py:39,Security,access,access,39,"# We prefer an extant hail token to an access token for the internal auth token; # during development of the idp access token feature because the production auth; # is not yet configured to accept access tokens. This can be changed to always prefer; # an idp access token when this change is in production.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py:113,Security,access,access,113,"# We prefer an extant hail token to an access token for the internal auth token; # during development of the idp access token feature because the production auth; # is not yet configured to accept access tokens. This can be changed to always prefer; # an idp access token when this change is in production.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py:197,Security,access,access,197,"# We prefer an extant hail token to an access token for the internal auth token; # during development of the idp access token feature because the production auth; # is not yet configured to accept access tokens. This can be changed to always prefer; # an idp access token when this change is in production.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py:259,Security,access,access,259,"# We prefer an extant hail token to an access token for the internal auth token; # during development of the idp access token feature because the production auth; # is not yet configured to accept access tokens. This can be changed to always prefer; # an idp access token when this change is in production.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/auth.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/auth.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:132,Security,secur,secure,132,"""""""; Initiates the OAuth2 flow. Usually run in response to a user clicking a login button.; The returned dict should be stored in a secure session so that the server can; identify to which OAuth2 flow a client is responding. In particular, the server must; pass this dict to :meth:`.receive_callback` in the OAuth2 callback.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:77,Testability,log,login,77,"""""""; Initiates the OAuth2 flow. Usually run in response to a user clicking a login button.; The returned dict should be stored in a secure session so that the server can; identify to which OAuth2 flow a client is responding. In particular, the server must; pass this dict to :meth:`.receive_callback` in the OAuth2 callback.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:43,Deployability,install,installed,43,"""""""Performs an OAuth2 flow for credentials installed on the user's machine.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:30,Security,access,access,30,"""""""; Validate a user-provided access token. If the token is valid, return the identity; to which it belongs. If it is not valid, return None.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:138,Security,authenticat,authentication-library-for-python,138,"# confusingly, scopes=[] is the only way to get the openid, profile, and; # offline_access scopes; # https://github.com/AzureAD/microsoft-authentication-library-for-python/blob/dev/msal/application.py#L568-L580",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:201,Performance,perform,perform,201,"# AAD does not support revocation of a single refresh token,; # only all refresh tokens issued to all applications for a particular; # user, which we neither wish nor should have the permissions; # to perform.; # See: https://learn.microsoft.com/en-us/answers/questions/1158831/invalidate-old-refresh-token-after-using-it-to-get",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:226,Usability,learn,learn,226,"# AAD does not support revocation of a single refresh token,; # only all refresh tokens issued to all applications for a particular; # user, which we neither wish nor should have the permissions; # to perform.; # See: https://learn.microsoft.com/en-us/answers/questions/1158831/invalidate-old-refresh-token-after-using-it-to-get",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:82,Security,authenticat,authentication-library-for-python,82,"# This code is taken nearly verbatim from; # https://github.com/AzureAD/microsoft-authentication-library-for-python/issues/147; # At time of writing, the community response in that issue is the recommended way to validate; # AAD access tokens in python as it is not a part of the MSAL library.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:213,Security,validat,validate,213,"# This code is taken nearly verbatim from; # https://github.com/AzureAD/microsoft-authentication-library-for-python/issues/147; # At time of writing, the community response in that issue is the recommended way to validate; # AAD access tokens in python as it is not a part of the MSAL library.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py:229,Security,access,access,229,"# This code is taken nearly verbatim from; # https://github.com/AzureAD/microsoft-authentication-library-for-python/issues/147; # At time of writing, the community response in that issue is the recommended way to validate; # AAD access tokens in python as it is not a part of the MSAL library.",MatchSource.CODE_COMMENT,hail/python/hailtop/auth/flow.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/auth/flow.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:118,Energy Efficiency,monitor,monitor,118,"""""""; The type of value returned by :py:meth:`.Backend._run`. The value returned by some backends; enables the user to monitor the asynchronous execution of a Batch.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:407,Modifiability,variab,variable,407,"""""""; Backend that executes batches on a local computer.; Examples; --------. >>> local_backend = LocalBackend(tmp_dir='/tmp/user/'); >>> b = Batch(backend=local_backend). Parameters; ----------; tmp_dir:; Temporary directory to use.; gsa_key_file:; Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a; job specifies a docker image. This option will override the value set by; the environment variable `HAIL_BATCH_GSA_KEY_FILE`.; extra_docker_run_flags:; Additional flags to pass to `docker run`. Only used if a job specifies; a docker image. This option will override the value set by the environment; variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:617,Modifiability,variab,variable,617,"""""""; Backend that executes batches on a local computer.; Examples; --------. >>> local_backend = LocalBackend(tmp_dir='/tmp/user/'); >>> b = Batch(backend=local_backend). Parameters; ----------; tmp_dir:; Temporary directory to use.; gsa_key_file:; Mount a file with a gsa key to `/gsa-key/key.json`. Only used if a; job specifies a docker image. This option will override the value set by; the environment variable `HAIL_BATCH_GSA_KEY_FILE`.; extra_docker_run_flags:; Additional flags to pass to `docker run`. Only used if a job specifies; a docker image. This option will override the value set by the environment; variable `HAIL_BATCH_EXTRA_DOCKER_RUN_FLAGS`.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:576,Availability,echo,echo,576,"""""""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but us",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:878,Availability,echo,echo,878,"""""""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but us",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:1275,Availability,echo,echo,1275,"iltop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* u",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:3888,Availability,avail,available,3888,"ch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:729,Deployability,configurat,configuration,729,"""""""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but us",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:1150,Deployability,configurat,configuration,1150,"lling project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGION",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:2007,Deployability,configurat,configuration,2007,"te_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` inste",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:2417,Deployability,configurat,configuration,2417,"//my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; l",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:3708,Deployability,configurat,configuration,3708,"ch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:729,Modifiability,config,configuration,729,"""""""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but us",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:921,Modifiability,config,config,921,"""""""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but us",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:982,Modifiability,config,config,982,"""""""Backend that executes batches on Hail's Batch Service on Google Cloud. Examples; --------. Create and use a backend that bills to the Hail Batch billing project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but us",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:1150,Modifiability,config,configuration,1150,"lling project named ""my-billing-account""; and stores temporary intermediate files in ""gs://my-bucket/temporary-files"". >>> import hailtop.batch as hb; >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGION",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:1318,Modifiability,config,config,1318,"='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above,",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:1379,Modifiability,config,config,1379,"='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above,",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:1451,Modifiability,config,config,1451,"='gs://my-bucket/temporary-files/'; ... ) # doctest: +SKIP; >>> b = hb.Batch(backend=service_backend) # doctest: +SKIP; >>> j = b.new_job() # doctest: +SKIP; >>> j.command('echo hello world!') # doctest: +SKIP; >>> b.run() # doctest: +SKIP. Same as above, but set the billing project and temporary intermediate folders via a; configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(backend=ServiceBackend()); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above,",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:2007,Modifiability,config,configuration,2007,"te_tmpdir gs://my-bucket/temporary-files/; python3 my-batch-script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` inste",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:2037,Modifiability,config,config,2037,"script.py. Same as above, but also specify the use of the :class:`.ServiceBackend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:2167,Modifiability,variab,variable,2167,"kend` via configuration file::. cat >my-batch-script.py >>EOF; import hailtop.batch as hb; b = hb.Batch(); j = b.new_job(); j.command('echo hello world!'); b.run(); EOF; hailctl config set batch/billing_project my-billing-account; hailctl config set batch/remote_tmpdir gs://my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optiona",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:2417,Modifiability,config,configuration,2417,"//my-bucket/temporary-files/; hailctl config set batch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; l",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:2447,Modifiability,config,config,2447,"ch/backend service; python3 my-batch-script.py. Create a backend which stores temporary intermediate files in; ""https://my-account.blob.core.windows.net/my-container/tempdir"". >>> service_backend = hb.ServiceBackend(; ... billing_project='my-billing-account',; ... remote_tmpdir='https://my-account.blob.core.windows.net/my-container/tempdir'; ... ) # doctest: +SKIP. Require all jobs in all batches in this backend to execute in us-central1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1; python3 my-batch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:3168,Modifiability,config,configure,3168,"ch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:3296,Modifiability,config,configure,3296,"ch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:3708,Modifiability,config,configuration,3708,"ch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:3723,Modifiability,variab,variable,3723,"ch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:3783,Modifiability,variab,variables,3783,"ch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:3432,Security,authoriz,authorization,3432,"ch-script.py. Same as above, but using the ``HAIL_BATCH_REGIONS`` environment variable::. export HAIL_BATCH_REGIONS=us-central1; python3 my-batch-script.py. Permit jobs to execute in *either* us-central1 or us-east1::. >>> b = hb.Batch(backend=hb.ServiceBackend(regions=['us-central1', 'us-east1'])). Same as above, but using a configuration file::. hailctl config set batch/regions us-central1,us-east1. Allow reading or writing to buckets even though they are ""cold"" storage:. >>> b = hb.Batch(; ... backend=hb.ServiceBackend(; ... gcs_bucket_allow_list=['cold-bucket', 'cold-bucket2'],; ... ),; ... ). Parameters; ----------; billing_project:; Name of billing project to use.; bucket:; This argument is deprecated. Use `remote_tmpdir` instead.; remote_tmpdir:; Temporary data will be stored in this cloud storage folder.; google_project:; This argument is deprecated. Use `gcs_requester_pays_configuration` instead.; gcs_requester_pays_configuration : either :class:`str` or :class:`tuple` of :class:`str` and :class:`list` of :class:`str`, optional; If a string is provided, configure the Google Cloud Storage file system to bill usage to the; project identified by that string. If a tuple is provided, configure the Google Cloud; Storage file system to bill usage to the specified project for buckets specified in the; list.; token:; The authorization token to pass to the batch client.; Should only be set for user delegation purposes.; regions:; Cloud regions in which jobs may run. :attr:`.ServiceBackend.ANY_REGION` indicates jobs may; run in any region. If unspecified or ``None``, the ``batch/regions`` Hail configuration; variable is consulted. See examples above. If none of these variables are set, then jobs may; run in any region. :meth:`.ServiceBackend.supported_regions` lists the available regions.; gcs_bucket_allow_list:; A list of buckets that the :class:`.ServiceBackend` should be permitted to read from or write to, even if their; default policy is to use ""cold"" storage. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py:599,Usability,progress bar,progress bar,599,"""""""Execute a batch. Warning; -------; This method should not be called directly. Instead, use :meth:`.batch.Batch.run`; and pass :class:`.ServiceBackend` specific arguments as key-word arguments. Parameters; ----------; batch:; Batch to execute.; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delete temporary directories with intermediate files.; wait:; If `True`, wait for the batch to finish executing before returning.; open:; If `True`, open the UI page for the batch.; disable_progress_bar:; If `True`, disable the progress bar.; callback:; If not `None`, a URL that will receive at most one POST request; after the entire batch completes.; token:; If not `None`, a string used for idempotency of batch submission.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:247,Availability,echo,echo,247,"""""""Object representing the distributed acyclic graph (DAG) of jobs to run. Examples; --------; Create a batch object:. >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints ""hello"":. >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:. >>> p.run(). Require all jobs in this batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; def",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:3707,Availability,failure,failures,3707,"a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`.ServiceBackend`. See :meth:`.Job.storage`.; default_regions:; Cloud regions in which jobs may run. When unspecified or ``None``, use the regions attribute of; :class:`.ServiceBackend`. See :class:`.ServiceBackend` for details.; default_timeout:; Maximum time in seconds for a job to run before being killed. Only; applicable for the :class:`.ServiceBackend`. If `None`, there is no; timeout.; default_python_image:; Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is `latest`). The image must have; the `dill` Python package installed and have the same version of Python installed that is; currently running. If `None`, a tag of the `hailgenetics/hail` image will be chosen; according to the current Hail and Python version.; default_spot:; If unspecified or ``True``, jobs will run by default on spot instances. If ``False``, jobs; will run by default on non-spot instances. Each job can override this setting with; :meth:`.Job.spot`.; project:; DEPRECATED: please specify `google_project` on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures:; Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the :class:`.ServiceBackend`. Must be greater than 0. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:3791,Availability,failure,failures,3791,"a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`.ServiceBackend`. See :meth:`.Job.storage`.; default_regions:; Cloud regions in which jobs may run. When unspecified or ``None``, use the regions attribute of; :class:`.ServiceBackend`. See :class:`.ServiceBackend` for details.; default_timeout:; Maximum time in seconds for a job to run before being killed. Only; applicable for the :class:`.ServiceBackend`. If `None`, there is no; timeout.; default_python_image:; Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is `latest`). The image must have; the `dill` Python package installed and have the same version of Python installed that is; currently running. If `None`, a tag of the `hailgenetics/hail` image will be chosen; according to the current Hail and Python version.; default_spot:; If unspecified or ``True``, jobs will run by default on spot instances. If ``False``, jobs; will run by default on non-spot instances. Each job can override this setting with; :meth:`.Job.spot`.; project:; DEPRECATED: please specify `google_project` on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures:; Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the :class:`.ServiceBackend`. Must be greater than 0. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1117,Deployability,configurat,configurations,1117,"ch(). Create a new job that prints ""hello"":. >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:. >>> p.run(). Require all jobs in this batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1309,Deployability,configurat,configurations,1309,"is batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:2943,Deployability,install,installed,2943,"a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`.ServiceBackend`. See :meth:`.Job.storage`.; default_regions:; Cloud regions in which jobs may run. When unspecified or ``None``, use the regions attribute of; :class:`.ServiceBackend`. See :class:`.ServiceBackend` for details.; default_timeout:; Maximum time in seconds for a job to run before being killed. Only; applicable for the :class:`.ServiceBackend`. If `None`, there is no; timeout.; default_python_image:; Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is `latest`). The image must have; the `dill` Python package installed and have the same version of Python installed that is; currently running. If `None`, a tag of the `hailgenetics/hail` image will be chosen; according to the current Hail and Python version.; default_spot:; If unspecified or ``True``, jobs will run by default on spot instances. If ``False``, jobs; will run by default on non-spot instances. Each job can override this setting with; :meth:`.Job.spot`.; project:; DEPRECATED: please specify `google_project` on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures:; Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the :class:`.ServiceBackend`. Must be greater than 0. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:2989,Deployability,install,installed,2989,"a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`.ServiceBackend`. See :meth:`.Job.storage`.; default_regions:; Cloud regions in which jobs may run. When unspecified or ``None``, use the regions attribute of; :class:`.ServiceBackend`. See :class:`.ServiceBackend` for details.; default_timeout:; Maximum time in seconds for a job to run before being killed. Only; applicable for the :class:`.ServiceBackend`. If `None`, there is no; timeout.; default_python_image:; Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is `latest`). The image must have; the `dill` Python package installed and have the same version of Python installed that is; currently running. If `None`, a tag of the `hailgenetics/hail` image will be chosen; according to the current Hail and Python version.; default_spot:; If unspecified or ``True``, jobs will run by default on spot instances. If ``False``, jobs; will run by default on non-spot instances. Each job can override this setting with; :meth:`.Job.spot`.; project:; DEPRECATED: please specify `google_project` on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures:; Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the :class:`.ServiceBackend`. Must be greater than 0. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1033,Modifiability,variab,variable,1033,"e distributed acyclic graph (DAG) of jobs to run. Examples; --------; Create a batch object:. >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints ""hello"":. >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:. >>> p.run(). Require all jobs in this batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1080,Modifiability,config,config,1080,"e distributed acyclic graph (DAG) of jobs to run. Examples; --------; Create a batch object:. >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints ""hello"":. >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:. >>> p.run(). Require all jobs in this batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1087,Modifiability,variab,variable,1087,"e distributed acyclic graph (DAG) of jobs to run. Examples; --------; Create a batch object:. >>> import hailtop.batch as hb; >>> p = hb.Batch(). Create a new job that prints ""hello"":. >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:. >>> p.run(). Require all jobs in this batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1117,Modifiability,config,configurations,1117,"ch(). Create a new job that prints ""hello"":. >>> t = p.new_job(); >>> t.command(f'echo ""hello"" '). Execute the DAG:. >>> p.run(). Require all jobs in this batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1309,Modifiability,config,configurations,1309,"is batch to execute in us-central1:. >>> b = hb.Batch(backend=hb.ServiceBackend(), default_regions=['us-central1']). Notes; -----. The methods :meth:`.Batch.read_input` and :meth:`.Batch.read_input_group`; are for adding input files to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:2701,Safety,timeout,timeout,2701,"he; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`.ServiceBackend`. See :meth:`.Job.storage`.; default_regions:; Cloud regions in which jobs may run. When unspecified or ``None``, use the regions attribute of; :class:`.ServiceBackend`. See :class:`.ServiceBackend` for details.; default_timeout:; Maximum time in seconds for a job to run before being killed. Only; applicable for the :class:`.ServiceBackend`. If `None`, there is no; timeout.; default_python_image:; Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is `latest`). The image must have; the `dill` Python package installed and have the same version of Python installed that is; currently running. If `None`, a tag of the `hailgenetics/hail` image will be chosen; according to the current Hail and Python version.; default_spot:; If unspecified or ``True``, jobs will run by default on spot instances. If ``False``, jobs; will run by default on non-spot instances. Each job can override this setting with; :meth:`.Job.spot`.; project:; DEPRECATED: please specify `google_project` on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures:; Automatically cancel the ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1567,Security,access,accessing,1567,"iles to a batch. An input file is a file that already; exists before executing a batch and is not present in the docker container; the job is being run in. Files generated by executing a job are temporary files and must be written; to a permanent location using the method :meth:`.Batch.write_output`. Parameters; ----------; name:; Name of the batch.; backend:; Backend used to execute the jobs. If no backend is specified, a backend; will be created by first looking at the environment variable HAIL_BATCH_BACKEND,; then the hailctl config variable batch/backend. These configurations, if set,; can be either `local` or `service`, and will result in the use of a; :class:`.LocalBackend` and :class:`.ServiceBackend` respectively. If no; argument is given and no configurations are set, the default is; :class:`.LocalBackend`.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; requester_pays_project:; The name of the Google project to be billed when accessing requester pays buckets.; default_image:; Default docker image to use for Bash jobs. This must be the full name of the; image including any repository prefix and tags if desired (default tag is `latest`).; default_memory:; Memory setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`.ServiceBackend`. See :meth:`.Job.storage`.; default_regions:; Cloud regions in which jobs may run. When unspecified or ``None``, use the regions attribute of; :class:`.ServiceBackend`. See :class:`.ServiceBackend` for details.; ",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:3479,Security,authenticat,authenticating,3479,"a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.memory`.; default_cpu:; CPU setting to use by default if not specified by a job. Only; applicable if a docker image is specified for the :class:`.LocalBackend`; or the :class:`.ServiceBackend`. See :meth:`.Job.cpu`.; default_storage:; Storage setting to use by default if not specified by a job. Only; applicable for the :class:`.ServiceBackend`. See :meth:`.Job.storage`.; default_regions:; Cloud regions in which jobs may run. When unspecified or ``None``, use the regions attribute of; :class:`.ServiceBackend`. See :class:`.ServiceBackend` for details.; default_timeout:; Maximum time in seconds for a job to run before being killed. Only; applicable for the :class:`.ServiceBackend`. If `None`, there is no; timeout.; default_python_image:; Default image to use for all Python jobs. This must be the full name of the image including; any repository prefix and tags if desired (default tag is `latest`). The image must have; the `dill` Python package installed and have the same version of Python installed that is; currently running. If `None`, a tag of the `hailgenetics/hail` image will be chosen; according to the current Hail and Python version.; default_spot:; If unspecified or ``True``, jobs will run by default on spot instances. If ``False``, jobs; will run by default on non-spot instances. Each job can override this setting with; :meth:`.Job.spot`.; project:; DEPRECATED: please specify `google_project` on the ServiceBackend instead. If specified,; the project to use when authenticating with Google Storage. Google Storage is used to; transfer serialized values between this computer and the cloud machines that execute Python; jobs.; cancel_after_n_failures:; Automatically cancel the batch after N failures have occurred. The default; behavior is there is no limit on the number of failures. Only; applicable for the :class:`.ServiceBackend`. Must be greater than 0. """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:350,Availability,echo,echo,350,"""""""; Initialize a :class:`.BashJob` object with default memory, storage,; image, and CPU settings (defined in :class:`.Batch`) upon batch creation. Examples; --------; Create and execute a batch `b` with one job `j` that prints ""hello world"":. >>> b = Batch(); >>> j = b.new_bash_job(name='hello', attributes={'language': 'english'}); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters; ----------; name:; Name of the job.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:739,Deployability,install,installed,739,"""""""; Initialize a new :class:`.PythonJob` object with default; Python image, memory, storage, and CPU settings (defined in :class:`.Batch`); upon batch creation. Examples; --------; Create and execute a batch `b` with one job `j` that prints ""hello alice"":. .. code-block:: python. b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; -----. The image to use for Python jobs can be specified by `default_python_image`; when constructing a :class:`.Batch`. The image specified must have the `dill`; package installed. If ``default_python_image`` is not specified, then a Docker; image will automatically be created for you with the base image; `hailgenetics/python-dill:[major_version].[minor_version]-slim` and the Python; packages specified by ``python_requirements`` will be installed. The default name; of the image is `batch-python` with a random string for the tag unless ``python_build_image_name``; is specified. If the :class:`.ServiceBackend` is the backend, the locally built; image will be pushed to the repository specified by ``image_repository``. Parameters; ----------; name:; Name of the job.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1010,Deployability,install,installed,1010,"""""""; Initialize a new :class:`.PythonJob` object with default; Python image, memory, storage, and CPU settings (defined in :class:`.Batch`); upon batch creation. Examples; --------; Create and execute a batch `b` with one job `j` that prints ""hello alice"":. .. code-block:: python. b = Batch(default_python_image='hailgenetics/python-dill:3.9-slim'). def hello(name):; return f'hello {name}'. j = b.new_python_job(); output = j.call(hello, 'alice'). # Write out the str representation of result to a file. b.write_output(output.as_str(), 'hello.txt'). b.run(). Notes; -----. The image to use for Python jobs can be specified by `default_python_image`; when constructing a :class:`.Batch`. The image specified must have the `dill`; package installed. If ``default_python_image`` is not specified, then a Docker; image will automatically be created for you with the base image; `hailgenetics/python-dill:[major_version].[minor_version]-slim` and the Python; packages specified by ``python_requirements`` will be installed. The default name; of the image is `batch-python` with a random string for the tag unless ``python_build_image_name``; is specified. If the :class:`.ServiceBackend` is the backend, the locally built; image will be pushed to the repository specified by ``image_repository``. Parameters; ----------; name:; Name of the job.; attributes:; Key-value pairs of additional attributes. 'name' is not a valid keyword.; Use the name argument instead.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:71,Availability,down,downloaded,71,"# Avoid os.fspath(), which causes some pathlikes to return a path to a downloaded copy instead.",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:113,Energy Efficiency,charge,charges,113,"""""""; Create a new input resource file object representing a single file. .. warning::. To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; --------. Read the file `hello.txt`:. >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters; ----------; path: :obj:`str`; File path to read.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:90,Safety,avoid,avoid,90,"""""""; Create a new input resource file object representing a single file. .. warning::. To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; --------. Read the file `hello.txt`:. >>> b = Batch(); >>> input = b.read_input('data/hello.txt'); >>> j = b.new_job(); >>> j.command(f'cat {input}'); >>> b.run(). Parameters; ----------; path: :obj:`str`; File path to read.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:134,Energy Efficiency,charge,charges,134,"""""""Create a new resource group representing a mapping of identifier to; input resource files. .. warning::. To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; --------. Read a binary PLINK file:. >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() # doctest: +SKIP. Read a FASTA file and it's index (file extensions matter!):. >>> fasta = b.read_input_group(**{'fasta': 'data/example.fasta',; ... 'fasta.idx': 'data/example.fasta.idx'}). Create a resource group where the identifiers don't match the file extensions:. >>> rg = b.read_input_group(foo='data/foo.txt',; ... bar='data/bar.txt'). `rg.foo` and `rg.bar` will not have the `.txt` file extension and; instead will be `{root}.foo` and `{root}.bar` where `{root}` is a random; identifier. Notes; -----; The identifier is used to refer to a specific resource file. For example,; given the resource group `rg`, you can use the attribute notation; `rg.identifier` or the get item notation `rg[identifier]`. The file extensions for each file are derived from the identifier. This; is equivalent to `""{root}.identifier""` from; :meth:`.BashJob.declare_resource_group`. We are planning on adding; flexibility to incorporate more complicated extensions in the future; such as `.vcf.bgz`. For now, use :meth:`.JobResourceFile.add_extension`; to add an extension to a resource file. Parameters; ----------; kwargs:; Key word arguments where the name/key is the identifier and the value; is the file path.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:111,Safety,avoid,avoid,111,"""""""Create a new resource group representing a mapping of identifier to; input resource files. .. warning::. To avoid expensive egress charges, input files should be located in buckets; that are in the same region in which your Batch jobs run. Examples; --------. Read a binary PLINK file:. >>> b = Batch(); >>> bfile = b.read_input_group(bed=""data/example.bed"",; ... bim=""data/example.bim"",; ... fam=""data/example.fam""); >>> j = b.new_job(); >>> j.command(f""plink --bfile {bfile} --geno --make-bed --out {j.geno}""); >>> j.command(f""wc -l {bfile.fam}""); >>> j.command(f""wc -l {bfile.bim}""); >>> b.run() # doctest: +SKIP. Read a FASTA file and it's index (file extensions matter!):. >>> fasta = b.read_input_group(**{'fasta': 'data/example.fasta',; ... 'fasta.idx': 'data/example.fasta.idx'}). Create a resource group where the identifiers don't match the file extensions:. >>> rg = b.read_input_group(foo='data/foo.txt',; ... bar='data/bar.txt'). `rg.foo` and `rg.bar` will not have the `.txt` file extension and; instead will be `{root}.foo` and `{root}.bar` where `{root}` is a random; identifier. Notes; -----; The identifier is used to refer to a specific resource file. For example,; given the resource group `rg`, you can use the attribute notation; `rg.identifier` or the get item notation `rg[identifier]`. The file extensions for each file are derived from the identifier. This; is equivalent to `""{root}.identifier""` from; :meth:`.BashJob.declare_resource_group`. We are planning on adding; flexibility to incorporate more complicated extensions in the future; such as `.vcf.bgz`. For now, use :meth:`.JobResourceFile.add_extension`; to add an extension to a resource file. Parameters; ----------; kwargs:; Key word arguments where the name/key is the identifier and the value; is the file path.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:198,Availability,echo,echo,198,"""""""; Write resource file or resource file group to an output destination. Examples; --------. Write a single job intermediate to a local file:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. .. warning::. To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.write_output` if the output needs; to be saved. Parameters; ----------; resource:; Resource to be written to a file.; dest:; Destination file path. For a single :class:`.ResourceFile`, this will; simply be `dest`. For a :class:`.ResourceGroup`, `dest` is the file; root and each resource file will be written to `{root}.identifier`; where `identifier` is the identifier of the file in the; :class:`.ResourceGroup` map.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:419,Availability,echo,echo,419,"""""""; Write resource file or resource file group to an output destination. Examples; --------. Write a single job intermediate to a local file:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. .. warning::. To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.write_output` if the output needs; to be saved. Parameters; ----------; resource:; Resource to be written to a file.; dest:; Destination file path. For a single :class:`.ResourceFile`, this will; simply be `dest`. For a :class:`.ResourceGroup`, `dest` is the file; root and each resource file will be written to `{root}.identifier`; where `identifier` is the identifier of the file in the; :class:`.ResourceGroup` map.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:648,Availability,echo,echo,648,"""""""; Write resource file or resource file group to an output destination. Examples; --------. Write a single job intermediate to a local file:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. .. warning::. To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.write_output` if the output needs; to be saved. Parameters; ----------; resource:; Resource to be written to a file.; dest:; Destination file path. For a single :class:`.ResourceFile`, this will; simply be `dest`. For a :class:`.ResourceGroup`, `dest` is the file; root and each resource file will be written to `{root}.identifier`; where `identifier` is the identifier of the file in the; :class:`.ResourceGroup` map.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:841,Energy Efficiency,charge,charges,841,"""""""; Write resource file or resource file group to an output destination. Examples; --------. Write a single job intermediate to a local file:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. .. warning::. To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.write_output` if the output needs; to be saved. Parameters; ----------; resource:; Resource to be written to a file.; dest:; Destination file path. For a single :class:`.ResourceFile`, this will; simply be `dest`. For a :class:`.ResourceGroup`, `dest` is the file; root and each resource file will be written to `{root}.identifier`; where `identifier` is the identifier of the file in the; :class:`.ResourceGroup` map.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:818,Safety,avoid,avoid,818,"""""""; Write resource file or resource file group to an output destination. Examples; --------. Write a single job intermediate to a local file:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. .. warning::. To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.write_output` if the output needs; to be saved. Parameters; ----------; resource:; Resource to be written to a file.; dest:; Destination file path. For a single :class:`.ResourceFile`, this will; simply be `dest`. For a :class:`.ResourceGroup`, `dest` is the file; root and each resource file will be written to `{root}.identifier`; where `identifier` is the identifier of the file in the; :class:`.ResourceGroup` map.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:1270,Usability,simpl,simply,1270,"""""""; Write resource file or resource file group to an output destination. Examples; --------. Write a single job intermediate to a local file:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Write a single job intermediate to a permanent location in GCS:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'gs://mybucket/output/hello.txt'); b.run(). Write a single job intermediate to a permanent location in Azure:. .. code-block:: python. b = Batch(); j = b.new_job(); j.command(f'echo ""hello"" > {j.ofile}'); b.write_output(j.ofile, 'https://my-account.blob.core.windows.net/my-container/output/hello.txt'); b.run() # doctest: +SKIP. .. warning::. To avoid expensive egress charges, output files should be located in buckets; that are in the same region in which your Batch jobs run. Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.write_output` if the output needs; to be saved. Parameters; ----------; resource:; Resource to be written to a file.; dest:; Destination file path. For a single :class:`.ResourceFile`, this will; simply be `dest`. For a :class:`.ResourceGroup`, `dest` is the file; root and each resource file will be written to `{root}.identifier`; where `identifier` is the identifier of the file in the; :class:`.ResourceGroup` map.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:207,Testability,assert,assert,207,"""""""; Select all jobs in the batch whose name matches `pattern`. Examples; --------. Select jobs in batch matching `qc`:. >>> b = Batch(); >>> j = b.new_job(name='qc'); >>> qc_jobs = b.select_jobs('qc'); >>> assert qc_jobs == [j]. Parameters; ----------; pattern:; Regex pattern matching job names.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:147,Availability,echo,echo,147,"""""""; Execute a batch. Examples; --------. Create a simple batch with one job and execute it:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters; ----------; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delete temporary directories with intermediate files.; backend_kwargs:; See :meth:`.Backend._run` for backend-specific arguments.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py:51,Usability,simpl,simple,51,"""""""; Execute a batch. Examples; --------. Create a simple batch with one job and execute it:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command('echo ""hello world""'); >>> b.run(). Parameters; ----------; dry_run:; If `True`, don't execute code.; verbose:; If `True`, print debugging output.; delete_scratch_on_exit:; If `True`, delete temporary directories with intermediate files.; backend_kwargs:; See :meth:`.Backend._run` for backend-specific arguments.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:205,Availability,avail,available,205,"""""""An executor which executes Python functions in the cloud. :class:`.concurrent.futures.ProcessPoolExecutor` and; :class:`.concurrent.futures.ThreadPoolExecutor` enable the use of all the; computer cores available on a single computer. :class:`.BatchPoolExecutor`; enables the use of an effectively arbitrary number of cloud computer cores. Functions provided to :meth:`.submit` are serialized using `dill; <https://dill.readthedocs.io/en/latest/dill.html>`__, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which :meth:`.submit` was; called. The Python version in the docker container will share a major and; minor verison with the local process. The `image` parameter overrides this; behavior. When used as a context manager (the ``with`` syntax), the executor will wait; for all jobs to finish before finishing the ``with`` statement. This; behavior can be controlled by the `wait_on_exit` parameter. This class creates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python pack",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:2815,Availability,down,down,2815,"reates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; ``numpy``, ``scipy``, and ``sklearn`` installed is used.; cpus_per_job:; The number of CPU cores to allocate to each job. The default value is; ``1``. The parameter is passed unaltered to :meth:`.Job.cpu`. This; parameter's value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit:; If ``True`` or unspecified, wait for all jobs to complete when exiting a; context. If ``False``, do not wait. This option has no effect if this; executor is not used with the ``with`` syntax.; cleanup_bucket:; If ``True`` or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project:; DEPRECATED. Please specify gcs_requester_pays_configuration in :class:`.ServiceBackend`.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:2006,Deployability,install,installed,2006," parameter. This class creates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; ``numpy``, ``scipy``, and ``sklearn`` installed is used.; cpus_per_job:; The number of CPU cores to allocate to each job. The default value is; ``1``. The parameter is passed unaltered to :meth:`.Job.cpu`. This; parameter's value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit:; If ``True`` or unspecified, wait for all jobs to complete when exiting a; context. If ``False``, do not wait. This option has no effect if this; executor is not used with the ``with`` syntax.; cleanup_bucket:; If ``True`` or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project:; DEPRECATED. Please specify gcs_requester_pays_configuration in :class",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:2079,Deployability,install,installed,2079,"reates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; ``numpy``, ``scipy``, and ``sklearn`` installed is used.; cpus_per_job:; The number of CPU cores to allocate to each job. The default value is; ``1``. The parameter is passed unaltered to :meth:`.Job.cpu`. This; parameter's value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit:; If ``True`` or unspecified, wait for all jobs to complete when exiting a; context. If ``False``, do not wait. This option has no effect if this; executor is not used with the ``with`` syntax.; cleanup_bucket:; If ``True`` or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project:; DEPRECATED. Please specify gcs_requester_pays_configuration in :class:`.ServiceBackend`.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:2189,Deployability,install,installed,2189,"reates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; ``numpy``, ``scipy``, and ``sklearn`` installed is used.; cpus_per_job:; The number of CPU cores to allocate to each job. The default value is; ``1``. The parameter is passed unaltered to :meth:`.Job.cpu`. This; parameter's value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit:; If ``True`` or unspecified, wait for all jobs to complete when exiting a; context. If ``False``, do not wait. This option has no effect if this; executor is not used with the ``with`` syntax.; cleanup_bucket:; If ``True`` or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project:; DEPRECATED. Please specify gcs_requester_pays_configuration in :class:`.ServiceBackend`.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:2251,Energy Efficiency,allocate,allocate,2251,"reates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; ``numpy``, ``scipy``, and ``sklearn`` installed is used.; cpus_per_job:; The number of CPU cores to allocate to each job. The default value is; ``1``. The parameter is passed unaltered to :meth:`.Job.cpu`. This; parameter's value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit:; If ``True`` or unspecified, wait for all jobs to complete when exiting a; context. If ``False``, do not wait. This option has no effect if this; executor is not used with the ``with`` syntax.; cleanup_bucket:; If ``True`` or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project:; DEPRECATED. Please specify gcs_requester_pays_configuration in :class:`.ServiceBackend`.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:2416,Modifiability,variab,variables,2416,"reates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Python verison and; ``numpy``, ``scipy``, and ``sklearn`` installed is used.; cpus_per_job:; The number of CPU cores to allocate to each job. The default value is; ``1``. The parameter is passed unaltered to :meth:`.Job.cpu`. This; parameter's value is used to set several environment variables; instructing BLAS and LAPACK to limit core use.; wait_on_exit:; If ``True`` or unspecified, wait for all jobs to complete when exiting a; context. If ``False``, do not wait. This option has no effect if this; executor is not used with the ``with`` syntax.; cleanup_bucket:; If ``True`` or unspecified, delete all temporary files in the cloud; storage bucket when this executor fully shuts down. If Python crashes; before the executor is shutdown, the files will not be deleted.; project:; DEPRECATED. Please specify gcs_requester_pays_configuration in :class:`.ServiceBackend`.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:70,Performance,concurren,concurrent,70,"""""""An executor which executes Python functions in the cloud. :class:`.concurrent.futures.ProcessPoolExecutor` and; :class:`.concurrent.futures.ThreadPoolExecutor` enable the use of all the; computer cores available on a single computer. :class:`.BatchPoolExecutor`; enables the use of an effectively arbitrary number of cloud computer cores. Functions provided to :meth:`.submit` are serialized using `dill; <https://dill.readthedocs.io/en/latest/dill.html>`__, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which :meth:`.submit` was; called. The Python version in the docker container will share a major and; minor verison with the local process. The `image` parameter overrides this; behavior. When used as a context manager (the ``with`` syntax), the executor will wait; for all jobs to finish before finishing the ``with`` statement. This; behavior can be controlled by the `wait_on_exit` parameter. This class creates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python pack",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:124,Performance,concurren,concurrent,124,"""""""An executor which executes Python functions in the cloud. :class:`.concurrent.futures.ProcessPoolExecutor` and; :class:`.concurrent.futures.ThreadPoolExecutor` enable the use of all the; computer cores available on a single computer. :class:`.BatchPoolExecutor`; enables the use of an effectively arbitrary number of cloud computer cores. Functions provided to :meth:`.submit` are serialized using `dill; <https://dill.readthedocs.io/en/latest/dill.html>`__, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which :meth:`.submit` was; called. The Python version in the docker container will share a major and; minor verison with the local process. The `image` parameter overrides this; behavior. When used as a context manager (the ``with`` syntax), the executor will wait; for all jobs to finish before finishing the ``with`` statement. This; behavior can be controlled by the `wait_on_exit` parameter. This class creates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python pack",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:1122,Safety,safe,safely,1122,"futures.ThreadPoolExecutor` enable the use of all the; computer cores available on a single computer. :class:`.BatchPoolExecutor`; enables the use of an effectively arbitrary number of cloud computer cores. Functions provided to :meth:`.submit` are serialized using `dill; <https://dill.readthedocs.io/en/latest/dill.html>`__, sent to a Python; docker container in the cloud, deserialized, and executed. The results are; serialized and returned to the machine from which :meth:`.submit` was; called. The Python version in the docker container will share a major and; minor verison with the local process. The `image` parameter overrides this; behavior. When used as a context manager (the ``with`` syntax), the executor will wait; for all jobs to finish before finishing the ``with`` statement. This; behavior can be controlled by the `wait_on_exit` parameter. This class creates a folder ``batch-pool-executor`` at the root of the; bucket specified by the `backend`. This folder can be safely deleted after; all jobs have completed. Examples; --------. Add ``3`` to ``6`` on a machine in the cloud and send the result back to; this machine:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future_nine = bpe.submit(lambda: 3 + 6); >>> future_nine.result() # doctest: +SKIP; 9. :meth:`.map` facilitates the common case of executing a function on many; values in parallel:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x * 3, range(4))); [0, 3, 6, 9]. Parameters; ----------; name:; A name for the executor. Executors produce many batches and each batch; will include this name as a prefix.; backend:; Backend used to execute the jobs. Must be a :class:`.ServiceBackend`.; image:; The name of a Docker image used for each submitted job. The image must; include Python 3.9 or later and must have the ``dill`` Python package; installed. If you intend to use ``numpy``, ensure that OpenBLAS is also; installed. If unspecified, an image with a matching Pyth",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:1646,Energy Efficiency,schedul,schedule,1646,"""""""Call `fn` on cloud machines with arguments from `iterables`. This function returns a generator which will produce each result in the; same order as the `iterables`, only blocking if the result is not yet; ready. You can convert the generator to a list with :class:`.list`. Examples; --------. Do nothing, but on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:. >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters; ----------; fn:; The function to execute.; iterables:; The `iterables` are zipped together and each tuple is used as; arguments to `fn`. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of `iterables` must; have the same length.; timeout:; This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator's; :class:`.BatchPoolFuture`; :meth:`.iterator.__next__` invokes :meth:`.BatchPoolFuture.result` with this; `timeout`.; chunksize:; The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:1857,Energy Efficiency,reduce,reduce,1857,"""""""Call `fn` on cloud machines with arguments from `iterables`. This function returns a generator which will produce each result in the; same order as the `iterables`, only blocking if the result is not yet; ready. You can convert the generator to a list with :class:`.list`. Examples; --------. Do nothing, but on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:. >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters; ----------; fn:; The function to execute.; iterables:; The `iterables` are zipped together and each tuple is used as; arguments to `fn`. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of `iterables` must; have the same length.; timeout:; This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator's; :class:`.BatchPoolFuture`; :meth:`.iterator.__next__` invokes :meth:`.BatchPoolFuture.result` with this; `timeout`.; chunksize:; The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:1361,Safety,timeout,timeout,1361,"""""""Call `fn` on cloud machines with arguments from `iterables`. This function returns a generator which will produce each result in the; same order as the `iterables`, only blocking if the result is not yet; ready. You can convert the generator to a list with :class:`.list`. Examples; --------. Do nothing, but on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:. >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters; ----------; fn:; The function to execute.; iterables:; The `iterables` are zipped together and each tuple is used as; arguments to `fn`. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of `iterables` must; have the same length.; timeout:; This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator's; :class:`.BatchPoolFuture`; :meth:`.iterator.__next__` invokes :meth:`.BatchPoolFuture.result` with this; `timeout`.; chunksize:; The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:1389,Safety,timeout,timeout,1389,"""""""Call `fn` on cloud machines with arguments from `iterables`. This function returns a generator which will produce each result in the; same order as the `iterables`, only blocking if the result is not yet; ready. You can convert the generator to a list with :class:`.list`. Examples; --------. Do nothing, but on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:. >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters; ----------; fn:; The function to execute.; iterables:; The `iterables` are zipped together and each tuple is used as; arguments to `fn`. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of `iterables` must; have the same length.; timeout:; This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator's; :class:`.BatchPoolFuture`; :meth:`.iterator.__next__` invokes :meth:`.BatchPoolFuture.result` with this; `timeout`.; chunksize:; The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:1600,Safety,timeout,timeout,1600,"""""""Call `fn` on cloud machines with arguments from `iterables`. This function returns a generator which will produce each result in the; same order as the `iterables`, only blocking if the result is not yet; ready. You can convert the generator to a list with :class:`.list`. Examples; --------. Do nothing, but on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x: x, range(4))); [0, 1, 2, 3]. Call a function with two parameters, on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(lambda x, y: x + y,; ... [""white"", ""cat"", ""best""],; ... [""house"", ""dog"", ""friend""])); [""whitehouse"", ""catdog"", ""bestfriend""]. Generate products of random matrices, on the cloud:. >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... list(bpe.map(random_product, range(4))); [24.440006386777277, 23.325755364428026, 23.920184804993806, 25.47912882125101]. Parameters; ----------; fn:; The function to execute.; iterables:; The `iterables` are zipped together and each tuple is used as; arguments to `fn`. See the second example for more detail. It is not; possible to pass keyword arguments. Each element of `iterables` must; have the same length.; timeout:; This is roughly a timeout on how long we wait on each function; call. Specifically, each call to the returned generator's; :class:`.BatchPoolFuture`; :meth:`.iterator.__next__` invokes :meth:`.BatchPoolFuture.result` with this; `timeout`.; chunksize:; The number of tasks to schedule in the same docker container. Docker; containers take about 5 seconds to start. Ideally, each task should; take an order of magnitude more time than start-up time. You can; make the chunksize larger to reduce parallelism but increase the; amount of meaningful work done per-container.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:588,Security,access,access,588,"""""""Call `fn` on a cloud machine with all remaining arguments and keyword arguments. The function, any objects it references, the arguments, and the keyword; arguments will be serialized to the cloud machine. Python modules are; not serialized, so you must ensure any needed Python modules and; packages already present in the underlying Docker image. For more; details see the `default_image` argument to :class:`.BatchPoolExecutor`. This function does not return the function's output, it returns a; :class:`.BatchPoolFuture` whose :meth:`.BatchPoolFuture.result` method; can be used to access the value. Examples; --------. Do nothing, but on the cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future = bpe.submit(lambda x: x, 4); ... future.result(); 4. Call a function with two arguments and one keyword argument, on the; cloud:. >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future = bpe.submit(lambda x, y, z: x + y + z,; ... ""poly"", ""ethyl"", z=""ene""); ... future.result(); ""polyethylene"". Generate a product of two random matrices, on the cloud:. >>> def random_product(seed):; ... np.random.seed(seed); ... w = np.random.rand(1, 100); ... u = np.random.rand(100, 1); ... return float(w @ u); >>> with BatchPoolExecutor() as bpe: # doctest: +SKIP; ... future = bpe.submit(random_product, 1); ... future.result(); [23.325755364428026]. Parameters; ----------; fn:; The function to execute.; args:; Arguments for the funciton.; kwargs:; Keyword arguments for the function.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:301,Availability,error,error,301,"""""""Blocks until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class:`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:99,Performance,concurren,concurrent,99,"""""""Blocks until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class:`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:193,Performance,concurren,concurrent,193,"""""""Blocks until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class:`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:251,Safety,timeout,timeout,251,"""""""Blocks until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class:`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:293,Safety,timeout,timeout,293,"""""""Blocks until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class:`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:314,Availability,error,error,314,"""""""Asynchronously wait until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class""`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:112,Performance,concurren,concurrent,112,"""""""Asynchronously wait until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class""`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:206,Performance,concurren,concurrent,206,"""""""Asynchronously wait until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class""`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:264,Safety,timeout,timeout,264,"""""""Asynchronously wait until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class""`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py:306,Safety,timeout,timeout,306,"""""""Asynchronously wait until the job is complete. If the job has been cancelled, this method raises a; :class:`.concurrent.futures.CancelledError`. If the job has timed out, this method raises an; :class""`.concurrent.futures.TimeoutError`. Parameters; ----------; timeout:; Wait this long before raising a timeout error.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/batch_pool_executor.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/batch_pool_executor.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/conftest.py:21,Testability,test,test,21,"# FIXME: remove once test output matches docs",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/conftest.py:47,Performance,race condition,race conditions,47,"# This gets run once per process -- must avoid race conditions",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/conftest.py:41,Safety,avoid,avoid,41,"# This gets run once per process -- must avoid race conditions",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docker.py:71,Deployability,install,installed,71,"""""""; Build a new Python image with dill and the specified pip packages installed. Notes; -----. This function is used to build Python images for :class:`.PythonJob`. Examples; --------. >>> image = build_python_image('us-docker.pkg.dev/<MY_GCP_PROJECT>/hail/batch-python',; ... requirements=['pandas']) # doctest: +SKIP. Parameters; ----------; fullname:; Full name of where to build the image including any repository prefix and tags; if desired (default tag is `latest`).; requirements:; List of pip packages to install.; python_version:; String in the format of `major_version.minor_version` (ex: `3.9`). Defaults to; current version of Python that is running.; _tmp_dir:; Location to place local temporary files used while building the image.; show_docker_output:; Print the output from Docker when building / pushing the image. Returns; -------; Full name where built image is located.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docker.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docker.py:514,Deployability,install,install,514,"""""""; Build a new Python image with dill and the specified pip packages installed. Notes; -----. This function is used to build Python images for :class:`.PythonJob`. Examples; --------. >>> image = build_python_image('us-docker.pkg.dev/<MY_GCP_PROJECT>/hail/batch-python',; ... requirements=['pandas']) # doctest: +SKIP. Parameters; ----------; fullname:; Full name of where to build the image including any repository prefix and tags; if desired (default tag is `latest`).; requirements:; List of pip packages to install.; python_version:; String in the format of `major_version.minor_version` (ex: `3.9`). Defaults to; current version of Python that is running.; _tmp_dir:; Location to place local temporary files used while building the image.; show_docker_output:; Print the output from Docker when building / pushing the image. Returns; -------; Full name where built image is located.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docker.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docker.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:170,Availability,echo,echo,170,"""""""; Explicitly set dependencies on other jobs. Examples; --------. Initialize the batch:. >>> b = Batch(). Create the first job:. >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job `j2` that depends on `j1`:. >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:. >>> b.run(). Notes; -----; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters; ----------; jobs:; Sequence of jobs to depend on. Returns; -------; Same job object with dependencies set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:298,Availability,echo,echo,298,"""""""; Explicitly set dependencies on other jobs. Examples; --------. Initialize the batch:. >>> b = Batch(). Create the first job:. >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job `j2` that depends on `j1`:. >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:. >>> b.run(). Notes; -----; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters; ----------; jobs:; Sequence of jobs to depend on. Returns; -------; Same job object with dependencies set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:20,Integrability,depend,dependencies,20,"""""""; Explicitly set dependencies on other jobs. Examples; --------. Initialize the batch:. >>> b = Batch(). Create the first job:. >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job `j2` that depends on `j1`:. >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:. >>> b.run(). Notes; -----; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters; ----------; jobs:; Sequence of jobs to depend on. Returns; -------; Same job object with dependencies set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:218,Integrability,depend,depends,218,"""""""; Explicitly set dependencies on other jobs. Examples; --------. Initialize the batch:. >>> b = Batch(). Create the first job:. >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job `j2` that depends on `j1`:. >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:. >>> b.run(). Notes; -----; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters; ----------; jobs:; Sequence of jobs to depend on. Returns; -------; Same job object with dependencies set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:545,Integrability,depend,dependency,545,"""""""; Explicitly set dependencies on other jobs. Examples; --------. Initialize the batch:. >>> b = Batch(). Create the first job:. >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job `j2` that depends on `j1`:. >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:. >>> b.run(). Notes; -----; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters; ----------; jobs:; Sequence of jobs to depend on. Returns; -------; Same job object with dependencies set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:636,Integrability,depend,depend,636,"""""""; Explicitly set dependencies on other jobs. Examples; --------. Initialize the batch:. >>> b = Batch(). Create the first job:. >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job `j2` that depends on `j1`:. >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:. >>> b.run(). Notes; -----; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters; ----------; jobs:; Sequence of jobs to depend on. Returns; -------; Same job object with dependencies set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:686,Integrability,depend,dependencies,686,"""""""; Explicitly set dependencies on other jobs. Examples; --------. Initialize the batch:. >>> b = Batch(). Create the first job:. >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello""'). Create the second job `j2` that depends on `j1`:. >>> j2 = b.new_job(); >>> j2.depends_on(j1); >>> j2.command(f'echo ""world""'). Execute the batch:. >>> b.run(). Notes; -----; Dependencies between jobs are automatically created when resources from; one job are used in a subsequent job. This method is only needed when; no intermediate resource exists and the dependency needs to be explicitly; set. Parameters; ----------; jobs:; Sequence of jobs to depend on. Returns; -------; Same job object with dependencies set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:173,Availability,echo,echo,173,"""""""; Set the job's storage size. Examples; --------. Set the job's disk requirements to 10 Gi:. >>> b = Batch(); >>> j = b.new_job(); >>> (j.storage('10Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; -----. The storage expression must be of the form {number}{suffix}; where valid optional suffixes are *K*, *Ki*, *M*, *Mi*,; *G*, *Gi*, *T*, *Ti*, *P*, and *Pi*. Omitting a suffix means; the value is in bytes. For the :class:`.ServiceBackend`, jobs requesting one or more cores receive; 5 GiB of storage for the root file system `/`. Jobs requesting a fraction of a core; receive the same fraction of 5 GiB of storage. If you need additional storage, you; can explicitly request more storage using this method and the extra storage space; will be mounted at `/io`. Batch automatically writes all :class:`.ResourceFile` to; `/io`. The default storage size is 0 Gi. The minimum storage size is 0 Gi and the; maximum storage size is 64 Ti. If storage is set to a value between 0 Gi; and 10 Gi, the storage request is rounded up to 10 Gi. All values are; rounded up to the nearest Gi. Parameters; ----------; storage:; Units are in bytes if `storage` is an :obj:`int`. If `None`, use the; default storage size for the :class:`.ServiceBackend` (0 Gi). Returns; -------; Same job object with storage set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:180,Availability,echo,echo,180,"""""""; Set the job's memory requirements. Examples; --------. Set the job's memory requirement to be 3Gi:. >>> b = Batch(); >>> j = b.new_job(); >>> (j.memory('3Gi'); ... .command(f'echo ""hello""')); >>> b.run(). Notes; -----. The memory expression must be of the form {number}{suffix}; where valid optional suffixes are *K*, *Ki*, *M*, *Mi*,; *G*, *Gi*, *T*, *Ti*, *P*, and *Pi*. Omitting a suffix means; the value is in bytes. For the :class:`.ServiceBackend`, the values 'lowmem', 'standard',; and 'highmem' are also valid arguments. 'lowmem' corresponds to; approximately 1 Gi/core, 'standard' corresponds to approximately; 4 Gi/core, and 'highmem' corresponds to approximately 7 Gi/core.; The default value is 'standard'. Parameters; ----------; memory:; Units are in bytes if `memory` is an :obj:`int`. If `None`,; use the default value for the :class:`.ServiceBackend` ('standard'). Returns; -------; Same job object with memory requirements set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:441,Availability,echo,echo,441,"""""""; Set the job's CPU requirements. Notes; -----. The string expression must be of the form {number}{suffix}; where the optional suffix is *m* representing millicpu.; Omitting a suffix means the value is in cpu. For the :class:`.ServiceBackend`, `cores` must be a power of; two between 0.25 and 16. Examples; --------. Set the job's CPU requirement to 250 millicpu:. >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters; ----------; cores:; Units are in cpu if `cores` is numeric. If `None`,; use the default value for the :class:`.ServiceBackend`; (1 cpu). Returns; -------; Same job object with CPU requirements set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:265,Energy Efficiency,power,power,265,"""""""; Set the job's CPU requirements. Notes; -----. The string expression must be of the form {number}{suffix}; where the optional suffix is *m* representing millicpu.; Omitting a suffix means the value is in cpu. For the :class:`.ServiceBackend`, `cores` must be a power of; two between 0.25 and 16. Examples; --------. Set the job's CPU requirement to 250 millicpu:. >>> b = Batch(); >>> j = b.new_job(); >>> (j.cpu('250m'); ... .command(f'echo ""hello""')); >>> b.run(). Parameters; ----------; cores:; Units are in cpu if `cores` is numeric. If `None`,; use the default value for the :class:`.ServiceBackend`; (1 cpu). Returns; -------; Same job object with CPU requirements set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:254,Availability,echo,echo,254,"""""""; Set the job to always run, even if dependencies fail. Warning; -------; Jobs set to always run are not cancellable!. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters; ----------; always_run:; If True, set job to always run. Returns; -------; Same job object set to always run.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:40,Integrability,depend,dependencies,40,"""""""; Set the job to always run, even if dependencies fail. Warning; -------; Jobs set to always run are not cancellable!. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters; ----------; always_run:; If True, set job to always run. Returns; -------; Same job object set to always run.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:188,Testability,test,test,188,"""""""; Set the job to always run, even if dependencies fail. Warning; -------; Jobs set to always run are not cancellable!. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_run(); ... .command(f'echo ""hello""')). Parameters; ----------; always_run:; If True, set job to always run. Returns; -------; Same job object set to always run.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:279,Availability,echo,echo,279,"""""""; Set whether a job is run on spot instances. By default, all jobs run on spot instances. Examples; --------. Ensure a job only runs on non-spot instances:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters; ----------; is_spot:; If False, this job will be run on non-spot instances. Returns; -------; Same job object.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:206,Testability,test,test,206,"""""""; Set whether a job is run on spot instances. By default, all jobs run on spot instances. Examples; --------. Ensure a job only runs on non-spot instances:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> j = j.spot(False); >>> j = j.command(f'echo ""hello""'). Parameters; ----------; is_spot:; If False, this job will be run on non-spot instances. Returns; -------; Same job object.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:453,Availability,echo,echo,453,"""""""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:623,Availability,echo,echo,623,"""""""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:760,Availability,avail,available,760,"""""""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:847,Availability,avail,available,847,"""""""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:231,Energy Efficiency,charge,charges,231,"""""""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:258,Performance,latency,latency,258,"""""""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:218,Safety,avoid,avoid,218,"""""""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:375,Testability,test,test,375,"""""""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:556,Testability,test,test,556,"""""""; Set the cloud regions a job can run in. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method may be used to ensure code executes in the same region as the data it reads.; This can avoid egress charges as well as improve latency. Examples; --------. Require the job to run in 'us-central1':. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(['us-central1']); ... .command(f'echo ""hello""')). Specify the job can run in any region:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.regions(None); ... .command(f'echo ""hello""')). Parameters; ----------; regions:; The cloud region(s) to run this job in. Use `None` to signify; the job can run in any available region. Use py:staticmethod:`.ServiceBackend.supported_regions`; to list the available regions to choose from. The default is the job can run in; any region. Returns; -------; Same job object with the cloud regions the job can run in set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:274,Availability,echo,echo,274,"""""""; Set the maximum amount of time this job can run for in seconds. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters; ----------; timeout:; Maximum amount of time in seconds for a job to run before being killed.; If `None`, there is no timeout. Returns; -------; Same job object set with a timeout in seconds.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:246,Safety,timeout,timeout,246,"""""""; Set the maximum amount of time this job can run for in seconds. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters; ----------; timeout:; Maximum amount of time in seconds for a job to run before being killed.; If `None`, there is no timeout. Returns; -------; Same job object set with a timeout in seconds.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:315,Safety,timeout,timeout,315,"""""""; Set the maximum amount of time this job can run for in seconds. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters; ----------; timeout:; Maximum amount of time in seconds for a job to run before being killed.; If `None`, there is no timeout. Returns; -------; Same job object set with a timeout in seconds.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:421,Safety,timeout,timeout,421,"""""""; Set the maximum amount of time this job can run for in seconds. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters; ----------; timeout:; Maximum amount of time in seconds for a job to run before being killed.; If `None`, there is no timeout. Returns; -------; Same job object set with a timeout in seconds.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:475,Safety,timeout,timeout,475,"""""""; Set the maximum amount of time this job can run for in seconds. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters; ----------; timeout:; Maximum amount of time in seconds for a job to run before being killed.; If `None`, there is no timeout. Returns; -------; Same job object set with a timeout in seconds.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:209,Testability,test,test,209,"""""""; Set the maximum amount of time this job can run for in seconds. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.timeout(10); ... .command(f'echo ""hello""')). Parameters; ----------; timeout:; Maximum amount of time in seconds for a job to run before being killed.; If `None`, there is no timeout. Returns; -------; Same job object set with a timeout in seconds.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:256,Performance,perform,performance,256,"""""""; Add a bucket to mount with gcsfuse. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method can; be called more than once. This method has been deprecated. Use :meth:`.Job.cloudfuse`; instead. Warning; -------; There are performance and cost implications of using `gcsfuse <https://cloud.google.com/storage/docs/gcs-fuse>`__. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters; ----------; bucket:; Name of the google storage bucket to mount.; mount_point:; The path at which the bucket should be mounted to in the Docker; container.; read_only:; If ``True``, mount the bucket in read-only mode. Returns; -------; Same job object set with a bucket to mount with gcsfuse.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:427,Testability,test,test,427,"""""""; Add a bucket to mount with gcsfuse. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method can; be called more than once. This method has been deprecated. Use :meth:`.Job.cloudfuse`; instead. Warning; -------; There are performance and cost implications of using `gcsfuse <https://cloud.google.com/storage/docs/gcs-fuse>`__. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.gcsfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-file')). Parameters; ----------; bucket:; Name of the google storage bucket to mount.; mount_point:; The path at which the bucket should be mounted to in the Docker; container.; read_only:; If ``True``, mount the bucket in read-only mode. Returns; -------; Same job object set with a bucket to mount with gcsfuse.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:239,Performance,perform,performance,239,"""""""; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method can; be called more than once. Warning; -------; There are performance and cost implications of using `gcsfuse <https://cloud.google.com/storage/docs/gcs-fuse>`__; or `blobfuse <https://github.com/Azure/azure-storage-fuse#considerations>`__. Examples; --------. Google Cloud Platform:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters; ----------; bucket:; Name of the google storage bucket to mount or the path to an Azure container in the; format of `<account>/<container>`.; mount_point:; The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only:; If ``True``, mount the cloud blob storage in read-only mode. Returns; -------; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:512,Testability,test,test,512,"""""""; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method can; be called more than once. Warning; -------; There are performance and cost implications of using `gcsfuse <https://cloud.google.com/storage/docs/gcs-fuse>`__; or `blobfuse <https://github.com/Azure/azure-storage-fuse#considerations>`__. Examples; --------. Google Cloud Platform:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters; ----------; bucket:; Name of the google storage bucket to mount or the path to an Azure container in the; format of `<account>/<container>`.; mount_point:; The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only:; If ``True``, mount the cloud blob storage in read-only mode. Returns; -------; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:690,Testability,test,test,690,"""""""; Add a bucket to mount with gcsfuse in GCP or a storage container with blobfuse in Azure. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. This method can; be called more than once. Warning; -------; There are performance and cost implications of using `gcsfuse <https://cloud.google.com/storage/docs/gcs-fuse>`__; or `blobfuse <https://github.com/Azure/azure-storage-fuse#considerations>`__. Examples; --------. Google Cloud Platform:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-bucket', '/my-bucket'); ... .command(f'cat /my-bucket/my-blob-object')). Azure:. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.cloudfuse('my-account/my-container', '/dest'); ... .command(f'cat /dest/my-blob-object')). Parameters; ----------; bucket:; Name of the google storage bucket to mount or the path to an Azure container in the; format of `<account>/<container>`.; mount_point:; The path at which the cloud blob storage should be mounted to in the Docker; container.; read_only:; If ``True``, mount the cloud blob storage in read-only mode. Returns; -------; Same job object set with a cloud storage path to mount with either gcsfuse or blobfuse.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:295,Availability,echo,echo,295,"""""""; Set the job to always copy output to cloud storage, even if the job failed. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters; ----------; always_copy_output:; If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Returns; -------; Same job object set to always copy output.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:221,Testability,test,test,221,"""""""; Set the job to always copy output to cloud storage, even if the job failed. Notes; -----; Can only be used with the :class:`.backend.ServiceBackend`. Examples; --------. >>> b = Batch(backend=backend.ServiceBackend('test')); >>> j = b.new_job(); >>> (j.always_copy_output(); ... .command(f'echo ""hello"" > {j.ofile} && false')). Parameters; ----------; always_copy_output:; If True, set job to always copy output to cloud storage regardless; of whether the job succeeded. Returns; -------; Same job object set to always copy output.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:225,Availability,echo,echo,225,"""""""; Object representing a single bash job to execute. Examples; --------. Create a batch object:. >>> b = Batch(). Create a new bash job that prints hello to a temporary file `t.ofile`:. >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'). Write the temporary file `t.ofile` to a permanent location. >>> b.write_output(j.ofile, 'hello.txt'). Execute the DAG:. >>> b.run(). Notes; -----; This class should never be created directly by the user. Use :meth:`.Batch.new_job`; or :meth:`.Batch.new_bash_job` instead.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:352,Testability,log,log,352,"""""""Declare a resource group for a job. Examples; --------. Declare a resource group:. >>> b = Batch(); >>> input = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'); >>> j = b.new_job(); >>> j.declare_resource_group(tmp1={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam',; ... 'log': '{root}.log'}); >>> j.command(f'plink --bfile {input} --make-bed --out {j.tmp1}'); >>> b.run() # doctest: +SKIP. Warning; -------; Be careful when specifying the expressions for each file as this is Python; code that is executed with `eval`!. Parameters; ----------; mappings:; Keywords (in the above example `tmp1`) are the name(s) of the; resource group(s). File names may contain arbitrary Python; expressions, which will be evaluated by Python `eval`. To use the; keyword as the file name, use `{root}` (in the above example {root}; will be replaced with `tmp1`). Returns; -------; Same job object with resource groups set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:366,Testability,log,log,366,"""""""Declare a resource group for a job. Examples; --------. Declare a resource group:. >>> b = Batch(); >>> input = b.read_input_group(bed='data/example.bed',; ... bim='data/example.bim',; ... fam='data/example.fam'); >>> j = b.new_job(); >>> j.declare_resource_group(tmp1={'bed': '{root}.bed',; ... 'bim': '{root}.bim',; ... 'fam': '{root}.fam',; ... 'log': '{root}.log'}); >>> j.command(f'plink --bfile {input} --make-bed --out {j.tmp1}'); >>> b.run() # doctest: +SKIP. Warning; -------; Be careful when specifying the expressions for each file as this is Python; code that is executed with `eval`!. Parameters; ----------; mappings:; Keywords (in the above example `tmp1`) are the name(s) of the; resource group(s). File names may contain arbitrary Python; expressions, which will be evaluated by Python `eval`. To use the; keyword as the file name, use `{root}` (in the above example {root}; will be replaced with `tmp1`). Returns; -------; Same job object with resource groups set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:183,Availability,echo,echo,183,"""""""; Set the job's docker image. Examples; --------. Set the job's docker image to `ubuntu:22.04`:. >>> b = Batch(); >>> j = b.new_job(); >>> (j.image('ubuntu:22.04'); ... .command(f'echo ""hello""')); >>> b.run() # doctest: +SKIP. Parameters; ----------; image:; Docker image to use. Returns; -------; Same job object with docker image set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:145,Availability,echo,echo,145,"""""""Set the job's command to execute. Examples; --------. Simple job with no output files:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file `j.ofile` that is written to a; permanent location:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:. >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:. >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; -----; This method can be called more than once. It's behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command. To declare a resource file of type :class:`.JobResourceFile`, use either; the get attribute syntax of `job.{identifier}` or the get item syntax of; `job['identifier']`. If an object for that identifier doesn't exist,; then one will be created automatically (only allowed in the; :meth:`.command` method). The identifier name can be any valid Python; identifier such as `ofile5000`. All :class:`.JobResourceFile` are temporary files and must be written to; a permanent location using :meth:`.Batch.write_output` if the output; needs to be saved. Only resources can be referred to in commands. Referencing a; :class:`.batch.Batch` or :class:`.Job` will result in an error. Parameters; ----------; command:; A ``bash`` command. Returns; -------; Same job object with command appended.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:316,Availability,echo,echo,316,"""""""Set the job's command to execute. Examples; --------. Simple job with no output files:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file `j.ofile` that is written to a; permanent location:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:. >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:. >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; -----; This method can be called more than once. It's behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command. To declare a resource file of type :class:`.JobResourceFile`, use either; the get attribute syntax of `job.{identifier}` or the get item syntax of; `job['identifier']`. If an object for that identifier doesn't exist,; then one will be created automatically (only allowed in the; :meth:`.command` method). The identifier name can be any valid Python; identifier such as `ofile5000`. All :class:`.JobResourceFile` are temporary files and must be written to; a permanent location using :meth:`.Batch.write_output` if the output; needs to be saved. Only resources can be referred to in commands. Referencing a; :class:`.batch.Batch` or :class:`.Job` will result in an error. Parameters; ----------; command:; A ``bash`` command. Returns; -------; Same job object with command appended.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:507,Availability,echo,echo,507,"""""""Set the job's command to execute. Examples; --------. Simple job with no output files:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file `j.ofile` that is written to a; permanent location:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:. >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:. >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; -----; This method can be called more than once. It's behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command. To declare a resource file of type :class:`.JobResourceFile`, use either; the get attribute syntax of `job.{identifier}` or the get item syntax of; `job['identifier']`. If an object for that identifier doesn't exist,; then one will be created automatically (only allowed in the; :meth:`.command` method). The identifier name can be any valid Python; identifier such as `ofile5000`. All :class:`.JobResourceFile` are temporary files and must be written to; a permanent location using :meth:`.Batch.write_output` if the output; needs to be saved. Only resources can be referred to in commands. Referencing a; :class:`.batch.Batch` or :class:`.Job` will result in an error. Parameters; ----------; command:; A ``bash`` command. Returns; -------; Same job object with command appended.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:777,Availability,echo,echo,777,"""""""Set the job's command to execute. Examples; --------. Simple job with no output files:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file `j.ofile` that is written to a; permanent location:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:. >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:. >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; -----; This method can be called more than once. It's behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command. To declare a resource file of type :class:`.JobResourceFile`, use either; the get attribute syntax of `job.{identifier}` or the get item syntax of; `job['identifier']`. If an object for that identifier doesn't exist,; then one will be created automatically (only allowed in the; :meth:`.command` method). The identifier name can be any valid Python; identifier such as `ofile5000`. All :class:`.JobResourceFile` are temporary files and must be written to; a permanent location using :meth:`.Batch.write_output` if the output; needs to be saved. Only resources can be referred to in commands. Referencing a; :class:`.batch.Batch` or :class:`.Job` will result in an error. Parameters; ----------; command:; A ``bash`` command. Returns; -------; Same job object with command appended.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:820,Availability,echo,echo,820,"""""""Set the job's command to execute. Examples; --------. Simple job with no output files:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file `j.ofile` that is written to a; permanent location:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:. >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:. >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; -----; This method can be called more than once. It's behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command. To declare a resource file of type :class:`.JobResourceFile`, use either; the get attribute syntax of `job.{identifier}` or the get item syntax of; `job['identifier']`. If an object for that identifier doesn't exist,; then one will be created automatically (only allowed in the; :meth:`.command` method). The identifier name can be any valid Python; identifier such as `ofile5000`. All :class:`.JobResourceFile` are temporary files and must be written to; a permanent location using :meth:`.Batch.write_output` if the output; needs to be saved. Only resources can be referred to in commands. Referencing a; :class:`.batch.Batch` or :class:`.Job` will result in an error. Parameters; ----------; command:; A ``bash`` command. Returns; -------; Same job object with command appended.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:863,Availability,echo,echo,863,"""""""Set the job's command to execute. Examples; --------. Simple job with no output files:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file `j.ofile` that is written to a; permanent location:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:. >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:. >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; -----; This method can be called more than once. It's behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command. To declare a resource file of type :class:`.JobResourceFile`, use either; the get attribute syntax of `job.{identifier}` or the get item syntax of; `job['identifier']`. If an object for that identifier doesn't exist,; then one will be created automatically (only allowed in the; :meth:`.command` method). The identifier name can be any valid Python; identifier such as `ofile5000`. All :class:`.JobResourceFile` are temporary files and must be written to; a permanent location using :meth:`.Batch.write_output` if the output; needs to be saved. Only resources can be referred to in commands. Referencing a; :class:`.batch.Batch` or :class:`.Job` will result in an error. Parameters; ----------; command:; A ``bash`` command. Returns; -------; Same job object with command appended.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:1868,Availability,error,error,1868,"""""""Set the job's command to execute. Examples; --------. Simple job with no output files:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello""'); >>> b.run(). Simple job with one temporary file `j.ofile` that is written to a; permanent location:. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.write_output(j.ofile, 'output/hello.txt'); >>> b.run(). Two jobs with a file interdependency:. >>> b = Batch(); >>> j1 = b.new_job(); >>> j1.command(f'echo ""hello"" > {j1.ofile}'); >>> j2 = b.new_bash_job(); >>> j2.command(f'cat {j1.ofile} > {j2.ofile}'); >>> b.write_output(j2.ofile, 'output/cat_output.txt'); >>> b.run(). Specify multiple commands in the same job:. >>> b = Batch(); >>> t = b.new_job(); >>> j.command(f'echo ""hello"" > {j.tmp1}'); >>> j.command(f'echo ""world"" > {j.tmp2}'); >>> j.command(f'echo ""!"" > {j.tmp3}'); >>> j.command(f'cat {j.tmp1} {j.tmp2} {j.tmp3} > {j.ofile}'); >>> b.write_output(j.ofile, 'output/concatenated.txt'); >>> b.run(). Notes; -----; This method can be called more than once. It's behavior is to append; commands to run to the set of previously defined commands rather than; overriding an existing command. To declare a resource file of type :class:`.JobResourceFile`, use either; the get attribute syntax of `job.{identifier}` or the get item syntax of; `job['identifier']`. If an object for that identifier doesn't exist,; then one will be created automatically (only allowed in the; :meth:`.command` method). The identifier name can be any valid Python; identifier such as `ofile5000`. All :class:`.JobResourceFile` are temporary files and must be written to; a permanent location using :meth:`.Batch.write_output` if the output; needs to be saved. Only resources can be referred to in commands. Referencing a; :class:`.batch.Batch` or :class:`.Job` will result in an error. Parameters; ----------; command:; A ``bash`` command. Returns; -------; Same job object with command appended.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:216,Deployability,install,installed,216,"""""""; Set the job's docker image. Notes; -----. `image` must already exist and have the same version of Python as what is; being used on the computer submitting the Batch. It also must have the; `dill` Python package installed. You can use the function :func:`.docker.build_python_image`; to build a new image containing `dill` and additional Python packages. Examples; --------. Set the job's docker image to `hailgenetics/python-dill:3.9-slim`:. >>> b = Batch(); >>> j = b.new_python_job(); >>> (j.image('hailgenetics/python-dill:3.9-slim'); ... .call(print, 'hello')); >>> b.run() # doctest: +SKIP. Parameters; ----------; image:; Docker image to use. Returns; -------; Same job object with docker image set.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:1456,Availability,down,downstream,1456,"a.append(d); return json.dumps(data). # Get all the multiplication and addition table results. b = Batch(name='add-mult-table'). formatted_results = []. for x in range(3):; for y in range(3):; j = b.new_python_job(name=f'{x}-{y}'); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; -----; Unlike the :class:`.BashJob`, a :class:`.PythonJob` returns a new; :class:`.PythonResult` for every invocation of :meth:`.PythonJob.call`. A; :class:`.PythonResult` can be used as an argument in subsequent invocations of; :meth:`.PythonJob.call`, as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, :class:`.InputResourceFile`,; :class:`.JobResourceFile`, and :class:`.ResourceGroup` can be passed to; :meth:`.PythonJob.call`. Batch automatically detects dependencies between jobs; including between python jobs and bash jobs. When a :class:`.ResourceFile` is passed as an argument, it is passed to the; function as a string to the local file path. When a :class:`.ResourceGroup`; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original :class:`.ResourceGroup`; and the values are the local file paths. Like :class:`.JobResourceFile`, all :class:`.PythonResult` are stored as; temporary files and must be written to a permanent location using; :meth:`.Batch.write_output` if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods :meth:`.PythonResult.as_str`, :meth:`.PythonResult.as_repr`,; or :meth:`.Pyth",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:3008,Availability,error,error,3008,"; :meth:`.PythonJob.call`. Batch automatically detects dependencies between jobs; including between python jobs and bash jobs. When a :class:`.ResourceFile` is passed as an argument, it is passed to the; function as a string to the local file path. When a :class:`.ResourceGroup`; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original :class:`.ResourceGroup`; and the values are the local file paths. Like :class:`.JobResourceFile`, all :class:`.PythonResult` are stored as; temporary files and must be written to a permanent location using; :meth:`.Batch.write_output` if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods :meth:`.PythonResult.as_str`, :meth:`.PythonResult.as_repr`,; or :meth:`.PythonResult.as_json` to convert a `PythonResult` to a; `JobResourceFile` with the desired output. Warning; -------. You must have any non-builtin packages that are used by `unapplied` installed; in your image. You can use :func:`.docker.build_python_image` to build a; Python image with additional Python packages installed that is compatible; with Python jobs. Here are some tips to make sure your function can be used with Batch:. - Only reference top-level modules in your functions: like numpy or pandas.; - If you get a serialization error, try moving your imports into your function.; - Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters; ----------; unapplied:; A reference to a Python function to execute.; args:; Positional arguments to the Python function. Must be either a builtin; Python object, a :class:`.Resource`, or a Dill serializable object.; kwargs:; Key-word arguments to the Python function. Must be either a builtin; Python object, a :class:`.Resource`, or a Dill serializable object. Returns; -------; :class:`.resource.PythonResult`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:2653,Deployability,install,installed,2653,"; :meth:`.PythonJob.call`. Batch automatically detects dependencies between jobs; including between python jobs and bash jobs. When a :class:`.ResourceFile` is passed as an argument, it is passed to the; function as a string to the local file path. When a :class:`.ResourceGroup`; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original :class:`.ResourceGroup`; and the values are the local file paths. Like :class:`.JobResourceFile`, all :class:`.PythonResult` are stored as; temporary files and must be written to a permanent location using; :meth:`.Batch.write_output` if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods :meth:`.PythonResult.as_str`, :meth:`.PythonResult.as_repr`,; or :meth:`.PythonResult.as_json` to convert a `PythonResult` to a; `JobResourceFile` with the desired output. Warning; -------. You must have any non-builtin packages that are used by `unapplied` installed; in your image. You can use :func:`.docker.build_python_image` to build a; Python image with additional Python packages installed that is compatible; with Python jobs. Here are some tips to make sure your function can be used with Batch:. - Only reference top-level modules in your functions: like numpy or pandas.; - If you get a serialization error, try moving your imports into your function.; - Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters; ----------; unapplied:; A reference to a Python function to execute.; args:; Positional arguments to the Python function. Must be either a builtin; Python object, a :class:`.Resource`, or a Dill serializable object.; kwargs:; Key-word arguments to the Python function. Must be either a builtin; Python object, a :class:`.Resource`, or a Dill serializable object. Returns; -------; :class:`.resource.PythonResult`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:2783,Deployability,install,installed,2783,"; :meth:`.PythonJob.call`. Batch automatically detects dependencies between jobs; including between python jobs and bash jobs. When a :class:`.ResourceFile` is passed as an argument, it is passed to the; function as a string to the local file path. When a :class:`.ResourceGroup`; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original :class:`.ResourceGroup`; and the values are the local file paths. Like :class:`.JobResourceFile`, all :class:`.PythonResult` are stored as; temporary files and must be written to a permanent location using; :meth:`.Batch.write_output` if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods :meth:`.PythonResult.as_str`, :meth:`.PythonResult.as_repr`,; or :meth:`.PythonResult.as_json` to convert a `PythonResult` to a; `JobResourceFile` with the desired output. Warning; -------. You must have any non-builtin packages that are used by `unapplied` installed; in your image. You can use :func:`.docker.build_python_image` to build a; Python image with additional Python packages installed that is compatible; with Python jobs. Here are some tips to make sure your function can be used with Batch:. - Only reference top-level modules in your functions: like numpy or pandas.; - If you get a serialization error, try moving your imports into your function.; - Instead of serializing a complex class, determine what information is essential; and only serialize that, perhaps as a dict or array. Parameters; ----------; unapplied:; A reference to a Python function to execute.; args:; Positional arguments to the Python function. Must be either a builtin; Python object, a :class:`.Resource`, or a Dill serializable object.; kwargs:; Key-word arguments to the Python function. Must be either a builtin; Python object, a :class:`.Resource`, or a Dill serializable object. Returns; -------; :class:`.resource.PythonResult`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:1680,Integrability,depend,dependencies,1680,"); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; -----; Unlike the :class:`.BashJob`, a :class:`.PythonJob` returns a new; :class:`.PythonResult` for every invocation of :meth:`.PythonJob.call`. A; :class:`.PythonResult` can be used as an argument in subsequent invocations of; :meth:`.PythonJob.call`, as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, :class:`.InputResourceFile`,; :class:`.JobResourceFile`, and :class:`.ResourceGroup` can be passed to; :meth:`.PythonJob.call`. Batch automatically detects dependencies between jobs; including between python jobs and bash jobs. When a :class:`.ResourceFile` is passed as an argument, it is passed to the; function as a string to the local file path. When a :class:`.ResourceGroup`; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original :class:`.ResourceGroup`; and the values are the local file paths. Like :class:`.JobResourceFile`, all :class:`.PythonResult` are stored as; temporary files and must be written to a permanent location using; :meth:`.Batch.write_output` if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods :meth:`.PythonResult.as_str`, :meth:`.PythonResult.as_repr`,; or :meth:`.PythonResult.as_json` to convert a `PythonResult` to a; `JobResourceFile` with the desired output. Warning; -------. You must have any non-builtin packages that are used by `unapplied` installed; in your image. You can use :func:`.do",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py:1672,Safety,detect,detects,1672,"); add_result = j.call(add, x, y); mult_result = j.call(multiply, x, y); result = j.call(format_as_csv, x, y, add_result, mult_result); formatted_results.append(result.as_str()). cat_j = b.new_bash_job(name='concatenate'); cat_j.command(f'cat {"" "".join(formatted_results)} > {cat_j.output}'). csv_to_json_j = b.new_python_job(name='csv-to-json'); json_output = csv_to_json_j.call(csv_to_json, cat_j.output). b.write_output(j.as_str(), '/output/add_mult_table.json'); b.run(). Notes; -----; Unlike the :class:`.BashJob`, a :class:`.PythonJob` returns a new; :class:`.PythonResult` for every invocation of :meth:`.PythonJob.call`. A; :class:`.PythonResult` can be used as an argument in subsequent invocations of; :meth:`.PythonJob.call`, as an argument in downstream python jobs,; or as inputs to other bash jobs. Likewise, :class:`.InputResourceFile`,; :class:`.JobResourceFile`, and :class:`.ResourceGroup` can be passed to; :meth:`.PythonJob.call`. Batch automatically detects dependencies between jobs; including between python jobs and bash jobs. When a :class:`.ResourceFile` is passed as an argument, it is passed to the; function as a string to the local file path. When a :class:`.ResourceGroup`; is passed as an argument, it is passed to the function as a dict where the; keys are the resource identifiers in the original :class:`.ResourceGroup`; and the values are the local file paths. Like :class:`.JobResourceFile`, all :class:`.PythonResult` are stored as; temporary files and must be written to a permanent location using; :meth:`.Batch.write_output` if the output needs to be saved. A; PythonResult is saved as a dill serialized object. However, you; can use one of the methods :meth:`.PythonResult.as_str`, :meth:`.PythonResult.as_repr`,; or :meth:`.PythonResult.as_json` to convert a `PythonResult` to a; `JobResourceFile` with the desired output. Warning; -------. You must have any non-builtin packages that are used by `unapplied` installed; in your image. You can use :func:`.do",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/job.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/job.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/resource.py:204,Availability,echo,echo,204,"""""""; Class representing an intermediate file from a job. Examples; --------; `j.ofile` is a :class:`.JobResourceFile` on the job`j`:. >>> b = Batch(); >>> j = b.new_job(name='hello-tmp'); >>> j.command(f'echo ""hello world"" > {j.ofile}'); >>> b.run(). Notes; -----; All :class:`.JobResourceFile` are temporary files and must be written; to a permanent location using :meth:`.Batch.write_output` if the output needs; to be saved.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/resource.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/resource.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/resource.py:114,Availability,echo,echo,114,"""""""; Specify the file extension to use. Examples; --------. >>> b = Batch(); >>> j = b.new_job(); >>> j.command(f'echo ""hello"" > {j.ofile}'); >>> j.ofile.add_extension('.txt'); >>> b.run(). Notes; -----; The default file name for a :class:`.JobResourceFile` is the name; of the identifier. Parameters; ----------; extension: :obj:`str`; File extension to use. Returns; -------; :class:`.JobResourceFile`; Same resource file with the extension specified; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/resource.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/resource.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py:242,Modifiability,config,config,242,"# -*- coding: utf-8 -*-; #; # Configuration file for the Sphinx documentation builder.; #; # This file does only contain a selection of the most common options. For a; # full list see the documentation:; # http://www.sphinx-doc.org/en/master/config; # -- Path setup --------------------------------------------------------------; # If extensions (or modules to document with autodoc) are in another directory,; # add these directories to sys.path here. If the directory is relative to the; # documentation root, use os.path.abspath to make it absolute, like shown here.; #; # import os; # import sys; # sys.path.insert(0, os.path.abspath('.'))",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py:13,Deployability,configurat,configuration,13,"# -- General configuration ---------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py:13,Modifiability,config,configuration,13,"# -- General configuration ---------------------------------------------------; # If your documentation needs a minimal Sphinx version, state it here.; #",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py:112,Availability,avail,available,112,"# Theme options are theme-specific and customize the look and feel of a theme; # further. For a list of options available for each theme, see the; # documentation.; #; # html_theme_options = {}; # Add any paths that contain custom static files (such as style sheets) here,; # relative to this directory. They are copied after the builtin static files,; # so a file named ""default.css"" will overwrite the builtin ""default.css"".; # html_static_path = ['_static']; # Custom sidebar templates, must be a dictionary that maps document names; # to template names.; #; # The default sidebars (for documents that don't match any pattern) are; # defined by theme itself. Builtin themes are using these templates by; # default: ``['localtoc.html', 'relations.html', 'sourcelink.html',; # 'searchbox.html']``.; #; # html_sidebars = {}; # https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py:15,Deployability,configurat,configuration,15,"# -- Extension configuration -------------------------------------------------",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py:15,Modifiability,config,configuration,15,"# -- Extension configuration -------------------------------------------------",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/conf.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/conf.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/batch_clumping.py:33,Testability,test,test,33,"""""""; QC data and get association test statistics; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/batch_clumping.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/batch_clumping.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint.py:21,Testability,test,testing,21,"# split training and testing data for the current window",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint.py:37,Testability,test,testing,37,"# apply the trained random forest on testing data",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint_batching.py:21,Testability,test,testing,21,"# split training and testing data for the current window",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint_batching.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint_batching.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint_batching.py:37,Testability,test,testing,37,"# apply the trained random forest on testing data",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint_batching.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_checkpoint_batching.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_simple.py:21,Testability,test,testing,21,"# split training and testing data for the current window",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/run_rf_simple.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_simple.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_simple.py:37,Testability,test,testing,37,"# apply the trained random forest on testing data",MatchSource.CODE_COMMENT,hail/python/hailtop/batch/docs/cookbook/files/run_rf_simple.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch/docs/cookbook/files/run_rf_simple.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch_client/aioclient.py:22,Availability,error,error,22,"# don't return status error",MatchSource.CODE_COMMENT,hail/python/hailtop/batch_client/aioclient.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch_client/aioclient.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch_client/aioclient.py:2,Deployability,update,updates,2,"# updates _last_known_status",MatchSource.CODE_COMMENT,hail/python/hailtop/batch_client/aioclient.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch_client/aioclient.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch_client/aioclient.py:2,Deployability,update,updates,2,"# updates _last_known_status",MatchSource.CODE_COMMENT,hail/python/hailtop/batch_client/aioclient.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/batch_client/aioclient.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/deploy_config.py:16,Integrability,depend,depend,16,"# FIXME: should depend on ssl context",MatchSource.CODE_COMMENT,hail/python/hailtop/config/deploy_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/deploy_config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/deploy_config.py:5,Security,encrypt,encryption,5,"# no encryption on the internal gateway",MatchSource.CODE_COMMENT,hail/python/hailtop/config/deploy_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/deploy_config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/deploy_config.py:27,Security,access,access,27,"# local mode does not have access to self-signed certs",MatchSource.CODE_COMMENT,hail/python/hailtop/config/deploy_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/deploy_config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/user_config.py:25,Modifiability,config,config,25,"# in older versions, the config file was accidentally named; # config.yaml, if the new config does not exist, and the old; # one does, silently rename it",MatchSource.CODE_COMMENT,hail/python/hailtop/config/user_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/user_config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/user_config.py:63,Modifiability,config,config,63,"# in older versions, the config file was accidentally named; # config.yaml, if the new config does not exist, and the old; # one does, silently rename it",MatchSource.CODE_COMMENT,hail/python/hailtop/config/user_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/user_config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/user_config.py:87,Modifiability,config,config,87,"# in older versions, the config file was accidentally named; # config.yaml, if the new config does not exist, and the old; # one does, silently rename it",MatchSource.CODE_COMMENT,hail/python/hailtop/config/user_config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/config/user_config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py:1740,Availability,error,error,1740,"d Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py:2003,Availability,error,error,2003,"d Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py:822,Security,access,access,822,"""""""Open a file from the local filesystem of from blob storage. Supported; blob storage providers are GCS, S3 and ABS. Examples; --------; Write a Pandas DataFrame as a CSV directly into Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/df.csv', 'w') as f: # doctest: +SKIP; ... pandas_df.to_csv(f). Read and print the lines of a text file stored in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws a",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py:2248,Security,access,access,2248,"d Storage:. >>> with hfs.open('gs://my-bucket/notes.txt') as f: # doctest: +SKIP; ... for line in f:; ... print(line.strip()). Access a text file stored in a Requester Pays Bucket in Google Cloud Storage:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config='my-project'; ... ) as f:; ... for line in f:; ... print(line.strip()). Specify multiple Requester Pays Buckets within a project that are acceptable; to access:. >>> with hfs.open( # doctest: +SKIP; ... 'gs://my-bucket/notes.txt',; ... requester_pays_config=('my-project', ['my-bucket', 'bucket-2']); ... ) as f:; ... for line in f:; ... print(line.strip()). Write two lines directly to a file in Google Cloud Storage:. >>> with hfs.open('gs://my-bucket/notes.txt', 'w') as f: # doctest: +SKIP; ... f.write('result1: %s\\n' % result1); ... f.write('result2: %s\\n' % result2). Unpack a packed Python struct directly from a file in Google Cloud Storage:. >>> from struct import unpack; >>> with hfs.open('gs://my-bucket/notes.txt', 'rb') as f: # doctest: +SKIP; ... print(unpack('<f', bytearray(f.read()))). Notes; -----; The supported modes are:. - ``'r'`` -- Readable text file (:class:`io.TextIOWrapper`). Default behavior.; - ``'w'`` -- Writable text file (:class:`io.TextIOWrapper`).; - ``'x'`` -- Exclusive writable text file (:class:`io.TextIOWrapper`).; Throws an error if a file already exists at the path.; - ``'rb'`` -- Readable binary file (:class:`io.BufferedReader`).; - ``'wb'`` -- Writable binary file (:class:`io.BufferedWriter`).; - ``'xb'`` -- Exclusive writable binary file (:class:`io.BufferedWriter`).; Throws an error if a file already exists at the path. The provided destination file path must be a URI (uniform resource identifier); or a path on the local filesystem. Parameters; ----------; path : :class:`str`; Path to file.; mode : :class:`str`; File access mode.; buffer_size : :obj:`int`; Buffer size, in bytes. Returns; -------; Readable or writable file handle.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py:360,Performance,load,load,360,"""""""Copy a file between filesystems. Filesystems can be local filesystem; or the blob storage providers GCS, S3 and ABS. Examples; --------; Copy a file from Google Cloud Storage to a local file:. >>> hfs.copy('gs://hail-common/LCR.interval_list',; ... 'file:///mnt/data/LCR.interval_list') # doctest: +SKIP. Notes; ----. If you are copying a file just to then load it into Python, you can use; :func:`.open` instead. For example:. >>> with hfs.open('gs://my_bucket/results.csv', 'r') as f: #doctest: +SKIP; ... df = pandas_df.read_csv(f). The provided source and destination file paths must be URIs; (uniform resource identifiers) or local filesystem paths. Parameters; ----------; src: :class:`str`; Source file URI.; dest: :class:`str`; Destination file URI.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py:92,Availability,error,error,92,"""""""Returns information about the file or directory at a given path. Notes; -----; Raises an error if `path` does not exist. The resulting dictionary contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`dict`; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py:70,Availability,error,error,70,"""""""Returns information about files at `path`. Notes; -----; Raises an error if `path` does not exist. If `path` is a file, returns a list with one element. If `path` is a; directory, returns an element for each file contained in `path` (does not; search recursively). Each dict element of the result list contains the following data:. - is_dir (:obj:`bool`) -- Path is a directory.; - size_bytes (:obj:`int`) -- Size in bytes.; - size (:class:`str`) -- Size as a readable string.; - modification_time (:class:`str`) -- Time of last file modification.; - owner (:class:`str`) -- Owner.; - path (:class:`str`) -- Path. Parameters; ----------; path : :class:`str`. Returns; -------; :obj:`list` [:obj:`dict`]; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/fs_utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/fs_utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/router_fs.py:74,Availability,error,error,74,"# Unless we are using a glob pattern, a path referring to no files should error",MatchSource.CODE_COMMENT,hail/python/hailtop/fs/router_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/fs/router_fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/__main__.py:9,Security,authenticat,authenticated,9,"""""""Issue authenticated curl requests to Hail infrastructure.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/auth/cli.py:55,Testability,log,login,55,"""""""; Create a new Hail user with username USERNAME and login ID LOGIN_ID.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/auth/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/auth/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/auth/login.py:19,Testability,log,logged,19,"# Confirm that the logged in user is registered with the hail service",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/auth/login.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/auth/login.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/batch/cli.py:11,Testability,log,log,11,"""""""Get the log for the job with id JOB_ID in the batch with id BATCH_ID.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/batch/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/batch/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:62,Deployability,configurat,configuration,62,"""""""; Parameters must contain at most one slash separating the configuration section; from the configuration parameter, for example: ""batch/billing_project"". Parameters may also have no slashes, indicating the parameter is a global; parameter, for example: ""domain"". A parameter with more than one slash is invalid, for example:; ""batch/billing/project"".; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:94,Deployability,configurat,configuration,94,"""""""; Parameters must contain at most one slash separating the configuration section; from the configuration parameter, for example: ""batch/billing_project"". Parameters may also have no slashes, indicating the parameter is a global; parameter, for example: ""domain"". A parameter with more than one slash is invalid, for example:; ""batch/billing/project"".; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:62,Modifiability,config,configuration,62,"""""""; Parameters must contain at most one slash separating the configuration section; from the configuration parameter, for example: ""batch/billing_project"". Parameters may also have no slashes, indicating the parameter is a global; parameter, for example: ""domain"". A parameter with more than one slash is invalid, for example:; ""batch/billing/project"".; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:94,Modifiability,config,configuration,94,"""""""; Parameters must contain at most one slash separating the configuration section; from the configuration parameter, for example: ""batch/billing_project"". Parameters may also have no slashes, indicating the parameter is a global; parameter, for example: ""domain"". A parameter with more than one slash is invalid, for example:; ""batch/billing/project"".; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:14,Deployability,configurat,configuration,14,"""""""Set a Hail configuration parameter.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:14,Modifiability,config,configuration,14,"""""""Set a Hail configuration parameter.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:16,Deployability,configurat,configuration,16,"""""""Unset a Hail configuration parameter (restore to default behavior).""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:16,Modifiability,config,configuration,16,"""""""Unset a Hail configuration parameter (restore to default behavior).""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:27,Deployability,configurat,configuration,27,"""""""Get the value of a Hail configuration parameter.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:27,Modifiability,config,configuration,27,"""""""Get the value of a Hail configuration parameter.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:29,Modifiability,config,config,29,"""""""Print the location of the config file.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:15,Modifiability,config,config,15,"""""""Lists every config variable in the section.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py:22,Modifiability,variab,variable,22,"""""""Lists every config variable in the section.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/config/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/config/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/cli.py:30,Modifiability,config,configured,30,"""""""; Start a Dataproc cluster configured for Hail.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/cli.py:10,Availability,down,down,10,"""""""; Shut down a Dataproc cluster.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/connect.py:31,Deployability,configurat,configuration,31,"# open Chrome with SOCKS proxy configuration",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/connect.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/connect.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/connect.py:31,Modifiability,config,configuration,31,"# open Chrome with SOCKS proxy configuration",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/connect.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/connect.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/gcloud.py:16,Deployability,configurat,configuration,16,"""""""Get a gcloud configuration value.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/gcloud.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/gcloud.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/gcloud.py:16,Modifiability,config,configuration,16,"""""""Get a gcloud configuration value.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/gcloud.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/gcloud.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py:49,Energy Efficiency,charge,charges,49,"# VEP is too expensive if you have to pay egress charges.",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py:45,Modifiability,variab,variable,45,"# if Python packages requested, add metadata variable",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py:133,Testability,log,log,133,"# 1. GCE only provides 51 GiB for an n1-highmem-8 (advertised as 52 GiB); # 2. System daemons use ~10 GiB based on syslog ""earlyoom"" log statements during VM startup",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py:229,Energy Efficiency,schedul,scheduler,229,"# A Google support engineer recommended the strategy of passing the YARN; # config params, and the default value of 95% of machine memory to give to YARN.; # yarn.nodemanager.resource.memory-mb - total memory per machine; # yarn.scheduler.maximum-allocation-mb - max memory to allocate to each container",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py:277,Energy Efficiency,allocate,allocate,277,"# A Google support engineer recommended the strategy of passing the YARN; # config params, and the default value of 95% of machine memory to give to YARN.; # yarn.nodemanager.resource.memory-mb - total memory per machine; # yarn.scheduler.maximum-allocation-mb - max memory to allocate to each container",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py:76,Modifiability,config,config,76,"# A Google support engineer recommended the strategy of passing the YARN; # config params, and the default value of 95% of machine memory to give to YARN.; # yarn.nodemanager.resource.memory-mb - total memory per machine; # yarn.scheduler.maximum-allocation-mb - max memory to allocate to each container",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py:2,Modifiability,rewrite,rewrite,2,"# rewrite metadata and properties to escape them",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/start.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/utils.py:23,Availability,error,error,23,"# only print output on error",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py:25,Deployability,install,install,25,"# additional packages to install",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py:82,Energy Efficiency,monitor,monitor,82,"# Update python3 kernel spec with the environment variables and the hail; # spark monitor",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py:50,Modifiability,variab,variables,50,"# Update python3 kernel spec with the environment variables and the hail; # spark monitor",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py:17,Deployability,configurat,configuration,17,"# create Jupyter configuration file",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py:17,Modifiability,config,configuration,17,"# create Jupyter configuration file",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py:29,Availability,echo,echo,29,"""""""ipython profile create && echo ""c.InteractiveShellApp.extensions.append('sparkmonitor.kernelextension')"" >> $(ipython profile locate default)/ipython_kernel_config.py""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dataproc/resources/init_notebook.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dev/config.py:11,Modifiability,config,config,11,"""""""Set dev config property PROPERTY to value VALUE.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dev/config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dev/config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dev/config.py:32,Modifiability,config,config,32,"""""""List the settings in the dev config.""""""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/dev/config.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/dev/config.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py:32,Modifiability,config,configured,32,"""""""; Start an HDInsight cluster configured for Hail.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py:31,Modifiability,config,configured,31,"""""""; Stop an HDInsight cluster configured for Hail.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py:42,Modifiability,config,configured,42,"""""""; Submit a job to an HDInsight cluster configured for Hail. If you wish to pass option-like arguments you should use ""--"". For example:. $ hailctl hdinsight submit name account password script.py --image-name docker.io/image my_script.py -- some-argument --animal dog; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py:180,Security,password,password,180,"""""""; Submit a job to an HDInsight cluster configured for Hail. If you wish to pass option-like arguments you should use ""--"". For example:. $ hailctl hdinsight submit name account password script.py --image-name docker.io/image my_script.py -- some-argument --animal dog; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py:29,Modifiability,config,configured,29,"""""""; List HDInsight clusters configured for Hail.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/cli.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/cli.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/start.py:186,Availability,error,error,186,"# I figured this out after looking at; # https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-manage-ambari-rest-api#restart-a-service-component; # and doing some trial and error",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/start.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/start.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/submit.py:21,Integrability,protocol,protocol,21,"# NB: Only the local protocol is permitted, the file protocol is banned #security",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/submit.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/submit.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/submit.py:53,Integrability,protocol,protocol,53,"# NB: Only the local protocol is permitted, the file protocol is banned #security",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/submit.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/submit.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/submit.py:73,Security,secur,security,73,"# NB: Only the local protocol is permitted, the file protocol is banned #security",MatchSource.CODE_COMMENT,hail/python/hailtop/hailctl/hdinsight/submit.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/hailctl/hdinsight/submit.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/filesize.py:124,Availability,down,down,124,"""""""; Renders the given number as a human-readable filesize using binary multiple-byte units (e.g. ""MiB"", not ""MB"").; Rounds down to the nearest integer value and multiple-byte unit (e.g. ``filesize((1024**3) - 1)`` returns; ``""1023MiB""``, not ``""1GiB""`` or ``""1023.9999990463257MiB""``).; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/filesize.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/filesize.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:25,Availability,error,error,25,"# pylint: disable=import-error",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:33,Testability,test,test,33,"# pylint: disable=using-constant-test",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:24,Testability,log,log,24,"# 22 characters is math.log(62 ** 22, 2) == ~130 bits of randomness. OWASP; # recommends at least 128 bits:; # https://owasp.org/www-community/vulnerabilities/Insufficient_Session-ID_Length",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:552,Availability,down,down,552,"""""""`OnlineBoundedGather2` provides the capability to run background; tasks with bounded parallelism. It is a context manager, and; waits for all background tasks to complete on exit. `OnlineBoundedGather2` supports cancellation of background tasks.; When a background task raises `asyncio.CancelledError`, the task; is considered complete and the pool and other background tasks; continue runnning. If a background task fails (raises an exception besides; `asyncio.CancelledError`), all running background tasks are; cancelled and the the pool is shut down. Subsequent calls to; `OnlineBoundedGather2.call()` raise `PoolShutdownError`. Because the pool runs tasks in the background, multiple exceptions; can occur simultaneously. The first exception raised, whether by; a background task or into the context manager exit, is raised by; the context manager exit, and any further exceptions are logged; and otherwise discarded.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:893,Testability,log,logged,893,"""""""`OnlineBoundedGather2` provides the capability to run background; tasks with bounded parallelism. It is a context manager, and; waits for all background tasks to complete on exit. `OnlineBoundedGather2` supports cancellation of background tasks.; When a background task raises `asyncio.CancelledError`, the task; is considered complete and the pool and other background tasks; continue runnning. If a background task fails (raises an exception besides; `asyncio.CancelledError`), all running background tasks are; cancelled and the the pool is shut down. Subsequent calls to; `OnlineBoundedGather2.call()` raise `PoolShutdownError`. Because the pool runs tasks in the background, multiple exceptions; can occur simultaneously. The first exception raised, whether by; a background task or into the context manager exit, is raised by; the context manager exit, and any further exceptions are logged; and otherwise discarded.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:8,Availability,down,down,8,"""""""Shut down the pool. Cancel all pending tasks and wait for them to complete.; Subsequent calls to call will raise `PoolShutdownError`.; """"""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:7,Availability,down,down,7,"# shut down the pending tasks",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:113,Availability,error,error,113,"# nginx returns 502 if it cannot connect to the upstream server; #; # 408 request timeout; # 500 internal server error; # 502 bad gateway; # 503 service unavailable; # 504 gateway timeout; # 429 ""Temporarily throttled, too many requests""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:208,Performance,throttle,throttled,208,"# nginx returns 502 if it cannot connect to the upstream server; #; # 408 request timeout; # 500 internal server error; # 502 bad gateway; # 503 service unavailable; # 504 gateway timeout; # 429 ""Temporarily throttled, too many requests""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:82,Safety,timeout,timeout,82,"# nginx returns 502 if it cannot connect to the upstream server; #; # 408 request timeout; # 500 internal server error; # 502 bad gateway; # 503 service unavailable; # 504 gateway timeout; # 429 ""Temporarily throttled, too many requests""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:180,Safety,timeout,timeout,180,"# nginx returns 502 if it cannot connect to the upstream server; #; # 408 request timeout; # 500 internal server error; # 502 bad gateway; # 503 service unavailable; # 504 gateway timeout; # 429 ""Temporarily throttled, too many requests""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:32,Availability,error,error,32,"# An exception is a ""retry once error"" if a rare, known bug in a dependency or in a cloud; # provider can manifest as this exception *and* that manifestation is indistinguishable from a; # true error.",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:194,Availability,error,error,194,"# An exception is a ""retry once error"" if a rare, known bug in a dependency or in a cloud; # provider can manifest as this exception *and* that manifestation is indistinguishable from a; # true error.",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:65,Integrability,depend,dependency,65,"# An exception is a ""retry once error"" if a rare, known bug in a dependency or in a cloud; # provider can manifest as this exception *and* that manifestation is indistinguishable from a; # true error.",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:431,Availability,error,error,431,"# observed exceptions:; #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host <host> ssl:None [Connect call failed ('<ip>', 80)]; #; # concurrent.futures._base.TimeoutError; # from aiohttp/helpers.py:585:TimerContext: raise asyncio.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; #; # Traceback (most recent call last):; # ...; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 505, in _request; # await resp.start(conn); # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 848, in start; # message, payload = await self._protocol.read() # type: ignore; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py"", line 592, in read; # await self._waiter; # aiohttp.client_exceptions.ServerDisconnectedError: None; #; # during aiohttp request; # aiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer; #; # urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); #; # requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # socket.timeout: The read operation timed out; #; # ConnectionResetError: [Errno 104] Connection reset by peer; #; # google.auth.exceptions.TransportError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # aiohttp.client_exceptions.ClientConnectorError: Cannot conne",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:400,Integrability,rout,route,400,"# observed exceptions:; #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host <host> ssl:None [Connect call failed ('<ip>', 80)]; #; # concurrent.futures._base.TimeoutError; # from aiohttp/helpers.py:585:TimerContext: raise asyncio.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; #; # Traceback (most recent call last):; # ...; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 505, in _request; # await resp.start(conn); # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 848, in start; # message, payload = await self._protocol.read() # type: ignore; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py"", line 592, in read; # await self._waiter; # aiohttp.client_exceptions.ServerDisconnectedError: None; #; # during aiohttp request; # aiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer; #; # urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); #; # requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # socket.timeout: The read operation timed out; #; # ConnectionResetError: [Errno 104] Connection reset by peer; #; # google.auth.exceptions.TransportError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # aiohttp.client_exceptions.ClientConnectorError: Cannot conne",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:460,Integrability,protocol,protocol,460,"# observed exceptions:; #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host <host> ssl:None [Connect call failed ('<ip>', 80)]; #; # concurrent.futures._base.TimeoutError; # from aiohttp/helpers.py:585:TimerContext: raise asyncio.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; #; # Traceback (most recent call last):; # ...; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 505, in _request; # await resp.start(conn); # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 848, in start; # message, payload = await self._protocol.read() # type: ignore; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py"", line 592, in read; # await self._waiter; # aiohttp.client_exceptions.ServerDisconnectedError: None; #; # during aiohttp request; # aiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer; #; # urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); #; # requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # socket.timeout: The read operation timed out; #; # ConnectionResetError: [Errno 104] Connection reset by peer; #; # google.auth.exceptions.TransportError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # aiohttp.client_exceptions.ClientConnectorError: Cannot conne",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:1098,Integrability,message,message,1098," <host> ssl:None [Connect call failed ('<ip>', 80)]; #; # concurrent.futures._base.TimeoutError; # from aiohttp/helpers.py:585:TimerContext: raise asyncio.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; #; # Traceback (most recent call last):; # ...; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 505, in _request; # await resp.start(conn); # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 848, in start; # message, payload = await self._protocol.read() # type: ignore; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py"", line 592, in read; # await self._waiter; # aiohttp.client_exceptions.ServerDisconnectedError: None; #; # during aiohttp request; # aiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer; #; # urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); #; # requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # socket.timeout: The read operation timed out; #; # ConnectionResetError: [Errno 104] Connection reset by peer; #; # google.auth.exceptions.TransportError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host batch.pr-6925-default-s24o4bgat8e8:80 ssl:None [Connect call failed ('10.36.7.86', 80)];",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:157,Performance,concurren,concurrent,157,"# observed exceptions:; #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host <host> ssl:None [Connect call failed ('<ip>', 80)]; #; # concurrent.futures._base.TimeoutError; # from aiohttp/helpers.py:585:TimerContext: raise asyncio.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; #; # Traceback (most recent call last):; # ...; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 505, in _request; # await resp.start(conn); # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 848, in start; # message, payload = await self._protocol.read() # type: ignore; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py"", line 592, in read; # await self._waiter; # aiohttp.client_exceptions.ServerDisconnectedError: None; #; # during aiohttp request; # aiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer; #; # urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); #; # requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # socket.timeout: The read operation timed out; #; # ConnectionResetError: [Errno 104] Connection reset by peer; #; # google.auth.exceptions.TransportError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # aiohttp.client_exceptions.ClientConnectorError: Cannot conne",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:1561,Safety,timeout,timeout,1561,"o.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; #; # Traceback (most recent call last):; # ...; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 505, in _request; # await resp.start(conn); # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 848, in start; # message, payload = await self._protocol.read() # type: ignore; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py"", line 592, in read; # await self._waiter; # aiohttp.client_exceptions.ServerDisconnectedError: None; #; # during aiohttp request; # aiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer; #; # urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); #; # requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # socket.timeout: The read operation timed out; #; # ConnectionResetError: [Errno 104] Connection reset by peer; #; # google.auth.exceptions.TransportError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host batch.pr-6925-default-s24o4bgat8e8:80 ssl:None [Connect call failed ('10.36.7.86', 80)]; #; # OSError: [Errno 51] Connect call failed ('35.188.91.25', 443); # https://hail.zulipchat.com/#narrow/stream/223457-Batch-support/topic/ssl.20error",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:1629,Safety,abort,aborted,1629,"o.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; #; # Traceback (most recent call last):; # ...; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 505, in _request; # await resp.start(conn); # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 848, in start; # message, payload = await self._protocol.read() # type: ignore; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py"", line 592, in read; # await self._waiter; # aiohttp.client_exceptions.ServerDisconnectedError: None; #; # during aiohttp request; # aiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer; #; # urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); #; # requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # socket.timeout: The read operation timed out; #; # ConnectionResetError: [Errno 104] Connection reset by peer; #; # google.auth.exceptions.TransportError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host batch.pr-6925-default-s24o4bgat8e8:80 ssl:None [Connect call failed ('10.36.7.86', 80)]; #; # OSError: [Errno 51] Connect call failed ('35.188.91.25', 443); # https://hail.zulipchat.com/#narrow/stream/223457-Batch-support/topic/ssl.20error",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:1708,Safety,timeout,timeout,1708,"o.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; #; # Traceback (most recent call last):; # ...; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 505, in _request; # await resp.start(conn); # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 848, in start; # message, payload = await self._protocol.read() # type: ignore; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py"", line 592, in read; # await self._waiter; # aiohttp.client_exceptions.ServerDisconnectedError: None; #; # during aiohttp request; # aiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer; #; # urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); #; # requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # socket.timeout: The read operation timed out; #; # ConnectionResetError: [Errno 104] Connection reset by peer; #; # google.auth.exceptions.TransportError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host batch.pr-6925-default-s24o4bgat8e8:80 ssl:None [Connect call failed ('10.36.7.86', 80)]; #; # OSError: [Errno 51] Connect call failed ('35.188.91.25', 443); # https://hail.zulipchat.com/#narrow/stream/223457-Batch-support/topic/ssl.20error",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:1869,Safety,abort,aborted,1869,"o.TimeoutError from None; #; # Connected call failed caused by:; # OSError: [Errno 113] Connect call failed ('<ip>', 80); # 113 is EHOSTUNREACH: No route to host; #; # Fatal read error on socket transport; # protocol: <asyncio.sslproto.SSLProtocol object at 0x12b47d320>; # transport: <_SelectorSocketTransport fd=13 read=polling write=<idle, bufsize=0>>; # Traceback (most recent call last):; # File ""/anaconda3/lib/python3.7/asyncio/selector_events.py"", line 812, in _read_ready__data_received; # data = self._sock.recv(self.max_size); # TimeoutError: [Errno 60] Operation timed out; #; # Traceback (most recent call last):; # ...; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 505, in _request; # await resp.start(conn); # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 848, in start; # message, payload = await self._protocol.read() # type: ignore; # File ""/usr/local/lib/python3.6/dist-packages/aiohttp/streams.py"", line 592, in read; # await self._waiter; # aiohttp.client_exceptions.ServerDisconnectedError: None; #; # during aiohttp request; # aiohttp.client_exceptions.ClientOSError: [Errno 104] Connection reset by peer; #; # urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='www.googleapis.com', port=443): Read timed out. (read timeout=60); #; # requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # socket.timeout: The read operation timed out; #; # ConnectionResetError: [Errno 104] Connection reset by peer; #; # google.auth.exceptions.TransportError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')); #; # aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host batch.pr-6925-default-s24o4bgat8e8:80 ssl:None [Connect call failed ('10.36.7.86', 80)]; #; # OSError: [Errno 51] Connect call failed ('35.188.91.25', 443); # https://hail.zulipchat.com/#narrow/stream/223457-Batch-support/topic/ssl.20error",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:41,Availability,failure,failure,41,"# socket.EAI_AGAIN: [Errno -3] Temporary failure in name resolution; # socket.EAI_NONAME: [Errno 8] nodename nor servname provided, or not known",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:46,Performance,throttle,throttled,46,"# 503 service unavailable; # 429 ""Temporarily throttled, too many requests""",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:31,Safety,avoid,avoid,31,"# do not set larger than 30 to avoid BigInt arithmetic",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:101,Energy Efficiency,reduce,reduce,101,"# 0.5 is arbitrary, but should be short enough not to greatly; # increase latency and long enough to reduce the impact of; # wasteful spinning when `should_wait` is always true and the; # event is constantly being set. This was instated to; # avoid wasteful repetition of scheduling loops, but; # might not always be desirable, especially in very low-latency batches.",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:272,Energy Efficiency,schedul,scheduling,272,"# 0.5 is arbitrary, but should be short enough not to greatly; # increase latency and long enough to reduce the impact of; # wasteful spinning when `should_wait` is always true and the; # event is constantly being set. This was instated to; # avoid wasteful repetition of scheduling loops, but; # might not always be desirable, especially in very low-latency batches.",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:74,Performance,latency,latency,74,"# 0.5 is arbitrary, but should be short enough not to greatly; # increase latency and long enough to reduce the impact of; # wasteful spinning when `should_wait` is always true and the; # event is constantly being set. This was instated to; # avoid wasteful repetition of scheduling loops, but; # might not always be desirable, especially in very low-latency batches.",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:351,Performance,latency,latency,351,"# 0.5 is arbitrary, but should be short enough not to greatly; # increase latency and long enough to reduce the impact of; # wasteful spinning when `should_wait` is always true and the; # event is constantly being set. This was instated to; # avoid wasteful repetition of scheduling loops, but; # might not always be desirable, especially in very low-latency batches.",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py:243,Safety,avoid,avoid,243,"# 0.5 is arbitrary, but should be short enough not to greatly; # increase latency and long enough to reduce the impact of; # wasteful spinning when `should_wait` is always true and the; # event is constantly being set. This was instated to; # avoid wasteful repetition of scheduling loops, but; # might not always be desirable, especially in very low-latency batches.",MatchSource.CODE_COMMENT,hail/python/hailtop/utils/utils.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/hailtop/utils/utils.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/conftest.py:93,Availability,avail,available-in-fixtures,93,"# from: https://docs.pytest.org/en/latest/example/simple.html#making-test-result-information-available-in-fixtures",MatchSource.CODE_COMMENT,hail/python/test/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/conftest.py:69,Testability,test,test-result-information-available-in-fixtures,69,"# from: https://docs.pytest.org/en/latest/example/simple.html#making-test-result-information-available-in-fixtures",MatchSource.CODE_COMMENT,hail/python/test/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/conftest.py:50,Usability,simpl,simple,50,"# from: https://docs.pytest.org/en/latest/example/simple.html#making-test-result-information-available-in-fixtures",MatchSource.CODE_COMMENT,hail/python/test/hail/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_call_caching.py:33,Performance,cache,cache,33,"""""""Asserts creation of execution cache folder""""""",MatchSource.CODE_COMMENT,hail/python/test/hail/test_call_caching.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_call_caching.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_context.py:15,Availability,error,error,15,"# Should be no error",MatchSource.CODE_COMMENT,hail/python/test/hail/test_context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_context.py:42,Availability,error,error,42,"# ensure functions are cleaned up without error",MatchSource.CODE_COMMENT,hail/python/test/hail/test_context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_context.py:15,Availability,error,error,15,"# Should be no error",MatchSource.CODE_COMMENT,hail/python/test/hail/test_context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_context.py:15,Availability,error,error,15,"# Should be no error",MatchSource.CODE_COMMENT,hail/python/test/hail/test_context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_ir.py:21,Availability,error,errors,21,"# TODO: Scala Pretty errors out with a StackOverflowError here; # ht._force_count()",MatchSource.CODE_COMMENT,hail/python/test/hail/test_ir.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_ir.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_no_context.py:15,Availability,error,error,15,"# Should be no error",MatchSource.CODE_COMMENT,hail/python/test/hail/test_no_context.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/test_no_context.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py:193,Deployability,pipeline,pipelines,193,"# DSP's variants team depends on these functions which are in experimental. Normally we do not; # guarantee backwards compatibility but given the practical importance of these to production; # pipelines at Broad, we ensure they continue to exist.",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py:22,Integrability,depend,depends,22,"# DSP's variants team depends on these functions which are in experimental. Normally we do not; # guarantee backwards compatibility but given the practical importance of these to production; # pipelines at Broad, we ensure they continue to exist.",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_dsp_necessary_functions_have_not_moved.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_experimental.py:2,Deployability,integrat,integration,2,"# integration test pulled out of test_ld_score_regression to isolate issues with lowered shuffles; # and RDD serialization, 2021-07-06; # if this comment no longer reflects the backend system, that's a really good thing",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_experimental.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_experimental.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_experimental.py:2,Integrability,integrat,integration,2,"# integration test pulled out of test_ld_score_regression to isolate issues with lowered shuffles; # and RDD serialization, 2021-07-06; # if this comment no longer reflects the backend system, that's a really good thing",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_experimental.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_experimental.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_experimental.py:14,Testability,test,test,14,"# integration test pulled out of test_ld_score_regression to isolate issues with lowered shuffles; # and RDD serialization, 2021-07-06; # if this comment no longer reflects the backend system, that's a really good thing",MatchSource.CODE_COMMENT,hail/python/test/hail/experimental/test_experimental.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/experimental/test_experimental.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:37,Testability,test,test,37,"# Tests MatrixMapCols initOp; # must test that call_stats isn't null, because equality doesn't test for that",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:95,Testability,test,test,95,"# Tests MatrixMapCols initOp; # must test that call_stats isn't null, because equality doesn't test for that",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:52,Security,hash,hashable,52,"# Test that it's evalable, since python sets aren't hashable.",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:53,Security,hash,hashable,53,"# Test that it's evalable, since python dicts aren't hashable.",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:10,Testability,test,test,10,"# need to test laziness, so we will overwrite a file",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test NA",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test If",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test StreamIota",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test ToArray",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test ToStream",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test StreamZip",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test StreamFilter",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test StreamFilter",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test StreamFold",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test StreamScan",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test StreamAgg",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test AggExplode",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test TableCount",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test TableGetGlobals",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test TableCollect",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test TableAggregate",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test MatrixCount",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py:2,Testability,test,test,2,"# test MatrixAggregate",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_expr.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_expr.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py:98,Security,hash,hashable,98,"# NB: We never return dict, only frozendict, so we assert that. However, dict *values* must be; # hashable if and only if the frozendict must be hashable. In this case, the frozendict *need; # not* be hashable because its inside a list. As a result, the value *should not* be frozen; # (i.e. hashable). This preserves backwards compatibility for users who expect normal Python; # lists when possible.",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_freezing.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py:145,Security,hash,hashable,145,"# NB: We never return dict, only frozendict, so we assert that. However, dict *values* must be; # hashable if and only if the frozendict must be hashable. In this case, the frozendict *need; # not* be hashable because its inside a list. As a result, the value *should not* be frozen; # (i.e. hashable). This preserves backwards compatibility for users who expect normal Python; # lists when possible.",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_freezing.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py:201,Security,hash,hashable,201,"# NB: We never return dict, only frozendict, so we assert that. However, dict *values* must be; # hashable if and only if the frozendict must be hashable. In this case, the frozendict *need; # not* be hashable because its inside a list. As a result, the value *should not* be frozen; # (i.e. hashable). This preserves backwards compatibility for users who expect normal Python; # lists when possible.",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_freezing.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py:292,Security,hash,hashable,292,"# NB: We never return dict, only frozendict, so we assert that. However, dict *values* must be; # hashable if and only if the frozendict must be hashable. In this case, the frozendict *need; # not* be hashable because its inside a list. As a result, the value *should not* be frozen; # (i.e. hashable). This preserves backwards compatibility for users who expect normal Python; # lists when possible.",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_freezing.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py:51,Testability,assert,assert,51,"# NB: We never return dict, only frozendict, so we assert that. However, dict *values* must be; # hashable if and only if the frozendict must be hashable. In this case, the frozendict *need; # not* be hashable because its inside a list. As a result, the value *should not* be frozen; # (i.e. hashable). This preserves backwards compatibility for users who expect normal Python; # lists when possible.",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_freezing.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_freezing.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_ndarrays.py:167,Testability,assert,assert,167,"# NDArrays don't correctly support elements that contain pointers at the moment.; # s = hl.nd.array([""hail"", ""is"", ""great""]); # s_lens = s.map(lambda e: hl.len(e)); # assert np.array_equal(hl.eval(s_lens), np.array([4, 2, 5]))",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_ndarrays.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_ndarrays.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_ndarrays.py:14,Testability,test,tests,14,"# Missingness tests",MatchSource.CODE_COMMENT,hail/python/test/hail/expr/test_ndarrays.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/expr/test_ndarrays.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/fs/test_worker_driver_fs.py:11,Testability,test,test,11,"# NB: this test uses a file with more rows than partitions because Hadoop's Seekable input; # streams do not permit seeking past the end of the input (ref:; # https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/fs/Seekable.html#seek-long-).; #; # Hail assumes that seeking past the end of the input does not raise an EOFException (see, for; # example `skip` in java.io.FileInputStream:; # https://docs.oracle.com/javase/7/docs/api/java/io/FileInputStream.html)",MatchSource.CODE_COMMENT,hail/python/test/hail/fs/test_worker_driver_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/fs/test_worker_driver_fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/genetics/test_reference_genome.py:7,Testability,test,test,7,"# this test doesn't behave properly if these reference genomes are already defined in scope.",MatchSource.CODE_COMMENT,hail/python/test/hail/genetics/test_reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/genetics/test_reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/genetics/test_reference_genome.py:2,Performance,load,loading,2,"# loading different reference genome with same name should fail; # (different `test_rg_o` definition)",MatchSource.CODE_COMMENT,hail/python/test/hail/genetics/test_reference_genome.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/genetics/test_reference_genome.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py:31,Usability,simpl,simple,31,"# BlockMatrixMap requires very simple IRs on the SparkBackend. If I use; # `from_ndarray` here, it generates an `NDArrayRef` expression that it can't handle.; # Will be fixed by improving FoldConstants handling of ndarrays or fully lowering BlockMatrix.",MatchSource.CODE_COMMENT,hail/python/test/hail/linalg/test_linalg.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py:78,Testability,assert,assertTrue,78,"# FIXME doesn't work in service, if test_is_sparse works, uncomment below; # .assertTrue(bm.is_sparse)",MatchSource.CODE_COMMENT,hail/python/test/hail/linalg/test_linalg.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py:77,Testability,assert,assert,77,"# FIXME doesn't work in service, if test_is_sparse works, uncomment below; # assert not bm.is_sparse; # assert bm.sparsify_triangle().is_sparse",MatchSource.CODE_COMMENT,hail/python/test/hail/linalg/test_linalg.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py:104,Testability,assert,assert,104,"# FIXME doesn't work in service, if test_is_sparse works, uncomment below; # assert not bm.is_sparse; # assert bm.sparsify_triangle().is_sparse",MatchSource.CODE_COMMENT,hail/python/test/hail/linalg/test_linalg.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py:21,Availability,error,error,21,"# should run without error",MatchSource.CODE_COMMENT,hail/python/test/hail/linalg/test_linalg.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/linalg/test_linalg.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_file_formats.py:72,Testability,test,tests,72,"# pytest sometimes uses background threads, named ""Dummy-1"", to collect tests. Asyncio dislikes; # automatically creating event loops in these threads, so we just explicitly create one.",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_file_formats.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_file_formats.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_grouped_matrix_table.py:2,Testability,test,tests,2,"# tests fixed indices",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_grouped_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_grouped_matrix_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_grouped_matrix_table.py:2,Testability,test,tests,2,"# tests fixed indices",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_grouped_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_grouped_matrix_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py:21,Availability,error,error,21,"# should run without error",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py:2,Testability,test,test,2,"# test different row schemas",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/matrixtable/test_matrix_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/matrixtable/test_matrix_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py:15,Integrability,depend,depends,15,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py:10,Testability,test,test,10,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py:98,Testability,test,test,98,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py:118,Testability,test,test,118,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py:15,Integrability,depend,depends,15,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py:10,Testability,test,test,10,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py:98,Testability,test,test,98,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py:118,Testability,test,test,118,"""""""; This test depends on certain properties of the trio matrix VCF and; pedigree structure. This test is NOT a valid test if the pedigree; includes quads: the trio_matrix method will duplicate the parents; appropriately, but the genotypes_table and samples_table orthogonal; paths would require another duplication/explode that we haven't written.; """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py:2,Testability,test,test,2,"# test annotations",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_family_methods.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_family_methods.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:44,Testability,assert,assertDictEqual,44,"# are py4 JavaMaps, not dicts, so can't use assertDictEqual",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:697,Availability,echo,echo,697,"# this routine was used to generate resources random.gen, random.sample; # random.bgen was generated with qctool v2.0rc9:; # qctool -g random.gen -s random.sample -bgen-bits 8 -og random.bgen; #; # random-a.bgen, random-b.bgen, random-c.bgen was generated as follows:; # head -n 10 random.gen > random-a.gen; head -n 20 random.gen | tail -n 10 > random-b.gen; tail -n 10 random.gen > random-c.gen; # qctool -g random-a.gen -s random.sample -og random-a.bgen -bgen-bits 8; # qctool -g random-b.gen -s random.sample -og random-b.bgen -bgen-bits 8; # qctool -g random-c.gen -s random.sample -og random-c.bgen -bgen-bits 8; #; # random-*-disjoint.bgen was generated as follows:; # while read line; do echo $RANDOM $line; done < src/test/resources/random.gen | sort -n | cut -f2- -d' ' > random-shuffled.gen; # head -n 10 random-shuffled.gen > random-a-disjoint.gen; head -n 20 random-shuffled.gen | tail -n 10 > random-b-disjoint.gen; tail -n 10 random-shuffled.gen > random-c-disjoint.gen; # qctool -g random-a-disjoint.gen -s random.sample -og random-a-disjoint.bgen -bgen-bits 8; # qctool -g random-b-disjoint.gen -s random.sample -og random-b-disjoint.bgen -bgen-bits 8; # qctool -g random-c-disjoint.gen -s random.sample -og random-c-disjoint.bgen -bgen-bits 8",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:7,Integrability,rout,routine,7,"# this routine was used to generate resources random.gen, random.sample; # random.bgen was generated with qctool v2.0rc9:; # qctool -g random.gen -s random.sample -bgen-bits 8 -og random.bgen; #; # random-a.bgen, random-b.bgen, random-c.bgen was generated as follows:; # head -n 10 random.gen > random-a.gen; head -n 20 random.gen | tail -n 10 > random-b.gen; tail -n 10 random.gen > random-c.gen; # qctool -g random-a.gen -s random.sample -og random-a.bgen -bgen-bits 8; # qctool -g random-b.gen -s random.sample -og random-b.bgen -bgen-bits 8; # qctool -g random-c.gen -s random.sample -og random-c.bgen -bgen-bits 8; #; # random-*-disjoint.bgen was generated as follows:; # while read line; do echo $RANDOM $line; done < src/test/resources/random.gen | sort -n | cut -f2- -d' ' > random-shuffled.gen; # head -n 10 random-shuffled.gen > random-a-disjoint.gen; head -n 20 random-shuffled.gen | tail -n 10 > random-b-disjoint.gen; tail -n 10 random-shuffled.gen > random-c-disjoint.gen; # qctool -g random-a-disjoint.gen -s random.sample -og random-a-disjoint.bgen -bgen-bits 8; # qctool -g random-b-disjoint.gen -s random.sample -og random-b-disjoint.bgen -bgen-bits 8; # qctool -g random-c-disjoint.gen -s random.sample -og random-c-disjoint.bgen -bgen-bits 8",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:728,Testability,test,test,728,"# this routine was used to generate resources random.gen, random.sample; # random.bgen was generated with qctool v2.0rc9:; # qctool -g random.gen -s random.sample -bgen-bits 8 -og random.bgen; #; # random-a.bgen, random-b.bgen, random-c.bgen was generated as follows:; # head -n 10 random.gen > random-a.gen; head -n 20 random.gen | tail -n 10 > random-b.gen; tail -n 10 random.gen > random-c.gen; # qctool -g random-a.gen -s random.sample -og random-a.bgen -bgen-bits 8; # qctool -g random-b.gen -s random.sample -og random-b.bgen -bgen-bits 8; # qctool -g random-c.gen -s random.sample -og random-c.bgen -bgen-bits 8; #; # random-*-disjoint.bgen was generated as follows:; # while read line; do echo $RANDOM $line; done < src/test/resources/random.gen | sort -n | cut -f2- -d' ' > random-shuffled.gen; # head -n 10 random-shuffled.gen > random-a-disjoint.gen; head -n 20 random-shuffled.gen | tail -n 10 > random-b-disjoint.gen; tail -n 10 random-shuffled.gen > random-c-disjoint.gen; # qctool -g random-a-disjoint.gen -s random.sample -og random-a-disjoint.bgen -bgen-bits 8; # qctool -g random-b-disjoint.gen -s random.sample -og random-b-disjoint.bgen -bgen-bits 8; # qctool -g random-c-disjoint.gen -s random.sample -og random-c-disjoint.bgen -bgen-bits 8",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:51,Testability,test,testing,51,"# forcing each variant to be its own partition for testing duplicates work properly",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:7,Testability,assert,assertEqual,7,"# self.assertEqual(everything.count(), (199, 500))",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:8,Testability,test,testing,8,"# FIXME testing block_size (in MB) requires large BGEN",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:1383,Energy Efficiency,adapt,adapted,1383,ream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); ,MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:1383,Modifiability,adapt,adapted,1383,ream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); ,MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:1941,Performance,concurren,concurrent,1941,"ct0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:2109,Performance,concurren,concurrent,2109,"ct0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:2173,Performance,concurren,concurrent,2173,"ct0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:2242,Performance,concurren,concurrent,2242,"ct0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:2318,Performance,concurren,concurrent,2318,"ct0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:2390,Performance,concurren,concurrent,2390,"ct0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py:2477,Performance,concurren,concurrent,2477,"ct0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174); E 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548); E 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509); E 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432); E 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178); E 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:119); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:118); E 	at is.hail.utils.package$.using(package.scala:638); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$2(ServiceBackend.scala:118); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:71); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$1(ServiceBackend.scala:117); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); E 	at scala.util.Success.$anonfun$map$1(Try.scala:255); E 	at scala.util.Success.map(Try.scala:213); E 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); E 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); E 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); E 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); """"""",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_impex.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_impex.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_misc.py:26,Safety,safe,safe,26,"# approximate, 1 place is safe",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_misc.py:26,Safety,safe,safe,26,"# approximate, 1 place is safe",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_misc.py:26,Safety,safe,safe,26,"# approximate, 1 place is safe",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_misc.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_misc.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py:38,Testability,test,test,38,"# dat <- as.matrix(read.csv('hail/src/test/resources/skat_genotype_matrix.csv', header=FALSE)); # cov = read.csv('hail/src/test/resources/skat_phenotypes.csv', header=FALSE); # cov$V1 = cov$V1 > 2; # weights <- rep(1, 100); # null_model <- SKAT_Null_Model(cov$V1 ~ 1, out_type=""D""); # result <- SKAT(dat, null_model, method=""davies"", weights=weights); #; # cat(result$p.value, result$Q); #; #; # SKAT expects rows to be samples, so we transpose from the original input",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_skat.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py:123,Testability,test,test,123,"# dat <- as.matrix(read.csv('hail/src/test/resources/skat_genotype_matrix.csv', header=FALSE)); # cov = read.csv('hail/src/test/resources/skat_phenotypes.csv', header=FALSE); # cov$V1 = cov$V1 > 2; # weights <- rep(1, 100); # null_model <- SKAT_Null_Model(cov$V1 ~ 1, out_type=""D""); # result <- SKAT(dat, null_model, method=""davies"", weights=weights); #; # cat(result$p.value, result$Q); #; #; # SKAT expects rows to be samples, so we transpose from the original input",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_skat.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py:38,Testability,test,test,38,"# dat <- as.matrix(read.csv('hail/src/test/resources/skat_genotype_matrix.csv', header=FALSE)); # cov = read.csv('hail/src/test/resources/skat_phenotypes.csv', header=FALSE); # weights <- rep(1, 100); # null_model <- SKAT_Null_Model(cov$V1 ~ 1, out_type=""C""); # result <- SKAT(dat, null_model, method=""davies"", weights=weights); #; # cat(result$p.value, result$Q); #; #; # SKAT expects rows to be samples, so we transpose from the original input",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_skat.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py:123,Testability,test,test,123,"# dat <- as.matrix(read.csv('hail/src/test/resources/skat_genotype_matrix.csv', header=FALSE)); # cov = read.csv('hail/src/test/resources/skat_phenotypes.csv', header=FALSE); # weights <- rep(1, 100); # null_model <- SKAT_Null_Model(cov$V1 ~ 1, out_type=""C""); # result <- SKAT(dat, null_model, method=""davies"", weights=weights); #; # cat(result$p.value, result$Q); #; #; # SKAT expects rows to be samples, so we transpose from the original input",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_skat.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py:6,Testability,log,logistic,6,"# The logistic settings are only used when fitting the null model, so we need to use a; # covariate that triggers nonconvergence",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_skat.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py:6,Testability,log,logistic,6,"# The logistic settings are only used when fitting the null model, so we need to use a; # covariate that triggers nonconvergence",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_skat.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_skat.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:2,Testability,test,test,2,"# test that chained linear regression can replicate separate calls with different missingness",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:2,Testability,test,test,2,"# test differential missingness against each other",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:186,Testability,log,logfit,186,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # waldtest <- coef(summary(logfit)); # beta <- waldtest[""x"", ""Estimate""]; # se <- waldtest[""x"", ""Std. Error""]; # zstat <- waldtest[""x"", ""z value""]; # pval <- waldtest[""x"", ""Pr(>|z|)""]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:239,Testability,log,logit,239,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # waldtest <- coef(summary(logfit)); # beta <- waldtest[""x"", ""Estimate""]; # se <- waldtest[""x"", ""Std. Error""]; # zstat <- waldtest[""x"", ""z value""]; # pval <- waldtest[""x"", ""Pr(>|z|)""]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:276,Testability,log,logfit,276,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # waldtest <- coef(summary(logfit)); # beta <- waldtest[""x"", ""Estimate""]; # se <- waldtest[""x"", ""Std. Error""]; # zstat <- waldtest[""x"", ""z value""]; # pval <- waldtest[""x"", ""Pr(>|z|)""]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:7,Testability,test,test,7,"# TODO test handling of missingness",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:201,Testability,log,logfit,201,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # beta <- coef(summary(logfit))[""x"", ""Estimate""]; # lrtest <- anova(logfitnull, logfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:254,Testability,log,logit,254,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # beta <- coef(summary(logfit))[""x"", ""Estimate""]; # lrtest <- anova(logfitnull, logfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:266,Testability,log,logfitnull,266,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # beta <- coef(summary(logfit))[""x"", ""Estimate""]; # lrtest <- anova(logfitnull, logfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:319,Testability,log,logit,319,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # beta <- coef(summary(logfit))[""x"", ""Estimate""]; # lrtest <- anova(logfitnull, logfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:352,Testability,log,logfit,352,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # beta <- coef(summary(logfit))[""x"", ""Estimate""]; # lrtest <- anova(logfitnull, logfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:397,Testability,log,logfitnull,397,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # beta <- coef(summary(logfit))[""x"", ""Estimate""]; # lrtest <- anova(logfitnull, logfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:409,Testability,log,logfit,409,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # beta <- coef(summary(logfit))[""x"", ""Estimate""]; # lrtest <- anova(logfitnull, logfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:417,Testability,test,test,417,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # beta <- coef(summary(logfit))[""x"", ""Estimate""]; # lrtest <- anova(logfitnull, logfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:201,Testability,log,logfit,201,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ c1 + c2 + x, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # scoretest <- anova(logfitnull, logfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:254,Testability,log,logit,254,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ c1 + c2 + x, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # scoretest <- anova(logfitnull, logfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:266,Testability,log,logfitnull,266,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ c1 + c2 + x, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # scoretest <- anova(logfitnull, logfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:319,Testability,log,logit,319,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ c1 + c2 + x, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # scoretest <- anova(logfitnull, logfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:350,Testability,log,logfitnull,350,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ c1 + c2 + x, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # scoretest <- anova(logfitnull, logfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:362,Testability,log,logfit,362,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ c1 + c2 + x, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # scoretest <- anova(logfitnull, logfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:370,Testability,test,test,370,"# comparing to output of R code:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 0, 1, 1, 1, 1, 0, 0, 1, 1); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ c1 + c2 + x, family=binomial(link=""logit"")); # logfitnull <- glm(y ~ c1 + c2, family=binomial(link=""logit"")); # scoretest <- anova(logfitnull, logfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:186,Testability,log,logfit,186,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 2, 5, 3, 6, 2, 1, 1, 0, 0); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=poisson(link=""log"")); # waldtest <- coef(summary(logfit)); # beta <- waldtest[""x"", ""Estimate""]; # se <- waldtest[""x"", ""Std. Error""]; # zstat <- waldtest[""x"", ""z value""]; # pval <- waldtest[""x"", ""Pr(>|z|)""]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:238,Testability,log,log,238,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 2, 5, 3, 6, 2, 1, 1, 0, 0); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=poisson(link=""log"")); # waldtest <- coef(summary(logfit)); # beta <- waldtest[""x"", ""Estimate""]; # se <- waldtest[""x"", ""Std. Error""]; # zstat <- waldtest[""x"", ""z value""]; # pval <- waldtest[""x"", ""Pr(>|z|)""]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:273,Testability,log,logfit,273,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 2, 5, 3, 6, 2, 1, 1, 0, 0); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # logfit <- glm(y ~ x + c1 + c2, family=poisson(link=""log"")); # waldtest <- coef(summary(logfit)); # beta <- waldtest[""x"", ""Estimate""]; # se <- waldtest[""x"", ""Std. Error""]; # zstat <- waldtest[""x"", ""z value""]; # pval <- waldtest[""x"", ""Pr(>|z|)""]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:239,Testability,log,log,239,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 2, 5, 3, 6, 2, 1, 1, 0, 0); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # poisfit <- glm(y ~ x + c1 + c2, family=poisson(link=""log"")); # poisfitnull <- glm(y ~ c1 + c2, family=poisson(link=""log"")); # beta <- coef(summary(poisfit))[""x"", ""Estimate""]; # lrtest <- anova(poisfitnull, poisfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:302,Testability,log,log,302,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 2, 5, 3, 6, 2, 1, 1, 0, 0); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # poisfit <- glm(y ~ x + c1 + c2, family=poisson(link=""log"")); # poisfitnull <- glm(y ~ c1 + c2, family=poisson(link=""log"")); # beta <- coef(summary(poisfit))[""x"", ""Estimate""]; # lrtest <- anova(poisfitnull, poisfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:401,Testability,test,test,401,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 2, 5, 3, 6, 2, 1, 1, 0, 0); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # poisfit <- glm(y ~ x + c1 + c2, family=poisson(link=""log"")); # poisfitnull <- glm(y ~ c1 + c2, family=poisson(link=""log"")); # beta <- coef(summary(poisfit))[""x"", ""Estimate""]; # lrtest <- anova(poisfitnull, poisfit, test=""LRT""); # chi2 <- lrtest[[""Deviance""]][2]; # pval <- lrtest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:239,Testability,log,log,239,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 2, 5, 3, 6, 2, 1, 1, 0, 0); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # poisfit <- glm(y ~ c1 + c2 + x, family=poisson(link=""log"")); # poisfitnull <- glm(y ~ c1 + c2, family=poisson(link=""log"")); # scoretest <- anova(poisfitnull, poisfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:302,Testability,log,log,302,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 2, 5, 3, 6, 2, 1, 1, 0, 0); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # poisfit <- glm(y ~ c1 + c2 + x, family=poisson(link=""log"")); # poisfitnull <- glm(y ~ c1 + c2, family=poisson(link=""log"")); # scoretest <- anova(poisfitnull, poisfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:353,Testability,test,test,353,"# comparing to R:; # x = c(0, 1, 0, 0, 0, 1, 0, 0, 0, 0); # y = c(0, 2, 5, 3, 6, 2, 1, 1, 0, 0); # c1 = c(0, 2, 1, -2, -2, 4, 1, 2, 3, 4); # c2 = c(-1, 3, 5, 0, -4, 3, 0, -2, -1, -4); # poisfit <- glm(y ~ c1 + c2 + x, family=poisson(link=""log"")); # poisfitnull <- glm(y ~ c1 + c2, family=poisson(link=""log"")); # scoretest <- anova(poisfitnull, poisfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:2,Testability,test,test,2,"# test pop distribution",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:2,Testability,test,test,2,"# test af distribution",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:2,Testability,test,test,2,"# test genotype distribution",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:19,Testability,test,test,19,"# The name of this test suggests it was originally a comparison to EPACTS. The original EPACTS; # values were slightly different from the output of lowered logistic regression. I regenerated; # this test's expected values using R.; #; # 1. Export the data into an R-friendly format:; #; # mt = logistic_epacts_mt(); # mt = mt.select_cols(; # y=hl.int32(mt.is_case),; # c1=1.0,; # c2=hl.int32(mt.is_female),; # c3=mt.PC1,; # c4=mt.PC2,; # x=hl.agg.collect(mt.GT.n_alt_alleles()); # ); # mt = mt.transmute_cols(**{; # f'x{i}': mt.x[i] for i in range(mt.count_rows()); # }); # mt.cols().export('phenos.tsv'); #; # 2. Run this model repeatedly for each x:; #; # df = read.table(file = 'phenos.csv', sep = '\t', header = TRUE); # poisfit <- glm(df$y ~ df$c1 + df$c2 + df$c3 + df$c4 + df$x0, family=""binomial""); # poisfitnull <- glm(df$y ~ df$c1 + df$c2 + df$c3 + df$c4, family=""binomial""); # scoretest <- anova(poisfitnull, poisfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]; #",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:156,Testability,log,logistic,156,"# The name of this test suggests it was originally a comparison to EPACTS. The original EPACTS; # values were slightly different from the output of lowered logistic regression. I regenerated; # this test's expected values using R.; #; # 1. Export the data into an R-friendly format:; #; # mt = logistic_epacts_mt(); # mt = mt.select_cols(; # y=hl.int32(mt.is_case),; # c1=1.0,; # c2=hl.int32(mt.is_female),; # c3=mt.PC1,; # c4=mt.PC2,; # x=hl.agg.collect(mt.GT.n_alt_alleles()); # ); # mt = mt.transmute_cols(**{; # f'x{i}': mt.x[i] for i in range(mt.count_rows()); # }); # mt.cols().export('phenos.tsv'); #; # 2. Run this model repeatedly for each x:; #; # df = read.table(file = 'phenos.csv', sep = '\t', header = TRUE); # poisfit <- glm(df$y ~ df$c1 + df$c2 + df$c3 + df$c4 + df$x0, family=""binomial""); # poisfitnull <- glm(df$y ~ df$c1 + df$c2 + df$c3 + df$c4, family=""binomial""); # scoretest <- anova(poisfitnull, poisfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]; #",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:199,Testability,test,test,199,"# The name of this test suggests it was originally a comparison to EPACTS. The original EPACTS; # values were slightly different from the output of lowered logistic regression. I regenerated; # this test's expected values using R.; #; # 1. Export the data into an R-friendly format:; #; # mt = logistic_epacts_mt(); # mt = mt.select_cols(; # y=hl.int32(mt.is_case),; # c1=1.0,; # c2=hl.int32(mt.is_female),; # c3=mt.PC1,; # c4=mt.PC2,; # x=hl.agg.collect(mt.GT.n_alt_alleles()); # ); # mt = mt.transmute_cols(**{; # f'x{i}': mt.x[i] for i in range(mt.count_rows()); # }); # mt.cols().export('phenos.tsv'); #; # 2. Run this model repeatedly for each x:; #; # df = read.table(file = 'phenos.csv', sep = '\t', header = TRUE); # poisfit <- glm(df$y ~ df$c1 + df$c2 + df$c3 + df$c4 + df$x0, family=""binomial""); # poisfitnull <- glm(df$y ~ df$c1 + df$c2 + df$c3 + df$c4, family=""binomial""); # scoretest <- anova(poisfitnull, poisfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]; #",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py:928,Testability,test,test,928,"# The name of this test suggests it was originally a comparison to EPACTS. The original EPACTS; # values were slightly different from the output of lowered logistic regression. I regenerated; # this test's expected values using R.; #; # 1. Export the data into an R-friendly format:; #; # mt = logistic_epacts_mt(); # mt = mt.select_cols(; # y=hl.int32(mt.is_case),; # c1=1.0,; # c2=hl.int32(mt.is_female),; # c3=mt.PC1,; # c4=mt.PC2,; # x=hl.agg.collect(mt.GT.n_alt_alleles()); # ); # mt = mt.transmute_cols(**{; # f'x{i}': mt.x[i] for i in range(mt.count_rows()); # }); # mt.cols().export('phenos.tsv'); #; # 2. Run this model repeatedly for each x:; #; # df = read.table(file = 'phenos.csv', sep = '\t', header = TRUE); # poisfit <- glm(df$y ~ df$c1 + df$c2 + df$c3 + df$c4 + df$x0, family=""binomial""); # poisfitnull <- glm(df$y ~ df$c1 + df$c2 + df$c3 + df$c4, family=""binomial""); # scoretest <- anova(poisfitnull, poisfit, test=""Rao""); # chi2 <- scoretest[[""Rao""]][2]; # pval <- scoretest[[""Pr(>Chi)""]][2]; #",MatchSource.CODE_COMMENT,hail/python/test/hail/methods/test_statgen.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/methods/test_statgen.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,test,2,"# test map side combine and shuffle aggregation",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,test,2,"# test sorted aggregation",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:16,Performance,optimiz,optimization,16,"# prevent count optimization",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:7,Testability,test,testing,7,"# Just testing import without segault",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,tests,2,"# tests left join right distinct requiredness",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:34,Testability,test,test,34,"# NOTE: we cannot use Hail during test parameter initialization",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:2,Testability,test,test,2,"# test with no consumer randomness",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py:18,Availability,avail,available,18,"# with sufficient available cores should take <=60s",MatchSource.CODE_COMMENT,hail/python/test/hail/table/test_table.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/table/test_table.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/typecheck/test_typecheck.py:2,Availability,error,error,2,"# error because it should be typecheck_method",MatchSource.CODE_COMMENT,hail/python/test/hail/typecheck/test_typecheck.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/typecheck/test_typecheck.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/utils/test_hl_hadoop_and_hail_fs.py:64,Testability,test,test,64,"# subdir1subdir2: will exist in cloud, but not local, so do not test for it",MatchSource.CODE_COMMENT,hail/python/test/hail/utils/test_hl_hadoop_and_hail_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/utils/test_hl_hadoop_and_hail_fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/utils/test_hl_hadoop_and_hail_fs.py:64,Testability,test,test,64,"# subdir1subdir2: will exist in cloud, but not local, so do not test for it",MatchSource.CODE_COMMENT,hail/python/test/hail/utils/test_hl_hadoop_and_hail_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/utils/test_hl_hadoop_and_hail_fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/vds/test_combiner.py:65,Testability,assert,assertions,65,"# see https://github.com/hail-is/hail/issues/13367 for why these assertions are here",MatchSource.CODE_COMMENT,hail/python/test/hail/vds/test_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/vds/test_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/vds/test_combiner.py:64,Testability,assert,assertion,64,"# see https://github.com/hail-is/hail/issues/14564 for why this assertion is here",MatchSource.CODE_COMMENT,hail/python/test/hail/vds/test_combiner.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/vds/test_combiner.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/vds/test_vds_functions.py:171,Testability,assert,assert,171,"# the below fails because phasing uses the sum of j and k for its second allele.; # we cannot represent this allele index in 28 bits; # c2 = hl.call(1, 1, phased=True); # assert hl.eval(hl.vds.lgt_to_gt(c2, [0, 17495])) == hl.Call([17495, 17495], phased=True)",MatchSource.CODE_COMMENT,hail/python/test/hail/vds/test_vds_functions.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hail/vds/test_vds_functions.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/test_aiogoogle.py:22,Testability,test,test,22,"# This is a white-box test. compose has a maximum of 32 inputs,; # so if we're composing more than 32 parts, the; # GoogleStorageAsyncFS does a multi-level hierarhical merge.",MatchSource.CODE_COMMENT,hail/python/test/hailtop/test_aiogoogle.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/test_aiogoogle.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/test_aiogoogle.py:13,Performance,perform,perform,13,"# > 32 so we perform at least 2 levels of merging",MatchSource.CODE_COMMENT,hail/python/test/hailtop/test_aiogoogle.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/test_aiogoogle.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/conftest.py:44,Testability,test,tests,44,"# create a unique URL for each split of the tests",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:14,Testability,assert,assert,14,"# too slow; # assert b.run().status()['state'] == 'success'",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:21,Security,access,access,21,"# bucket is a public access bucket (https://cloud.google.com/storage/docs/access-public-data)",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:74,Security,access,access-public-data,74,"# bucket is a public access bucket (https://cloud.google.com/storage/docs/access-public-data)",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:40,Availability,error,error,40,"# no configuration, nonexistent buckets error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:5,Deployability,configurat,configuration,5,"# no configuration, nonexistent buckets error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:5,Modifiability,config,configuration,5,"# no configuration, nonexistent buckets error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:49,Availability,error,error,49,"# no configuration, public access bucket doesn't error unless the object doesn't exist",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:5,Deployability,configurat,configuration,5,"# no configuration, public access bucket doesn't error unless the object doesn't exist",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:5,Modifiability,config,configuration,5,"# no configuration, public access bucket doesn't error unless the object doesn't exist",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:27,Security,access,access,27,"# no configuration, public access bucket doesn't error unless the object doesn't exist",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:36,Availability,error,errors,36,"# no configuration, no perms bucket errors",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:5,Deployability,configurat,configuration,5,"# no configuration, no perms bucket errors",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:5,Modifiability,config,configuration,5,"# no configuration, no perms bucket errors",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:32,Availability,error,errors,32,"# no configuration, cold bucket errors",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:5,Deployability,configurat,configuration,5,"# no configuration, cold bucket errors",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:5,Modifiability,config,configuration,5,"# no configuration, cold bucket errors",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:56,Availability,error,error,56,"# hailctl config, allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:10,Modifiability,config,config,10,"# hailctl config, allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:74,Availability,error,error,74,"# environment variable config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:14,Modifiability,variab,variable,14,"# environment variable config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:23,Modifiability,config,config,23,"# environment variable config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:72,Availability,error,error,72,"# arg to constructor config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py:21,Modifiability,config,config,21,"# arg to constructor config, only allowlisted nonexistent buckets don't error",MatchSource.CODE_COMMENT,hail/python/test/hailtop/batch/test_batch_service_backend.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/batch/test_batch_service_backend.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py:22,Deployability,configurat,configuration,22,"""""""Fixture for gcloud configuration values.""""""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/hailctl/dataproc/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py:22,Modifiability,config,configuration,22,"""""""Fixture for gcloud configuration values.""""""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/hailctl/dataproc/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py:47,Testability,mock,mocks,47,"""""""Automatically replace gcloud functions with mocks.""""""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/hailctl/dataproc/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py:15,Deployability,deploy,deploy,15,"""""""Fixture for deploy.yaml values.""""""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/hailctl/dataproc/conftest.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/conftest.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/test_connect.py:17,Testability,mock,mock,17,"""""""Automatically mock subprocess module.""""""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/hailctl/dataproc/test_connect.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/hailctl/dataproc/test_connect.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py:272,Deployability,configurat,configuration,272,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py:359,Deployability,configurat,configuration,359,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py:60,Integrability,depend,depends,60,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py:272,Modifiability,config,configuration,272,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py:359,Modifiability,config,configuration,359,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py:25,Testability,test,test,25,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py:50,Testability,test,test,50,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py:104,Testability,test,testing,104,"""""""Create a directory of test data. The directory test data depends on the name (src or dest) so, when; testing overwriting for example, there is a file in src which does; not exist in dest, a file in dest that does not exist in src, and; one that exists in both. The src configuration looks like:; - {base}/src/a/file1; - {base}/src/a/subdir/file2. The dest configuration looks like:; - {base}/dest/a/subdir/file2; - {base}/dest/a/file3; """"""",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/generate_copy_test_specs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_copy.py:329,Availability,error,error,329,"# This fixture is for test_copy_behavior. It runs a series of copy; # test ""specifications"" by calling run_test_spec. The set of; # specifications is enumerated by; # generate_copy_test_specs.py::copy_test_configurations which are then; # run against the local file system. This tests that (1) that copy; # runs without expected error for each enumerated spec, and that the; # semantics of each filesystem agree.",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/test_copy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_copy.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_copy.py:70,Testability,test,test,70,"# This fixture is for test_copy_behavior. It runs a series of copy; # test ""specifications"" by calling run_test_spec. The set of; # specifications is enumerated by; # generate_copy_test_specs.py::copy_test_configurations which are then; # run against the local file system. This tests that (1) that copy; # runs without expected error for each enumerated spec, and that the; # semantics of each filesystem agree.",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/test_copy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_copy.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_copy.py:279,Testability,test,tests,279,"# This fixture is for test_copy_behavior. It runs a series of copy; # test ""specifications"" by calling run_test_spec. The set of; # specifications is enumerated by; # generate_copy_test_specs.py::copy_test_configurations which are then; # run against the local file system. This tests that (1) that copy; # runs without expected error for each enumerated spec, and that the; # semantics of each filesystem agree.",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/test_copy.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_copy.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_fs.py:8,Testability,test,test,8,"# can't test this until after creating foo",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/test_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_fs.py:8,Testability,test,test,8,"# can't test this until after creating foo",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/test_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_fs.py:2,Testability,test,test,2,"# test FileListEntry.status raises on directory",MatchSource.CODE_COMMENT,hail/python/test/hailtop/inter_cloud/test_fs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/python/test/hailtop/inter_cloud/test_fs.py
https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py:9,Testability,benchmark,benchmarks,9,"# If the benchmarks fail, we always want this file to exist otherwise; # later combine jobs will fail when localising.",MatchSource.CODE_COMMENT,hail/scripts/benchmark_in_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py:247,Availability,down,down,247,"# pytest keeps 3 test sessions worth of temp files by default.; # some benchmarks generate very large output files which can quickly; # fill the tmpfs and so we'll make pytest always delete tmp files; # immediately when tmp_path fixtures are torn-down.",MatchSource.CODE_COMMENT,hail/scripts/benchmark_in_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py:17,Testability,test,test,17,"# pytest keeps 3 test sessions worth of temp files by default.; # some benchmarks generate very large output files which can quickly; # fill the tmpfs and so we'll make pytest always delete tmp files; # immediately when tmp_path fixtures are torn-down.",MatchSource.CODE_COMMENT,hail/scripts/benchmark_in_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py
https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py:71,Testability,benchmark,benchmarks,71,"# pytest keeps 3 test sessions worth of temp files by default.; # some benchmarks generate very large output files which can quickly; # fill the tmpfs and so we'll make pytest always delete tmp files; # immediately when tmp_path fixtures are torn-down.",MatchSource.CODE_COMMENT,hail/scripts/benchmark_in_batch.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/hail/scripts/benchmark_in_batch.py
https://github.com/hail-is/hail/tree/0.2.133/monitoring/monitoring/__main__.py:2,Modifiability,config,configure,2,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,monitoring/monitoring/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/monitoring/monitoring/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/monitoring/monitoring/__main__.py:12,Testability,log,logging,12,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,monitoring/monitoring/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/monitoring/monitoring/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/tls/create_certs.py:40,Availability,avail,available,40,"# gear, hailtop, and web_common are not available in the create_certs image",MatchSource.CODE_COMMENT,tls/create_certs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/tls/create_certs.py
https://github.com/hail-is/hail/tree/0.2.133/tls/create_certs.py:206,Security,secur,security,206,"# this whole extfile nonsense is because OpenSSL has known, unfixed bugs; # in the x509 command. These really ought to be in the CSR.; # https://www.openssl.org/docs/man1.1.0/man1/x509.html#BUGS; # https://security.stackexchange.com/questions/150078/missing-x509-extensions-with-an-openssl-generated-certificate",MatchSource.CODE_COMMENT,tls/create_certs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/tls/create_certs.py
https://github.com/hail-is/hail/tree/0.2.133/tls/create_certs.py:300,Security,certificate,certificate,300,"# this whole extfile nonsense is because OpenSSL has known, unfixed bugs; # in the x509 command. These really ought to be in the CSR.; # https://www.openssl.org/docs/man1.1.0/man1/x509.html#BUGS; # https://security.stackexchange.com/questions/150078/missing-x509-extensions-with-an-openssl-generated-certificate",MatchSource.CODE_COMMENT,tls/create_certs.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/tls/create_certs.py
https://github.com/hail-is/hail/tree/0.2.133/website/website/website.py:18,Availability,down,download,18,"# Chrome fails to download the tutorials.tar.gz file without the Content-Type header.",MatchSource.CODE_COMMENT,website/website/website.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/website.py
https://github.com/hail-is/hail/tree/0.2.133/website/website/__main__.py:2,Modifiability,config,configure,2,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,website/website/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/__main__.py
https://github.com/hail-is/hail/tree/0.2.133/website/website/__main__.py:12,Testability,log,logging,12,"# configure logging before importing anything else",MatchSource.CODE_COMMENT,website/website/__main__.py,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/tree/0.2.133/website/website/__main__.py
