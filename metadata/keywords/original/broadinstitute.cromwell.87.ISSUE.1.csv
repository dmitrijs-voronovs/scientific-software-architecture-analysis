id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/pull/986:31,Testability,test,tests,31,"…. Closes #940 . Note that the tests for aborting were already commented out, and as this would be tricky to test anyways I took that as a sign from above. I've manually verified it, however. @abaumann - this will not clean up previously stuck workflows, I'm going to provide a script or something like that to clean up the old crap, fixing those from within Cromwell was building up too many tendrils for a hack fix to something which is going to be replaced in the near future.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/986
https://github.com/broadinstitute/cromwell/pull/986:109,Testability,test,test,109,"…. Closes #940 . Note that the tests for aborting were already commented out, and as this would be tricky to test anyways I took that as a sign from above. I've manually verified it, however. @abaumann - this will not clean up previously stuck workflows, I'm going to provide a script or something like that to clean up the old crap, fixing those from within Cromwell was building up too many tendrils for a hack fix to something which is going to be replaced in the near future.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/986
https://github.com/broadinstitute/cromwell/issues/993:61,Deployability,pipeline,pipelines,61,"Using the script in . https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/test_staging/launch_50_cloud_workflows.sh. as a template, run a single sample (e.g. J464) through cromwell v0.20. That script currently runs 2 samples, 25x each against a gotc server and using files on the broad filesystem, but it provides a good starting point.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/993
https://github.com/broadinstitute/cromwell/issues/995:61,Deployability,pipeline,pipelines,61,"Using the script in . https://github.com/broadinstitute/dsde-pipelines/blob/master/genomes_in_the_cloud/single_sample/test_staging/launch_50_cloud_workflows.sh. as a template, run a single sample (e.g. J464) through cromwell v0.20. That script currently runs 2 samples, 25x each against a gotc server and using files on the broad filesystem, but it provides a good starting point. Just change the 25x to be 5x and let 'er rip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/995
https://github.com/broadinstitute/cromwell/issues/996:85,Deployability,configurat,configuration,85,"Currently, we have a number of ignored tests that are part of the default 'sbt test' configuration which we are not planning on re-enabling for MVP. By excluding these we (a) don't lose them and (b) should be able to drive the ignored tests to 0 (yay!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/996
https://github.com/broadinstitute/cromwell/issues/996:85,Modifiability,config,configuration,85,"Currently, we have a number of ignored tests that are part of the default 'sbt test' configuration which we are not planning on re-enabling for MVP. By excluding these we (a) don't lose them and (b) should be able to drive the ignored tests to 0 (yay!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/996
https://github.com/broadinstitute/cromwell/issues/996:39,Testability,test,tests,39,"Currently, we have a number of ignored tests that are part of the default 'sbt test' configuration which we are not planning on re-enabling for MVP. By excluding these we (a) don't lose them and (b) should be able to drive the ignored tests to 0 (yay!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/996
https://github.com/broadinstitute/cromwell/issues/996:79,Testability,test,test,79,"Currently, we have a number of ignored tests that are part of the default 'sbt test' configuration which we are not planning on re-enabling for MVP. By excluding these we (a) don't lose them and (b) should be able to drive the ignored tests to 0 (yay!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/996
https://github.com/broadinstitute/cromwell/issues/996:235,Testability,test,tests,235,"Currently, we have a number of ignored tests that are part of the default 'sbt test' configuration which we are not planning on re-enabling for MVP. By excluding these we (a) don't lose them and (b) should be able to drive the ignored tests to 0 (yay!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/996
https://github.com/broadinstitute/cromwell/issues/997:86,Performance,concurren,concurrent,86,Work as an ambassador to GOTC to have them test out v0.20 in their standard tests (50 concurrent genomes) to quick resolve issues and file any necessary tickets.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/997
https://github.com/broadinstitute/cromwell/issues/997:43,Testability,test,test,43,Work as an ambassador to GOTC to have them test out v0.20 in their standard tests (50 concurrent genomes) to quick resolve issues and file any necessary tickets.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/997
https://github.com/broadinstitute/cromwell/issues/997:76,Testability,test,tests,76,Work as an ambassador to GOTC to have them test out v0.20 in their standard tests (50 concurrent genomes) to quick resolve issues and file any necessary tickets.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/997
https://github.com/broadinstitute/cromwell/issues/998:99,Security,access,access,99,Work as an ambassador to FireCloud to have them test out v0.20 in their standard tests (especially access token related) to quick resolve issues and file any necessary tickets.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/998
https://github.com/broadinstitute/cromwell/issues/998:48,Testability,test,test,48,Work as an ambassador to FireCloud to have them test out v0.20 in their standard tests (especially access token related) to quick resolve issues and file any necessary tickets.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/998
https://github.com/broadinstitute/cromwell/issues/998:81,Testability,test,tests,81,Work as an ambassador to FireCloud to have them test out v0.20 in their standard tests (especially access token related) to quick resolve issues and file any necessary tickets.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/998
https://github.com/broadinstitute/cromwell/issues/999:5,Security,encrypt,encrypted,5,When encrypted fields are specified in the application.conf they're not being encrypted in the DB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/999
https://github.com/broadinstitute/cromwell/issues/999:78,Security,encrypt,encrypted,78,When encrypted fields are specified in the application.conf they're not being encrypted in the DB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/999
https://github.com/broadinstitute/cromwell/issues/1005:212,Testability,test,test,212,"The following workflow options are currently unsupported: . ```; ""workflowFailureMode"": ""ContinueWhilePossible""; ```. and . ```; ""workflowFailureMode"": ""NoNewCalls""; ```. Ruchi's PR in centaur (PR 69) contains a test for this which should be un-ignored.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1005
https://github.com/broadinstitute/cromwell/issues/1012:386,Energy Efficiency,reduce,reduce,386,"Currently we have a lot of information that gets logged by Cromwell and some of it seems like it may not be useful to our customers. For example, every actor state transition gets logged, but it's unclear who is using that information. . We tech talked today an the suggestion was to meet with some key customer's, figure out if there are aspects of our logging they don't need to help reduce, and instead just redirect the unwanted logging to debug level, so that it can still be used for debugging purposes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1012
https://github.com/broadinstitute/cromwell/issues/1012:49,Testability,log,logged,49,"Currently we have a lot of information that gets logged by Cromwell and some of it seems like it may not be useful to our customers. For example, every actor state transition gets logged, but it's unclear who is using that information. . We tech talked today an the suggestion was to meet with some key customer's, figure out if there are aspects of our logging they don't need to help reduce, and instead just redirect the unwanted logging to debug level, so that it can still be used for debugging purposes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1012
https://github.com/broadinstitute/cromwell/issues/1012:180,Testability,log,logged,180,"Currently we have a lot of information that gets logged by Cromwell and some of it seems like it may not be useful to our customers. For example, every actor state transition gets logged, but it's unclear who is using that information. . We tech talked today an the suggestion was to meet with some key customer's, figure out if there are aspects of our logging they don't need to help reduce, and instead just redirect the unwanted logging to debug level, so that it can still be used for debugging purposes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1012
https://github.com/broadinstitute/cromwell/issues/1012:354,Testability,log,logging,354,"Currently we have a lot of information that gets logged by Cromwell and some of it seems like it may not be useful to our customers. For example, every actor state transition gets logged, but it's unclear who is using that information. . We tech talked today an the suggestion was to meet with some key customer's, figure out if there are aspects of our logging they don't need to help reduce, and instead just redirect the unwanted logging to debug level, so that it can still be used for debugging purposes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1012
https://github.com/broadinstitute/cromwell/issues/1012:433,Testability,log,logging,433,"Currently we have a lot of information that gets logged by Cromwell and some of it seems like it may not be useful to our customers. For example, every actor state transition gets logged, but it's unclear who is using that information. . We tech talked today an the suggestion was to meet with some key customer's, figure out if there are aspects of our logging they don't need to help reduce, and instead just redirect the unwanted logging to debug level, so that it can still be used for debugging purposes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1012
https://github.com/broadinstitute/cromwell/pull/1014:9,Usability,undo,undocumented,9,Replaced undocumented `timings` and `outputs` flags on `/metadata` endpoint with `includeKey`.; Added a `excludeKey` mirror of `includeKey`.; DRY-ed out the `SqlDatabase.queryMetadataEventsWithWildcardKey` by using the very similar `SqlDatabase.queryMetadataEventsWithWildcardKeys`.; Fixed compiler warnings in `BackendWorkflowInitializationActor` and `jes.Run`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1014
https://github.com/broadinstitute/cromwell/issues/1016:70,Availability,error,error,70,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:154,Availability,error,error,154,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:3972,Availability,error,error,3972,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:3216,Performance,concurren,concurrent,3216,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:3309,Performance,concurren,concurrent,3309,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:3562,Performance,concurren,concurrent,3562,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:3637,Performance,concurren,concurrent,3637,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:3725,Performance,concurren,concurrent,3725,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:3805,Performance,concurren,concurrent,3805,"1.apply(TraversableLike .scala:245); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike .scala:245); > at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray. scala:59); > at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); > at scala.collection.AbstractTraversable.map(Traversable.scala:104); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInpu tPaths(SharedFileSystem.scala:220); > at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(Loc alBackend.scala:94); > at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBack end.scala:96); > at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBa ckend.scala:246); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$l zycompute(JobDescriptor.scala:52); > at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(J obDescriptor.scala:52); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:115); > at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(L ocalBackend.scala:113); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1( Future.scala:24); > at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.sca la:24); > at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); > at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(Abst ractDispatcher.scala:397); > at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool .java:1339); > at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19 79); > at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThre ad.java:107). I tried to use Cygwin, cause it transforms `c:\` into `/cygdrive/c` , but i get the same error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:197,Testability,test,testTask,197,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:272,Testability,test,test,272,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:319,Testability,test,testTask,319,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:469,Testability,test,test,469,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:516,Testability,test,testTask,516,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/issues/1016:538,Testability,test,test,538,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016
https://github.com/broadinstitute/cromwell/pull/1018:28,Deployability,configurat,configuration,28,- [x] Clarify the options / configuration names,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1018
https://github.com/broadinstitute/cromwell/pull/1018:28,Modifiability,config,configuration,28,- [x] Clarify the options / configuration names,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1018
https://github.com/broadinstitute/cromwell/issues/1024:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1024
https://github.com/broadinstitute/cromwell/issues/1024:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1024
https://github.com/broadinstitute/cromwell/issues/1025:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1025
https://github.com/broadinstitute/cromwell/issues/1025:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1025
https://github.com/broadinstitute/cromwell/issues/1026:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1026
https://github.com/broadinstitute/cromwell/issues/1026:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1026
https://github.com/broadinstitute/cromwell/issues/1027:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1027
https://github.com/broadinstitute/cromwell/issues/1027:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1027
https://github.com/broadinstitute/cromwell/issues/1028:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1028
https://github.com/broadinstitute/cromwell/issues/1028:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1028
https://github.com/broadinstitute/cromwell/issues/1029:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1029
https://github.com/broadinstitute/cromwell/issues/1029:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1029
https://github.com/broadinstitute/cromwell/issues/1030:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1030
https://github.com/broadinstitute/cromwell/issues/1030:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1030
https://github.com/broadinstitute/cromwell/issues/1031:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1031
https://github.com/broadinstitute/cromwell/issues/1031:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1031
https://github.com/broadinstitute/cromwell/issues/1032:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1032
https://github.com/broadinstitute/cromwell/issues/1032:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1032
https://github.com/broadinstitute/cromwell/issues/1033:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1033
https://github.com/broadinstitute/cromwell/issues/1033:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1033
https://github.com/broadinstitute/cromwell/issues/1034:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1034
https://github.com/broadinstitute/cromwell/issues/1034:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1034
https://github.com/broadinstitute/cromwell/issues/1035:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1035
https://github.com/broadinstitute/cromwell/issues/1035:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1035
https://github.com/broadinstitute/cromwell/issues/1036:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1036
https://github.com/broadinstitute/cromwell/issues/1036:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1036
https://github.com/broadinstitute/cromwell/issues/1037:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1037
https://github.com/broadinstitute/cromwell/issues/1037:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1037
https://github.com/broadinstitute/cromwell/issues/1038:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1038
https://github.com/broadinstitute/cromwell/issues/1038:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1038
https://github.com/broadinstitute/cromwell/issues/1039:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1039
https://github.com/broadinstitute/cromwell/issues/1039:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1039
https://github.com/broadinstitute/cromwell/issues/1040:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1040
https://github.com/broadinstitute/cromwell/issues/1040:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1040
https://github.com/broadinstitute/cromwell/issues/1041:32,Testability,test,tests,32,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1041
https://github.com/broadinstitute/cromwell/issues/1041:78,Testability,test,test,78,"The goal here is to look at the tests in the spec, and determine; 1. does the test assess something valuable; 2. if no, delete it (with an explanation in the PR); 3. if yes, either fix it up so it passes, or reimplement... whatever is more expedient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1041
https://github.com/broadinstitute/cromwell/issues/1042:62,Availability,error,error,62,"We do this for preemptible VMs, when a job fails with certain error codes (13/14 I believe) which indicate that a VM shut down unexpectedly, we retry a certain number of times which is user specified via the WDL task definition. @dvoet has requested this same feature for non-preemptible VMs for FireCloud",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1042
https://github.com/broadinstitute/cromwell/issues/1042:122,Availability,down,down,122,"We do this for preemptible VMs, when a job fails with certain error codes (13/14 I believe) which indicate that a VM shut down unexpectedly, we retry a certain number of times which is user specified via the WDL task definition. @dvoet has requested this same feature for non-preemptible VMs for FireCloud",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1042
https://github.com/broadinstitute/cromwell/issues/1043:32,Deployability,update,update,32,"When PBE is drawing to a close, update the README and Swagger with then-current versions of output from the various endpoints. Some of this documentation is already very out of date.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1043
https://github.com/broadinstitute/cromwell/pull/1046:20,Testability,test,tests,20,Pretty much all the tests have already been re-written in each backend implementation. The missing ones were added.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1046
https://github.com/broadinstitute/cromwell/issues/1047:38,Integrability,message,message,38,"The service registry actor internals (message flow, service lookup by name...) should be tested. An empty `ServiceRegistryActorSpec` file already exists for that purpose.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1047
https://github.com/broadinstitute/cromwell/issues/1047:89,Testability,test,tested,89,"The service registry actor internals (message flow, service lookup by name...) should be tested. An empty `ServiceRegistryActorSpec` file already exists for that purpose.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1047
https://github.com/broadinstitute/cromwell/pull/1048:174,Integrability,message,message,174,"This test was actually testing the KV service (which already has a test spec) through the ServiceRegistry.; The service registry should have its own tests to assert that the message flow / service lookup is working, but not to test all possible services it knows about. See https://github.com/broadinstitute/cromwell/issues/1047",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1048
https://github.com/broadinstitute/cromwell/pull/1048:5,Testability,test,test,5,"This test was actually testing the KV service (which already has a test spec) through the ServiceRegistry.; The service registry should have its own tests to assert that the message flow / service lookup is working, but not to test all possible services it knows about. See https://github.com/broadinstitute/cromwell/issues/1047",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1048
https://github.com/broadinstitute/cromwell/pull/1048:23,Testability,test,testing,23,"This test was actually testing the KV service (which already has a test spec) through the ServiceRegistry.; The service registry should have its own tests to assert that the message flow / service lookup is working, but not to test all possible services it knows about. See https://github.com/broadinstitute/cromwell/issues/1047",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1048
https://github.com/broadinstitute/cromwell/pull/1048:67,Testability,test,test,67,"This test was actually testing the KV service (which already has a test spec) through the ServiceRegistry.; The service registry should have its own tests to assert that the message flow / service lookup is working, but not to test all possible services it knows about. See https://github.com/broadinstitute/cromwell/issues/1047",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1048
https://github.com/broadinstitute/cromwell/pull/1048:149,Testability,test,tests,149,"This test was actually testing the KV service (which already has a test spec) through the ServiceRegistry.; The service registry should have its own tests to assert that the message flow / service lookup is working, but not to test all possible services it knows about. See https://github.com/broadinstitute/cromwell/issues/1047",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1048
https://github.com/broadinstitute/cromwell/pull/1048:158,Testability,assert,assert,158,"This test was actually testing the KV service (which already has a test spec) through the ServiceRegistry.; The service registry should have its own tests to assert that the message flow / service lookup is working, but not to test all possible services it knows about. See https://github.com/broadinstitute/cromwell/issues/1047",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1048
https://github.com/broadinstitute/cromwell/pull/1048:227,Testability,test,test,227,"This test was actually testing the KV service (which already has a test spec) through the ServiceRegistry.; The service registry should have its own tests to assert that the message flow / service lookup is working, but not to test all possible services it knows about. See https://github.com/broadinstitute/cromwell/issues/1047",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1048
https://github.com/broadinstitute/cromwell/pull/1050:97,Deployability,rolling,rolling,97,"…n. Closes #1022 . Known sharp corners I'm inclined to fix later in the interest of getting this rolling:; - The docker image shouldn't be pointing at my own organization; - 32 CPU seems like overkill. OTOH I've seen futures time out at 16 cpu. One could argue (and I would) that this is BS, however for this testing I want to make sure futures never time out. We can/should address the ""why"" separately; - It's painful to dig into why a test failed on the centaur side. I don't know yet a way to make that easier, but this at least makes it possible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1050
https://github.com/broadinstitute/cromwell/pull/1050:309,Testability,test,testing,309,"…n. Closes #1022 . Known sharp corners I'm inclined to fix later in the interest of getting this rolling:; - The docker image shouldn't be pointing at my own organization; - 32 CPU seems like overkill. OTOH I've seen futures time out at 16 cpu. One could argue (and I would) that this is BS, however for this testing I want to make sure futures never time out. We can/should address the ""why"" separately; - It's painful to dig into why a test failed on the centaur side. I don't know yet a way to make that easier, but this at least makes it possible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1050
https://github.com/broadinstitute/cromwell/pull/1050:438,Testability,test,test,438,"…n. Closes #1022 . Known sharp corners I'm inclined to fix later in the interest of getting this rolling:; - The docker image shouldn't be pointing at my own organization; - 32 CPU seems like overkill. OTOH I've seen futures time out at 16 cpu. One could argue (and I would) that this is BS, however for this testing I want to make sure futures never time out. We can/should address the ""why"" separately; - It's painful to dig into why a test failed on the centaur side. I don't know yet a way to make that easier, but this at least makes it possible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1050
https://github.com/broadinstitute/cromwell/issues/1051:3099,Performance,concurren,concurrent,3099,cess.cromwell$engine$db$slick$SlickDataAccess$$wdlValueToDbValue(SlickDataAccess.scala:160); at cromwell.engine.db.slick.SlickDataAccess$$anonfun$44$$anonfun$apply$35.apply(SlickDataAccess.scala:612); at cromwell.engine.db.slick.SlickDataAccess$$anonfun$44$$anonfun$apply$35.apply(SlickDataAccess.scala:608); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245); at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778); at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245); at scala.collection.MapLike$MappedValues$$anonfun$foreach$3.apply(MapLike.scala:245); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778); at scala.collection.immutable.Map$Map2.foreach(Map.scala:137); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777); at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245); at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777); at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:245); at scala.collection.TraversableLike$class.map(TraversableLike.scala:245); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.db.slick.SlickDataAccess$$anonfun$44.apply(SlickDataAccess.scala:608); at cromwell.engine.db.slick.SlickDataAccess$$anonfun$44.apply(SlickDataAccess.scala:607); at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146); at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1051
https://github.com/broadinstitute/cromwell/pull/1052:22,Testability,test,tests,22,"Mainly this is moving tests for functionality which isn't there yet into different files. One test remains, and two placeholders",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1052
https://github.com/broadinstitute/cromwell/pull/1052:94,Testability,test,test,94,"Mainly this is moving tests for functionality which isn't there yet into different files. One test remains, and two placeholders",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1052
https://github.com/broadinstitute/cromwell/issues/1058:5,Deployability,pipeline,pipeline,5,"As a pipeline author, sometimes I have pipelines which require lots of input files. For example, the joint calling pipeline at one point has a step where I need to combine genotypes from all the samples. This can be in the 10,000s range and growing. Currently, this causes lots of problems. In Cromwell having that many inputs causes memory and database problems because parameters are first class citizens and there are so many of them. In addition they are file paths which can be very long. This can lead to GBs of footprint. Similarly this causes problem for the underlying backend (e.g. JES) because of the volume. Recently our requests to JES were truncated because a load-balancer in front of the service had a maximum request size. I would like to be able to instead specify a file, which is full of file names. My task will know what to do with this. I need a way to indicate this in wdl (perhaps a new type, like FOFN instead of File?). With this information, the Cromwell backend can do the correct thing during localization. For example in JES, we would tell the JES Api this is a FOFN. . Each backend would then need to handle this type. When receiving a FOFN input type the backend would first localize the FOFN and then iterate through the contents to localize each file. A new FOFN would then be rewritten to reference the local paths, and that FOFN would be used in place of the original FOFN as parameters to the tasks. . First, we should conduct a feasibility effort on this with a thought experiment on the joint calling workflow to see if FOFNs would solve the parameter space problem ( #1059 )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058
https://github.com/broadinstitute/cromwell/issues/1058:39,Deployability,pipeline,pipelines,39,"As a pipeline author, sometimes I have pipelines which require lots of input files. For example, the joint calling pipeline at one point has a step where I need to combine genotypes from all the samples. This can be in the 10,000s range and growing. Currently, this causes lots of problems. In Cromwell having that many inputs causes memory and database problems because parameters are first class citizens and there are so many of them. In addition they are file paths which can be very long. This can lead to GBs of footprint. Similarly this causes problem for the underlying backend (e.g. JES) because of the volume. Recently our requests to JES were truncated because a load-balancer in front of the service had a maximum request size. I would like to be able to instead specify a file, which is full of file names. My task will know what to do with this. I need a way to indicate this in wdl (perhaps a new type, like FOFN instead of File?). With this information, the Cromwell backend can do the correct thing during localization. For example in JES, we would tell the JES Api this is a FOFN. . Each backend would then need to handle this type. When receiving a FOFN input type the backend would first localize the FOFN and then iterate through the contents to localize each file. A new FOFN would then be rewritten to reference the local paths, and that FOFN would be used in place of the original FOFN as parameters to the tasks. . First, we should conduct a feasibility effort on this with a thought experiment on the joint calling workflow to see if FOFNs would solve the parameter space problem ( #1059 )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058
https://github.com/broadinstitute/cromwell/issues/1058:115,Deployability,pipeline,pipeline,115,"As a pipeline author, sometimes I have pipelines which require lots of input files. For example, the joint calling pipeline at one point has a step where I need to combine genotypes from all the samples. This can be in the 10,000s range and growing. Currently, this causes lots of problems. In Cromwell having that many inputs causes memory and database problems because parameters are first class citizens and there are so many of them. In addition they are file paths which can be very long. This can lead to GBs of footprint. Similarly this causes problem for the underlying backend (e.g. JES) because of the volume. Recently our requests to JES were truncated because a load-balancer in front of the service had a maximum request size. I would like to be able to instead specify a file, which is full of file names. My task will know what to do with this. I need a way to indicate this in wdl (perhaps a new type, like FOFN instead of File?). With this information, the Cromwell backend can do the correct thing during localization. For example in JES, we would tell the JES Api this is a FOFN. . Each backend would then need to handle this type. When receiving a FOFN input type the backend would first localize the FOFN and then iterate through the contents to localize each file. A new FOFN would then be rewritten to reference the local paths, and that FOFN would be used in place of the original FOFN as parameters to the tasks. . First, we should conduct a feasibility effort on this with a thought experiment on the joint calling workflow to see if FOFNs would solve the parameter space problem ( #1059 )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058
https://github.com/broadinstitute/cromwell/issues/1058:674,Performance,load,load-balancer,674,"As a pipeline author, sometimes I have pipelines which require lots of input files. For example, the joint calling pipeline at one point has a step where I need to combine genotypes from all the samples. This can be in the 10,000s range and growing. Currently, this causes lots of problems. In Cromwell having that many inputs causes memory and database problems because parameters are first class citizens and there are so many of them. In addition they are file paths which can be very long. This can lead to GBs of footprint. Similarly this causes problem for the underlying backend (e.g. JES) because of the volume. Recently our requests to JES were truncated because a load-balancer in front of the service had a maximum request size. I would like to be able to instead specify a file, which is full of file names. My task will know what to do with this. I need a way to indicate this in wdl (perhaps a new type, like FOFN instead of File?). With this information, the Cromwell backend can do the correct thing during localization. For example in JES, we would tell the JES Api this is a FOFN. . Each backend would then need to handle this type. When receiving a FOFN input type the backend would first localize the FOFN and then iterate through the contents to localize each file. A new FOFN would then be rewritten to reference the local paths, and that FOFN would be used in place of the original FOFN as parameters to the tasks. . First, we should conduct a feasibility effort on this with a thought experiment on the joint calling workflow to see if FOFNs would solve the parameter space problem ( #1059 )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1058
https://github.com/broadinstitute/cromwell/pull/1060:20,Testability,test,test,20,There was already a test for this in WorkflowExecutionActor. Just added some more metadata verification.; This also attempts to provide a different ServiceRegistryActor for every WorkflowManagerActor being build in the tests (which might help travis slowness ? @mcovarr).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1060
https://github.com/broadinstitute/cromwell/pull/1060:219,Testability,test,tests,219,There was already a test for this in WorkflowExecutionActor. Just added some more metadata verification.; This also attempts to provide a different ServiceRegistryActor for every WorkflowManagerActor being build in the tests (which might help travis slowness ? @mcovarr).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1060
https://github.com/broadinstitute/cromwell/pull/1061:160,Modifiability,config,config,160,"Removed `WorkflowDescriptorSpec`. Most tests were about copying logs and outputs which lives in a different (better) place now.; The one test regarding caching config that might be useful later was moved to the backend project and left ignored, with some pre-work to prepare it for whenever it will be prioritized.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1061
https://github.com/broadinstitute/cromwell/pull/1061:39,Testability,test,tests,39,"Removed `WorkflowDescriptorSpec`. Most tests were about copying logs and outputs which lives in a different (better) place now.; The one test regarding caching config that might be useful later was moved to the backend project and left ignored, with some pre-work to prepare it for whenever it will be prioritized.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1061
https://github.com/broadinstitute/cromwell/pull/1061:64,Testability,log,logs,64,"Removed `WorkflowDescriptorSpec`. Most tests were about copying logs and outputs which lives in a different (better) place now.; The one test regarding caching config that might be useful later was moved to the backend project and left ignored, with some pre-work to prepare it for whenever it will be prioritized.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1061
https://github.com/broadinstitute/cromwell/pull/1061:137,Testability,test,test,137,"Removed `WorkflowDescriptorSpec`. Most tests were about copying logs and outputs which lives in a different (better) place now.; The one test regarding caching config that might be useful later was moved to the backend project and left ignored, with some pre-work to prepare it for whenever it will be prioritized.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1061
https://github.com/broadinstitute/cromwell/pull/1062:52,Integrability,message,messages,52,- Job Outputs not being printed; - Removed some log messages marked as PBE candidates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1062
https://github.com/broadinstitute/cromwell/pull/1062:48,Testability,log,log,48,- Job Outputs not being printed; - Removed some log messages marked as PBE candidates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1062
https://github.com/broadinstitute/cromwell/issues/1064:111,Deployability,update,update,111,"Flexible date times no longer work in the cromwell 0.20. (e.g. ""2016-06-20"" is no longer a valid date). Either update the README and notify customers of this change or revert to the flexible date time parser. On behalf of green team, Vivek votes for keeping the stricter date time parser.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1064
https://github.com/broadinstitute/cromwell/issues/1064:222,Energy Efficiency,green,green,222,"Flexible date times no longer work in the cromwell 0.20. (e.g. ""2016-06-20"" is no longer a valid date). Either update the README and notify customers of this change or revert to the flexible date time parser. On behalf of green team, Vivek votes for keeping the stricter date time parser.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1064
https://github.com/broadinstitute/cromwell/issues/1064:0,Modifiability,Flexible,Flexible,0,"Flexible date times no longer work in the cromwell 0.20. (e.g. ""2016-06-20"" is no longer a valid date). Either update the README and notify customers of this change or revert to the flexible date time parser. On behalf of green team, Vivek votes for keeping the stricter date time parser.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1064
https://github.com/broadinstitute/cromwell/issues/1064:182,Modifiability,flexible,flexible,182,"Flexible date times no longer work in the cromwell 0.20. (e.g. ""2016-06-20"" is no longer a valid date). Either update the README and notify customers of this change or revert to the flexible date time parser. On behalf of green team, Vivek votes for keeping the stricter date time parser.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1064
https://github.com/broadinstitute/cromwell/issues/1065:84,Performance,concurren,concurrently,84,People are reporting significant slowdown when running more than about 15 workflows concurrently,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065
https://github.com/broadinstitute/cromwell/issues/1066:277,Availability,failure,failures,277,"I submitted a workflow with `defaultRuntimeOptions` instead of `default_runtime_attributes` and when I then called `/metadata`, I got this:. ```; {; ""calls"": {; },; ""id"": ""93215d4d-1341-4aba-9ded-ce83beaeedce"",; ""submission"": ""2016-06-23T20:11:26.122Z"",; ""status"": ""Failed"",; ""failures"": [""Workflow contains invalid options JSON: Unsupported key/value pair in WorkflowOptions: defaultRuntimeOptions -> {\""zones\"":\""us-central1-b\""}""],; ""end"": ""2016-06-23T20:11:26.509Z"",; ""start"": ""2016-06-23T20:11:26.137Z""; }; ```. Note that the `""inputs""` field is missing, but I didn't ask to exclude it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1066
https://github.com/broadinstitute/cromwell/issues/1067:41,Availability,failure,failures,41,"Re-enable and make the `""handle coercion failures gracefully""` test in `MaterializeWorkflowDescriptorActorSpec` work again",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1067
https://github.com/broadinstitute/cromwell/issues/1067:63,Testability,test,test,63,"Re-enable and make the `""handle coercion failures gracefully""` test in `MaterializeWorkflowDescriptorActorSpec` work again",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1067
https://github.com/broadinstitute/cromwell/issues/1068:103,Safety,avoid,avoid,103,"This is the test `""assign default runtime attributes""` in `MaterializeWorkflowDescriptorActorSpec`. To avoid having to recompute the defaults separately in every backend, it should be filled in by the MWDA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1068
https://github.com/broadinstitute/cromwell/issues/1068:12,Testability,test,test,12,"This is the test `""assign default runtime attributes""` in `MaterializeWorkflowDescriptorActorSpec`. To avoid having to recompute the defaults separately in every backend, it should be filled in by the MWDA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1068
https://github.com/broadinstitute/cromwell/issues/1070:689,Availability,ERROR,ERROR,689,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:745,Availability,Failure,Failures,745,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:965,Availability,Failure,Failures,965,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:2310,Performance,concurren,concurrent,2310,19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. Full log:; [workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt](https://github.com/broadinstitute/cromwell/files/332863/workflow.8a4e2219-90e2-4210-a753-,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:2426,Performance,concurren,concurrent,2426,"omwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. Full log:; [workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt](https://github.com/broadinstitute/cromwell/files/332863/workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt). ``` bash; $ java -version; java version ""1.8.0_25""; Java(TM) SE Runtime Environment (build 1.8.0_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:2747,Performance,concurren,concurrent,2747,"[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. Full log:; [workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt](https://github.com/broadinstitute/cromwell/files/332863/workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt). ``` bash; $ java -version; java version ""1.8.0_25""; Java(TM) SE Runtime Environment (build 1.8.0_25-b17); Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode); ```. ``` bash; $ uname -a; Darwin aer 13.4.0 Darwin Kernel Version 13.4.0: Wed Mar 18 16:20:14 PDT 2015; root:xnu-2422.115.14~1/RELEASE_X86_64 x86_64; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:2845,Performance,concurren,concurrent,2845,"[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. Full log:; [workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt](https://github.com/broadinstitute/cromwell/files/332863/workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt). ``` bash; $ java -version; java version ""1.8.0_25""; Java(TM) SE Runtime Environment (build 1.8.0_25-b17); Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode); ```. ``` bash; $ uname -a; Darwin aer 13.4.0 Darwin Kernel Version 13.4.0: Wed Mar 18 16:20:14 PDT 2015; root:xnu-2422.115.14~1/RELEASE_X86_64 x86_64; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:2955,Performance,concurren,concurrent,2955,"[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. Full log:; [workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt](https://github.com/broadinstitute/cromwell/files/332863/workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt). ``` bash; $ java -version; java version ""1.8.0_25""; Java(TM) SE Runtime Environment (build 1.8.0_25-b17); Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode); ```. ``` bash; $ uname -a; Darwin aer 13.4.0 Darwin Kernel Version 13.4.0: Wed Mar 18 16:20:14 PDT 2015; root:xnu-2422.115.14~1/RELEASE_X86_64 x86_64; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:3057,Performance,concurren,concurrent,3057,"[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. Full log:; [workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt](https://github.com/broadinstitute/cromwell/files/332863/workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt). ``` bash; $ java -version; java version ""1.8.0_25""; Java(TM) SE Runtime Environment (build 1.8.0_25-b17); Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode); ```. ``` bash; $ uname -a; Darwin aer 13.4.0 Darwin Kernel Version 13.4.0: Wed Mar 18 16:20:14 PDT 2015; root:xnu-2422.115.14~1/RELEASE_X86_64 x86_64; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:308,Testability,test,test,308,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:337,Testability,test,test,337,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:438,Testability,test,test,438,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:466,Testability,test,test,466,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:477,Testability,test,test,477,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:617,Testability,test,test,617,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:626,Testability,test,test,626,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:792,Testability,test,test,792,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:847,Testability,test,test,847,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:920,Testability,test,test,920,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:1012,Testability,test,test,1012,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:1067,Testability,test,test,1067,"Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:1140,Testability,test,test,1140,"Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1070:3169,Testability,log,log,3169,"[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:115) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) [cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. Full log:; [workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt](https://github.com/broadinstitute/cromwell/files/332863/workflow.8a4e2219-90e2-4210-a753-26b149c18b51.txt). ``` bash; $ java -version; java version ""1.8.0_25""; Java(TM) SE Runtime Environment (build 1.8.0_25-b17); Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode); ```. ``` bash; $ uname -a; Darwin aer 13.4.0 Darwin Kernel Version 13.4.0: Wed Mar 18 16:20:14 PDT 2015; root:xnu-2422.115.14~1/RELEASE_X86_64 x86_64; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070
https://github.com/broadinstitute/cromwell/issues/1071:73,Availability,error,error,73,"I've got a server up and running and I tried out my first WDL. I get the error below. I've pasted my WDL and JSON below the error: . ```; 2016-06-27 08:01:07,766 cromwell-system-akka.actor.default-dispatcher-9 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq -> /cil/shed/apps/internal/cromwell/cromwell-executions/smartseq_amr/21612af1-2c5f-400a-a53f-6ac66ec47674/call-RunSTARAlignment/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq ; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.localizeWdlValue(SgeBackend.scala:60) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:225) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:224) ~[cromwell-0.19.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:124,Availability,error,error,124,"I've got a server up and running and I tried out my first WDL. I get the error below. I've pasted my WDL and JSON below the error: . ```; 2016-06-27 08:01:07,766 cromwell-system-akka.actor.default-dispatcher-9 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq -> /cil/shed/apps/internal/cromwell/cromwell-executions/smartseq_amr/21612af1-2c5f-400a-a53f-6ac66ec47674/call-RunSTARAlignment/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq ; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.localizeWdlValue(SgeBackend.scala:60) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:225) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:224) ~[cromwell-0.19.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:210,Availability,ERROR,ERROR,210,"I've got a server up and running and I tried out my first WDL. I get the error below. I've pasted my WDL and JSON below the error: . ```; 2016-06-27 08:01:07,766 cromwell-system-akka.actor.default-dispatcher-9 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq -> /cil/shed/apps/internal/cromwell/cromwell-executions/smartseq_amr/21612af1-2c5f-400a-a53f-6ac66ec47674/call-RunSTARAlignment/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq ; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.localizeWdlValue(SgeBackend.scala:60) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:225) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:224) ~[cromwell-0.19.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:218,Availability,Failure,Failures,218,"I've got a server up and running and I tried out my first WDL. I get the error below. I've pasted my WDL and JSON below the error: . ```; 2016-06-27 08:01:07,766 cromwell-system-akka.actor.default-dispatcher-9 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq -> /cil/shed/apps/internal/cromwell/cromwell-executions/smartseq_amr/21612af1-2c5f-400a-a53f-6ac66ec47674/call-RunSTARAlignment/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq ; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.localizeWdlValue(SgeBackend.scala:60) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:225) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:224) ~[cromwell-0.19.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:5754,Modifiability,sandbox,sandbox,5754,"cutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {; sh ${pipeline_path}/RunSTARAlignment.sh ${fastq1} ${fastq2} ${sample_name} ${genome_dir} ${output_dir}; }; output {; File genome_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.sortedByCoord.out.bam""; File trans_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.Aligned.toTranscriptome.out.bam""; }; runtime {; docker: ""cowmoo/cil-dev:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }. workflow smartseq_amr {; call RunSTARAlignment; }; ```. JSON:. ```; {; ""smartseq_amr.RunSTARAlignment.fastq1"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq "",; ""smartseq_amr.RunSTARAlignment.fastq2"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R2_001.fastq"",; ""smartseq_amr.RunSTARAlignment.sample_name"": ""Mouse-A2-single"",; ""smartseq_amr.RunSTARAlignment.genome_dir"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/Resources/Mouse10"",; ""smartseq_amr.RunSTARAlignment.output_dir"": ""/cil/shed/sandbox/amr/smartseq/test"",; ""smartseq_amr.RunSTARAlignment.pipeline_path"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:3591,Performance,concurren,concurrent,3591,cala:104) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:224) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.adjustSharedInputPaths(SgeBackend.scala:60) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.adjustInputPaths(SgeBackend.scala:67) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.instantiateCommand(SgeBackend.scala:260) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:84) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:82) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:3707,Performance,concurren,concurrent,3707,SharedFileSystem.scala:224) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.adjustSharedInputPaths(SgeBackend.scala:60) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.adjustInputPaths(SgeBackend.scala:67) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.instantiateCommand(SgeBackend.scala:260) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:84) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:82) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {; sh ${pipeline_path}/RunSTARAlignment.sh ${fastq1} ${fastq2} ${sample_name} ${genome_dir} ${output_dir}; }; output ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:4029,Performance,concurren,concurrent,4029,"ntiateCommand(SgeBackend.scala:260) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:84) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:82) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {; sh ${pipeline_path}/RunSTARAlignment.sh ${fastq1} ${fastq2} ${sample_name} ${genome_dir} ${output_dir}; }; output {; File genome_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.sortedByCoord.out.bam""; File trans_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.Aligned.toTranscriptome.out.bam""; }; runtime {; docker: ""cowmoo/cil-dev:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:4127,Performance,concurren,concurrent,4127,"allJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:84) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:82) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {; sh ${pipeline_path}/RunSTARAlignment.sh ${fastq1} ${fastq2} ${sample_name} ${genome_dir} ${output_dir}; }; output {; File genome_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.sortedByCoord.out.bam""; File trans_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.Aligned.toTranscriptome.out.bam""; }; runtime {; docker: ""cowmoo/cil-dev:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }. workflow smartseq_amr {; call RunSTARAlignment; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:4237,Performance,concurren,concurrent,4237,"engine.backend.BackendCallJobDescriptor.instantiateCommand(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:84) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:82) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {; sh ${pipeline_path}/RunSTARAlignment.sh ${fastq1} ${fastq2} ${sample_name} ${genome_dir} ${output_dir}; }; output {; File genome_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.sortedByCoord.out.bam""; File trans_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.Aligned.toTranscriptome.out.bam""; }; runtime {; docker: ""cowmoo/cil-dev:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }. workflow smartseq_amr {; call RunSTARAlignment; }; ```. JSON:. ```; {; ""smartseq_amr.RunSTARAlignment.fastq1"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:4339,Performance,concurren,concurrent,4339,":0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:84) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend$$anonfun$execute$1.apply(SgeBackend.scala:82) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40) ~[cromwell-0.19.jar:0.19]; at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {; sh ${pipeline_path}/RunSTARAlignment.sh ${fastq1} ${fastq2} ${sample_name} ${genome_dir} ${output_dir}; }; output {; File genome_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.sortedByCoord.out.bam""; File trans_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.Aligned.toTranscriptome.out.bam""; }; runtime {; docker: ""cowmoo/cil-dev:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }. workflow smartseq_amr {; call RunSTARAlignment; }; ```. JSON:. ```; {; ""smartseq_amr.RunSTARAlignment.fastq1"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq "",; ""smartseq_amr.RunS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:5754,Testability,sandbox,sandbox,5754,"cutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {; sh ${pipeline_path}/RunSTARAlignment.sh ${fastq1} ${fastq2} ${sample_name} ${genome_dir} ${output_dir}; }; output {; File genome_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.sortedByCoord.out.bam""; File trans_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.Aligned.toTranscriptome.out.bam""; }; runtime {; docker: ""cowmoo/cil-dev:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }. workflow smartseq_amr {; call RunSTARAlignment; }; ```. JSON:. ```; {; ""smartseq_amr.RunSTARAlignment.fastq1"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq "",; ""smartseq_amr.RunSTARAlignment.fastq2"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R2_001.fastq"",; ""smartseq_amr.RunSTARAlignment.sample_name"": ""Mouse-A2-single"",; ""smartseq_amr.RunSTARAlignment.genome_dir"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/Resources/Mouse10"",; ""smartseq_amr.RunSTARAlignment.output_dir"": ""/cil/shed/sandbox/amr/smartseq/test"",; ""smartseq_amr.RunSTARAlignment.pipeline_path"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1071:5775,Testability,test,test,5775,"cutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell-0.19.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell-0.19.jar:0.19]; ```. WDL:. ```; task RunSTARAlignment {; File fastq1; File fastq2; String sample_name; String genome_dir; String output_dir; String pipeline_path; command {; sh ${pipeline_path}/RunSTARAlignment.sh ${fastq1} ${fastq2} ${sample_name} ${genome_dir} ${output_dir}; }; output {; File genome_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.sortedByCoord.out.bam""; File trans_alignment = ""${output_dir}/alignments/${sample_name}.Aligned.Aligned.toTranscriptome.out.bam""; }; runtime {; docker: ""cowmoo/cil-dev:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }. workflow smartseq_amr {; call RunSTARAlignment; }; ```. JSON:. ```; {; ""smartseq_amr.RunSTARAlignment.fastq1"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq "",; ""smartseq_amr.RunSTARAlignment.fastq2"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R2_001.fastq"",; ""smartseq_amr.RunSTARAlignment.sample_name"": ""Mouse-A2-single"",; ""smartseq_amr.RunSTARAlignment.genome_dir"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/Resources/Mouse10"",; ""smartseq_amr.RunSTARAlignment.output_dir"": ""/cil/shed/sandbox/amr/smartseq/test"",; ""smartseq_amr.RunSTARAlignment.pipeline_path"": ""/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071
https://github.com/broadinstitute/cromwell/issues/1072:1814,Availability,error,error,1814,"rwx 1 dshih broad 72 Jun 27 10:44 seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta -> /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta*; ```. However, the pon file in the same task is hard-linked:. ``` bash; $ ls -l xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; -rw-r--r--+ 3 dshih broad 1020487624 May 10 11:35 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon; ```. The number of references for this file is 3 (instead of 1), because I ran the workflow twice and both times, the localized file is hard-linked. All 3 files have the same inode number (14733355). I looked at the log file, but it was not terribly revealing... No error had occurred. No attempt at soft linking was logged. I was able to manually soft link the file in question. ``` bash; $ uname -a; Linux cga02 2.6.32-573.26.1.el6.x86_64 #1 SMP Tue Apr 12 01:47:01 EDT 2016 x86_64 x86_64 x86_64 GNU/Linux. $ java -version; java version ""1.8.0_92""; Java(TM) SE Runtime Environment (build 1.8.0_92-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode); ```. Both the wdl file and the log file are attached. [WesCopyNumber.wdl.txt](https://github.com/broadinstitute/cromwell/files/335331/WesCopyNumber.wdl.txt); [workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt](https://github.com/broadinstitute/cromwell/files/335332/workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072
https://github.com/broadinstitute/cromwell/issues/1072:87,Deployability,release,release,87,"I compiled the develop branch (after commit b246001).; (I had previously used the 0.19 release and 0.19_hotfix, but both refused to soft link or hard link.). When cromwell-0.20-b246001 is invoked with. ```; cromwell -jar ~/java/cromwell-0.20-b246001.jar \; -Dbackend.shared-filesystem.localization.0=soft-link; ```. Some of the files are soft linked, while other files are hard-linked in the same run of a task. The fasta file is soft-linked:. ``` bash; $ ls -l seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; lrwxrwxrwx 1 dshih broad 72 Jun 27 10:44 seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta -> /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta*; ```. However, the pon file in the same task is hard-linked:. ``` bash; $ ls -l xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; -rw-r--r--+ 3 dshih broad 1020487624 May 10 11:35 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon; ```. The number of references for this file is 3 (instead of 1), because I ran the workflow twice and both times, the localized file is hard-linked. All 3 files have the same inode number (14733355). I looked at the log file, but it was not terribly revealing... No error had occurred. No attempt at soft linking was logged. I was able to manually soft link the file in question. ``` bash; $ uname -a; Linux cga02 2.6.32-573.26.1.el6.x86_64 #1 SMP Tue A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072
https://github.com/broadinstitute/cromwell/issues/1072:1764,Testability,log,log,1764,"rwx 1 dshih broad 72 Jun 27 10:44 seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta -> /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta*; ```. However, the pon file in the same task is hard-linked:. ``` bash; $ ls -l xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; -rw-r--r--+ 3 dshih broad 1020487624 May 10 11:35 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon; ```. The number of references for this file is 3 (instead of 1), because I ran the workflow twice and both times, the localized file is hard-linked. All 3 files have the same inode number (14733355). I looked at the log file, but it was not terribly revealing... No error had occurred. No attempt at soft linking was logged. I was able to manually soft link the file in question. ``` bash; $ uname -a; Linux cga02 2.6.32-573.26.1.el6.x86_64 #1 SMP Tue Apr 12 01:47:01 EDT 2016 x86_64 x86_64 x86_64 GNU/Linux. $ java -version; java version ""1.8.0_92""; Java(TM) SE Runtime Environment (build 1.8.0_92-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode); ```. Both the wdl file and the log file are attached. [WesCopyNumber.wdl.txt](https://github.com/broadinstitute/cromwell/files/335331/WesCopyNumber.wdl.txt); [workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt](https://github.com/broadinstitute/cromwell/files/335332/workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072
https://github.com/broadinstitute/cromwell/issues/1072:1865,Testability,log,logged,1865,"rwx 1 dshih broad 72 Jun 27 10:44 seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta -> /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta*; ```. However, the pon file in the same task is hard-linked:. ``` bash; $ ls -l xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; -rw-r--r--+ 3 dshih broad 1020487624 May 10 11:35 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon; ```. The number of references for this file is 3 (instead of 1), because I ran the workflow twice and both times, the localized file is hard-linked. All 3 files have the same inode number (14733355). I looked at the log file, but it was not terribly revealing... No error had occurred. No attempt at soft linking was logged. I was able to manually soft link the file in question. ``` bash; $ uname -a; Linux cga02 2.6.32-573.26.1.el6.x86_64 #1 SMP Tue Apr 12 01:47:01 EDT 2016 x86_64 x86_64 x86_64 GNU/Linux. $ java -version; java version ""1.8.0_92""; Java(TM) SE Runtime Environment (build 1.8.0_92-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode); ```. Both the wdl file and the log file are attached. [WesCopyNumber.wdl.txt](https://github.com/broadinstitute/cromwell/files/335331/WesCopyNumber.wdl.txt); [workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt](https://github.com/broadinstitute/cromwell/files/335332/workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072
https://github.com/broadinstitute/cromwell/issues/1072:2249,Testability,log,log,2249,"rwx 1 dshih broad 72 Jun 27 10:44 seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta -> /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta*; ```. However, the pon file in the same task is hard-linked:. ``` bash; $ ls -l xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; -rw-r--r--+ 3 dshih broad 1020487624 May 10 11:35 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon; ```. The number of references for this file is 3 (instead of 1), because I ran the workflow twice and both times, the localized file is hard-linked. All 3 files have the same inode number (14733355). I looked at the log file, but it was not terribly revealing... No error had occurred. No attempt at soft linking was logged. I was able to manually soft link the file in question. ``` bash; $ uname -a; Linux cga02 2.6.32-573.26.1.el6.x86_64 #1 SMP Tue Apr 12 01:47:01 EDT 2016 x86_64 x86_64 x86_64 GNU/Linux. $ java -version; java version ""1.8.0_92""; Java(TM) SE Runtime Environment (build 1.8.0_92-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode); ```. Both the wdl file and the log file are attached. [WesCopyNumber.wdl.txt](https://github.com/broadinstitute/cromwell/files/335331/WesCopyNumber.wdl.txt); [workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt](https://github.com/broadinstitute/cromwell/files/335332/workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072
https://github.com/broadinstitute/cromwell/issues/1072:2423,Testability,log,log,2423,"rwx 1 dshih broad 72 Jun 27 10:44 seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta -> /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta*; ```. However, the pon file in the same task is hard-linked:. ``` bash; $ ls -l xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; -rw-r--r--+ 3 dshih broad 1020487624 May 10 11:35 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon; ```. The number of references for this file is 3 (instead of 1), because I ran the workflow twice and both times, the localized file is hard-linked. All 3 files have the same inode number (14733355). I looked at the log file, but it was not terribly revealing... No error had occurred. No attempt at soft linking was logged. I was able to manually soft link the file in question. ``` bash; $ uname -a; Linux cga02 2.6.32-573.26.1.el6.x86_64 #1 SMP Tue Apr 12 01:47:01 EDT 2016 x86_64 x86_64 x86_64 GNU/Linux. $ java -version; java version ""1.8.0_92""; Java(TM) SE Runtime Environment (build 1.8.0_92-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode); ```. Both the wdl file and the log file are attached. [WesCopyNumber.wdl.txt](https://github.com/broadinstitute/cromwell/files/335331/WesCopyNumber.wdl.txt); [workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt](https://github.com/broadinstitute/cromwell/files/335332/workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072
https://github.com/broadinstitute/cromwell/issues/1072:2534,Testability,log,log,2534,"rwx 1 dshih broad 72 Jun 27 10:44 seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta -> /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta*; ```. However, the pon file in the same task is hard-linked:. ``` bash; $ ls -l xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; -rw-r--r--+ 3 dshih broad 1020487624 May 10 11:35 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon. $ ls -i /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon ; 14733355 /xchip/scarter/dshih/local/opt/gatk4cnv-20160112/pons/ice_female_pd250/create_pon/ice_female_pd250_spark.pon; ```. The number of references for this file is 3 (instead of 1), because I ran the workflow twice and both times, the localized file is hard-linked. All 3 files have the same inode number (14733355). I looked at the log file, but it was not terribly revealing... No error had occurred. No attempt at soft linking was logged. I was able to manually soft link the file in question. ``` bash; $ uname -a; Linux cga02 2.6.32-573.26.1.el6.x86_64 #1 SMP Tue Apr 12 01:47:01 EDT 2016 x86_64 x86_64 x86_64 GNU/Linux. $ java -version; java version ""1.8.0_92""; Java(TM) SE Runtime Environment (build 1.8.0_92-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode); ```. Both the wdl file and the log file are attached. [WesCopyNumber.wdl.txt](https://github.com/broadinstitute/cromwell/files/335331/WesCopyNumber.wdl.txt); [workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt](https://github.com/broadinstitute/cromwell/files/335332/workflow.e9597059-476c-4797-8e8c-fca4382b9ef3.log.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072
https://github.com/broadinstitute/cromwell/issues/1075:24,Deployability,configurat,configuration,24,"The MWDA should use the configuration it is passed when determining backend assignments, not rely on the `shadowy` variables in `CromwellBackends` as it does currently. Re-enable the `""assign backends based on runtime attributes""`test in `MaterialiseWorkflowDescriptorActorSpec`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1075
https://github.com/broadinstitute/cromwell/issues/1075:24,Modifiability,config,configuration,24,"The MWDA should use the configuration it is passed when determining backend assignments, not rely on the `shadowy` variables in `CromwellBackends` as it does currently. Re-enable the `""assign backends based on runtime attributes""`test in `MaterialiseWorkflowDescriptorActorSpec`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1075
https://github.com/broadinstitute/cromwell/issues/1075:115,Modifiability,variab,variables,115,"The MWDA should use the configuration it is passed when determining backend assignments, not rely on the `shadowy` variables in `CromwellBackends` as it does currently. Re-enable the `""assign backends based on runtime attributes""`test in `MaterialiseWorkflowDescriptorActorSpec`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1075
https://github.com/broadinstitute/cromwell/issues/1075:230,Testability,test,test,230,"The MWDA should use the configuration it is passed when determining backend assignments, not rely on the `shadowy` variables in `CromwellBackends` as it does currently. Re-enable the `""assign backends based on runtime attributes""`test in `MaterialiseWorkflowDescriptorActorSpec`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1075
https://github.com/broadinstitute/cromwell/issues/1076:24,Modifiability,variab,variables,24,"The assignment of these variables into each task's descriptor naturally fits in MaterializeWorkflowDescriptorActor rather than relying on the backends to re-implement it. Re-enable test `""assign default runtime attributes""` in `MaterializeWorkflowDescriptorActorSpec`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076
https://github.com/broadinstitute/cromwell/issues/1076:181,Testability,test,test,181,"The assignment of these variables into each task's descriptor naturally fits in MaterializeWorkflowDescriptorActor rather than relying on the backends to re-implement it. Re-enable test `""assign default runtime attributes""` in `MaterializeWorkflowDescriptorActorSpec`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076
https://github.com/broadinstitute/cromwell/pull/1077:89,Testability,test,test,89,"Ok, ok. This is less ""fixup"" and more ""start again and try to preserve the spirit of the test cases""...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1077
https://github.com/broadinstitute/cromwell/issues/1079:97,Integrability,message,message,97,"If `BackendWorkflowInitializationActor` fails to instantiate, it never receives the `Initialize` message and the WorkflowActor never gets a response. So the workflow hangs forever. From a more general perspective, if any actor in the lifecycle craches/dies unexpectedly, the workfow will hang at whichever state it was in..",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1079
https://github.com/broadinstitute/cromwell/pull/1080:40,Availability,down,down,40,Alternative title: kicking the SDAS can down the road until 0.21 (when these methods will make sense),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1080
https://github.com/broadinstitute/cromwell/issues/1081:63,Testability,test,test,63,"This is an extension of #1026, that will not fix this specific test. If mocking is not possible that it may require that an actual refresh token is used?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1081
https://github.com/broadinstitute/cromwell/issues/1081:72,Testability,mock,mocking,72,"This is an extension of #1026, that will not fix this specific test. If mocking is not possible that it may require that an actual refresh token is used?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1081
https://github.com/broadinstitute/cromwell/issues/1082:63,Testability,test,test,63,"This is an extension of #1026, that will not fix this specific test. If mocking is not possible that it may require that an actual refresh token is used?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1082
https://github.com/broadinstitute/cromwell/issues/1082:72,Testability,mock,mocking,72,"This is an extension of #1026, that will not fix this specific test. If mocking is not possible that it may require that an actual refresh token is used?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1082
https://github.com/broadinstitute/cromwell/pull/1091:82,Availability,down,down,82,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091
https://github.com/broadinstitute/cromwell/pull/1091:363,Deployability,integrat,integration,363,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091
https://github.com/broadinstitute/cromwell/pull/1091:435,Deployability,Integrat,IntegrationTest,435,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091
https://github.com/broadinstitute/cromwell/pull/1091:363,Integrability,integrat,integration,363,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091
https://github.com/broadinstitute/cromwell/pull/1091:435,Integrability,Integrat,IntegrationTest,435,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091
https://github.com/broadinstitute/cromwell/pull/1091:126,Testability,Test,TestKitSuite,126,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091
https://github.com/broadinstitute/cromwell/pull/1091:155,Testability,Test,TestKit,155,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091
https://github.com/broadinstitute/cromwell/pull/1091:375,Testability,test,test,375,"Additionally:; Ignoring `PostMVP` specs to allow `sbt alltests` to run.; Shutting down many more actor systems by creating a `TestKitSuite` mixing Akka's `TestKit` and ScalaTest's `Suite`.; DRYed out some backend specs with a `BackendSpec`.; Moved more classes/files from `engine` to `core`, including `SampleWdl`, `application.conf`, etc.; Gave more time to the integration test `SprayDockerRegistryApiClientSpec`.; `DockerTest` and `IntegrationTest` tags now in `crowell.core`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1091
https://github.com/broadinstitute/cromwell/issues/1094:21,Testability,test,test,21,"After a restart, the test may run just fine. ```; [info] MainSpec:; [info] Main; [info] - should print usage; [info] - should print usage when no args; [info] - should run *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run using args *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run and locate the default inputs path *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run if the inputs path is ""-"" *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run reading options *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run writing metadata *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run if the inputs path is not found, and not set to -; [info] - should fail run if the inputs path is not readable; [info] - should fail run if the inputs path is not valid inputs json *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run with not enough args; [info] - should fail run with too many args; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094
https://github.com/broadinstitute/cromwell/issues/1094:199,Testability,test,test,199,"After a restart, the test may run just fine. ```; [info] MainSpec:; [info] Main; [info] - should print usage; [info] - should print usage when no args; [info] - should run *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run using args *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run and locate the default inputs path *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run if the inputs path is ""-"" *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run reading options *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run writing metadata *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run if the inputs path is not found, and not set to -; [info] - should fail run if the inputs path is not readable; [info] - should fail run if the inputs path is not valid inputs json *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run with not enough args; [info] - should fail run with too many args; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094
https://github.com/broadinstitute/cromwell/issues/1094:366,Testability,test,test,366,"After a restart, the test may run just fine. ```; [info] MainSpec:; [info] Main; [info] - should print usage; [info] - should print usage when no args; [info] - should run *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run using args *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run and locate the default inputs path *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run if the inputs path is ""-"" *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run reading options *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run writing metadata *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run if the inputs path is not found, and not set to -; [info] - should fail run if the inputs path is not readable; [info] - should fail run if the inputs path is not valid inputs json *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run with not enough args; [info] - should fail run with too many args; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094
https://github.com/broadinstitute/cromwell/issues/1094:557,Testability,test,test,557,"After a restart, the test may run just fine. ```; [info] MainSpec:; [info] Main; [info] - should print usage; [info] - should print usage when no args; [info] - should run *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run using args *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run and locate the default inputs path *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run if the inputs path is ""-"" *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run reading options *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run writing metadata *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run if the inputs path is not found, and not set to -; [info] - should fail run if the inputs path is not readable; [info] - should fail run if the inputs path is not valid inputs json *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run with not enough args; [info] - should fail run with too many args; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094
https://github.com/broadinstitute/cromwell/issues/1094:739,Testability,test,test,739,"After a restart, the test may run just fine. ```; [info] MainSpec:; [info] Main; [info] - should print usage; [info] - should print usage when no args; [info] - should run *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run using args *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run and locate the default inputs path *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run if the inputs path is ""-"" *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run reading options *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run writing metadata *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run if the inputs path is not found, and not set to -; [info] - should fail run if the inputs path is not readable; [info] - should fail run if the inputs path is not valid inputs json *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run with not enough args; [info] - should fail run with too many args; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094
https://github.com/broadinstitute/cromwell/issues/1094:911,Testability,test,test,911,"After a restart, the test may run just fine. ```; [info] MainSpec:; [info] Main; [info] - should print usage; [info] - should print usage when no args; [info] - should run *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run using args *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run and locate the default inputs path *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run if the inputs path is ""-"" *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run reading options *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run writing metadata *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run if the inputs path is not found, and not set to -; [info] - should fail run if the inputs path is not readable; [info] - should fail run if the inputs path is not valid inputs json *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run with not enough args; [info] - should fail run with too many args; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094
https://github.com/broadinstitute/cromwell/issues/1094:1084,Testability,test,test,1084,"After a restart, the test may run just fine. ```; [info] MainSpec:; [info] Main; [info] - should print usage; [info] - should print usage when no args; [info] - should run *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run using args *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run and locate the default inputs path *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run if the inputs path is ""-"" *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run reading options *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run writing metadata *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run if the inputs path is not found, and not set to -; [info] - should fail run if the inputs path is not readable; [info] - should fail run if the inputs path is not valid inputs json *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run with not enough args; [info] - should fail run with too many args; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094
https://github.com/broadinstitute/cromwell/issues/1094:1426,Testability,test,test,1426,"After a restart, the test may run just fine. ```; [info] MainSpec:; [info] Main; [info] - should print usage; [info] - should print usage when no args; [info] - should run *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run using args *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run and locate the default inputs path *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run if the inputs path is ""-"" *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run reading options *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should run writing metadata *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run if the inputs path is not found, and not set to -; [info] - should fail run if the inputs path is not readable; [info] - should fail run if the inputs path is not valid inputs json *** FAILED ***; [info] The test did not complete within the specified 60000000000 nanoseconds time limit. (TimeLimitedTests.scala:137); [info] - should fail run with not enough args; [info] - should fail run with too many args; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1094
https://github.com/broadinstitute/cromwell/issues/1101:144,Integrability,message,message,144,Looks to be caused by . ```; [WARN] [06/30/2016 11:18:20.900] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/workflow-actor-747f3560-2635-4390-b126-fd8249b44bea]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1101
https://github.com/broadinstitute/cromwell/issues/1101:63,Testability,test,test-system-akka,63,Looks to be caused by . ```; [WARN] [06/30/2016 11:18:20.900] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/workflow-actor-747f3560-2635-4390-b126-fd8249b44bea]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1101
https://github.com/broadinstitute/cromwell/issues/1101:116,Testability,test,test-system,116,Looks to be caused by . ```; [WARN] [06/30/2016 11:18:20.900] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/workflow-actor-747f3560-2635-4390-b126-fd8249b44bea]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1101
https://github.com/broadinstitute/cromwell/issues/1101:157,Testability,Test,TestActor,157,Looks to be caused by . ```; [WARN] [06/30/2016 11:18:20.900] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/workflow-actor-747f3560-2635-4390-b126-fd8249b44bea]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1101
https://github.com/broadinstitute/cromwell/issues/1101:174,Testability,test,test-system,174,Looks to be caused by . ```; [WARN] [06/30/2016 11:18:20.900] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/workflow-actor-747f3560-2635-4390-b126-fd8249b44bea]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1101
https://github.com/broadinstitute/cromwell/pull/1102:79,Testability,test,test-only,79,"With apologies to @Horneth, I merged too soon! This unfortunately caused `sbt ""test-only cromwellSimpleWorkflowActorSpec""` to hang indefinitely.; - Lazy-fiy workflowPaths to prevent actor from throwing during instantiation; - Handle ActorInitFailure with supervision; - with default'; - cleanup Workflow finalization; - clean imports; - PR commnet",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1102
https://github.com/broadinstitute/cromwell/issues/1103:61,Availability,down,download,61,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103
https://github.com/broadinstitute/cromwell/issues/1103:155,Availability,down,download,155,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103
https://github.com/broadinstitute/cromwell/issues/1103:52,Deployability,release,releases,52,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103
https://github.com/broadinstitute/cromwell/issues/1103:146,Deployability,release,releases,146,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103
https://github.com/broadinstitute/cromwell/issues/1103:223,Deployability,release,release,223,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103
https://github.com/broadinstitute/cromwell/issues/1103:429,Deployability,update,update,429,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103
https://github.com/broadinstitute/cromwell/issues/1103:546,Deployability,update,updated,546,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103
https://github.com/broadinstitute/cromwell/issues/1103:383,Testability,log,logic,383,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103
https://github.com/broadinstitute/cromwell/issues/1103:422,Usability,simpl,simply,422,"0.19.3:; https://github.com/broadinstitute/cromwell/releases/download/0.19.3/cromwell-0.19.jar. 0.19:; https://github.com/broadinstitute/cromwell/releases/download/0.19/cromwell-0.19.jar. I noticed you haven't done a point release before, so I wasn't sure whether the 0.19.jar for 0.19.3 was intentional or a mistake. It tripped up the Homebrew formula because we have `-#{version}` logic in the formula itself, but I can simply update that section if the identical naming was intentional. Or if not, I can leave that section alone if the URL is updated to point to a file named cromwell-0.19.3.jar. Cf. https://github.com/Homebrew/homebrew-core/pull/2511",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1103
https://github.com/broadinstitute/cromwell/pull/1106:264,Modifiability,refactor,refactor,264,"I offer for your consideration a PBSPro/Torque backend that we are using here at QIMR Berghofer (based on SGE one), if you think it might be useful to encourage external user uptake in the short term. I was going to hold off until you had finished the PBE work to refactor this functionality to work with that but I see that you've accepted a PR for LSF backend using the current structure and I don't know what your broader priorities are. If you don't think it's worth your time to review then that's ok too :)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106
https://github.com/broadinstitute/cromwell/issues/1109:38,Safety,abort,abortInitialization,38,Actual code:. ``` scala; override def abortInitialization(): Unit = ???; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1109
https://github.com/broadinstitute/cromwell/issues/1110:38,Safety,abort,abortInitialization,38,Actual code:. ``` scala; override def abortInitialization(): Unit = ???; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1110
https://github.com/broadinstitute/cromwell/issues/1111:38,Safety,abort,abortInitialization,38,Actual code:. ``` scala; override def abortInitialization(): Unit = ???; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1111
https://github.com/broadinstitute/cromwell/issues/1112:176,Safety,abort,abortInitialization,176,(We don't actually have a pluggable SGE backend ATM but just entering this for completeness since the other backends had the same issue). Actual code:. ``` scala; override def abortInitialization(): Unit = ???; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1112
https://github.com/broadinstitute/cromwell/issues/1115:369,Modifiability,extend,extends,369,"Add a Job Store backed by _filesystem_ stuff, which receives the following commands:. ```; registerJobComplete(jobKey: JobKey, jobResults: JobResults); isJobAlreadyCompleted(jobKey: JobKey): JobCompletedYetStatus; notifyWorkflowComplete(workflowId: WorkflowId); ```. with JobCompletedYetStatus being one of:. ```; case class JobCompletedAlready(jobResults: JobResults) extends JobCompletedYetStatus; case object JobNotCompletedYet extends JobCompletedYetStatus; ```. `registerJobComplete` adds to the database.; `notifyWorkflowComplete` removes all appropriate jobs from the database. (all class names given above are **not** final!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1115
https://github.com/broadinstitute/cromwell/issues/1115:431,Modifiability,extend,extends,431,"Add a Job Store backed by _filesystem_ stuff, which receives the following commands:. ```; registerJobComplete(jobKey: JobKey, jobResults: JobResults); isJobAlreadyCompleted(jobKey: JobKey): JobCompletedYetStatus; notifyWorkflowComplete(workflowId: WorkflowId); ```. with JobCompletedYetStatus being one of:. ```; case class JobCompletedAlready(jobResults: JobResults) extends JobCompletedYetStatus; case object JobNotCompletedYet extends JobCompletedYetStatus; ```. `registerJobComplete` adds to the database.; `notifyWorkflowComplete` removes all appropriate jobs from the database. (all class names given above are **not** final!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1115
https://github.com/broadinstitute/cromwell/issues/1116:379,Deployability,a/b,a/broadinstitute,379,"Add an **Engine** JobExecutionActor which can act as the representative of a single Job within the engine. When asked to restart a job, it should first reference the JobStore (#1115) to see whether it has already been completed. If a job is new or not-yet-completed, it should be forwarded to the BackendJobExecutionActor, as before. See actor diagram in https://docs.google.com/a/broadinstitute.com/document/d/1U3SbbRMyNUCs2uHHhG-NvC7P6sPBJXN2WvP0SJ4vMco/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1116
https://github.com/broadinstitute/cromwell/issues/1117:0,Integrability,Depend,Depends,0,Depends on #1116. This is currently in the WorkflowExecutionActor but is Job-specific and probably belongs more in the EJEA.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117
https://github.com/broadinstitute/cromwell/issues/1118:18,Deployability,Update,Update,18,Depends on #1114. Update the WorkflowStore to resubmit non-complete jobs as restarts when the system comes up.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1118
https://github.com/broadinstitute/cromwell/issues/1118:0,Integrability,Depend,Depends,0,Depends on #1114. Update the WorkflowStore to resubmit non-complete jobs as restarts when the system comes up.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1118
https://github.com/broadinstitute/cromwell/issues/1119:12,Deployability,upgrade,upgrades,12,"When a user upgrades a production instance of cromwell from 0.19 -> 0.21, we need to be able to preserve the state of running workflows so that when the server restarts their workflows are resumed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1119
https://github.com/broadinstitute/cromwell/issues/1119:189,Usability,resume,resumed,189,"When a user upgrades a production instance of cromwell from 0.19 -> 0.21, we need to be able to preserve the state of running workflows so that when the server restarts their workflows are resumed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1119
https://github.com/broadinstitute/cromwell/issues/1120:82,Deployability,upgrade,upgrade,82,"Review the documentation for 0.21 and make sure it is still accurate, and provide upgrade instructions for users coming from 0.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1120
https://github.com/broadinstitute/cromwell/issues/1123:30,Testability,log,logic,30,"As with the restart and retry logic, this is more naturally part of `EngineJobExecutionActor` than the `WorkflowExecutionActor`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1123
https://github.com/broadinstitute/cromwell/issues/1126:54,Safety,abort,aborted,54,"When running a task on docker locally, if the task is aborted, the script that feeds the wdl command into the docker container will be destroyed - but not the docker itself. ; Before abort:. ```; wm0e9-683:cromwell tjeandet$ ps -ef | grep c72df249-ccf5-460b-8654-9606a655c669; 1250915218 11002 10990 0 6:13PM ?? 0:00.00 /bin/bash -c cat cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t/script | docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash <&0; 1250915218 11004 11002 0 6:13PM ?? 0:00.15 docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash; 1250915218 11015 2783 0 6:13PM ttys000 0:00.00 grep c72df249-ccf5-460b-8654-9606a655c669; ```. After:. ```; wm0e9-683:cromwell tjeandet$ ps -ef | grep c72df249-ccf5-460b-8654-9606a655c669; 1250915218 11004 1 0 6:13PM ?? 0:00.15 docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash; 1250915218 11021 2783 0 6:14PM ttys000 0:00.00 grep c72df249-ccf5-460b-8654-9606a655c669; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126
https://github.com/broadinstitute/cromwell/issues/1126:183,Safety,abort,abort,183,"When running a task on docker locally, if the task is aborted, the script that feeds the wdl command into the docker container will be destroyed - but not the docker itself. ; Before abort:. ```; wm0e9-683:cromwell tjeandet$ ps -ef | grep c72df249-ccf5-460b-8654-9606a655c669; 1250915218 11002 10990 0 6:13PM ?? 0:00.00 /bin/bash -c cat cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t/script | docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash <&0; 1250915218 11004 11002 0 6:13PM ?? 0:00.15 docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash; 1250915218 11015 2783 0 6:13PM ttys000 0:00.00 grep c72df249-ccf5-460b-8654-9606a655c669; ```. After:. ```; wm0e9-683:cromwell tjeandet$ ps -ef | grep c72df249-ccf5-460b-8654-9606a655c669; 1250915218 11004 1 0 6:13PM ?? 0:00.15 docker run --rm -v /Users/tjeandet/Codebase/cromwell/cromwell-executions/w/c72df249-ccf5-460b-8654-9606a655c669/call-t:/root/w/c72df249-ccf5-460b-8654-9606a655c669/call-t -i ubuntu:latest /bin/bash; 1250915218 11021 2783 0 6:14PM ttys000 0:00.00 grep c72df249-ccf5-460b-8654-9606a655c669; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126
https://github.com/broadinstitute/cromwell/issues/1128:509,Availability,failure,failure,509,"While debugging an issue with @helgridly we noticed that when Cromwell comes up & it tries to start up the DB (currently via the EngineMetadataServiceActor) it will eventually call SchemaManager.updateSchema. . If this call fails for some reason Cromwell continues to run even though any call to the metadata service (He noticed it via metadata requests) will fail. Recommendation: In any subsystem which a) we require to be running and b) it requires a DB, we should fail to initialize Cromwell if there's a failure in that space",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128
https://github.com/broadinstitute/cromwell/issues/1128:195,Deployability,update,updateSchema,195,"While debugging an issue with @helgridly we noticed that when Cromwell comes up & it tries to start up the DB (currently via the EngineMetadataServiceActor) it will eventually call SchemaManager.updateSchema. . If this call fails for some reason Cromwell continues to run even though any call to the metadata service (He noticed it via metadata requests) will fail. Recommendation: In any subsystem which a) we require to be running and b) it requires a DB, we should fail to initialize Cromwell if there's a failure in that space",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1128
https://github.com/broadinstitute/cromwell/pull/1132:286,Integrability,wrap,wrapping,286,"Spark backend pull request, Which is capable of running dockerized Spark jobs on a standalone Spark cluster (Note: There is not yet any resource manager involve between Spark driver and worker i.e. Mesos or Yarn or etc.). The way we are submitting jobs is like other backend HtCondor ( wrapping commands in script and expect rc code). Here goal is to show Cromwell able to submit/support containerized Spark application through wdl.; NOTE: **This PR is just to add Spark backend, so it is not creating Spark command internally right now rather than accepting full command in wdl command, Next PR i will add Spark submit command through backend like in HtCondor.** ; Formal reviewers: @geoffjentry @francares",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1132
https://github.com/broadinstitute/cromwell/issues/1134:3,Testability,test,testing,3,In testing my async submit branch I noticed that the batch submit endpoint will hang if you only supply a WDL and no inputs - which somewhat makes sense as it is batching on the inputs. I just checked and the same behavior occurs in develop so I'm (probably) not going to fix in the async submit branch,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1134
https://github.com/broadinstitute/cromwell/issues/1137:191,Availability,error,error,191,"As a user, like @yfarjoun, if I run a workflow against the JES backend and an input file for a task doesn't exist (e.g. gs://foo/bar/baz.txt does not exist). I would like to get back a clear error message that this is why the task failed. This is important because currently ""the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had an inkling that it's a missing file problem. so [you] have to resort to divide and conquer in order to identify the missing file...and that's a pain.""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137
https://github.com/broadinstitute/cromwell/issues/1137:280,Availability,error,error,280,"As a user, like @yfarjoun, if I run a workflow against the JES backend and an input file for a task doesn't exist (e.g. gs://foo/bar/baz.txt does not exist). I would like to get back a clear error message that this is why the task failed. This is important because currently ""the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had an inkling that it's a missing file problem. so [you] have to resort to divide and conquer in order to identify the missing file...and that's a pain.""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137
https://github.com/broadinstitute/cromwell/issues/1137:197,Integrability,message,message,197,"As a user, like @yfarjoun, if I run a workflow against the JES backend and an input file for a task doesn't exist (e.g. gs://foo/bar/baz.txt does not exist). I would like to get back a clear error message that this is why the task failed. This is important because currently ""the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had an inkling that it's a missing file problem. so [you] have to resort to divide and conquer in order to identify the missing file...and that's a pain.""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137
https://github.com/broadinstitute/cromwell/issues/1137:185,Usability,clear,clear,185,"As a user, like @yfarjoun, if I run a workflow against the JES backend and an input file for a task doesn't exist (e.g. gs://foo/bar/baz.txt does not exist). I would like to get back a clear error message that this is why the task failed. This is important because currently ""the error is so cryptic one cannot tell; that a file is causing the problem, nor which file it is, even if one had an inkling that it's a missing file problem. so [you] have to resort to divide and conquer in order to identify the missing file...and that's a pain.""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137
https://github.com/broadinstitute/cromwell/issues/1139:36,Safety,abort,abort,36,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139
https://github.com/broadinstitute/cromwell/issues/1139:99,Safety,abort,abort,99,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139
https://github.com/broadinstitute/cromwell/issues/1139:142,Safety,abort,abort,142,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139
https://github.com/broadinstitute/cromwell/issues/1139:247,Safety,abort,aborted,247,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139
https://github.com/broadinstitute/cromwell/issues/1139:269,Safety,abort,abort,269,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139
https://github.com/broadinstitute/cromwell/issues/1139:381,Safety,risk,risk,381,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139
https://github.com/broadinstitute/cromwell/issues/1139:616,Safety,abort,aborting,616,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139
https://github.com/broadinstitute/cromwell/issues/1139:991,Safety,Abort,Aborted,991,"As FireCloud ( @cbirger ) , I often abort workflows. Although JES currently makes a best effort to abort calls, sometimes those calls fail to abort. I would like to know through the cromwell call/workflow status the difference between ""definitely aborted"" and ""unknown abort status"". The reason this is important is that if I know the status is ""unknown"" I know that I might be at risk for being billed for machines I don't want and should take further action. </end of PO comment>. Technically this might require a little research and specifically work on JES. Ideally:; 1. We change our overall workflow status to aborting; 2. When we cancel a JES operation, the status of that operation will reflect reality.; 3. We can poll that operation until it reaches a terminal state (e.g. cancelled). This may actually just be for us to keep running the workflow like normal and just add canceled to the list of terminal states).; 4. Once all tasks are in a terminal state, the workflow status is Aborted. Question will be... if JES fails to terminate a job, should we change it's status to something like 'LOST' or 'UNKNOWN' after N minutes, or should we wait indefinitely. Since it's a best effort cancellation in JES we should handle this case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1139
https://github.com/broadinstitute/cromwell/issues/1140:697,Availability,down,downsampled-bams,697,"Cromwell deforms the path to JNI library while running wdl workflow. Printout of the **cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/script** file:; `... -Djava.library.path=/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux ...`. Path that is specified in the json file:; `""case_gatk_acnv_workflow.jni_lib"": ""/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux/`. JSON file path: **/dsde/working/asmirnov/TestCromwell/downsampled-bams.json**; Cromwell output directory: **/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140
https://github.com/broadinstitute/cromwell/issues/1140:269,Testability,Test,TestCromwell,269,"Cromwell deforms the path to JNI library while running wdl workflow. Printout of the **cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/script** file:; `... -Djava.library.path=/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux ...`. Path that is specified in the json file:; `""case_gatk_acnv_workflow.jni_lib"": ""/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux/`. JSON file path: **/dsde/working/asmirnov/TestCromwell/downsampled-bams.json**; Cromwell output directory: **/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140
https://github.com/broadinstitute/cromwell/issues/1140:684,Testability,Test,TestCromwell,684,"Cromwell deforms the path to JNI library while running wdl workflow. Printout of the **cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/script** file:; `... -Djava.library.path=/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux ...`. Path that is specified in the json file:; `""case_gatk_acnv_workflow.jni_lib"": ""/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux/`. JSON file path: **/dsde/working/asmirnov/TestCromwell/downsampled-bams.json**; Cromwell output directory: **/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140
https://github.com/broadinstitute/cromwell/issues/1140:774,Testability,Test,TestCromwell,774,"Cromwell deforms the path to JNI library while running wdl workflow. Printout of the **cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/script** file:; `... -Djava.library.path=/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux ...`. Path that is specified in the json file:; `""case_gatk_acnv_workflow.jni_lib"": ""/broad/software/free/Linux/redhat_6_x86_64/pkgs/hdfview_2.9/HDFView/lib/linux/`. JSON file path: **/dsde/working/asmirnov/TestCromwell/downsampled-bams.json**; Cromwell output directory: **/dsde/working/asmirnov/TestCromwell/cromwell-executions/case_gatk_acnv_workflow/2ed49a7f-a6bb-4fac-8516-406eb6416268/call-TumorNormalizeSomaticReadCounts/**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1140
https://github.com/broadinstitute/cromwell/pull/1141:889,Availability,heartbeat,heartbeat,889,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:983,Availability,echo,echoing,983,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:131,Deployability,release,releases,131,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:461,Integrability,depend,dependency,461,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:504,Integrability,depend,depend,504,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:615,Integrability,depend,depend,615,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:653,Integrability,depend,dependencies,653,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:689,Integrability,depend,dependencies,689,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:401,Testability,test,test,401,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:456,Testability,test,test,456,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:537,Testability,test,tests,537,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:684,Testability,Test,Test,684,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:770,Testability,test,testkit,770,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1141:942,Testability,log,log,942,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141
https://github.com/broadinstitute/cromwell/pull/1143:47,Availability,avail,available,47,Minor change in order to make services library available for publishing. Formal reviewer: @kshakir.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1143
https://github.com/broadinstitute/cromwell/issues/1146:5,Deployability,pipeline,pipeline,5,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1146:237,Deployability,pipeline,pipelines,237,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1146:324,Deployability,pipeline,pipeline,324,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1146:172,Integrability,message,message,172,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1146:266,Safety,detect,detected,266,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1146:456,Safety,predict,predict,456,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1146:54,Testability,assert,assert-like,54,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1146:143,Testability,log,log,143,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1146:363,Testability,test,tests,363,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1146:514,Testability,test,test,514,"As a pipeline author, I would like the ability to add assert-like statements to my tasks. For example, after the task has run check the output log for the presence of some message. This is important because we often find problems in the pipelines that we could have detected with some basic checks that could be part of the pipeline. They are different than unit tests because these problems often only arise under unusual data conditions that are hard to predict. The current approach on this is for @yfarjoun to test run these ideas just within the command block of WDL as a way to figure out what this feature really looks like. Then from that set of we can figure out what features need to be added in order to support this in a natural, DRY way",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146
https://github.com/broadinstitute/cromwell/issues/1149:69,Testability,test,tests,69,This will be a precursor to then having tickets that un-ignore those tests (either by fixing or removing),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1149
https://github.com/broadinstitute/cromwell/issues/1150:381,Availability,down,downstream,381,"From @yfarjoun . > I've been playing around with some wdl on the methods cromwell that has caching turned on. I have some results I do not understand. I modified the bwa step in a small way for input that has already run all the way to HaplotypeCaller. Re-running that wdl shows that the upstream steps are fast (as expected), the BWA step is re-done (again, as expected), but the downstream steps have been used from cache...this seems wrong to me, after all, if the BWA has been re-run, all the downstream steps need to be re-run as well...could you explain what's going on?. For example, the ""PairedEndSingleSampleWorkflow.MergeBamAlignment"" step should have been recomputed as it is downstream of the BWA step. Here's a [timing diagram](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/timing) and [metadata](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/metadata): . metadata also attached here. Although we don't have call caching in 0.20+ we should understand this problem (or clarify why it's not a problem) so that we don't replicate the bug (if that's what it is)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1150
https://github.com/broadinstitute/cromwell/issues/1150:497,Availability,down,downstream,497,"From @yfarjoun . > I've been playing around with some wdl on the methods cromwell that has caching turned on. I have some results I do not understand. I modified the bwa step in a small way for input that has already run all the way to HaplotypeCaller. Re-running that wdl shows that the upstream steps are fast (as expected), the BWA step is re-done (again, as expected), but the downstream steps have been used from cache...this seems wrong to me, after all, if the BWA has been re-run, all the downstream steps need to be re-run as well...could you explain what's going on?. For example, the ""PairedEndSingleSampleWorkflow.MergeBamAlignment"" step should have been recomputed as it is downstream of the BWA step. Here's a [timing diagram](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/timing) and [metadata](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/metadata): . metadata also attached here. Although we don't have call caching in 0.20+ we should understand this problem (or clarify why it's not a problem) so that we don't replicate the bug (if that's what it is)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1150
https://github.com/broadinstitute/cromwell/issues/1150:687,Availability,down,downstream,687,"From @yfarjoun . > I've been playing around with some wdl on the methods cromwell that has caching turned on. I have some results I do not understand. I modified the bwa step in a small way for input that has already run all the way to HaplotypeCaller. Re-running that wdl shows that the upstream steps are fast (as expected), the BWA step is re-done (again, as expected), but the downstream steps have been used from cache...this seems wrong to me, after all, if the BWA has been re-run, all the downstream steps need to be re-run as well...could you explain what's going on?. For example, the ""PairedEndSingleSampleWorkflow.MergeBamAlignment"" step should have been recomputed as it is downstream of the BWA step. Here's a [timing diagram](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/timing) and [metadata](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/metadata): . metadata also attached here. Although we don't have call caching in 0.20+ we should understand this problem (or clarify why it's not a problem) so that we don't replicate the bug (if that's what it is)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1150
https://github.com/broadinstitute/cromwell/issues/1150:418,Performance,cache,cache,418,"From @yfarjoun . > I've been playing around with some wdl on the methods cromwell that has caching turned on. I have some results I do not understand. I modified the bwa step in a small way for input that has already run all the way to HaplotypeCaller. Re-running that wdl shows that the upstream steps are fast (as expected), the BWA step is re-done (again, as expected), but the downstream steps have been used from cache...this seems wrong to me, after all, if the BWA has been re-run, all the downstream steps need to be re-run as well...could you explain what's going on?. For example, the ""PairedEndSingleSampleWorkflow.MergeBamAlignment"" step should have been recomputed as it is downstream of the BWA step. Here's a [timing diagram](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/timing) and [metadata](https://cromwell.dsde-; methods.broadinstitute.org/api/workflows/v1/d69172b2-3b5d-44b3-aaec-5ed12dbb771f/metadata): . metadata also attached here. Although we don't have call caching in 0.20+ we should understand this problem (or clarify why it's not a problem) so that we don't replicate the bug (if that's what it is)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1150
https://github.com/broadinstitute/cromwell/pull/1151:9,Testability,test,tests,9,"The only tests remaining in JesBackendSpec were all related to RefreshToken behavior, so just renaming the spec to reflect what it's testing. The tests in this spec check if workflow options are incomplete and that the refresh token is decrypted when retrieving it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1151
https://github.com/broadinstitute/cromwell/pull/1151:133,Testability,test,testing,133,"The only tests remaining in JesBackendSpec were all related to RefreshToken behavior, so just renaming the spec to reflect what it's testing. The tests in this spec check if workflow options are incomplete and that the refresh token is decrypted when retrieving it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1151
https://github.com/broadinstitute/cromwell/pull/1151:146,Testability,test,tests,146,"The only tests remaining in JesBackendSpec were all related to RefreshToken behavior, so just renaming the spec to reflect what it's testing. The tests in this spec check if workflow options are incomplete and that the refresh token is decrypted when retrieving it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1151
https://github.com/broadinstitute/cromwell/pull/1152:642,Modifiability,extend,extending,642,"Create 2 new dispatchers: `api-dispatcher` and `engine-dispatcher`.; `api-dispatcher` isolates all actors / futures to ensure that every request will be using resources in this thread pool.; `engine-dispatcher` isolates engine actors from the rest of the system (specifically the backends that can run an arbitrary amount of blocking/slow code out of the control of the engine). ; `slow-actor-dispatcher` has been renamed to `io-dispatcher` and is used for io / blocking operations (copying outputs / logs / uploading files to gcs etc..). Other side quests:; - Re-use the WorkflowManagerSystem created in Main in `CromwellServer`, instead of extending `CromwellServer` with `WorkflowManagerSystem` which duplicates everything a second time.; - Add an `afterAll` on `CromwellTestKitSpec` to shutdown the test actor system at the end of the spec.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152
https://github.com/broadinstitute/cromwell/pull/1152:501,Testability,log,logs,501,"Create 2 new dispatchers: `api-dispatcher` and `engine-dispatcher`.; `api-dispatcher` isolates all actors / futures to ensure that every request will be using resources in this thread pool.; `engine-dispatcher` isolates engine actors from the rest of the system (specifically the backends that can run an arbitrary amount of blocking/slow code out of the control of the engine). ; `slow-actor-dispatcher` has been renamed to `io-dispatcher` and is used for io / blocking operations (copying outputs / logs / uploading files to gcs etc..). Other side quests:; - Re-use the WorkflowManagerSystem created in Main in `CromwellServer`, instead of extending `CromwellServer` with `WorkflowManagerSystem` which duplicates everything a second time.; - Add an `afterAll` on `CromwellTestKitSpec` to shutdown the test actor system at the end of the spec.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152
https://github.com/broadinstitute/cromwell/pull/1152:803,Testability,test,test,803,"Create 2 new dispatchers: `api-dispatcher` and `engine-dispatcher`.; `api-dispatcher` isolates all actors / futures to ensure that every request will be using resources in this thread pool.; `engine-dispatcher` isolates engine actors from the rest of the system (specifically the backends that can run an arbitrary amount of blocking/slow code out of the control of the engine). ; `slow-actor-dispatcher` has been renamed to `io-dispatcher` and is used for io / blocking operations (copying outputs / logs / uploading files to gcs etc..). Other side quests:; - Re-use the WorkflowManagerSystem created in Main in `CromwellServer`, instead of extending `CromwellServer` with `WorkflowManagerSystem` which duplicates everything a second time.; - Add an `afterAll` on `CromwellTestKitSpec` to shutdown the test actor system at the end of the spec.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1152
https://github.com/broadinstitute/cromwell/pull/1155:575,Availability,down,down,575,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155
https://github.com/broadinstitute/cromwell/pull/1155:422,Deployability,update,update,422,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155
https://github.com/broadinstitute/cromwell/pull/1155:589,Integrability,rout,route,589,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155
https://github.com/broadinstitute/cromwell/pull/1155:665,Safety,avoid,avoid,665,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155
https://github.com/broadinstitute/cromwell/pull/1155:504,Testability,test,tests,504,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155
https://github.com/broadinstitute/cromwell/pull/1155:353,Usability,clear,clear,353,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155
https://github.com/broadinstitute/cromwell/issues/1156:136,Availability,error,error,136,"I had accidently left my `filesystems` stanza to `refresh-token` mode instead of what it was supposed to be: `application-default`. The error was very cryptic and left me guessing. ```; [2016-07-13 10:12:45,36] [info] Slf4jLogger started; [2016-07-13 10:12:45,59] [info] RUN sub-command; [2016-07-13 10:12:45,59] [info] WDL file: 3step.wdl; [2016-07-13 10:12:45,59] [info] Inputs: 3step.inputs; [2016-07-13 10:12:45,59] [info] Workflow Options: 3step.options; [2016-07-13 10:12:45,61] [info] Slf4jLogger started; [2016-07-13 10:12:45,63] [info] SingleWorkflowRunnerActor: Launching workflow; [2016-07-13 10:12:45,63] [info] WorkflowManagerActor Starting workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,64] [info] SingleWorkflowRunnerActor: Workflow submitted dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,64] [info] WorkflowManagerActor Successfully started WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,67] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [2016-07-13 10:12:46,07] [info] Running with database db.url = jdbc:hsqldb:mem:937e84db-703a-4f18-8e6d-1a2a18227cf5;shutdown=false;hsqldb.tx=mvcc; [2016-07-13 10:12:46,43] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: Call-to-Backend assignments: three_step.ps -> JES, three_step.cgrep -> JES, three_step.wc -> JES; [2016-07-13 10:12:46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156
https://github.com/broadinstitute/cromwell/issues/1156:1682,Availability,down,down,1682," submitted dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,64] [info] WorkflowManagerActor Successfully started WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,67] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [2016-07-13 10:12:46,07] [info] Running with database db.url = jdbc:hsqldb:mem:937e84db-703a-4f18-8e6d-1a2a18227cf5;shutdown=false;hsqldb.tx=mvcc; [2016-07-13 10:12:46,43] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: Call-to-Backend assignments: three_step.ps -> JES, three_step.cgrep -> JES, three_step.wc -> JES; [2016-07-13 10:12:46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from InitializationPendingState to InitializationInProgressState.; [2016-07-13 10:12:46,62] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is now terminal. Shutting down.; [2016-07-13 10:12:46,62] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from InitializingWorkflowState to FinalizingWorkflowState; [2016-07-13 10:12:46,63] [info] WorkflowFinalizationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from FinalizationPendingState to WorkflowFinalizationFailedState.; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from FinalizingWorkflowState to WorkflowFailedState; [2016-07-13 10:12:46,63] [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156
https://github.com/broadinstitute/cromwell/issues/1156:2209,Availability,down,down,2209,"db-703a-4f18-8e6d-1a2a18227cf5;shutdown=false;hsqldb.tx=mvcc; [2016-07-13 10:12:46,43] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: Call-to-Backend assignments: three_step.ps -> JES, three_step.cgrep -> JES, three_step.wc -> JES; [2016-07-13 10:12:46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from InitializationPendingState to InitializationInProgressState.; [2016-07-13 10:12:46,62] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is now terminal. Shutting down.; [2016-07-13 10:12:46,62] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from InitializingWorkflowState to FinalizingWorkflowState; [2016-07-13 10:12:46,63] [info] WorkflowFinalizationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from FinalizationPendingState to WorkflowFinalizationFailedState.; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from FinalizingWorkflowState to WorkflowFailedState; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from FinalizingWorkflowState to WorkflowFailedState: shutting down; [2016-07-13 10:12:46,64] [error] WorkflowManagerActor Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 failed (during FinalizingWorkflowState): java.lang.Throwable: Google credentials are invalid: 401 Unauthorized; java.util.NoSuchElementException: None.get; [2016-07-13 10:12:46,64] [info] Wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156
https://github.com/broadinstitute/cromwell/issues/1156:2911,Availability,down,down,2911,"46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from InitializationPendingState to InitializationInProgressState.; [2016-07-13 10:12:46,62] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is now terminal. Shutting down.; [2016-07-13 10:12:46,62] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from InitializingWorkflowState to FinalizingWorkflowState; [2016-07-13 10:12:46,63] [info] WorkflowFinalizationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from FinalizationPendingState to WorkflowFinalizationFailedState.; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from FinalizingWorkflowState to WorkflowFailedState; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from FinalizingWorkflowState to WorkflowFailedState: shutting down; [2016-07-13 10:12:46,64] [error] WorkflowManagerActor Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 failed (during FinalizingWorkflowState): java.lang.Throwable: Google credentials are invalid: 401 Unauthorized; java.util.NoSuchElementException: None.get; [2016-07-13 10:12:46,64] [info] WorkflowManagerActor WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 is in a terminal state: WorkflowFailedState; [2016-07-13 10:12:49,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156
https://github.com/broadinstitute/cromwell/issues/1156:2943,Availability,error,error,2943,"46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from InitializationPendingState to InitializationInProgressState.; [2016-07-13 10:12:46,62] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is now terminal. Shutting down.; [2016-07-13 10:12:46,62] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from InitializingWorkflowState to FinalizingWorkflowState; [2016-07-13 10:12:46,63] [info] WorkflowFinalizationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning from FinalizationPendingState to WorkflowFinalizationFailedState.; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from FinalizingWorkflowState to WorkflowFailedState; [2016-07-13 10:12:46,63] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from FinalizingWorkflowState to WorkflowFailedState: shutting down; [2016-07-13 10:12:46,64] [error] WorkflowManagerActor Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 failed (during FinalizingWorkflowState): java.lang.Throwable: Google credentials are invalid: 401 Unauthorized; java.util.NoSuchElementException: None.get; [2016-07-13 10:12:46,64] [info] WorkflowManagerActor WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 is in a terminal state: WorkflowFailedState; [2016-07-13 10:12:49,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; Workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259 transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156
https://github.com/broadinstitute/cromwell/issues/1157:254,Availability,down,down,254,"I've been running the 3step workflow on JES. It gets to this point and stops. ```; [2016-07-13 12:52:24,77] [info] WorkflowActor-59abeeeb-eb34-49c6-83d0-c806e6968cb1 [59abeeeb]: transition from FinalizingWorkflowState to WorkflowSucceededState: shutting down; [2016-07-13 12:52:24,77] [info] WorkflowManagerActor Workflow 59abeeeb-eb34-49c6-83d0-c806e6968cb1 succeeded!; [2016-07-13 12:52:24,77] [info] WorkflowManagerActor WorkflowActor-59abeeeb-eb34-49c6-83d0-c806e6968cb1 is in a terminal state: WorkflowSucceededState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1157
https://github.com/broadinstitute/cromwell/issues/1158:199,Availability,echo,echo,199,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158
https://github.com/broadinstitute/cromwell/issues/1158:210,Availability,echo,echo,210,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158
https://github.com/broadinstitute/cromwell/issues/1158:219,Availability,echo,echo,219,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158
https://github.com/broadinstitute/cromwell/issues/1158:227,Availability,echo,echo,227,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158
https://github.com/broadinstitute/cromwell/issues/1158:133,Energy Efficiency,monitor,monitor,133,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158
https://github.com/broadinstitute/cromwell/issues/1158:159,Energy Efficiency,monitor,monitor,159,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158
https://github.com/broadinstitute/cromwell/issues/1158:334,Energy Efficiency,monitor,monitoring,334,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158
https://github.com/broadinstitute/cromwell/issues/1158:232,Testability,test,test,232,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158
https://github.com/broadinstitute/cromwell/issues/1158:345,Testability,log,log,345,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158
https://github.com/broadinstitute/cromwell/issues/1168:147,Availability,echo,echo,147,"If my understanding is correct, this should run call c. ```; task sleeper {; Int seconds; Int output_seconds; Int rc; command {; sleep ${seconds}; echo ${output_seconds} > done. echo ""exit ${rc}"" >> script.sh; chmod +x script.sh; ./script.sh; }; runtime {docker: ""ubuntu:latest""}; output {; Int o = read_int(""done""); }; }. workflow w {; call sleeper as a {input: seconds=1, output_seconds=0, rc=1}; call sleeper as b {input: seconds=300, output_seconds=30, rc=0}; call sleeper as c {input: seconds=b.o, output_seconds=0, rc=0}; }; ```. Options. ```; {; ""workflow_failure_mode"": ""ContinueWhilePossible""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1168
https://github.com/broadinstitute/cromwell/issues/1168:178,Availability,echo,echo,178,"If my understanding is correct, this should run call c. ```; task sleeper {; Int seconds; Int output_seconds; Int rc; command {; sleep ${seconds}; echo ${output_seconds} > done. echo ""exit ${rc}"" >> script.sh; chmod +x script.sh; ./script.sh; }; runtime {docker: ""ubuntu:latest""}; output {; Int o = read_int(""done""); }; }. workflow w {; call sleeper as a {input: seconds=1, output_seconds=0, rc=1}; call sleeper as b {input: seconds=300, output_seconds=30, rc=0}; call sleeper as c {input: seconds=b.o, output_seconds=0, rc=0}; }; ```. Options. ```; {; ""workflow_failure_mode"": ""ContinueWhilePossible""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1168
https://github.com/broadinstitute/cromwell/pull/1170:19,Testability,test,test,19,Associated centaur test: https://github.com/broadinstitute/centaur/pull/89,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1170
https://github.com/broadinstitute/cromwell/pull/1171:8,Deployability,update,updated,8,"haven't updated the call caching or restart parts yet. migration doc is missing database migration instructions for 0.21. Also @ruchim wanted to take over the API documentation, so I did not update that as part of this PR.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1171
https://github.com/broadinstitute/cromwell/pull/1171:191,Deployability,update,update,191,"haven't updated the call caching or restart parts yet. migration doc is missing database migration instructions for 0.21. Also @ruchim wanted to take over the API documentation, so I did not update that as part of this PR.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1171
https://github.com/broadinstitute/cromwell/issues/1176:348,Availability,error,error,348,"Hello. Here is the simple example of workflow with optional input: . ```; workflow wf {; String? inn; call tsk_grep { input: input_381012639=inn }; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. When i do not specify any input for workflow(json file below):. ```; {; }; ```. i get an the error:; `wdl4s.WdlExpressionException: Could not resolve inn as a scatter variable, namespace, call, or declaration`. But when my workflow looks like this:. ```; workflow wf {; call tsk_grep; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. Cromwell works as expected executing `ps aux | grep j`.; Is this a bug or i can only use optional variables inside of **task scope**?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1176
https://github.com/broadinstitute/cromwell/issues/1176:422,Modifiability,variab,variable,422,"Hello. Here is the simple example of workflow with optional input: . ```; workflow wf {; String? inn; call tsk_grep { input: input_381012639=inn }; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. When i do not specify any input for workflow(json file below):. ```; {; }; ```. i get an the error:; `wdl4s.WdlExpressionException: Could not resolve inn as a scatter variable, namespace, call, or declaration`. But when my workflow looks like this:. ```; workflow wf {; call tsk_grep; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. Cromwell works as expected executing `ps aux | grep j`.; Is this a bug or i can only use optional variables inside of **task scope**?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1176
https://github.com/broadinstitute/cromwell/issues/1176:745,Modifiability,variab,variables,745,"Hello. Here is the simple example of workflow with optional input: . ```; workflow wf {; String? inn; call tsk_grep { input: input_381012639=inn }; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. When i do not specify any input for workflow(json file below):. ```; {; }; ```. i get an the error:; `wdl4s.WdlExpressionException: Could not resolve inn as a scatter variable, namespace, call, or declaration`. But when my workflow looks like this:. ```; workflow wf {; call tsk_grep; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. Cromwell works as expected executing `ps aux | grep j`.; Is this a bug or i can only use optional variables inside of **task scope**?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1176
https://github.com/broadinstitute/cromwell/issues/1176:19,Usability,simpl,simple,19,"Hello. Here is the simple example of workflow with optional input: . ```; workflow wf {; String? inn; call tsk_grep { input: input_381012639=inn }; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. When i do not specify any input for workflow(json file below):. ```; {; }; ```. i get an the error:; `wdl4s.WdlExpressionException: Could not resolve inn as a scatter variable, namespace, call, or declaration`. But when my workflow looks like this:. ```; workflow wf {; call tsk_grep; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. Cromwell works as expected executing `ps aux | grep j`.; Is this a bug or i can only use optional variables inside of **task scope**?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1176
https://github.com/broadinstitute/cromwell/issues/1177:548,Integrability,wrap,wrapper,548,"For more specific details see the DB meeting notes. Remove the biz logic-y portions of the DataAccess trait and move them into something alongside/in the appropriate locations (metadata store, kv store, etc) and have that code take a SlickDatabase. Move globalDataAccess to be a globally (in core package? Something like that) accessible singleton SlickDatabase instance, which the individual ex-DataAccess stuff can use. The DataAccess trait also had a withRetry function which does seem inappropriate for SlickDatabase, at the moment it's just a wrapper around the main withRetry w/ some defaults set - perhaps that lives w/ the global SlickDatabase? Perhaps allow individual ex-DataAccess stuff set their own? Up to you.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1177
https://github.com/broadinstitute/cromwell/issues/1177:327,Security,access,accessible,327,"For more specific details see the DB meeting notes. Remove the biz logic-y portions of the DataAccess trait and move them into something alongside/in the appropriate locations (metadata store, kv store, etc) and have that code take a SlickDatabase. Move globalDataAccess to be a globally (in core package? Something like that) accessible singleton SlickDatabase instance, which the individual ex-DataAccess stuff can use. The DataAccess trait also had a withRetry function which does seem inappropriate for SlickDatabase, at the moment it's just a wrapper around the main withRetry w/ some defaults set - perhaps that lives w/ the global SlickDatabase? Perhaps allow individual ex-DataAccess stuff set their own? Up to you.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1177
https://github.com/broadinstitute/cromwell/issues/1177:67,Testability,log,logic-y,67,"For more specific details see the DB meeting notes. Remove the biz logic-y portions of the DataAccess trait and move them into something alongside/in the appropriate locations (metadata store, kv store, etc) and have that code take a SlickDatabase. Move globalDataAccess to be a globally (in core package? Something like that) accessible singleton SlickDatabase instance, which the individual ex-DataAccess stuff can use. The DataAccess trait also had a withRetry function which does seem inappropriate for SlickDatabase, at the moment it's just a wrapper around the main withRetry w/ some defaults set - perhaps that lives w/ the global SlickDatabase? Perhaps allow individual ex-DataAccess stuff set their own? Up to you.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1177
https://github.com/broadinstitute/cromwell/issues/1179:217,Security,access,access,217,See user question at: http://gatkforums.broadinstitute.org/wdl/discussion/8015/usage-of-wdl-objects-and-the-read-objects-function#latest. When the user calls read_objects and then correctly (as per the spec) tries to access the value an exception occurs.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1179
https://github.com/broadinstitute/cromwell/issues/1180:49,Testability,test,testing,49,"Khalid mentioned that we need to incorporate SGE testing into Centaur soon and some thoughts that came up from today's tech talk:; - There's a docker image with SGE on it, and we can use that through Centaur to test SGE backend (Similar to how we make a special JES-Refresh backend to test refresh tokens); - Re-use existing WDL's from Centaur instead of writing anything new, and confirm that all applicable WDL functionalities work as expected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180
https://github.com/broadinstitute/cromwell/issues/1180:211,Testability,test,test,211,"Khalid mentioned that we need to incorporate SGE testing into Centaur soon and some thoughts that came up from today's tech talk:; - There's a docker image with SGE on it, and we can use that through Centaur to test SGE backend (Similar to how we make a special JES-Refresh backend to test refresh tokens); - Re-use existing WDL's from Centaur instead of writing anything new, and confirm that all applicable WDL functionalities work as expected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180
https://github.com/broadinstitute/cromwell/issues/1180:285,Testability,test,test,285,"Khalid mentioned that we need to incorporate SGE testing into Centaur soon and some thoughts that came up from today's tech talk:; - There's a docker image with SGE on it, and we can use that through Centaur to test SGE backend (Similar to how we make a special JES-Refresh backend to test refresh tokens); - Re-use existing WDL's from Centaur instead of writing anything new, and confirm that all applicable WDL functionalities work as expected.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180
https://github.com/broadinstitute/cromwell/pull/1183:54,Testability,test,test,54,"Making small changes to accommodate the Refresh_Token test in Centaur. NOTE: There's a todo that requires changing the centaur.wdl command from ""git checkout rm_refreshToken_test"" --> ""git checkout develop"". Must merge the refresh_token test into Centaur first before the develop branch will pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1183
https://github.com/broadinstitute/cromwell/pull/1183:237,Testability,test,test,237,"Making small changes to accommodate the Refresh_Token test in Centaur. NOTE: There's a todo that requires changing the centaur.wdl command from ""git checkout rm_refreshToken_test"" --> ""git checkout develop"". Must merge the refresh_token test into Centaur first before the develop branch will pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1183
https://github.com/broadinstitute/cromwell/pull/1184:130,Integrability,message,message,130,1. Added cacheEnabled and cacheForceRw support in workflow options.; 2. Added missing keys during initialization to log a warning message if a key is not in that list. Reviewers: @jainh and @geoffjentry.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1184
https://github.com/broadinstitute/cromwell/pull/1184:9,Performance,cache,cacheEnabled,9,1. Added cacheEnabled and cacheForceRw support in workflow options.; 2. Added missing keys during initialization to log a warning message if a key is not in that list. Reviewers: @jainh and @geoffjentry.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1184
https://github.com/broadinstitute/cromwell/pull/1184:26,Performance,cache,cacheForceRw,26,1. Added cacheEnabled and cacheForceRw support in workflow options.; 2. Added missing keys during initialization to log a warning message if a key is not in that list. Reviewers: @jainh and @geoffjentry.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1184
https://github.com/broadinstitute/cromwell/pull/1184:116,Testability,log,log,116,1. Added cacheEnabled and cacheForceRw support in workflow options.; 2. Added missing keys during initialization to log a warning message if a key is not in that list. Reviewers: @jainh and @geoffjentry.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1184
https://github.com/broadinstitute/cromwell/issues/1185:147,Availability,error,error,147,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:211,Availability,error,error,211,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:309,Availability,ERROR,ERROR,309,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:832,Availability,error,errors,832,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:890,Availability,Error,Error,890,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:953,Availability,Error,Error,953,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:870,Integrability,message,message,870,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:933,Integrability,message,message,933,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:18,Performance,cache,cache,18,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:115,Performance,cache,cache,115,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1185:194,Performance,cache,cached,194,"When getting call cache hits for a large (900+) amount of jobs, there is a highish likelihood that one of the call cache hits will fail with a 503 error when trying to copy the outputs from the cached call. The error looks like:. ```; 2016-07-17 18:31:09,715 cromwell-system-akka.actor.default-dispatcher-153 ERROR - JesBackend [UUID(14410fce):GenotypeFilterAndMakeSitesOnlyVcf:1]: Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-cromwell-execution/JointGenotyping/e71eba09-ef37-4cda-a89e-b0c4398fcc35/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1 to gs://broad-gotc-dev-cromwell-execution/JointGenotyping/14410fce-5b88-4166-b21b-e33a1e02911f/call-GenotypeFilterAndMakeSitesOnlyVcf/shard-1; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1185
https://github.com/broadinstitute/cromwell/issues/1188:21,Security,access,access,21,"Hi, in order to have access to all akka features (e.g. ask-Pattern) it would be good to use the latest version of the framework for Cromwell (as per today 2.4.8)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1188
https://github.com/broadinstitute/cromwell/issues/1190:24,Integrability,depend,dependency,24,So we can remove engine dependency on JES,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1190
https://github.com/broadinstitute/cromwell/issues/1191:115,Integrability,message,messages,115,"The `JobStore` sends responses for all attempted writes, but nothing is expecting them. The symptom is that we see messages like these in the logs:. For `WorkflowManagerActor`:. ```; 2016-07-20 15:02:17,967 cromwell-system-akka.dispatchers.engine-dispatcher-7 WARN - WorkflowManagerActor Unhandled message: JobStoreWriteSuccess(RegisterWorkflowCompleted(f8af17ab-2ea2-4f8e-b84b-959e45eeab06)); ```. and for `WorkflowExecutionActor`:. ```; 2016-07-20 15:02:08,782 cromwell-system-akka.dispatchers.engine-dispatcher-7 WARN - WorkflowExecutionActor-f8af17ab-2ea2-4f8e-b84b-959e45eeab06 [UUID(f8af17ab)]: WorkflowExecutionActor [UUID(f8af17ab)] received an unhandled message: JobStoreWriteSuccess(RegisterJobCompleted([...])) in state: WorkflowExecutionInProgressState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1191
https://github.com/broadinstitute/cromwell/issues/1191:298,Integrability,message,message,298,"The `JobStore` sends responses for all attempted writes, but nothing is expecting them. The symptom is that we see messages like these in the logs:. For `WorkflowManagerActor`:. ```; 2016-07-20 15:02:17,967 cromwell-system-akka.dispatchers.engine-dispatcher-7 WARN - WorkflowManagerActor Unhandled message: JobStoreWriteSuccess(RegisterWorkflowCompleted(f8af17ab-2ea2-4f8e-b84b-959e45eeab06)); ```. and for `WorkflowExecutionActor`:. ```; 2016-07-20 15:02:08,782 cromwell-system-akka.dispatchers.engine-dispatcher-7 WARN - WorkflowExecutionActor-f8af17ab-2ea2-4f8e-b84b-959e45eeab06 [UUID(f8af17ab)]: WorkflowExecutionActor [UUID(f8af17ab)] received an unhandled message: JobStoreWriteSuccess(RegisterJobCompleted([...])) in state: WorkflowExecutionInProgressState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1191
https://github.com/broadinstitute/cromwell/issues/1191:663,Integrability,message,message,663,"The `JobStore` sends responses for all attempted writes, but nothing is expecting them. The symptom is that we see messages like these in the logs:. For `WorkflowManagerActor`:. ```; 2016-07-20 15:02:17,967 cromwell-system-akka.dispatchers.engine-dispatcher-7 WARN - WorkflowManagerActor Unhandled message: JobStoreWriteSuccess(RegisterWorkflowCompleted(f8af17ab-2ea2-4f8e-b84b-959e45eeab06)); ```. and for `WorkflowExecutionActor`:. ```; 2016-07-20 15:02:08,782 cromwell-system-akka.dispatchers.engine-dispatcher-7 WARN - WorkflowExecutionActor-f8af17ab-2ea2-4f8e-b84b-959e45eeab06 [UUID(f8af17ab)]: WorkflowExecutionActor [UUID(f8af17ab)] received an unhandled message: JobStoreWriteSuccess(RegisterJobCompleted([...])) in state: WorkflowExecutionInProgressState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1191
https://github.com/broadinstitute/cromwell/issues/1191:142,Testability,log,logs,142,"The `JobStore` sends responses for all attempted writes, but nothing is expecting them. The symptom is that we see messages like these in the logs:. For `WorkflowManagerActor`:. ```; 2016-07-20 15:02:17,967 cromwell-system-akka.dispatchers.engine-dispatcher-7 WARN - WorkflowManagerActor Unhandled message: JobStoreWriteSuccess(RegisterWorkflowCompleted(f8af17ab-2ea2-4f8e-b84b-959e45eeab06)); ```. and for `WorkflowExecutionActor`:. ```; 2016-07-20 15:02:08,782 cromwell-system-akka.dispatchers.engine-dispatcher-7 WARN - WorkflowExecutionActor-f8af17ab-2ea2-4f8e-b84b-959e45eeab06 [UUID(f8af17ab)]: WorkflowExecutionActor [UUID(f8af17ab)] received an unhandled message: JobStoreWriteSuccess(RegisterJobCompleted([...])) in state: WorkflowExecutionInProgressState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1191
https://github.com/broadinstitute/cromwell/pull/1193:109,Testability,log,logic,109,Just to be clear on this - The SqlDatabase and its Slick implementation are not split out. Only the business logic code calling these SQL methods with primitive types has been scattered across sub-projects.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1193
https://github.com/broadinstitute/cromwell/pull/1193:11,Usability,clear,clear,11,Just to be clear on this - The SqlDatabase and its Slick implementation are not split out. Only the business logic code calling these SQL methods with primitive types has been scattered across sub-projects.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1193
https://github.com/broadinstitute/cromwell/pull/1197:447,Integrability,message,message,447,"Doesn't close #751 but stabs in its general direction. There is no persistent DB here, I replaced the DB-based approach in `KeyValueServiceActor` with a simple `Map` since the DB-based approach always failed to write due to referential integrity constraints that aren't going to be fixed (no `EXECUTION` or `WORKFLOW_EXECUTION` data for FK constraints). Ruchi and/or I will have a follow-on PR to put a DB table behind this. Also fixed ""abort"" to message the KV service instead of metadata service, removed now-unused `EXECUTION_INFO`-related methods, other assorted cleanup.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197
https://github.com/broadinstitute/cromwell/pull/1197:437,Safety,abort,abort,437,"Doesn't close #751 but stabs in its general direction. There is no persistent DB here, I replaced the DB-based approach in `KeyValueServiceActor` with a simple `Map` since the DB-based approach always failed to write due to referential integrity constraints that aren't going to be fixed (no `EXECUTION` or `WORKFLOW_EXECUTION` data for FK constraints). Ruchi and/or I will have a follow-on PR to put a DB table behind this. Also fixed ""abort"" to message the KV service instead of metadata service, removed now-unused `EXECUTION_INFO`-related methods, other assorted cleanup.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197
https://github.com/broadinstitute/cromwell/pull/1197:236,Security,integrity,integrity,236,"Doesn't close #751 but stabs in its general direction. There is no persistent DB here, I replaced the DB-based approach in `KeyValueServiceActor` with a simple `Map` since the DB-based approach always failed to write due to referential integrity constraints that aren't going to be fixed (no `EXECUTION` or `WORKFLOW_EXECUTION` data for FK constraints). Ruchi and/or I will have a follow-on PR to put a DB table behind this. Also fixed ""abort"" to message the KV service instead of metadata service, removed now-unused `EXECUTION_INFO`-related methods, other assorted cleanup.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197
https://github.com/broadinstitute/cromwell/pull/1197:153,Usability,simpl,simple,153,"Doesn't close #751 but stabs in its general direction. There is no persistent DB here, I replaced the DB-based approach in `KeyValueServiceActor` with a simple `Map` since the DB-based approach always failed to write due to referential integrity constraints that aren't going to be fixed (no `EXECUTION` or `WORKFLOW_EXECUTION` data for FK constraints). Ruchi and/or I will have a follow-on PR to put a DB table behind this. Also fixed ""abort"" to message the KV service instead of metadata service, removed now-unused `EXECUTION_INFO`-related methods, other assorted cleanup.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197
https://github.com/broadinstitute/cromwell/pull/1202:1095,Deployability,integrat,integration,1095,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:1095,Integrability,integrat,integration,1095,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:149,Modifiability,config,configs,149,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:345,Modifiability,config,config,345,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:410,Modifiability,refactor,refactored,410,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:623,Modifiability,config,config,623,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:726,Modifiability,Config,Config,726,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:840,Modifiability,extend,extending,840,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:937,Modifiability,Config,Config,937,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:986,Modifiability,extend,extending,986,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:509,Security,Validat,ValidatedType,509,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:1107,Testability,test,tests,1107,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/pull/1202:1137,Testability,mock,mock,1137,"Added a new Shared File System (SFS) Backend package as a base for any/most backends wanting to execute cromwell calls on an SFS.; Added SGE and LSF configs (commented out) to the main `application.conf`, plus other small tweaks.; Non-main `application.conf` files only set the keys that they need to add or override from the main.; More of the config is defaulted across the board.; A number of classes moved/refactored from other backends for use by the SFS backend.; New trait `RuntimeAttributesValidation[ValidatedType]` for proccessing runtime attributes.; First pass at a `WorkflowFileSystemProvider` that combines a config and workflow options to create a filesystem. Only implemented for local and GCS files.; Added a Config backend that implements the SFS backend by reading wdl-lite command strings and `job-id-regex`.; Currently extending SFS backend to create the SGE backend until one can specify runtime attributes via the Config backend.; Added a ""shadow"" local backend, extending the SFS backend, that will be evaluated for a while before hopefully usurping the original.; Fixed integration tests that were attempting to mock the gcs file system.; Gave `MetadataDatabaseAccessSpec` a little bit more time the non-split up ""create and query a workflow"".; Fixed the `SprayDockerRegistryApiClientSpec` for v1 registries by switching from the `ubuntu` repo to the `registry` repo (we query the registry for a repo hosting images of the docker registry-- inception).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1202
https://github.com/broadinstitute/cromwell/issues/1211:0,Energy Efficiency,Reduce,Reduce,0,Reduce the reliance on lenthall,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1211
https://github.com/broadinstitute/cromwell/issues/1217:298,Deployability,configurat,configuration,298,"> We use lots of different runtime attributes (bsub job submission parameters) in our workflows.  ; > ; > With existing Cromwell backends, you need to put all the possible runtime attributes in the code itself.  Would it be possible to let the user specify arbitrary runtime parameters only in the configuration file (not in the source code) that Cromwell just passes to the submit command?  ; > ; > We want to avoid asking for Cromwell code changes every time we decide to use a new bsub parameter. -- Pfizer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217
https://github.com/broadinstitute/cromwell/issues/1217:298,Modifiability,config,configuration,298,"> We use lots of different runtime attributes (bsub job submission parameters) in our workflows.  ; > ; > With existing Cromwell backends, you need to put all the possible runtime attributes in the code itself.  Would it be possible to let the user specify arbitrary runtime parameters only in the configuration file (not in the source code) that Cromwell just passes to the submit command?  ; > ; > We want to avoid asking for Cromwell code changes every time we decide to use a new bsub parameter. -- Pfizer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217
https://github.com/broadinstitute/cromwell/issues/1217:411,Safety,avoid,avoid,411,"> We use lots of different runtime attributes (bsub job submission parameters) in our workflows.  ; > ; > With existing Cromwell backends, you need to put all the possible runtime attributes in the code itself.  Would it be possible to let the user specify arbitrary runtime parameters only in the configuration file (not in the source code) that Cromwell just passes to the submit command?  ; > ; > We want to avoid asking for Cromwell code changes every time we decide to use a new bsub parameter. -- Pfizer",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1217
https://github.com/broadinstitute/cromwell/issues/1218:286,Integrability,message,message,286,"AsyncBackendJobExecutionActor (ABJEA) uses a `completionPromise: Promise` to tell its parent actor, the BackendJobExecutionActor (BJEA), when the async job is done. I doesn't seem obvious why we're passing around references to `scala.concurrent.Promise`s when akka has a perfectly good message system already. Worst case, if one needed two different states and didn't want the framework to use an FSM, the BJEA could `context.become(initializing)`, start the async job, then `context.become(asyncRunning)`, and wait for the message from the ABJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218
https://github.com/broadinstitute/cromwell/issues/1218:524,Integrability,message,message,524,"AsyncBackendJobExecutionActor (ABJEA) uses a `completionPromise: Promise` to tell its parent actor, the BackendJobExecutionActor (BJEA), when the async job is done. I doesn't seem obvious why we're passing around references to `scala.concurrent.Promise`s when akka has a perfectly good message system already. Worst case, if one needed two different states and didn't want the framework to use an FSM, the BJEA could `context.become(initializing)`, start the async job, then `context.become(asyncRunning)`, and wait for the message from the ABJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218
https://github.com/broadinstitute/cromwell/issues/1218:234,Performance,concurren,concurrent,234,"AsyncBackendJobExecutionActor (ABJEA) uses a `completionPromise: Promise` to tell its parent actor, the BackendJobExecutionActor (BJEA), when the async job is done. I doesn't seem obvious why we're passing around references to `scala.concurrent.Promise`s when akka has a perfectly good message system already. Worst case, if one needed two different states and didn't want the framework to use an FSM, the BJEA could `context.become(initializing)`, start the async job, then `context.become(asyncRunning)`, and wait for the message from the ABJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1218
https://github.com/broadinstitute/cromwell/pull/1223:30,Integrability,message,messages,30,These were already handled as messages so mainly a no-op. I did do some tidying up around the edges though,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1223
https://github.com/broadinstitute/cromwell/issues/1224:50,Security,Hash,Hashes,50,We want the following tables:; - [x] Call Caching Hashes. | HashID | HashKey | HashValue | CCRID |; | --- | --- | --- | --- |; | (Primary key) | | | (FK to Call Caching Job Result MetaInfo) |; - [x] Call Caching Job Result MetaInfo. | CCRID | Workflow UUID | Call FQN | Index | AllowsResultReuse |; | --- | --- | --- | --- | --- |; | (Primary key) | | | | |; - [x] Call Caching Job Result Store. | JRSID | Key | Value | CCRID |; | --- | --- | --- | --- |; | (Primary Key) | (Like metadata key) | (Like metadata value) | (FK to Call Caching Job Result MetaInfo) |,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1224
https://github.com/broadinstitute/cromwell/issues/1224:60,Security,Hash,HashID,60,We want the following tables:; - [x] Call Caching Hashes. | HashID | HashKey | HashValue | CCRID |; | --- | --- | --- | --- |; | (Primary key) | | | (FK to Call Caching Job Result MetaInfo) |; - [x] Call Caching Job Result MetaInfo. | CCRID | Workflow UUID | Call FQN | Index | AllowsResultReuse |; | --- | --- | --- | --- | --- |; | (Primary key) | | | | |; - [x] Call Caching Job Result Store. | JRSID | Key | Value | CCRID |; | --- | --- | --- | --- |; | (Primary Key) | (Like metadata key) | (Like metadata value) | (FK to Call Caching Job Result MetaInfo) |,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1224
https://github.com/broadinstitute/cromwell/issues/1224:69,Security,Hash,HashKey,69,We want the following tables:; - [x] Call Caching Hashes. | HashID | HashKey | HashValue | CCRID |; | --- | --- | --- | --- |; | (Primary key) | | | (FK to Call Caching Job Result MetaInfo) |; - [x] Call Caching Job Result MetaInfo. | CCRID | Workflow UUID | Call FQN | Index | AllowsResultReuse |; | --- | --- | --- | --- | --- |; | (Primary key) | | | | |; - [x] Call Caching Job Result Store. | JRSID | Key | Value | CCRID |; | --- | --- | --- | --- |; | (Primary Key) | (Like metadata key) | (Like metadata value) | (FK to Call Caching Job Result MetaInfo) |,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1224
https://github.com/broadinstitute/cromwell/issues/1224:79,Security,Hash,HashValue,79,We want the following tables:; - [x] Call Caching Hashes. | HashID | HashKey | HashValue | CCRID |; | --- | --- | --- | --- |; | (Primary key) | | | (FK to Call Caching Job Result MetaInfo) |; - [x] Call Caching Job Result MetaInfo. | CCRID | Workflow UUID | Call FQN | Index | AllowsResultReuse |; | --- | --- | --- | --- | --- |; | (Primary key) | | | | |; - [x] Call Caching Job Result Store. | JRSID | Key | Value | CCRID |; | --- | --- | --- | --- |; | (Primary Key) | (Like metadata key) | (Like metadata value) | (FK to Call Caching Job Result MetaInfo) |,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1224
https://github.com/broadinstitute/cromwell/issues/1230:651,Deployability,configurat,configuration,651,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:638,Integrability,depend,depending,638,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:651,Modifiability,config,configuration,651,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:43,Security,hash,hashes,43,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:139,Security,hash,hash,139,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:167,Security,hash,hash,167,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:327,Security,hash,hashes,327,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:396,Security,hash,hashes,396,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:473,Security,hash,hashes,473,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:512,Security,hash,hash,512,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:620,Security,hash,hashes,620,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1230:24,Testability,log,logic,24,"Implement any necessary logic to calculate hashes. Follow the pattern of the FileHasherActor: The EJHA creates an actor which can make the hash. This actor sends each hash result back to the EngineJobHashingActor individually and lets it do the magic of combining everything together.; - [x] command string (one of the initial hashes in `EJHA`); - [x] all outputs expressions (one of the initial hashes in `EJHA`); - [x] all non-file inputs (included in the set of initial hashes in `EJHA`); - [x] dockerhub/GCR hash (`EngineDockerHashingActor`) or docker name (tuple3 of (namespace, repository, tag, one of the initial hashes in `EJHA`) depending on configuration; - [x] runtime attributes (`BackendRuntimeAttributeHashingActor`); - [x] file inputs (`FileHasherActor`, and onwards to GCS, SFS specific `XYZFileHasherActor`s)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1230
https://github.com/broadinstitute/cromwell/issues/1232:130,Security,Hash,HashKey,130,"Create an actor, for use by the EJHA, which can dive into the DB (#1224) and retrieve the list of all CCRIDs which match a given (HashKey, HashResult) combination.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1232
https://github.com/broadinstitute/cromwell/issues/1232:139,Security,Hash,HashResult,139,"Create an actor, for use by the EJHA, which can dive into the DB (#1224) and retrieve the list of all CCRIDs which match a given (HashKey, HashResult) combination.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1232
https://github.com/broadinstitute/cromwell/issues/1233:63,Performance,cache,cache,63,"A backend-specific actor to replace the BJEA in case of a call cache hit.; - [ ] The EJEA should create an instance of this instead of a BJEA, if a call cache hit has occurred; - [ ] Given a BackendJobDescriptor and a CCRID (see #1224), should copy results as appropriate and send a BackendJobExecutionResponse to the EJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1233
https://github.com/broadinstitute/cromwell/issues/1233:153,Performance,cache,cache,153,"A backend-specific actor to replace the BJEA in case of a call cache hit.; - [ ] The EJEA should create an instance of this instead of a BJEA, if a call cache hit has occurred; - [ ] Given a BackendJobDescriptor and a CCRID (see #1224), should copy results as appropriate and send a BackendJobExecutionResponse to the EJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1233
https://github.com/broadinstitute/cromwell/issues/1236:67,Performance,cache,cache,67,"A SFS backend-specific actor to replace the BJEA in case of a call cache hit.; - [ ] The EJEA should create an instance of this instead of a BJEA, if a call cache hit has occurred; - [ ] Given a BackendJobDescriptor and a CCRID (see #1224), should copy results as appropriate and send a BackendJobExecutionResponse to the EJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1236
https://github.com/broadinstitute/cromwell/issues/1236:157,Performance,cache,cache,157,"A SFS backend-specific actor to replace the BJEA in case of a call cache hit.; - [ ] The EJEA should create an instance of this instead of a BJEA, if a call cache hit has occurred; - [ ] Given a BackendJobDescriptor and a CCRID (see #1224), should copy results as appropriate and send a BackendJobExecutionResponse to the EJEA.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1236
https://github.com/broadinstitute/cromwell/issues/1238:113,Safety,sanity check,sanity check,113,These inputs used to live in the WORKFLOW_EXECUTION_AUX table in .19. We use this table as kind of a back pocket sanity check if our workflow results are not making sense. Is it possible to get this table revived in .20?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1238
https://github.com/broadinstitute/cromwell/issues/1242:25,Integrability,depend,dependencies,25,There are still some JES dependencies in engine library: ; 1. cromwell/engine/src/test/scala/cromwell/engine/backend/jes/JesRuntimeInfoSpec.scala; 2. cromwell/engine/backend/jes/RunSpec.scala. Those are for testing purposes but it makes us to bring JES during our build process.; These should be moved to JES Backend.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1242
https://github.com/broadinstitute/cromwell/issues/1242:82,Testability,test,test,82,There are still some JES dependencies in engine library: ; 1. cromwell/engine/src/test/scala/cromwell/engine/backend/jes/JesRuntimeInfoSpec.scala; 2. cromwell/engine/backend/jes/RunSpec.scala. Those are for testing purposes but it makes us to bring JES during our build process.; These should be moved to JES Backend.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1242
https://github.com/broadinstitute/cromwell/issues/1242:207,Testability,test,testing,207,There are still some JES dependencies in engine library: ; 1. cromwell/engine/src/test/scala/cromwell/engine/backend/jes/JesRuntimeInfoSpec.scala; 2. cromwell/engine/backend/jes/RunSpec.scala. Those are for testing purposes but it makes us to bring JES during our build process.; These should be moved to JES Backend.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1242
https://github.com/broadinstitute/cromwell/issues/1243:173,Deployability,update,update,173,"Akka HTTP is effectively the next version of Spray. Spray is for all intents and purposes a dead technology. Considering that's been the case for over a year now, we should update at some point just because eventually we'll run into a problem that won't be resolved by developers. Also, Akka HTTP is now hitting a point in its development cycle where not only is it deemed ""Better"" :tm: but they've switched to ease of use and performance and it's eclipsing Spray in those categories now as well. I've labeled this as developers choice as it certainly doesn't _need_ to happen any time soon, although if a someone wearing a PO hat thought it was otherwise important it could certainly be pushed in elsewhere (since it likely _would_ include performance enhancements and such)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1243
https://github.com/broadinstitute/cromwell/issues/1243:753,Modifiability,enhance,enhancements,753,"Akka HTTP is effectively the next version of Spray. Spray is for all intents and purposes a dead technology. Considering that's been the case for over a year now, we should update at some point just because eventually we'll run into a problem that won't be resolved by developers. Also, Akka HTTP is now hitting a point in its development cycle where not only is it deemed ""Better"" :tm: but they've switched to ease of use and performance and it's eclipsing Spray in those categories now as well. I've labeled this as developers choice as it certainly doesn't _need_ to happen any time soon, although if a someone wearing a PO hat thought it was otherwise important it could certainly be pushed in elsewhere (since it likely _would_ include performance enhancements and such)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1243
https://github.com/broadinstitute/cromwell/issues/1243:427,Performance,perform,performance,427,"Akka HTTP is effectively the next version of Spray. Spray is for all intents and purposes a dead technology. Considering that's been the case for over a year now, we should update at some point just because eventually we'll run into a problem that won't be resolved by developers. Also, Akka HTTP is now hitting a point in its development cycle where not only is it deemed ""Better"" :tm: but they've switched to ease of use and performance and it's eclipsing Spray in those categories now as well. I've labeled this as developers choice as it certainly doesn't _need_ to happen any time soon, although if a someone wearing a PO hat thought it was otherwise important it could certainly be pushed in elsewhere (since it likely _would_ include performance enhancements and such)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1243
https://github.com/broadinstitute/cromwell/issues/1243:741,Performance,perform,performance,741,"Akka HTTP is effectively the next version of Spray. Spray is for all intents and purposes a dead technology. Considering that's been the case for over a year now, we should update at some point just because eventually we'll run into a problem that won't be resolved by developers. Also, Akka HTTP is now hitting a point in its development cycle where not only is it deemed ""Better"" :tm: but they've switched to ease of use and performance and it's eclipsing Spray in those categories now as well. I've labeled this as developers choice as it certainly doesn't _need_ to happen any time soon, although if a someone wearing a PO hat thought it was otherwise important it could certainly be pushed in elsewhere (since it likely _would_ include performance enhancements and such)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1243
https://github.com/broadinstitute/cromwell/issues/1245:1064,Testability,Test,Testing,1064,"ussion/8095/glob-function-is-inconsistent-between-jes-backend-and-local). Using the WDL file from that example, I run the WDL and get this for the output of `makeFilesTask`:. ```; /Users/sfrazer/projects/cromwell/cromwell-executions/globTestWorkflow/baa45456-4ff4-4c03-84b3-2df0c9bb9e0b/call-makeFilesTask; ├── Reads; │   ├── four.unique.txt; │   └── three.unique.txt; ├── one.unique.txt; ├── rc; ├── script; ├── stderr; ├── stdout; └── two.unique.txt; ```. It seems that the [globbing code in Cromwell](https://github.com/broadinstitute/cromwell/blob/3f81d4e4ad52f9a551f190d8a40beac241d77f0e/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemExpressionFunctions.scala#L65) uses `.glob(s""**/$pattern"")`, which in this case would expand to `.glob(s""**/./Reads/*.unique.txt"")`. For reference here is the [documentation for the Java pattern matcher](https://docs.oracle.com/javase/7/docs/api/java/nio/file/FileSystem.html#getPathMatcher%28java.lang.String%29). Testing this out via the Scala REPL:. ```; scala> import java.nio.file.Paths; import java.nio.file.Paths. scala> import better.files._; import better.files._. scala> val p = Paths.get(""/Users/sfrazer/projects/cromwell/cromwell-executions/globTestWorkflow/baa45456-4ff4-4c03-84b3-2df0c9bb9e0b/call-makeFilesTask""); p: java.nio.file.Path = /Users/sfrazer/projects/cromwell/cromwell-executions/globTestWorkflow/baa45456-4ff4-4c03-84b3-2df0c9bb9e0b/call-makeFilesTask. scala> p.glob(""**./Reads/*.unique.txt"").map(_.name).foreach(println). scala> p.glob(""./Reads/*.unique.txt"").map(_.name).foreach(println). scala> p.glob(""**/Reads/*.unique.txt"").map(_.name).foreach(println); four.unique.txt; three.unique.txt. scala> p.glob(""**/*.unique.txt"").map(_.name).foreach(println); one.unique.txt; four.unique.txt; three.unique.txt; two.unique.txt; ```. Ideally, the user should be able to:; - Use pattern `./Reads/*.unique.txt` and retrieve `four.unique.txt` and `three.unique.txt`; - Use pattern `*.unique.txt` and retrieve `one",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1245
https://github.com/broadinstitute/cromwell/pull/1250:8,Availability,recover,recovery,8,"- Added recovery functionality using KV service.; - In the next iteration will refactor to use a Doc store (Mongo, Couchbase) generic service implementation or continue using KV service but with a refactor in order to support not just SQL DBs as KV store but any other kind of DB. I think the best may be to work on a DAL or if it's not possible just modify the service to support other providers. Let me know what do you think on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250
https://github.com/broadinstitute/cromwell/pull/1250:79,Modifiability,refactor,refactor,79,"- Added recovery functionality using KV service.; - In the next iteration will refactor to use a Doc store (Mongo, Couchbase) generic service implementation or continue using KV service but with a refactor in order to support not just SQL DBs as KV store but any other kind of DB. I think the best may be to work on a DAL or if it's not possible just modify the service to support other providers. Let me know what do you think on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250
https://github.com/broadinstitute/cromwell/pull/1250:197,Modifiability,refactor,refactor,197,"- Added recovery functionality using KV service.; - In the next iteration will refactor to use a Doc store (Mongo, Couchbase) generic service implementation or continue using KV service but with a refactor in order to support not just SQL DBs as KV store but any other kind of DB. I think the best may be to work on a DAL or if it's not possible just modify the service to support other providers. Let me know what do you think on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250
https://github.com/broadinstitute/cromwell/pull/1250:8,Safety,recover,recovery,8,"- Added recovery functionality using KV service.; - In the next iteration will refactor to use a Doc store (Mongo, Couchbase) generic service implementation or continue using KV service but with a refactor in order to support not just SQL DBs as KV store but any other kind of DB. I think the best may be to work on a DAL or if it's not possible just modify the service to support other providers. Let me know what do you think on this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250
https://github.com/broadinstitute/cromwell/issues/1251:401,Deployability,update,updated,401,"The `SprayDockerRegistryApiClientSpec` check for the v1 docker repositories on the hub.docker.com registry are all returning 404. Tried switching the repository searched on the registry from [`ubuntu`](https://hub.docker.com/r/_/ubuntu/) to [`registry`](https://hub.docker.com/r/_/registry/), but that only worked for a day or two. Either the handshake for accessing the v1 hub.docker.com needs to be updated, or perhaps the test needs an edit / removal. Perhaps we may also decide to only support v2 registries. `sbt engine/alltests:test-only *SprayDockerRegistryApiClient` will run the tests including ""resolve docker hub image v1 layer ids"", that may be marked with ""ignore"" by the time this ticket is addressed. Haven't checked GCR (or artifactory, quay, etc.) recently, but perhaps most registries are all using v2 for looking up repositories and we may remove v1 support. NOTE: Docker Hub, even though it is technically a _docker registry_, did NOT implement the official Docker Registry API for a long time.; - http://dev.hpcloud.com/blog/2016/wrangling-different-docker-apis . See also:; - https://docs.docker.com/registry/spec/api/; - https://github.com/docker/distribution; - https://docs.docker.com/v1.7/docker/reference/api/docker-io_api/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1251
https://github.com/broadinstitute/cromwell/issues/1251:357,Security,access,accessing,357,"The `SprayDockerRegistryApiClientSpec` check for the v1 docker repositories on the hub.docker.com registry are all returning 404. Tried switching the repository searched on the registry from [`ubuntu`](https://hub.docker.com/r/_/ubuntu/) to [`registry`](https://hub.docker.com/r/_/registry/), but that only worked for a day or two. Either the handshake for accessing the v1 hub.docker.com needs to be updated, or perhaps the test needs an edit / removal. Perhaps we may also decide to only support v2 registries. `sbt engine/alltests:test-only *SprayDockerRegistryApiClient` will run the tests including ""resolve docker hub image v1 layer ids"", that may be marked with ""ignore"" by the time this ticket is addressed. Haven't checked GCR (or artifactory, quay, etc.) recently, but perhaps most registries are all using v2 for looking up repositories and we may remove v1 support. NOTE: Docker Hub, even though it is technically a _docker registry_, did NOT implement the official Docker Registry API for a long time.; - http://dev.hpcloud.com/blog/2016/wrangling-different-docker-apis . See also:; - https://docs.docker.com/registry/spec/api/; - https://github.com/docker/distribution; - https://docs.docker.com/v1.7/docker/reference/api/docker-io_api/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1251
https://github.com/broadinstitute/cromwell/issues/1251:425,Testability,test,test,425,"The `SprayDockerRegistryApiClientSpec` check for the v1 docker repositories on the hub.docker.com registry are all returning 404. Tried switching the repository searched on the registry from [`ubuntu`](https://hub.docker.com/r/_/ubuntu/) to [`registry`](https://hub.docker.com/r/_/registry/), but that only worked for a day or two. Either the handshake for accessing the v1 hub.docker.com needs to be updated, or perhaps the test needs an edit / removal. Perhaps we may also decide to only support v2 registries. `sbt engine/alltests:test-only *SprayDockerRegistryApiClient` will run the tests including ""resolve docker hub image v1 layer ids"", that may be marked with ""ignore"" by the time this ticket is addressed. Haven't checked GCR (or artifactory, quay, etc.) recently, but perhaps most registries are all using v2 for looking up repositories and we may remove v1 support. NOTE: Docker Hub, even though it is technically a _docker registry_, did NOT implement the official Docker Registry API for a long time.; - http://dev.hpcloud.com/blog/2016/wrangling-different-docker-apis . See also:; - https://docs.docker.com/registry/spec/api/; - https://github.com/docker/distribution; - https://docs.docker.com/v1.7/docker/reference/api/docker-io_api/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1251
https://github.com/broadinstitute/cromwell/issues/1251:534,Testability,test,test-only,534,"The `SprayDockerRegistryApiClientSpec` check for the v1 docker repositories on the hub.docker.com registry are all returning 404. Tried switching the repository searched on the registry from [`ubuntu`](https://hub.docker.com/r/_/ubuntu/) to [`registry`](https://hub.docker.com/r/_/registry/), but that only worked for a day or two. Either the handshake for accessing the v1 hub.docker.com needs to be updated, or perhaps the test needs an edit / removal. Perhaps we may also decide to only support v2 registries. `sbt engine/alltests:test-only *SprayDockerRegistryApiClient` will run the tests including ""resolve docker hub image v1 layer ids"", that may be marked with ""ignore"" by the time this ticket is addressed. Haven't checked GCR (or artifactory, quay, etc.) recently, but perhaps most registries are all using v2 for looking up repositories and we may remove v1 support. NOTE: Docker Hub, even though it is technically a _docker registry_, did NOT implement the official Docker Registry API for a long time.; - http://dev.hpcloud.com/blog/2016/wrangling-different-docker-apis . See also:; - https://docs.docker.com/registry/spec/api/; - https://github.com/docker/distribution; - https://docs.docker.com/v1.7/docker/reference/api/docker-io_api/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1251
https://github.com/broadinstitute/cromwell/issues/1251:588,Testability,test,tests,588,"The `SprayDockerRegistryApiClientSpec` check for the v1 docker repositories on the hub.docker.com registry are all returning 404. Tried switching the repository searched on the registry from [`ubuntu`](https://hub.docker.com/r/_/ubuntu/) to [`registry`](https://hub.docker.com/r/_/registry/), but that only worked for a day or two. Either the handshake for accessing the v1 hub.docker.com needs to be updated, or perhaps the test needs an edit / removal. Perhaps we may also decide to only support v2 registries. `sbt engine/alltests:test-only *SprayDockerRegistryApiClient` will run the tests including ""resolve docker hub image v1 layer ids"", that may be marked with ""ignore"" by the time this ticket is addressed. Haven't checked GCR (or artifactory, quay, etc.) recently, but perhaps most registries are all using v2 for looking up repositories and we may remove v1 support. NOTE: Docker Hub, even though it is technically a _docker registry_, did NOT implement the official Docker Registry API for a long time.; - http://dev.hpcloud.com/blog/2016/wrangling-different-docker-apis . See also:; - https://docs.docker.com/registry/spec/api/; - https://github.com/docker/distribution; - https://docs.docker.com/v1.7/docker/reference/api/docker-io_api/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1251
https://github.com/broadinstitute/cromwell/pull/1252:73,Deployability,Patch,Patches,73,"Moved parts of the shadow local backend into a new background executor.; Patches to tests based on changed to local: now with job id, fewer processes running in three step, using sfs config.; Replaced SGE backend with a config based version and added LSF example.; Revert: Jes's poll backoff starting at 30s up to 10 minutes is completely inappropriate for shared file system polling.; Docker Hub appears to have busted the v1 enpoint so disabling the test for now.; Sleep a second before letting the `WorkflowExecutionActorSpec` check for metadata status.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1252
https://github.com/broadinstitute/cromwell/pull/1252:183,Modifiability,config,config,183,"Moved parts of the shadow local backend into a new background executor.; Patches to tests based on changed to local: now with job id, fewer processes running in three step, using sfs config.; Replaced SGE backend with a config based version and added LSF example.; Revert: Jes's poll backoff starting at 30s up to 10 minutes is completely inappropriate for shared file system polling.; Docker Hub appears to have busted the v1 enpoint so disabling the test for now.; Sleep a second before letting the `WorkflowExecutionActorSpec` check for metadata status.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1252
https://github.com/broadinstitute/cromwell/pull/1252:220,Modifiability,config,config,220,"Moved parts of the shadow local backend into a new background executor.; Patches to tests based on changed to local: now with job id, fewer processes running in three step, using sfs config.; Replaced SGE backend with a config based version and added LSF example.; Revert: Jes's poll backoff starting at 30s up to 10 minutes is completely inappropriate for shared file system polling.; Docker Hub appears to have busted the v1 enpoint so disabling the test for now.; Sleep a second before letting the `WorkflowExecutionActorSpec` check for metadata status.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1252
https://github.com/broadinstitute/cromwell/pull/1252:84,Testability,test,tests,84,"Moved parts of the shadow local backend into a new background executor.; Patches to tests based on changed to local: now with job id, fewer processes running in three step, using sfs config.; Replaced SGE backend with a config based version and added LSF example.; Revert: Jes's poll backoff starting at 30s up to 10 minutes is completely inappropriate for shared file system polling.; Docker Hub appears to have busted the v1 enpoint so disabling the test for now.; Sleep a second before letting the `WorkflowExecutionActorSpec` check for metadata status.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1252
https://github.com/broadinstitute/cromwell/pull/1252:452,Testability,test,test,452,"Moved parts of the shadow local backend into a new background executor.; Patches to tests based on changed to local: now with job id, fewer processes running in three step, using sfs config.; Replaced SGE backend with a config based version and added LSF example.; Revert: Jes's poll backoff starting at 30s up to 10 minutes is completely inappropriate for shared file system polling.; Docker Hub appears to have busted the v1 enpoint so disabling the test for now.; Sleep a second before letting the `WorkflowExecutionActorSpec` check for metadata status.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1252
https://github.com/broadinstitute/cromwell/issues/1253:468,Availability,echo,echoHelloWorld,468,"I ran a super small WDL, then checked its status to make sure it was running. Then, I tried to abort it -- the request returned a 500: ""The server was not able to produce a timely response to your request."". However, when I check the status of the workflow, it says it has been successfully aborted. I included the WDL i ran against https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml in case you'd like to try it yourself. task echoHelloWorld {; command {; echo 'Hello, World!'; }; runtime {; docker: ""phusion/baseimage""; disks: ""local-disk 10 HDD""; memory: ""1 GB""; preemptible: 3; }; }. workflow printHelloAndGoodbye {; call echoHelloWorld; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253
https://github.com/broadinstitute/cromwell/issues/1253:497,Availability,echo,echo,497,"I ran a super small WDL, then checked its status to make sure it was running. Then, I tried to abort it -- the request returned a 500: ""The server was not able to produce a timely response to your request."". However, when I check the status of the workflow, it says it has been successfully aborted. I included the WDL i ran against https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml in case you'd like to try it yourself. task echoHelloWorld {; command {; echo 'Hello, World!'; }; runtime {; docker: ""phusion/baseimage""; disks: ""local-disk 10 HDD""; memory: ""1 GB""; preemptible: 3; }; }. workflow printHelloAndGoodbye {; call echoHelloWorld; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253
https://github.com/broadinstitute/cromwell/issues/1253:666,Availability,echo,echoHelloWorld,666,"I ran a super small WDL, then checked its status to make sure it was running. Then, I tried to abort it -- the request returned a 500: ""The server was not able to produce a timely response to your request."". However, when I check the status of the workflow, it says it has been successfully aborted. I included the WDL i ran against https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml in case you'd like to try it yourself. task echoHelloWorld {; command {; echo 'Hello, World!'; }; runtime {; docker: ""phusion/baseimage""; disks: ""local-disk 10 HDD""; memory: ""1 GB""; preemptible: 3; }; }. workflow printHelloAndGoodbye {; call echoHelloWorld; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253
https://github.com/broadinstitute/cromwell/issues/1253:95,Safety,abort,abort,95,"I ran a super small WDL, then checked its status to make sure it was running. Then, I tried to abort it -- the request returned a 500: ""The server was not able to produce a timely response to your request."". However, when I check the status of the workflow, it says it has been successfully aborted. I included the WDL i ran against https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml in case you'd like to try it yourself. task echoHelloWorld {; command {; echo 'Hello, World!'; }; runtime {; docker: ""phusion/baseimage""; disks: ""local-disk 10 HDD""; memory: ""1 GB""; preemptible: 3; }; }. workflow printHelloAndGoodbye {; call echoHelloWorld; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253
https://github.com/broadinstitute/cromwell/issues/1253:291,Safety,abort,aborted,291,"I ran a super small WDL, then checked its status to make sure it was running. Then, I tried to abort it -- the request returned a 500: ""The server was not able to produce a timely response to your request."". However, when I check the status of the workflow, it says it has been successfully aborted. I included the WDL i ran against https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml in case you'd like to try it yourself. task echoHelloWorld {; command {; echo 'Hello, World!'; }; runtime {; docker: ""phusion/baseimage""; disks: ""local-disk 10 HDD""; memory: ""1 GB""; preemptible: 3; }; }. workflow printHelloAndGoodbye {; call echoHelloWorld; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253
https://github.com/broadinstitute/cromwell/pull/1254:340,Performance,perform,perform,340,Minor change in order to support different implementations of KV stores.; I think with this minor change we will be able to use different dbs for this service and not just SQL.; Our business requires to use Doc dbs so that why I'm proposing this change.; Later in the future we can discuss if it's worthy to define a DAL. FYI... I tried to perform minimal changes to what is there...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254
https://github.com/broadinstitute/cromwell/issues/1261:1106,Availability,avail,available,1106,"ava -version; java version ""1.8.0_73""; Java(TM) SE Runtime Environment (build 1.8.0_73-b02); Java HotSpot(TM) Client VM (build 25.73-b02, mixed mode); ```. Cromwell: (https://github.com/broadinstitute/cromwell/releases/tag/0.19.3). ```; >java -jar cromwell.jar run hello.wdl hello.json; [2016-08-08 08:33:03,503] [info] Slf4jLogger started; [2016-08-08 08:33:03,533] [info] RUN sub-command; [2016-08-08 08:33:03,533] [info] WDL file: hello.wdl; [2016-08-08 08:33:03,533] [info] Inputs: hello.json; [2016-08-08 08:33:03,573] [info] SingleWorkflowRunnerActor: launching workflow; [2016-08-08 08:33:04,203] [info] Running with database db.url = jdbc:hsqldb:mem:7e19faf1-d831-4edc-83fa-086ef9b16cd3;shutdown=false;hsqldb.tx=mvcc; [2016-08-08 08:33:08,947] [info] WorkflowManagerActor submitWorkflow input id =None, effective id = 4e20eafc-baae-4605-a010-adfa5f32ae46; [2016-08-08 08:33:09,687] [←[38;5;220mwarn←[0m] Failed to get application default credentials; java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:93); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191); at cromwell.util.google.GoogleCredentialFactory.forApplicationDefaultCredentials(GoogleCredentialFactory.scala:125); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme$lzycompute(GoogleCredentialFactory.scala:64); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme(GoogleCredent",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:1126,Availability,avail,available,1126,"ironment (build 1.8.0_73-b02); Java HotSpot(TM) Client VM (build 25.73-b02, mixed mode); ```. Cromwell: (https://github.com/broadinstitute/cromwell/releases/tag/0.19.3). ```; >java -jar cromwell.jar run hello.wdl hello.json; [2016-08-08 08:33:03,503] [info] Slf4jLogger started; [2016-08-08 08:33:03,533] [info] RUN sub-command; [2016-08-08 08:33:03,533] [info] WDL file: hello.wdl; [2016-08-08 08:33:03,533] [info] Inputs: hello.json; [2016-08-08 08:33:03,573] [info] SingleWorkflowRunnerActor: launching workflow; [2016-08-08 08:33:04,203] [info] Running with database db.url = jdbc:hsqldb:mem:7e19faf1-d831-4edc-83fa-086ef9b16cd3;shutdown=false;hsqldb.tx=mvcc; [2016-08-08 08:33:08,947] [info] WorkflowManagerActor submitWorkflow input id =None, effective id = 4e20eafc-baae-4605-a010-adfa5f32ae46; [2016-08-08 08:33:09,687] [←[38;5;220mwarn←[0m] Failed to get application default credentials; java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:93); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191); at cromwell.util.google.GoogleCredentialFactory.forApplicationDefaultCredentials(GoogleCredentialFactory.scala:125); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme$lzycompute(GoogleCredentialFactory.scala:64); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme(GoogleCredentialFactory.scala:61); at cromwell.engine.backend.io.filesyste",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:7835,Availability,error,error,7835,"eafc←[0m]: starting calls: test.hello; [2016-08-08 08:33:10,111] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Starting.; [2016-08-08 08:33:10,121] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: inputs for call 'hello': name -> WdlString(String); [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: createdcall actor for hello.; [2016-08-08 08:33:10,271] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,291] [info] LocalBackend [←[38;5;2m4e20eafc←[0m:hello]: ←[38;5;5mecho 'Hello String!'←[0m; [2016-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:7952,Availability,error,error,7952,"8:33:10,121] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: inputs for call 'hello': name -> WdlString(String); [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: createdcall actor for hello.; [2016-08-08 08:33:10,271] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,291] [info] LocalBackend [←[38;5;2m4e20eafc←[0m:hello]: ←[38;5;5mecho 'Hello String!'←[0m; [2016-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDisp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:9338,Availability,error,error,9338,"6-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:292,Deployability,release,releases,292,"Hello,; I'm try to run Cromwell on ""Win7 Home Premium SP1, 32bit"". Java:. ```; > java -version; java version ""1.8.0_73""; Java(TM) SE Runtime Environment (build 1.8.0_73-b02); Java HotSpot(TM) Client VM (build 25.73-b02, mixed mode); ```. Cromwell: (https://github.com/broadinstitute/cromwell/releases/tag/0.19.3). ```; >java -jar cromwell.jar run hello.wdl hello.json; [2016-08-08 08:33:03,503] [info] Slf4jLogger started; [2016-08-08 08:33:03,533] [info] RUN sub-command; [2016-08-08 08:33:03,533] [info] WDL file: hello.wdl; [2016-08-08 08:33:03,533] [info] Inputs: hello.json; [2016-08-08 08:33:03,573] [info] SingleWorkflowRunnerActor: launching workflow; [2016-08-08 08:33:04,203] [info] Running with database db.url = jdbc:hsqldb:mem:7e19faf1-d831-4edc-83fa-086ef9b16cd3;shutdown=false;hsqldb.tx=mvcc; [2016-08-08 08:33:08,947] [info] WorkflowManagerActor submitWorkflow input id =None, effective id = 4e20eafc-baae-4605-a010-adfa5f32ae46; [2016-08-08 08:33:09,687] [←[38;5;220mwarn←[0m] Failed to get application default credentials; java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:93); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191); at cromwell.util.google.GoogleCredentialFactory.forApplicationDefaultCredentials(GoogleCredentialFactory.scala:125); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme$lzycompute(GoogleCredentialFactory.scala:64); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:6155,Integrability,message,message,6155,"rCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baae-4605-a010-adfa5f32ae46←[0m; [2016-08-08 08:33:10,104] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Beginning transition from Submitted to Running.; [2016-08-08 08:33:10,106] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: transitioning from Submitted to Running.; [2016-08-08 08:33:10,111] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: starting calls: test.hello; [2016-08-08 08:33:10,111] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Starting.; [2016-08-08 08:33:10,121] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: inputs for call 'hello': name -> WdlString(String); [2016-08-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:6350,Integrability,message,message,6350,"rCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baae-4605-a010-adfa5f32ae46←[0m; [2016-08-08 08:33:10,104] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Beginning transition from Submitted to Running.; [2016-08-08 08:33:10,106] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: transitioning from Submitted to Running.; [2016-08-08 08:33:10,111] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: starting calls: test.hello; [2016-08-08 08:33:10,111] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Starting.; [2016-08-08 08:33:10,121] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: inputs for call 'hello': name -> WdlString(String); [2016-08-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:1200,Modifiability,variab,variable,1200,". Cromwell: (https://github.com/broadinstitute/cromwell/releases/tag/0.19.3). ```; >java -jar cromwell.jar run hello.wdl hello.json; [2016-08-08 08:33:03,503] [info] Slf4jLogger started; [2016-08-08 08:33:03,533] [info] RUN sub-command; [2016-08-08 08:33:03,533] [info] WDL file: hello.wdl; [2016-08-08 08:33:03,533] [info] Inputs: hello.json; [2016-08-08 08:33:03,573] [info] SingleWorkflowRunnerActor: launching workflow; [2016-08-08 08:33:04,203] [info] Running with database db.url = jdbc:hsqldb:mem:7e19faf1-d831-4edc-83fa-086ef9b16cd3;shutdown=false;hsqldb.tx=mvcc; [2016-08-08 08:33:08,947] [info] WorkflowManagerActor submitWorkflow input id =None, effective id = 4e20eafc-baae-4605-a010-adfa5f32ae46; [2016-08-08 08:33:09,687] [←[38;5;220mwarn←[0m] Failed to get application default credentials; java.io.IOException: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.; at com.google.api.client.googleapis.auth.oauth2.DefaultCredentialProvider.getDefaultCredential(DefaultCredentialProvider.java:93); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:213); at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.getApplicationDefault(GoogleCredential.java:191); at cromwell.util.google.GoogleCredentialFactory.forApplicationDefaultCredentials(GoogleCredentialFactory.scala:125); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme$lzycompute(GoogleCredentialFactory.scala:64); at cromwell.util.google.GoogleCredentialFactory.fromCromwellAuthScheme(GoogleCredentialFactory.scala:61); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$$anonfun$cromwellAuthenticated$1.apply(StorageFactory.scala:20); at cro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:5454,Performance,concurren,concurrent,5454,"torActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:5527,Performance,concurren,concurrent,5527,"tor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baae-4605-a010-adfa5f32ae46←[0m; [2016-08-08 08:33:10,104] [info] WorkflowAc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:5612,Performance,concurren,concurrent,5612,"l.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baae-4605-a010-adfa5f32ae46←[0m; [2016-08-08 08:33:10,104] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Beginning transition from Submitted to Running.; [2016-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:5689,Performance,concurren,concurrent,5689,"$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baae-4605-a010-adfa5f32ae46←[0m; [2016-08-08 08:33:10,104] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Beginning transition from Submitted to Running.; [2016-08-08 08:33:10,106] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: transitionin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:8645,Performance,concurren,concurrent,8645,"6-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:8735,Performance,concurren,concurrent,8735,"6-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:8980,Performance,concurren,concurrent,8980,"6-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:9053,Performance,concurren,concurrent,9053,"6-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:9138,Performance,concurren,concurrent,9138,"6-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:9215,Performance,concurren,concurrent,9215,"6-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.io.IOException: CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessImpl.create(Native Method); at java.lang.ProcessImpl.<init>(Unknown Source); at java.lang.ProcessImpl.start(Unknown Source); ... 15 common frames omitted; ```. Riccardo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:3362,Security,Validat,ValidationFlatMap,3362,y$.apply(Try.scala:192); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated$lzycompute(StorageFactory.scala:20); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated(StorageFactory.scala:18); at cromwell.engine.backend.local.LocalBackend$$anonfun$14.apply(LocalBackend.scala:239); at cromwell.engine.backend.local.LocalBackend$$anonfun$14.apply(LocalBackend.scala:239); at scala.util.Try.orElse(Try.scala:84); at cromwell.engine.backend.local.LocalBackend.fileSystems(LocalBackend.scala:239); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:89); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.Materia,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:3388,Security,Validat,Validation,3388,:192); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated$lzycompute(StorageFactory.scala:20); at cromwell.engine.backend.io.filesystem.gcs.StorageFactory$.cromwellAuthenticated(StorageFactory.scala:18); at cromwell.engine.backend.local.LocalBackend$$anonfun$14.apply(LocalBackend.scala:239); at cromwell.engine.backend.local.LocalBackend$$anonfun$14.apply(LocalBackend.scala:239); at scala.util.Try.orElse(Try.scala:84); at cromwell.engine.backend.local.LocalBackend.fileSystems(LocalBackend.scala:239); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:89); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescrip,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:4075,Security,Validat,ValidationFlatMap,4075,$1.apply(MaterializeWorkflowDescriptorActor.scala:89); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(Materia,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:4101,Security,Validat,Validation,4101,zeWorkflowDescriptorActor.scala:89); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescrip,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:4558,Security,Validat,ValidationFlatMap,4558,alizeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQue,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:4584,Security,Validat,Validation,4584,iptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:6813,Testability,test,test,6813,"6-08-08 08:33:09,747] [info] Updating WorkflowManager state. New Data: (4e20eafc-baae-4605-a010-adfa5f32ae46,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-4e20eafc-baae-4605-a010-adfa5f32ae46#-904922324]); [2016-08-08 08:33:09,767] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#576120716])) message received; [2016-08-08 08:33:10,91] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwellsystem/user/SingleWorkflowRunnerActor#576120716]))) message received; [2016-08-08 08:33:10,94] [info] SingleWorkflowRunnerActor: workflow ID ←[38;5;2m4e20eafc-baae-4605-a010-adfa5f32ae46←[0m; [2016-08-08 08:33:10,104] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: Beginning transition from Submitted to Running.; [2016-08-08 08:33:10,106] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: transitioning from Submitted to Running.; [2016-08-08 08:33:10,111] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: starting calls: test.hello; [2016-08-08 08:33:10,111] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Starting.; [2016-08-08 08:33:10,121] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: inputs for call 'hello': name -> WdlString(String); [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: createdcall actor for hello.; [2016-08-08 08:33:10,271] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,291] [info] LocalBackend [←[38;5;2m4e20eafc←[0m:hello]: ←[38;5;5mecho 'Hello String!'←[0m; [2016-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1261:8093,Usability,Simpl,Simple,8093,"2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: inputs for call 'hello': name -> WdlString(String); [2016-08-08 08:33:10,251] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: createdcall actor for hello.; [2016-08-08 08:33:10,271] [←[38;5;220mwarn←[0m] Found unsupported keys for backend 'LOCAL': bootDiskSizeGb, cpu, disks, memory, preemptible, zones; [2016-08-08 08:33:10,291] [info] LocalBackend [←[38;5;2m4e20eafc←[0m:hello]: ←[38;5;5mecho 'Hello String!'←[0m; [2016-08-08 08:33:10,291] [info] WorkflowActor [←[38;5;2m4e20eafc←[0m]: persisting status of hello to Running.; [2016-08-08 08:33:10,311] [←[38;5;1merror←[0m] BackendCallExecutionActor [←[38;5 ;2m4e20eafc←[0m:hello]: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; java.io.IOException: Cannot run program ""/bin/bash"": CreateProcess error=2, Impossibile trovare il file specificato; at java.lang.ProcessBuilder.start(Unknown Source); at scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:69); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:100); at scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:99); at cromwell.engine.backend.local.LocalBackend.cromwell$engine$backend$local$LocalBackend$$runSubprocess(LocalBackend.scala:172); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:119); at cromwell.engine.backend.local.LocalBackend$$anonfun$execute$1.apply(LocalBackend.scala:113); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24); at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$W",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1261
https://github.com/broadinstitute/cromwell/issues/1263:249,Availability,error,error,249,"Note that in order to aggregate files after a scatter step, I must include the python code (see `run_plot_purity_series` task) in the next task. The next task takes in a file of files. ; The python inside the wdl is very counter-intuitive, prone to error, and unnecessary in other execution managers. See my real example below... ``` wdl; workflow crsp_validation_workflow {. ....snip....; Array[Array[File]] triplet_file_array = read_tsv(input_triplet_file_list); Float ploidy=""2"". scatter (triplet in triplet_file_array) {; ....snip.... call run_sensitivity_precision {; input:; entity_id=triplet[0],; oncotated_target_seg_gt_file = oncotate.oncotated_target_seg_gt_file,; ploidy=ploidy; }; }. call run_plot_purity_series {; input:; output_dir=""plots/"",; amp_sens_prec=run_sensitivity_precision.amp_sens_prec_file,; del_sens_prec=run_sensitivity_precision.del_sens_prec_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263
https://github.com/broadinstitute/cromwell/issues/1263:1212,Energy Efficiency,reduce,reduced,1212,"xt task. The next task takes in a file of files. ; The python inside the wdl is very counter-intuitive, prone to error, and unnecessary in other execution managers. See my real example below... ``` wdl; workflow crsp_validation_workflow {. ....snip....; Array[Array[File]] triplet_file_array = read_tsv(input_triplet_file_list); Float ploidy=""2"". scatter (triplet in triplet_file_array) {; ....snip.... call run_sensitivity_precision {; input:; entity_id=triplet[0],; oncotated_target_seg_gt_file = oncotate.oncotated_target_seg_gt_file,; ploidy=ploidy; }; }. call run_plot_purity_series {; input:; output_dir=""plots/"",; amp_sens_prec=run_sensitivity_precision.amp_sens_prec_file,; del_sens_prec=run_sensitivity_precision.del_sens_prec_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split("","")); with open(""sens_prec_aggregate.txt"", ""w"") as fp:; fp.write('\n'.join(files)); CODE; wc -l sens_prec_aggregate.txt. python <<CODE;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263
https://github.com/broadinstitute/cromwell/issues/1263:1961,Modifiability,extend,extend,1961,"c_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split("","")); with open(""sens_prec_aggregate.txt"", ""w"") as fp:; fp.write('\n'.join(files)); CODE; wc -l sens_prec_aggregate.txt. python <<CODE; files = ""${sep="","" small_sens}"".split("",""); with open(""small_sens_aggregate.txt"", ""w"") as fp:; fp.write('\n'.join(files)); CODE; wc -l small_sens_aggregate.txt. run_plot_purity_series sens_prec_aggregate.txt small_sens_aggregate.txt /root/eval-gatk-protected/sample_purity_table.tsv ${output_dir}; }. output {; File purity_series_amp = ""${output_dir}/purity_series_Amplifications.png""; File purity_series_del = ""${output_dir}/purity_series_Deletions.png""; File purity_series_small_amp = ""${output_dir}/purity_series_small_Amplifications.png""; File purity_series_small_del = ""${output_dir}/purity_series_small_Deletions.png""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263
https://github.com/broadinstitute/cromwell/issues/1263:1180,Safety,detect,detection,1180,"xt task. The next task takes in a file of files. ; The python inside the wdl is very counter-intuitive, prone to error, and unnecessary in other execution managers. See my real example below... ``` wdl; workflow crsp_validation_workflow {. ....snip....; Array[Array[File]] triplet_file_array = read_tsv(input_triplet_file_list); Float ploidy=""2"". scatter (triplet in triplet_file_array) {; ....snip.... call run_sensitivity_precision {; input:; entity_id=triplet[0],; oncotated_target_seg_gt_file = oncotate.oncotated_target_seg_gt_file,; ploidy=ploidy; }; }. call run_plot_purity_series {; input:; output_dir=""plots/"",; amp_sens_prec=run_sensitivity_precision.amp_sens_prec_file,; del_sens_prec=run_sensitivity_precision.del_sens_prec_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split("","")); with open(""sens_prec_aggregate.txt"", ""w"") as fp:; fp.write('\n'.join(files)); CODE; wc -l sens_prec_aggregate.txt. python <<CODE;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263
https://github.com/broadinstitute/cromwell/issues/1263:229,Usability,intuit,intuitive,229,"Note that in order to aggregate files after a scatter step, I must include the python code (see `run_plot_purity_series` task) in the next task. The next task takes in a file of files. ; The python inside the wdl is very counter-intuitive, prone to error, and unnecessary in other execution managers. See my real example below... ``` wdl; workflow crsp_validation_workflow {. ....snip....; Array[Array[File]] triplet_file_array = read_tsv(input_triplet_file_list); Float ploidy=""2"". scatter (triplet in triplet_file_array) {; ....snip.... call run_sensitivity_precision {; input:; entity_id=triplet[0],; oncotated_target_seg_gt_file = oncotate.oncotated_target_seg_gt_file,; ploidy=ploidy; }; }. call run_plot_purity_series {; input:; output_dir=""plots/"",; amp_sens_prec=run_sensitivity_precision.amp_sens_prec_file,; del_sens_prec=run_sensitivity_precision.del_sens_prec_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263
https://github.com/broadinstitute/cromwell/issues/1264:4,Modifiability,plugin,plugin,4,"The plugin is really nice when developing in the IDEs. Unfortunately, pycharm is not supported and evaluation workflows will probably be majority python scripts.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1264
https://github.com/broadinstitute/cromwell/issues/1266:58,Deployability,configurat,configuration,58,"To reproduce: Add an invalid actor for KeyValueService in configuration. Startup should fail, but instead the service continues running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1266
https://github.com/broadinstitute/cromwell/issues/1266:58,Modifiability,config,configuration,58,"To reproduce: Add an invalid actor for KeyValueService in configuration. Startup should fail, but instead the service continues running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1266
https://github.com/broadinstitute/cromwell/pull/1268:54,Modifiability,refactor,refactoring,54,"~~There's a few slick horrors in here. Any advice for refactoring the worst offenders would be welcomed~~. Thanks to @mcovarr, horrors are mostly erased",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1268
https://github.com/broadinstitute/cromwell/issues/1269:731,Integrability,wrap,wrapped,731,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:839,Security,hash,hash,839,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:88,Testability,log,log,88,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:235,Testability,log,logs,235,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:245,Testability,test,tests,245,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:458,Testability,test,tests,458,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:583,Testability,test,test,583,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:652,Testability,test,tests,652,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:718,Testability,log,logic,718,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:865,Testability,test,test,865,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:893,Testability,test,test,893,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1269:566,Usability,feedback,feedback,566,"Note to @kcibul - this is just a placeholder for now, not a would-be punching bag like ""log smarter, not harder"" :). As I'm rewriting CromwellTestKitSpec and thus going through all of the variations of the ""run a wdl and check outputs/logs/etc"" tests in the codebase, I'm realizing that the extent of overlap with Centaur is even larger than I'd previously thought. We need to bring sanity to this ...; - On one hand, all of these ""run a full workflow"" type tests belong in Centaur. If not, define what Centaur is for, if anything; - On the other hand, having rapid feedback via sbt test is useful; - And people find it annoying to write proper ""unit"" tests on behavior; - I'd argue this is a code smell, our business logic is too wrapped up with the actor-y stuff. But that's part of what we need to decide. At the very least, we need to hash out WTF _is_ a ""unit test"" (i.e. running by 'sbt test'), what belongs in Centaur, etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1269
https://github.com/broadinstitute/cromwell/issues/1270:152,Availability,error,error,152,I accidentally submitted a WDL and inputs to the batch submission endpoint instead of the regular submission endpoint. Cromwell did not respond with an error and eventually just timed out.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1270
https://github.com/broadinstitute/cromwell/issues/1271:343,Integrability,contract,contract,343,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271
https://github.com/broadinstitute/cromwell/issues/1271:454,Performance,perform,performance,454,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271
https://github.com/broadinstitute/cromwell/issues/1271:317,Safety,risk,risk,317,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271
https://github.com/broadinstitute/cromwell/issues/1271:292,Security,hash,hash,292,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271
https://github.com/broadinstitute/cromwell/issues/1272:38,Availability,failure,failures,38,"There seem to be intermittent Centaur failures on `invalid_runtime_attributes`. I investigated this for a while and found the following:. `WorkflowActor` will always try to run finalization, even if initialization fails. And if initialization fails there will be no initialization data, which means a `.get` on the optional initialization data will throw. Unfortunately this is exactly what `toJes` in `JesBackendLifecycleActorFactory` is doing:. ``` scala; def toJes = genericInitializationData collectFirst { case d: JesBackendInitializationData => d } get; ```. My guess is that this doesn't fail 100% of the time because there's already failure metadata generated (intentionally), and there's a race condition as to which failure event shows up first. If the intentional failure is generated first the test would pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272
https://github.com/broadinstitute/cromwell/issues/1272:641,Availability,failure,failure,641,"There seem to be intermittent Centaur failures on `invalid_runtime_attributes`. I investigated this for a while and found the following:. `WorkflowActor` will always try to run finalization, even if initialization fails. And if initialization fails there will be no initialization data, which means a `.get` on the optional initialization data will throw. Unfortunately this is exactly what `toJes` in `JesBackendLifecycleActorFactory` is doing:. ``` scala; def toJes = genericInitializationData collectFirst { case d: JesBackendInitializationData => d } get; ```. My guess is that this doesn't fail 100% of the time because there's already failure metadata generated (intentionally), and there's a race condition as to which failure event shows up first. If the intentional failure is generated first the test would pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272
https://github.com/broadinstitute/cromwell/issues/1272:726,Availability,failure,failure,726,"There seem to be intermittent Centaur failures on `invalid_runtime_attributes`. I investigated this for a while and found the following:. `WorkflowActor` will always try to run finalization, even if initialization fails. And if initialization fails there will be no initialization data, which means a `.get` on the optional initialization data will throw. Unfortunately this is exactly what `toJes` in `JesBackendLifecycleActorFactory` is doing:. ``` scala; def toJes = genericInitializationData collectFirst { case d: JesBackendInitializationData => d } get; ```. My guess is that this doesn't fail 100% of the time because there's already failure metadata generated (intentionally), and there's a race condition as to which failure event shows up first. If the intentional failure is generated first the test would pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272
https://github.com/broadinstitute/cromwell/issues/1272:775,Availability,failure,failure,775,"There seem to be intermittent Centaur failures on `invalid_runtime_attributes`. I investigated this for a while and found the following:. `WorkflowActor` will always try to run finalization, even if initialization fails. And if initialization fails there will be no initialization data, which means a `.get` on the optional initialization data will throw. Unfortunately this is exactly what `toJes` in `JesBackendLifecycleActorFactory` is doing:. ``` scala; def toJes = genericInitializationData collectFirst { case d: JesBackendInitializationData => d } get; ```. My guess is that this doesn't fail 100% of the time because there's already failure metadata generated (intentionally), and there's a race condition as to which failure event shows up first. If the intentional failure is generated first the test would pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272
https://github.com/broadinstitute/cromwell/issues/1272:699,Performance,race condition,race condition,699,"There seem to be intermittent Centaur failures on `invalid_runtime_attributes`. I investigated this for a while and found the following:. `WorkflowActor` will always try to run finalization, even if initialization fails. And if initialization fails there will be no initialization data, which means a `.get` on the optional initialization data will throw. Unfortunately this is exactly what `toJes` in `JesBackendLifecycleActorFactory` is doing:. ``` scala; def toJes = genericInitializationData collectFirst { case d: JesBackendInitializationData => d } get; ```. My guess is that this doesn't fail 100% of the time because there's already failure metadata generated (intentionally), and there's a race condition as to which failure event shows up first. If the intentional failure is generated first the test would pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272
https://github.com/broadinstitute/cromwell/issues/1272:806,Testability,test,test,806,"There seem to be intermittent Centaur failures on `invalid_runtime_attributes`. I investigated this for a while and found the following:. `WorkflowActor` will always try to run finalization, even if initialization fails. And if initialization fails there will be no initialization data, which means a `.get` on the optional initialization data will throw. Unfortunately this is exactly what `toJes` in `JesBackendLifecycleActorFactory` is doing:. ``` scala; def toJes = genericInitializationData collectFirst { case d: JesBackendInitializationData => d } get; ```. My guess is that this doesn't fail 100% of the time because there's already failure metadata generated (intentionally), and there's a race condition as to which failure event shows up first. If the intentional failure is generated first the test would pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272
https://github.com/broadinstitute/cromwell/issues/1274:343,Integrability,contract,contract,343,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1274
https://github.com/broadinstitute/cromwell/issues/1274:454,Performance,perform,performance,454,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1274
https://github.com/broadinstitute/cromwell/issues/1274:317,Safety,risk,risk,317,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1274
https://github.com/broadinstitute/cromwell/issues/1274:292,Security,hash,hash,292,"As a user with a controlled file system (like GOTC or the cromwell execution directory) where the I know that a file path is immutable and uniquely identifying, I would like to run the cromwell server in a mode where the file path can be used in call caching rather than computing the actual hash. I will take on the risk that if I break that contract (by modifying files), workflows will not execute properly. I want to do this because it will be a big performance gain when I have many files and I know that their paths are unique. @cjllanwarne gets credit for raising this as a cool feature, @jsotobroad and @dshiga agreed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1274
https://github.com/broadinstitute/cromwell/pull/1275:310,Testability,test,tests,310,The outputs query was changed such that the key the `MetadataBuilderActor` was looking for in the `WorkflowOutputsResponse` handler was never returned. `MetadataBuilderActor` quietly (some might say blithely) fell back to returning empty outputs. . It's alarming that this total breakage was not caught by any tests. But rather than add to the morass that is `CromwellTestKitSpec` I'm going to look at enhancing Centaur to be able to hit the outputs endpoint.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1275
https://github.com/broadinstitute/cromwell/issues/1276:65,Testability,test,tests,65,These may exist. Or they might not. They ought to. Also add unit tests as appropriate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1276
https://github.com/broadinstitute/cromwell/pull/1277:180,Testability,Test,Test,180,Uses liquibase [custom change](http://www.liquibase.org/documentation/changes/custom_change.html) to write classes that transform pre-0.20 existing data into 0.20 metadata.; - [x] Test at scale,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1277
https://github.com/broadinstitute/cromwell/pull/1278:149,Energy Efficiency,reduce,reduce,149,"Three tests are still ignored-- not sure if they are worth keeping. Small changes to the structure of the case classes of KeyValueService. Hoping to reduce having to pass around scopes just to gather basic call information like fqn, index and attempt.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1278
https://github.com/broadinstitute/cromwell/pull/1278:6,Testability,test,tests,6,"Three tests are still ignored-- not sure if they are worth keeping. Small changes to the structure of the case classes of KeyValueService. Hoping to reduce having to pass around scopes just to gather basic call information like fqn, index and attempt.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1278
https://github.com/broadinstitute/cromwell/issues/1279:45,Energy Efficiency,green,green,45,"When investigating the GCS ls issue from the green folks in 0.19 I saw that it was using one of our old style blocking retries. I then assumed that there's no way we'd still be doing that and told everyone that we weren't. Then I went and double checked what I said and sadly I was wrong. GcsFileSystemProvider.withRetry blocks threads. With the new dispatcher bulkheading it should be better, but considering how many of these might be in flight at once in a joint genotyping sort of environment, this seems bad.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279
https://github.com/broadinstitute/cromwell/issues/1280:9,Performance,Cache,Cache,9,"The Call Cache persists job results as `simpleton`s to prevent large arrays breaking database limits. The JobStore write out job results as a single giant `JSON` blob. It should use `simpleton`s too (preferably in the same way, and using the same code, as the Call Cache)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1280
https://github.com/broadinstitute/cromwell/issues/1280:265,Performance,Cache,Cache,265,"The Call Cache persists job results as `simpleton`s to prevent large arrays breaking database limits. The JobStore write out job results as a single giant `JSON` blob. It should use `simpleton`s too (preferably in the same way, and using the same code, as the Call Cache)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1280
https://github.com/broadinstitute/cromwell/issues/1280:40,Usability,simpl,simpleton,40,"The Call Cache persists job results as `simpleton`s to prevent large arrays breaking database limits. The JobStore write out job results as a single giant `JSON` blob. It should use `simpleton`s too (preferably in the same way, and using the same code, as the Call Cache)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1280
https://github.com/broadinstitute/cromwell/issues/1280:183,Usability,simpl,simpleton,183,"The Call Cache persists job results as `simpleton`s to prevent large arrays breaking database limits. The JobStore write out job results as a single giant `JSON` blob. It should use `simpleton`s too (preferably in the same way, and using the same code, as the Call Cache)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1280
https://github.com/broadinstitute/cromwell/pull/1289:383,Deployability,update,updated,383,- Creates a `CallCacheReadActor` to find cache hits in the database.; - Fixes a bug on cache hit checking that was referencing obsolete state data and missing legitimate cache hits.; - Makes data types that are logically sets actually `Set`s.; - Fix data type of `ALLOW_RESULT_REUSE` to match the 0.19 equivalent.; - Fix `CallCachingResultMetaInfoComponent` file naming to match the updated class name.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1289
https://github.com/broadinstitute/cromwell/pull/1289:41,Performance,cache,cache,41,- Creates a `CallCacheReadActor` to find cache hits in the database.; - Fixes a bug on cache hit checking that was referencing obsolete state data and missing legitimate cache hits.; - Makes data types that are logically sets actually `Set`s.; - Fix data type of `ALLOW_RESULT_REUSE` to match the 0.19 equivalent.; - Fix `CallCachingResultMetaInfoComponent` file naming to match the updated class name.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1289
https://github.com/broadinstitute/cromwell/pull/1289:87,Performance,cache,cache,87,- Creates a `CallCacheReadActor` to find cache hits in the database.; - Fixes a bug on cache hit checking that was referencing obsolete state data and missing legitimate cache hits.; - Makes data types that are logically sets actually `Set`s.; - Fix data type of `ALLOW_RESULT_REUSE` to match the 0.19 equivalent.; - Fix `CallCachingResultMetaInfoComponent` file naming to match the updated class name.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1289
https://github.com/broadinstitute/cromwell/pull/1289:170,Performance,cache,cache,170,- Creates a `CallCacheReadActor` to find cache hits in the database.; - Fixes a bug on cache hit checking that was referencing obsolete state data and missing legitimate cache hits.; - Makes data types that are logically sets actually `Set`s.; - Fix data type of `ALLOW_RESULT_REUSE` to match the 0.19 equivalent.; - Fix `CallCachingResultMetaInfoComponent` file naming to match the updated class name.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1289
https://github.com/broadinstitute/cromwell/pull/1289:211,Testability,log,logically,211,- Creates a `CallCacheReadActor` to find cache hits in the database.; - Fixes a bug on cache hit checking that was referencing obsolete state data and missing legitimate cache hits.; - Makes data types that are logically sets actually `Set`s.; - Fix data type of `ALLOW_RESULT_REUSE` to match the 0.19 equivalent.; - Fix `CallCachingResultMetaInfoComponent` file naming to match the updated class name.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1289
https://github.com/broadinstitute/cromwell/pull/1290:365,Availability,failure,failure,365,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:151,Modifiability,config,config,151,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:523,Performance,cache,cache,523,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:349,Safety,avoid,avoid,349,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:8,Security,hash,hashes,8,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:205,Security,hash,hashes,205,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:357,Security,hash,hashing,357,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:392,Security,hash,hash,392,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:408,Security,hash,hash-docker-names,408,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:437,Security,hash,hash-file-paths,437,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:576,Security,hash,hash-file-contents,576,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1290:105,Usability,simpl,simpletons,105,"Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1290
https://github.com/broadinstitute/cromwell/pull/1291:415,Availability,failure,failure,415,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:201,Modifiability,config,config,201,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:573,Performance,cache,cache,573,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:399,Safety,avoid,avoid,399,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:58,Security,hash,hashes,58,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:255,Security,hash,hashes,255,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:407,Security,hash,hashing,407,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:442,Security,hash,hash,442,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:458,Security,hash,hash-docker-names,458,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:487,Security,hash,hash-file-paths,487,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:626,Security,hash,hash-file-contents,626,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/pull/1291:155,Usability,simpl,simpletons,155,"With compliments to @mcovarr for the rebase base. Creates hashes for the following:; - command; - backend name; - output expression; - non-file inputs (as simpletons); - file input paths (according to config). Not included in this PR:; - backend specific hashes (runtime attributes, docker, file contents). Note that if you want anything to actually be written you'll want the following options (to avoid a hashing failure); - `lookup-docker-hash=false`; - `hash-docker-names=false`; - `hash-file-paths=true` -- actually you could leave this false but... then you'd always cache hit regardless of what files you're using!; - `hash-file-contents=false`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1291
https://github.com/broadinstitute/cromwell/issues/1292:684,Availability,error,error,684,"As a user who runs cromwell in a production setting (like @ktibbett), I need to be able to manage the lifecycle of workflows in the system. After running many workflows, they consume a lot of space on disk and even within the cromwell environment. I woul to be able to delete them through a REST endpoint. Add a new endpoint at DELETE/workflows/{version}/{id} which effectively removes this workflow from the system. This should include; - removing all output files for the workflows and calls; - removing all metadata from the metadata service; - removing all workflows/calls from the call caching service. attempting to remove a workflow in a non-terminal state should result in an error (it should either finish or be aborted first). --; In detail specification:. https://docs.google.com/document/d/1aJn5HzvDgYbvBlEG4z0KO8oZgaQ3lFu2hE8QzRC0_18/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292
https://github.com/broadinstitute/cromwell/issues/1292:721,Safety,abort,aborted,721,"As a user who runs cromwell in a production setting (like @ktibbett), I need to be able to manage the lifecycle of workflows in the system. After running many workflows, they consume a lot of space on disk and even within the cromwell environment. I woul to be able to delete them through a REST endpoint. Add a new endpoint at DELETE/workflows/{version}/{id} which effectively removes this workflow from the system. This should include; - removing all output files for the workflows and calls; - removing all metadata from the metadata service; - removing all workflows/calls from the call caching service. attempting to remove a workflow in a non-terminal state should result in an error (it should either finish or be aborted first). --; In detail specification:. https://docs.google.com/document/d/1aJn5HzvDgYbvBlEG4z0KO8oZgaQ3lFu2hE8QzRC0_18/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1292
https://github.com/broadinstitute/cromwell/issues/1297:65,Performance,cache,caches-are-coming-to-everyone,65,Could speed up our builds. https://blog.travis-ci.com/2016-05-03-caches-are-coming-to-everyone. https://github.com/spray/spray/blob/master/.travis.yml,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1297
https://github.com/broadinstitute/cromwell/issues/1299:581,Availability,down,downstream,581,"JES has added an additional ""event"" to their metadata. For example:; events: ; - description: copied 1 file(s) to ""gs://some/path/sample.vcf"" ; startTime: '2016-08-15T19:39:49.261235611Z'. gcloud alpha genomics operations describe EJ7K1P3oKhjukpy15ueGiy0gw7vetLsXKg9wcm9kdWN0aW9uUXVldWU (for more details). Cromwell tries to record these execution events to the db, but sometimes the new events exceed the 255 char limit on the 'DESCRIPTION' column. . AC: Filter out these file copying events in 0.19_hotfix so that they don't clog up the execution events table, which can lead to downstream status update failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1299
https://github.com/broadinstitute/cromwell/issues/1299:606,Availability,failure,failures,606,"JES has added an additional ""event"" to their metadata. For example:; events: ; - description: copied 1 file(s) to ""gs://some/path/sample.vcf"" ; startTime: '2016-08-15T19:39:49.261235611Z'. gcloud alpha genomics operations describe EJ7K1P3oKhjukpy15ueGiy0gw7vetLsXKg9wcm9kdWN0aW9uUXVldWU (for more details). Cromwell tries to record these execution events to the db, but sometimes the new events exceed the 255 char limit on the 'DESCRIPTION' column. . AC: Filter out these file copying events in 0.19_hotfix so that they don't clog up the execution events table, which can lead to downstream status update failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1299
https://github.com/broadinstitute/cromwell/issues/1299:599,Deployability,update,update,599,"JES has added an additional ""event"" to their metadata. For example:; events: ; - description: copied 1 file(s) to ""gs://some/path/sample.vcf"" ; startTime: '2016-08-15T19:39:49.261235611Z'. gcloud alpha genomics operations describe EJ7K1P3oKhjukpy15ueGiy0gw7vetLsXKg9wcm9kdWN0aW9uUXVldWU (for more details). Cromwell tries to record these execution events to the db, but sometimes the new events exceed the 255 char limit on the 'DESCRIPTION' column. . AC: Filter out these file copying events in 0.19_hotfix so that they don't clog up the execution events table, which can lead to downstream status update failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1299
https://github.com/broadinstitute/cromwell/issues/1301:58,Security,hash,hash,58,@leetl1220 is seeing an issue on 0.19 with a large Docker hash overflowing the DOCKER_IMAGE_HASH. Expanding from 100 chars to 255 gets him around the problem.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1301
https://github.com/broadinstitute/cromwell/issues/1302:109,Availability,failure,failure,109,"Note how the wdl below attempts to use `output_dir` in its output for `reproducibility_files`. This causes a failure. If I remove the `String output_dir` from the task and update the output to a hardcoded path, `Array[File] reproducibility_files = glob(""reproducibility_output/*.*"")`, it works fine. . ``` wdl; task run_plot_reproducibility {; File file1; File file2; String output_dir; command {; run_plot_reproducibility ${file1} ${file2} ${output_dir} 3.7; }; output {; File reproducibility_table = ""${output_dir}/reproducibility.tsv""; File reproducibility_final_results = ""${output_dir}/final_results.tsv""; File reproducibility_plot = ""${output_dir}/reproducibility_Reproducibility.png""; #### HERE; Array[File] reproducibility_files = glob(""${output_dir}/*.*""); }; runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302
https://github.com/broadinstitute/cromwell/issues/1302:172,Deployability,update,update,172,"Note how the wdl below attempts to use `output_dir` in its output for `reproducibility_files`. This causes a failure. If I remove the `String output_dir` from the task and update the output to a hardcoded path, `Array[File] reproducibility_files = glob(""reproducibility_output/*.*"")`, it works fine. . ``` wdl; task run_plot_reproducibility {; File file1; File file2; String output_dir; command {; run_plot_reproducibility ${file1} ${file2} ${output_dir} 3.7; }; output {; File reproducibility_table = ""${output_dir}/reproducibility.tsv""; File reproducibility_final_results = ""${output_dir}/final_results.tsv""; File reproducibility_plot = ""${output_dir}/reproducibility_Reproducibility.png""; #### HERE; Array[File] reproducibility_files = glob(""${output_dir}/*.*""); }; runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302
https://github.com/broadinstitute/cromwell/pull/1305:83,Security,hash,hashing,83,The extracted `WdlValueSimpleton` file was thieved from @cjllanwarne's magnum opus hashing branch and some metacharacter handling added to it.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1305
https://github.com/broadinstitute/cromwell/issues/1306:149,Availability,error,error,149,"If you have spaces or braces or other symbols which should be escaped in input file names, bash will be sad about this:. > /bin/bash: line 6: syntax error near unexpected token `('. Example input:; {; ""elc.input"": ""/home/vagrant/example (1).file""; }. because in resulting bash file it will be transformed into:. > /home/vagrant/cromwell/cromwell-executions/elc/5646fba9-3bdc-4c63-aeba-16adf80ae7d2/call-tsk_ELC_/home/vagrant/example (1).file. I think Cromwell should just put paths between single quotation marks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1306
https://github.com/broadinstitute/cromwell/pull/1307:230,Performance,cache,cache,230,Known TODOs (maybe in later tickets); - Backends should use runtime attributes in jobDescriptor instead of recomputing them; - SharedFileSystem `runtimeAttributeDefinitions` are static instead of dynamic; - No short-circuiting if cache miss is known and cache writing is off; - Test writing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1307
https://github.com/broadinstitute/cromwell/pull/1307:254,Performance,cache,cache,254,Known TODOs (maybe in later tickets); - Backends should use runtime attributes in jobDescriptor instead of recomputing them; - SharedFileSystem `runtimeAttributeDefinitions` are static instead of dynamic; - No short-circuiting if cache miss is known and cache writing is off; - Test writing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1307
https://github.com/broadinstitute/cromwell/pull/1307:278,Testability,Test,Test,278,Known TODOs (maybe in later tickets); - Backends should use runtime attributes in jobDescriptor instead of recomputing them; - SharedFileSystem `runtimeAttributeDefinitions` are static instead of dynamic; - No short-circuiting if cache miss is known and cache writing is off; - Test writing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1307
https://github.com/broadinstitute/cromwell/pull/1310:266,Integrability,protocol,protocol,266,"- Removed one unnecessary capture each from the array and map regexps; - Factored out the backslash on escaped metacharacters in the map regexp; - Transformed the map regexp to standard, non-""free spacing"" form; - Added some docs on the `WdlValueSimpleton` encoding protocol",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1310
https://github.com/broadinstitute/cromwell/issues/1311:636,Availability,recover,recoverWith,636,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1311:718,Availability,recover,recoverWith,718,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1311:609,Performance,concurren,concurrent,609,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1311:691,Performance,concurren,concurrent,691,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1311:773,Performance,concurren,concurrent,773,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1311:1263,Performance,concurren,concurrent,1263,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1311:636,Safety,recover,recoverWith,636,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1311:718,Safety,recover,recoverWith,718,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1311:5,Testability,test,test,5,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1311:360,Testability,Test,TestFailedException,360,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311
https://github.com/broadinstitute/cromwell/issues/1312:184,Availability,error,error,184,"Current scheme:; - If a workflow option is specified, use that; - If a runtime attribute is specified, use that; - If neither are specified, use the default backend (silently, without error). There are a few things wrong with this.; - If a user specifies a particular backend we should _not_ be silently giving them some other backend. IMO we shouldn't be giving them another backend period.; - The order of workflow option & runtime attribute makes sense in a single backend workflow but in a multi-backend workflow (which apparently _does_ work, at least in some cases) it's wrong. I think the best thing would be to expand workflow option to allow per-call backend specification or something like that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1312
https://github.com/broadinstitute/cromwell/issues/1314:382,Integrability,Depend,Depends,382,The runtime attributes are now included in the JobDescriptor for a task. Backends should use those rather than recalculating them themselves.; - Coercions could still be done by the Backend.; - Definitions of what attributes the backend wants is part of the `BackendXFactory` trait.; - Defaults are calculated automatically. The Backend shouldn't need to fill those in any more.; - Depends on #1307.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1314
https://github.com/broadinstitute/cromwell/issues/1315:262,Integrability,Depend,Depends,262,"The Config (SFS) backend trait should dynamically fill in the `runtimeAttributeDefinitions` field based on its config. This will allow the JobPreparation to correctly assign the defaults, and allow the call cache hashing to hash the attributes (if appropriate). Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1315
https://github.com/broadinstitute/cromwell/issues/1315:4,Modifiability,Config,Config,4,"The Config (SFS) backend trait should dynamically fill in the `runtimeAttributeDefinitions` field based on its config. This will allow the JobPreparation to correctly assign the defaults, and allow the call cache hashing to hash the attributes (if appropriate). Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1315
https://github.com/broadinstitute/cromwell/issues/1315:111,Modifiability,config,config,111,"The Config (SFS) backend trait should dynamically fill in the `runtimeAttributeDefinitions` field based on its config. This will allow the JobPreparation to correctly assign the defaults, and allow the call cache hashing to hash the attributes (if appropriate). Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1315
https://github.com/broadinstitute/cromwell/issues/1315:207,Performance,cache,cache,207,"The Config (SFS) backend trait should dynamically fill in the `runtimeAttributeDefinitions` field based on its config. This will allow the JobPreparation to correctly assign the defaults, and allow the call cache hashing to hash the attributes (if appropriate). Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1315
https://github.com/broadinstitute/cromwell/issues/1315:213,Security,hash,hashing,213,"The Config (SFS) backend trait should dynamically fill in the `runtimeAttributeDefinitions` field based on its config. This will allow the JobPreparation to correctly assign the defaults, and allow the call cache hashing to hash the attributes (if appropriate). Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1315
https://github.com/broadinstitute/cromwell/issues/1315:224,Security,hash,hash,224,"The Config (SFS) backend trait should dynamically fill in the `runtimeAttributeDefinitions` field based on its config. This will allow the JobPreparation to correctly assign the defaults, and allow the call cache hashing to hash the attributes (if appropriate). Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1315
https://github.com/broadinstitute/cromwell/issues/1316:70,Deployability,update,update,70,"If call cache writing is disabled and a cache miss is known, we could update the various File Hasher actors and tell them not to bother computing no-longer-required file hashes. - [X] Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316
https://github.com/broadinstitute/cromwell/issues/1316:184,Integrability,Depend,Depends,184,"If call cache writing is disabled and a cache miss is known, we could update the various File Hasher actors and tell them not to bother computing no-longer-required file hashes. - [X] Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316
https://github.com/broadinstitute/cromwell/issues/1316:8,Performance,cache,cache,8,"If call cache writing is disabled and a cache miss is known, we could update the various File Hasher actors and tell them not to bother computing no-longer-required file hashes. - [X] Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316
https://github.com/broadinstitute/cromwell/issues/1316:40,Performance,cache,cache,40,"If call cache writing is disabled and a cache miss is known, we could update the various File Hasher actors and tell them not to bother computing no-longer-required file hashes. - [X] Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316
https://github.com/broadinstitute/cromwell/issues/1316:94,Security,Hash,Hasher,94,"If call cache writing is disabled and a cache miss is known, we could update the various File Hasher actors and tell them not to bother computing no-longer-required file hashes. - [X] Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316
https://github.com/broadinstitute/cromwell/issues/1316:170,Security,hash,hashes,170,"If call cache writing is disabled and a cache miss is known, we could update the various File Hasher actors and tell them not to bother computing no-longer-required file hashes. - [X] Depends on #1307",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1316
https://github.com/broadinstitute/cromwell/issues/1317:72,Integrability,depend,depends,72,The `DockerHashLookupWorkerActor` is not yet implemented... ... do so. (depends on #1307),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1317
https://github.com/broadinstitute/cromwell/issues/1321:73,Deployability,pipeline,pipeline,73,"The attached wdl and json file have an issue with the second call in the pipeline. It is scattered over the bam files output by the first call (RevertSam). The problem is that shard-5 grabs the wrong bam file to input into the second call (FirstSortSam). All of the other shards grab the correct file except for shard-5 which grabs the bam file that is the input to the RevertSam task (rather than its output). The workaround to fix this was to change the glob command to be more specific: if I change it from *.bam to *.unmapped.bam, then shard-5 is correct. [BrokenShard.wdl.txt](https://github.com/broadinstitute/cromwell/files/432898/BrokenShard.wdl.txt); [testBam.json.txt](https://github.com/broadinstitute/cromwell/files/432900/testBam.json.txt). Note the example files that are attached need access to the Broad file system to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1321
https://github.com/broadinstitute/cromwell/issues/1321:800,Security,access,access,800,"The attached wdl and json file have an issue with the second call in the pipeline. It is scattered over the bam files output by the first call (RevertSam). The problem is that shard-5 grabs the wrong bam file to input into the second call (FirstSortSam). All of the other shards grab the correct file except for shard-5 which grabs the bam file that is the input to the RevertSam task (rather than its output). The workaround to fix this was to change the glob command to be more specific: if I change it from *.bam to *.unmapped.bam, then shard-5 is correct. [BrokenShard.wdl.txt](https://github.com/broadinstitute/cromwell/files/432898/BrokenShard.wdl.txt); [testBam.json.txt](https://github.com/broadinstitute/cromwell/files/432900/testBam.json.txt). Note the example files that are attached need access to the Broad file system to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1321
https://github.com/broadinstitute/cromwell/issues/1321:661,Testability,test,testBam,661,"The attached wdl and json file have an issue with the second call in the pipeline. It is scattered over the bam files output by the first call (RevertSam). The problem is that shard-5 grabs the wrong bam file to input into the second call (FirstSortSam). All of the other shards grab the correct file except for shard-5 which grabs the bam file that is the input to the RevertSam task (rather than its output). The workaround to fix this was to change the glob command to be more specific: if I change it from *.bam to *.unmapped.bam, then shard-5 is correct. [BrokenShard.wdl.txt](https://github.com/broadinstitute/cromwell/files/432898/BrokenShard.wdl.txt); [testBam.json.txt](https://github.com/broadinstitute/cromwell/files/432900/testBam.json.txt). Note the example files that are attached need access to the Broad file system to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1321
https://github.com/broadinstitute/cromwell/issues/1321:735,Testability,test,testBam,735,"The attached wdl and json file have an issue with the second call in the pipeline. It is scattered over the bam files output by the first call (RevertSam). The problem is that shard-5 grabs the wrong bam file to input into the second call (FirstSortSam). All of the other shards grab the correct file except for shard-5 which grabs the bam file that is the input to the RevertSam task (rather than its output). The workaround to fix this was to change the glob command to be more specific: if I change it from *.bam to *.unmapped.bam, then shard-5 is correct. [BrokenShard.wdl.txt](https://github.com/broadinstitute/cromwell/files/432898/BrokenShard.wdl.txt); [testBam.json.txt](https://github.com/broadinstitute/cromwell/files/432900/testBam.json.txt). Note the example files that are attached need access to the Broad file system to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1321
https://github.com/broadinstitute/cromwell/issues/1325:31,Deployability,pipeline,pipelines,31,"As a user who runs large scale pipelines in Google, I may run into quota limitations on external IP addresses as they are consumed by my JES compute VMs. In many cases (using GCR and using GCS only) I would like to have JES create a node without an external IP address. JES has added this feature (see https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines) through the ""noAddress"" flag. noAddress must be set in both ephermeralPipeline and pipelineArgs. NOTE: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If your project is not whitelisted and you use noAddress, the operation will hang. This should be specified in the WDL as a runtime attribute, similar to the way cpus, memory or preemptible VMs are requested",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1325
https://github.com/broadinstitute/cromwell/issues/1325:360,Deployability,pipeline,pipelines,360,"As a user who runs large scale pipelines in Google, I may run into quota limitations on external IP addresses as they are consumed by my JES compute VMs. In many cases (using GCR and using GCS only) I would like to have JES create a node without an external IP address. JES has added this feature (see https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines) through the ""noAddress"" flag. noAddress must be set in both ephermeralPipeline and pipelineArgs. NOTE: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If your project is not whitelisted and you use noAddress, the operation will hang. This should be specified in the WDL as a runtime attribute, similar to the way cpus, memory or preemptible VMs are requested",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1325
https://github.com/broadinstitute/cromwell/issues/1325:454,Deployability,pipeline,pipelineArgs,454,"As a user who runs large scale pipelines in Google, I may run into quota limitations on external IP addresses as they are consumed by my JES compute VMs. In many cases (using GCR and using GCS only) I would like to have JES create a node without an external IP address. JES has added this feature (see https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines) through the ""noAddress"" flag. noAddress must be set in both ephermeralPipeline and pipelineArgs. NOTE: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If your project is not whitelisted and you use noAddress, the operation will hang. This should be specified in the WDL as a runtime attribute, similar to the way cpus, memory or preemptible VMs are requested",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1325
https://github.com/broadinstitute/cromwell/issues/1325:518,Security,Access,Access,518,"As a user who runs large scale pipelines in Google, I may run into quota limitations on external IP addresses as they are consumed by my JES compute VMs. In many cases (using GCR and using GCS only) I would like to have JES create a node without an external IP address. JES has added this feature (see https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines) through the ""noAddress"" flag. noAddress must be set in both ephermeralPipeline and pipelineArgs. NOTE: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If your project is not whitelisted and you use noAddress, the operation will hang. This should be specified in the WDL as a runtime attribute, similar to the way cpus, memory or preemptible VMs are requested",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1325
https://github.com/broadinstitute/cromwell/issues/1325:547,Security,Access,Access,547,"As a user who runs large scale pipelines in Google, I may run into quota limitations on external IP addresses as they are consumed by my JES compute VMs. In many cases (using GCR and using GCS only) I would like to have JES create a node without an external IP address. JES has added this feature (see https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines) through the ""noAddress"" flag. noAddress must be set in both ephermeralPipeline and pipelineArgs. NOTE: Your project must be whitelisted in ""Google Access for Private IPs Early Access Program"". If your project is not whitelisted and you use noAddress, the operation will hang. This should be specified in the WDL as a runtime attribute, similar to the way cpus, memory or preemptible VMs are requested",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1325
https://github.com/broadinstitute/cromwell/pull/1326:357,Availability,down,downside,357,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1326:91,Deployability,update,updated,91,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1326:137,Integrability,rout,routed,137,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1326:178,Integrability,inject,inject,178,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1326:261,Integrability,inject,inject,261,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1326:114,Security,access,accessed,114,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1326:178,Security,inject,inject,178,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1326:261,Security,inject,inject,261,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1326:167,Testability,Test,Tests,167,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1326:228,Testability,test,testing,228,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326
https://github.com/broadinstitute/cromwell/pull/1328:127,Testability,test,tests,127,"- Needs to wait for the corresponding centaur PR before merging; - Also will need to wait for cleanup of failing local centaur tests before merging, but that's a centaur thing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1328
https://github.com/broadinstitute/cromwell/issues/1329:374,Availability,down,down,374,"Put in testing for restarts. . A possible (and preferred by this author) solution would be to seed a database with a known state and make sure a launching cromwell picks things up properly. That sounds like it'd probably be in the Centaur-ish space, but perhaps not. Another possibility (and not preferred by this author) would be to have a system that submits stuff, shuts down that cromwell, launches another cromwell and goes to town.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1329
https://github.com/broadinstitute/cromwell/issues/1329:7,Testability,test,testing,7,"Put in testing for restarts. . A possible (and preferred by this author) solution would be to seed a database with a known state and make sure a launching cromwell picks things up properly. That sounds like it'd probably be in the Centaur-ish space, but perhaps not. Another possibility (and not preferred by this author) would be to have a system that submits stuff, shuts down that cromwell, launches another cromwell and goes to town.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1329
https://github.com/broadinstitute/cromwell/pull/1331:36,Deployability,patch,patch,36,Oops - forgot that I only made this patch in my not-ready-yet cromwell branch but the check is live in centaur now,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1331
https://github.com/broadinstitute/cromwell/issues/1332:46,Availability,echo,echo,46,"If I run a WDL like the following in JES, the echo'ed path is relative and I would like it to be absolute. This will allow me to reference it from other locations than the current working directory more easily. ```; task echo {; File in_file; command {; echo ${in_file}; }; runtime {; docker: ""ubuntu:14.04""; }; }. workflow abs {; call echo ; }; ```. with this input json:. `{; ""abs.echo.in_file"": ""gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict""; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332
https://github.com/broadinstitute/cromwell/issues/1332:221,Availability,echo,echo,221,"If I run a WDL like the following in JES, the echo'ed path is relative and I would like it to be absolute. This will allow me to reference it from other locations than the current working directory more easily. ```; task echo {; File in_file; command {; echo ${in_file}; }; runtime {; docker: ""ubuntu:14.04""; }; }. workflow abs {; call echo ; }; ```. with this input json:. `{; ""abs.echo.in_file"": ""gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict""; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332
https://github.com/broadinstitute/cromwell/issues/1332:254,Availability,echo,echo,254,"If I run a WDL like the following in JES, the echo'ed path is relative and I would like it to be absolute. This will allow me to reference it from other locations than the current working directory more easily. ```; task echo {; File in_file; command {; echo ${in_file}; }; runtime {; docker: ""ubuntu:14.04""; }; }. workflow abs {; call echo ; }; ```. with this input json:. `{; ""abs.echo.in_file"": ""gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict""; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332
https://github.com/broadinstitute/cromwell/issues/1332:336,Availability,echo,echo,336,"If I run a WDL like the following in JES, the echo'ed path is relative and I would like it to be absolute. This will allow me to reference it from other locations than the current working directory more easily. ```; task echo {; File in_file; command {; echo ${in_file}; }; runtime {; docker: ""ubuntu:14.04""; }; }. workflow abs {; call echo ; }; ```. with this input json:. `{; ""abs.echo.in_file"": ""gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict""; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332
https://github.com/broadinstitute/cromwell/issues/1332:383,Availability,echo,echo,383,"If I run a WDL like the following in JES, the echo'ed path is relative and I would like it to be absolute. This will allow me to reference it from other locations than the current working directory more easily. ```; task echo {; File in_file; command {; echo ${in_file}; }; runtime {; docker: ""ubuntu:14.04""; }; }. workflow abs {; call echo ; }; ```. with this input json:. `{; ""abs.echo.in_file"": ""gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict""; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332
https://github.com/broadinstitute/cromwell/pull/1334:36,Testability,test,tested,36,This is all functionality now being tested by Centaur,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1334
https://github.com/broadinstitute/cromwell/issues/1335:290,Availability,echo,echo,290,"@geoffjentry ; Here is an example of a situation:. ```; workflow wf { ; String firstname; String lastname; call Greeting as GreetingFirstName { input: name=firstname }; call Greeting as GreetingLastName { input: name=lastname }; }; task Greeting {; String name; String greeting; command {; echo ""${greeting} ${name}""; }; }; ```. The idea is to make it possible to give a value to greetings for all aliased tasks in json like so:. ```; {; ""wf.firstname"": ""Andrey"",; ""wf.lastname"": ""Smirnov"",; ""wf.Greeting.greeting"": ""howdy""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1335
https://github.com/broadinstitute/cromwell/pull/1338:214,Availability,down,down,214,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:528,Availability,recover,recoverSpec,528,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:810,Availability,failure,failures,810,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:665,Deployability,Update,Updated,665,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:105,Integrability,depend,dependencies,105,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:164,Modifiability,Refactor,Refactored,164,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:477,Safety,timeout,timeout,477,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:528,Safety,recover,recoverSpec,528,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:237,Testability,log,logic,237,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:752,Testability,test,test,752,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1338:804,Testability,test,tests,804,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338
https://github.com/broadinstitute/cromwell/pull/1339:1721,Availability,error,error,1721," provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for restriction on environment:;   In spark cluster mode, the assembly jar file containing the application has to exist in all the nodes of the cluster since the driver program can be started on any of the nodes in the cluster.;   Known solution to this is to put the jar file in a shared file system like hdfs or a network file system, or a parallel distributed file system like lustre ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:394,Deployability,configurat,configuration,394,"PR Documentation describing the SparkBackend added to Cromwell. This PR adds support for execution of spark jobs as task in a workflow using the existing wdl format, with restrictions on the environment like having a local file system, a shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily bec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:653,Deployability,configurat,configurations,653,"PR Documentation describing the SparkBackend added to Cromwell. This PR adds support for execution of spark jobs as task in a workflow using the existing wdl format, with restrictions on the environment like having a local file system, a shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily bec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:714,Deployability,deploy,deployMode,714,"PR Documentation describing the SparkBackend added to Cromwell. This PR adds support for execution of spark jobs as task in a workflow using the existing wdl format, with restrictions on the environment like having a local file system, a shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily bec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:973,Deployability,deploy,deploy,973,"shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:1029,Deployability,deploy,deploy,1029,"shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:1099,Deployability,deploy,deploy,1099,"shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:1168,Deployability,deploy,deploy,1168,"shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:1223,Deployability,deploy,deploy,1223,"shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:2930,Deployability,release,release,2930,"er;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for restriction on environment:;   In spark cluster mode, the assembly jar file containing the application has to exist in all the nodes of the cluster since the driver program can be started on any of the nodes in the cluster.;   Known solution to this is to put the jar file in a shared file system like hdfs or a network file system, or a parallel distributed file system like lustre where all nodes in the cluster can access the file.;   For this reason, we did all our testing using the lustre file system, though it works just fine on a local file system with replication.;   ;   Also, note that as pointed out in the future PR plans section, the hadoop file system is not supported on this release. Supported File Systems:;   Local File System;   Network File System;   Distributed file system. PS: Please find attached read me for examples on How to use Spark Backend; [readMe.md.zip](https://github.com/broadinstitute/cromwell/files/437890/readMe.md.zip). contributor: @iyanuobidele. Reviewers: @geoffjentry @francares",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:483,Energy Efficiency,monitor,monitoring,483,"PR Documentation describing the SparkBackend added to Cromwell. This PR adds support for execution of spark jobs as task in a workflow using the existing wdl format, with restrictions on the environment like having a local file system, a shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily bec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:820,Integrability,depend,depending,820,"PR Documentation describing the SparkBackend added to Cromwell. This PR adds support for execution of spark jobs as task in a workflow using the existing wdl format, with restrictions on the environment like having a local file system, a shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily bec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:1419,Integrability,protocol,protocol,1419,"shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:1647,Integrability,protocol,protocol,1647," provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for restriction on environment:;   In spark cluster mode, the assembly jar file containing the application has to exist in all the nodes of the cluster since the driver program can be started on any of the nodes in the cluster.;   Known solution to this is to put the jar file in a shared file system like hdfs or a network file system, or a parallel distributed file system like lustre ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:394,Modifiability,config,configuration,394,"PR Documentation describing the SparkBackend added to Cromwell. This PR adds support for execution of spark jobs as task in a workflow using the existing wdl format, with restrictions on the environment like having a local file system, a shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily bec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:653,Modifiability,config,configurations,653,"PR Documentation describing the SparkBackend added to Cromwell. This PR adds support for execution of spark jobs as task in a workflow using the existing wdl format, with restrictions on the environment like having a local file system, a shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.  . Current deploy modes supported for any spark job:;   a - Client deploy mode using the spark standalone cluster manager;   b - Cluster deploy mode using the spark standalone cluster manager;   c - Client deploy mode using Yarn resource manager;   d - Cluster deploy mode using Yarn resource manager;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily bec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:2655,Security,access,access,2655,"er;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for restriction on environment:;   In spark cluster mode, the assembly jar file containing the application has to exist in all the nodes of the cluster since the driver program can be started on any of the nodes in the cluster.;   Known solution to this is to put the jar file in a shared file system like hdfs or a network file system, or a parallel distributed file system like lustre where all nodes in the cluster can access the file.;   For this reason, we did all our testing using the lustre file system, though it works just fine on a local file system with replication.;   ;   Also, note that as pointed out in the future PR plans section, the hadoop file system is not supported on this release. Supported File Systems:;   Local File System;   Network File System;   Distributed file system. PS: Please find attached read me for examples on How to use Spark Backend; [readMe.md.zip](https://github.com/broadinstitute/cromwell/files/437890/readMe.md.zip). contributor: @iyanuobidele. Reviewers: @geoffjentry @francares",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/pull/1339:2707,Testability,test,testing,2707,"er;   ; Future PR Plans:;   In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).;   This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation.   In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.;   Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for restriction on environment:;   In spark cluster mode, the assembly jar file containing the application has to exist in all the nodes of the cluster since the driver program can be started on any of the nodes in the cluster.;   Known solution to this is to put the jar file in a shared file system like hdfs or a network file system, or a parallel distributed file system like lustre where all nodes in the cluster can access the file.;   For this reason, we did all our testing using the lustre file system, though it works just fine on a local file system with replication.;   ;   Also, note that as pointed out in the future PR plans section, the hadoop file system is not supported on this release. Supported File Systems:;   Local File System;   Network File System;   Distributed file system. PS: Please find attached read me for examples on How to use Spark Backend; [readMe.md.zip](https://github.com/broadinstitute/cromwell/files/437890/readMe.md.zip). contributor: @iyanuobidele. Reviewers: @geoffjentry @francares",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339
https://github.com/broadinstitute/cromwell/issues/1342:60,Availability,down,down,60,"<< being discussed in Google Doc, copy here when it settles down >>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1342
https://github.com/broadinstitute/cromwell/issues/1343:180,Usability,simpl,simply,180,"Our documentation is a mess. It's a big, giant wall of text. I've been hearing an increasingly large number of complaints that it is so voluminous and hard to navigate that people simply don't bother to read it. We should put some effort to identify what's good, bad and indifferent. Find a better organization scheme. Etc etc etc. If they're up for it, it'd be good to work with @vdauwera and friends when the time comes. (note to @kcibul - I realize this is currently nearly as amorphous as that other ticket, I just wanted an official placeholder)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1343
https://github.com/broadinstitute/cromwell/issues/1344:396,Availability,down,down,396,"During the PR process, two commits are tested by Travis:; - The actually committed code: aka ""push""; - The results of the PR if it were to be merged: aka ""pr"". The latter test is currently producing false positives during tests of centaur local. The way Travis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Altern",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1344:2029,Modifiability,variab,variables,2029,"avis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Alternatively, the various travis variables including [`TRAVIS_PULL_REQUEST`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables) should provide enough information for a script to correctly fetch the correct commit from github.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1344:2117,Modifiability,variab,variables,2117,"avis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Alternatively, the various travis variables including [`TRAVIS_PULL_REQUEST`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables) should provide enough information for a script to correctly fetch the correct commit from github.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1344:2148,Modifiability,Variab,Variables,2148,"avis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Alternatively, the various travis variables including [`TRAVIS_PULL_REQUEST`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables) should provide enough information for a script to correctly fetch the correct commit from github.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1344:39,Testability,test,tested,39,"During the PR process, two commits are tested by Travis:; - The actually committed code: aka ""push""; - The results of the PR if it were to be merged: aka ""pr"". The latter test is currently producing false positives during tests of centaur local. The way Travis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Altern",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1344:171,Testability,test,test,171,"During the PR process, two commits are tested by Travis:; - The actually committed code: aka ""push""; - The results of the PR if it were to be merged: aka ""pr"". The latter test is currently producing false positives during tests of centaur local. The way Travis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Altern",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1344:222,Testability,test,tests,222,"During the PR process, two commits are tested by Travis:; - The actually committed code: aka ""push""; - The results of the PR if it were to be merged: aka ""pr"". The latter test is currently producing false positives during tests of centaur local. The way Travis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Altern",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1344:261,Testability,test,tests,261,"During the PR process, two commits are tested by Travis:; - The actually committed code: aka ""push""; - The results of the PR if it were to be merged: aka ""pr"". The latter test is currently producing false positives during tests of centaur local. The way Travis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Altern",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1344:612,Testability,log,log,612,"During the PR process, two commits are tested by Travis:; - The actually committed code: aka ""push""; - The results of the PR if it were to be merged: aka ""pr"". The latter test is currently producing false positives during tests of centaur local. The way Travis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Altern",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1344:690,Testability,log,log,690,"During the PR process, two commits are tested by Travis:; - The actually committed code: aka ""push""; - The results of the PR if it were to be merged: aka ""pr"". The latter test is currently producing false positives during tests of centaur local. The way Travis tests the results of the merged PR is by using github's extra `refs/pull/<PR>/merge`. These merge refs are special, and are not pulled down by default during `git fetch`, that only retrieves commits from origin under `refs/heads/*`. One can see Travis doing some of this extra retrieval in this excerpt of a branch that wouldn't build if merged, full log file [here](https://s3.amazonaws.com/archive.travis-ci.org/jobs/155209618/log.txt):. ``` bash; $ git clone --depth=50 https://github.com/broadinstitute/cromwell.git broadinstitute/cromwell; Cloning into 'broadinstitute/cromwell'...; remote: Counting objects: 2676, done.; remote: Compressing objects: 100% (1294/1294), done.; remote: Total 2676 (delta 879), reused 2196 (delta 632), pack-reused 0; Receiving objects: 100% (2676/2676), 909.71 KiB | 0 bytes/s, done.; Resolving deltas: 100% (879/879), done.; Checking connectivity... done.; $ cd broadinstitute/cromwell; $ git fetch origin +refs/pull/1339/merge:; remote: Counting objects: 17809, done.; remote: Compressing objects: 100% (5010/5010), done.; remote: Total 17809 (delta 9885), reused 17612 (delta 9710), pack-reused 0; Receiving objects: 100% (17809/17809), 4.56 MiB | 0 bytes/s, done.; Resolving deltas: 100% (9885/9885), completed with 116 local objects.; From https://github.com/broadinstitute/cromwell; * branch refs/pull/1339/merge -> FETCH_HEAD; $ git checkout -qf FETCH_HEAD; ```. But later, cromwell passes the wrong commit to centaur:. ``` bash; cd centaur; ./test_cromwell.sh -bdevelop -p5; ```. Perhaps centuar local could just use this existing checkout, already provided by Travis. If it didn't want to point to a directory, centaur local could just assemble the jar like centaur jes does and use that. Altern",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1344
https://github.com/broadinstitute/cromwell/issues/1347:32,Performance,cache,cached,32,"Call caching should verify that cached files exist before coping, and if they don’t then fall back to running the task. In FireCloud, users may delete files out of their bucket. If a cache hit occurs, it should check that the files to be copied exist. And if they don't Cromwell should fall back to just running the task as if a call cache hit didn't occur. Bonus points: should we then make that call ineligible for caching going forward?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1347
https://github.com/broadinstitute/cromwell/issues/1347:183,Performance,cache,cache,183,"Call caching should verify that cached files exist before coping, and if they don’t then fall back to running the task. In FireCloud, users may delete files out of their bucket. If a cache hit occurs, it should check that the files to be copied exist. And if they don't Cromwell should fall back to just running the task as if a call cache hit didn't occur. Bonus points: should we then make that call ineligible for caching going forward?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1347
https://github.com/broadinstitute/cromwell/issues/1347:334,Performance,cache,cache,334,"Call caching should verify that cached files exist before coping, and if they don’t then fall back to running the task. In FireCloud, users may delete files out of their bucket. If a cache hit occurs, it should check that the files to be copied exist. And if they don't Cromwell should fall back to just running the task as if a call cache hit didn't occur. Bonus points: should we then make that call ineligible for caching going forward?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1347
https://github.com/broadinstitute/cromwell/issues/1348:265,Testability,Mock,Mock,265,"- local backend; - no docker. If a workflow input File is from a bucket and it is dispatched to two tasks, the file will be localized twice, taking up twice the storage. Preferable behavior would be to localize the file once and have the second task use a symlink. Mock WDL for illustration:. ```; workflow process_file {; # gs://some_bucket/my_file; File input_file; call task1 {; input:; f=input_file; }. call task2 {; input:; f=input_file; }. }. task task1 {; File f; .......; }. task task2 {; File f; .......; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348
https://github.com/broadinstitute/cromwell/issues/1349:204,Performance,cache,cache,204,"Currently the Job Store doesn't cope with File outputs at all. See the discussion at #1340 for thoughts about doing this in a futureproof way. Also think about the how much logic will be shared with call cache results in #1233, and particularly about factoring out the simpleton nature in the job store and call cache result simpletons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349
https://github.com/broadinstitute/cromwell/issues/1349:312,Performance,cache,cache,312,"Currently the Job Store doesn't cope with File outputs at all. See the discussion at #1340 for thoughts about doing this in a futureproof way. Also think about the how much logic will be shared with call cache results in #1233, and particularly about factoring out the simpleton nature in the job store and call cache result simpletons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349
https://github.com/broadinstitute/cromwell/issues/1349:173,Testability,log,logic,173,"Currently the Job Store doesn't cope with File outputs at all. See the discussion at #1340 for thoughts about doing this in a futureproof way. Also think about the how much logic will be shared with call cache results in #1233, and particularly about factoring out the simpleton nature in the job store and call cache result simpletons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349
https://github.com/broadinstitute/cromwell/issues/1349:269,Usability,simpl,simpleton,269,"Currently the Job Store doesn't cope with File outputs at all. See the discussion at #1340 for thoughts about doing this in a futureproof way. Also think about the how much logic will be shared with call cache results in #1233, and particularly about factoring out the simpleton nature in the job store and call cache result simpletons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349
https://github.com/broadinstitute/cromwell/issues/1349:325,Usability,simpl,simpletons,325,"Currently the Job Store doesn't cope with File outputs at all. See the discussion at #1340 for thoughts about doing this in a futureproof way. Also think about the how much logic will be shared with call cache results in #1233, and particularly about factoring out the simpleton nature in the job store and call cache result simpletons.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1349
https://github.com/broadinstitute/cromwell/pull/1350:114,Performance,cache,cache,114,"- Handle ""File"" output types for the JobStore; - Create a common `DatabaseSimpleton` trait for Job Store and call cache result code to share; - Centralize logic for conversion of `DatabaseSimpleton`s to `WdlValueSimpleton`s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350
https://github.com/broadinstitute/cromwell/pull/1350:155,Testability,log,logic,155,"- Handle ""File"" output types for the JobStore; - Create a common `DatabaseSimpleton` trait for Job Store and call cache result code to share; - Centralize logic for conversion of `DatabaseSimpleton`s to `WdlValueSimpleton`s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1350
https://github.com/broadinstitute/cromwell/issues/1354:672,Availability,avail,available,672,"Currently, the local backend will spawn the maximum number of processes to run a workflow. . Why is this a problem? ; - This can cripple a machine with fewer CPUs than number of tasks that can be executed.; - This can cause all of the RAM to be used running a workflow. Again, crippling a machine.; - If either of the two above conditions are met, total wall clock time will increase and/or jobs will be killed and/or the cromwell process will be killed.; - This hinders the development of pipelines. Proposed solution:; - Allow users to specify the maximum number of simultaneous jobs to run for a workflow. Similar to how it is done in Queue. Workaround:; - Use JES, if available; - Use SGE, if available. Who?; - Pipeline engineers; - Method developers; - External researchers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354
https://github.com/broadinstitute/cromwell/issues/1354:697,Availability,avail,available,697,"Currently, the local backend will spawn the maximum number of processes to run a workflow. . Why is this a problem? ; - This can cripple a machine with fewer CPUs than number of tasks that can be executed.; - This can cause all of the RAM to be used running a workflow. Again, crippling a machine.; - If either of the two above conditions are met, total wall clock time will increase and/or jobs will be killed and/or the cromwell process will be killed.; - This hinders the development of pipelines. Proposed solution:; - Allow users to specify the maximum number of simultaneous jobs to run for a workflow. Similar to how it is done in Queue. Workaround:; - Use JES, if available; - Use SGE, if available. Who?; - Pipeline engineers; - Method developers; - External researchers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354
https://github.com/broadinstitute/cromwell/issues/1354:490,Deployability,pipeline,pipelines,490,"Currently, the local backend will spawn the maximum number of processes to run a workflow. . Why is this a problem? ; - This can cripple a machine with fewer CPUs than number of tasks that can be executed.; - This can cause all of the RAM to be used running a workflow. Again, crippling a machine.; - If either of the two above conditions are met, total wall clock time will increase and/or jobs will be killed and/or the cromwell process will be killed.; - This hinders the development of pipelines. Proposed solution:; - Allow users to specify the maximum number of simultaneous jobs to run for a workflow. Similar to how it is done in Queue. Workaround:; - Use JES, if available; - Use SGE, if available. Who?; - Pipeline engineers; - Method developers; - External researchers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354
https://github.com/broadinstitute/cromwell/issues/1354:716,Deployability,Pipeline,Pipeline,716,"Currently, the local backend will spawn the maximum number of processes to run a workflow. . Why is this a problem? ; - This can cripple a machine with fewer CPUs than number of tasks that can be executed.; - This can cause all of the RAM to be used running a workflow. Again, crippling a machine.; - If either of the two above conditions are met, total wall clock time will increase and/or jobs will be killed and/or the cromwell process will be killed.; - This hinders the development of pipelines. Proposed solution:; - Allow users to specify the maximum number of simultaneous jobs to run for a workflow. Similar to how it is done in Queue. Workaround:; - Use JES, if available; - Use SGE, if available. Who?; - Pipeline engineers; - Method developers; - External researchers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354
https://github.com/broadinstitute/cromwell/issues/1354:638,Performance,Queue,Queue,638,"Currently, the local backend will spawn the maximum number of processes to run a workflow. . Why is this a problem? ; - This can cripple a machine with fewer CPUs than number of tasks that can be executed.; - This can cause all of the RAM to be used running a workflow. Again, crippling a machine.; - If either of the two above conditions are met, total wall clock time will increase and/or jobs will be killed and/or the cromwell process will be killed.; - This hinders the development of pipelines. Proposed solution:; - Allow users to specify the maximum number of simultaneous jobs to run for a workflow. Similar to how it is done in Queue. Workaround:; - Use JES, if available; - Use SGE, if available. Who?; - Pipeline engineers; - Method developers; - External researchers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354
https://github.com/broadinstitute/cromwell/issues/1355:243,Availability,recover,recovering,243,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355
https://github.com/broadinstitute/cromwell/issues/1355:486,Performance,queue,queue,486,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355
https://github.com/broadinstitute/cromwell/issues/1355:551,Performance,queue,queue,551,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355
https://github.com/broadinstitute/cromwell/issues/1355:596,Performance,queue,queue,596,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355
https://github.com/broadinstitute/cromwell/issues/1355:243,Safety,recover,recovering,243,At this moment there is a way to submit jobs to a HPC with a command line that is executed. With drmaa this is also possible. The only problem is that with drmaa v1 you can only get status of jobs submitted in the same session. This means for recovering after a restart you must rely on command line methods like in the current implementation. Drmaa v2 have the possibility to track jobs outside it's session but there is almost no support for v2 yet. Here is the implementation inside queue:; https://github.com/broadgsa/gatk/tree/master/public/gatk-queue/src/main/scala/org/broadinstitute/gatk/queue/engine/drmaa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1355
https://github.com/broadinstitute/cromwell/issues/1357:156,Availability,Error,Error,156,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357
https://github.com/broadinstitute/cromwell/issues/1357:203,Availability,error,error,203,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357
https://github.com/broadinstitute/cromwell/issues/1357:399,Availability,error,error,399,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357
https://github.com/broadinstitute/cromwell/issues/1357:162,Integrability,message,message,162,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357
https://github.com/broadinstitute/cromwell/issues/1357:413,Integrability,Message,Message,413,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357
https://github.com/broadinstitute/cromwell/issues/1357:102,Testability,log,logs,102,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357
https://github.com/broadinstitute/cromwell/issues/1357:685,Testability,log,log,685,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357
https://github.com/broadinstitute/cromwell/issues/1357:709,Testability,log,log,709,"Backend: JES. Unfortunately, the `cromwell-executions` directory doesn't exist and `cromwell-workflow-logs` doesn't exist. So I am not sure how to proceed. Error message:. ```; [2016-08-30 13:28:27,81] [error] WorkflowManagerActor Workflow 3afe1c22-1216-4ff7-95a3-5305843b7310 failed (during ExecutingWorkflowState): java.lang.Throwable: Task 3afe1c22-1216-4ff7-95a3-5305843b7310:PadTargets failed: error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/targets.padded.tsv -> gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://broad-dsde-methods/case_gatk_acnv_workflow/3afe1c22-1216-4ff7-95a3-5305843b7310/call-PadTargets/targets.padded.tsv, command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357
https://github.com/broadinstitute/cromwell/pull/1366:106,Modifiability,refactor,refactored,106,"Also:; Splitting the database sql and migration, plus other cleanup.; Bits of call caching business logic refactored out of `database`.; Moved all simpleton conversion out of the `database`.; PK columns that cannot be filled in by business logic are defaulted to `None`.; Renamed `Database` to `ServicesStore`.; Renamed `CromwellDatabase` to `SingletonServicesStore`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1366
https://github.com/broadinstitute/cromwell/pull/1366:100,Testability,log,logic,100,"Also:; Splitting the database sql and migration, plus other cleanup.; Bits of call caching business logic refactored out of `database`.; Moved all simpleton conversion out of the `database`.; PK columns that cannot be filled in by business logic are defaulted to `None`.; Renamed `Database` to `ServicesStore`.; Renamed `CromwellDatabase` to `SingletonServicesStore`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1366
https://github.com/broadinstitute/cromwell/pull/1366:240,Testability,log,logic,240,"Also:; Splitting the database sql and migration, plus other cleanup.; Bits of call caching business logic refactored out of `database`.; Moved all simpleton conversion out of the `database`.; PK columns that cannot be filled in by business logic are defaulted to `None`.; Renamed `Database` to `ServicesStore`.; Renamed `CromwellDatabase` to `SingletonServicesStore`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1366
https://github.com/broadinstitute/cromwell/pull/1366:147,Usability,simpl,simpleton,147,"Also:; Splitting the database sql and migration, plus other cleanup.; Bits of call caching business logic refactored out of `database`.; Moved all simpleton conversion out of the `database`.; PK columns that cannot be filled in by business logic are defaulted to `None`.; Renamed `Database` to `ServicesStore`.; Renamed `CromwellDatabase` to `SingletonServicesStore`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1366
https://github.com/broadinstitute/cromwell/issues/1367:207,Availability,error,error,207,"- We have a lot of actors.; - The actor hierarchy can be quite deep in places; - Each actor is given a name, which must make the overall path unique; - Unwieldy actor paths are making debugging awkward. The error lines are often hundreds of characters long!; - We should decide how to name actors, in a way that ensures they have workflow, call and attempt information in the name... but **at most** once.; - We should also retroactively update the actor naming so that all existing actors have appropriate paths.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1367
https://github.com/broadinstitute/cromwell/issues/1367:438,Deployability,update,update,438,"- We have a lot of actors.; - The actor hierarchy can be quite deep in places; - Each actor is given a name, which must make the overall path unique; - Unwieldy actor paths are making debugging awkward. The error lines are often hundreds of characters long!; - We should decide how to name actors, in a way that ensures they have workflow, call and attempt information in the name... but **at most** once.; - We should also retroactively update the actor naming so that all existing actors have appropriate paths.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1367
https://github.com/broadinstitute/cromwell/issues/1368:9,Testability,log,log,9,having a log of every call made to google is of tremendous value when trouble shooting google related issues. Here is the code that rawls uses: https://github.com/broadinstitute/rawls/blob/0ce87caf4e52cc4df83d286ef49d4d56d91b590f/src/main/scala/org/broadinstitute/dsde/rawls/dataaccess/HttpGoogleServicesDAO.scala#L867. This information is also valuable because we have hard evidence when we say api calls are slow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1368
https://github.com/broadinstitute/cromwell/pull/1371:11,Availability,failure,failures,11,Known test failures:; - [x] cromwell.SimpleWorkflowActorSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithBadMetadataSpec; - [x] cromwell.ArrayWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataSpec; - [x] cromwell.ArrayOfArrayCoercionSpec; - [x] cromwell.WdlFunctionsAtWorkflowLevelSpec; - [x] cromwell.MapWorkflowSpec; - [x] cromwell.WorkflowOutputsSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataOnFailureSpec; - [x] cromwell.WorkflowFailSlowSpec; - [x] cromwell.FilePassingWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorNormalSpec; - [x] cromwell.engine.WorkflowManagerActorSpec; - [x] cromwell.MultipleFilesWithSameNameWorkflowSpec; - [x] cromwell.CopyWorkflowOutputsSpec; - [x] cromwell.PostfixQuantifierWorkflowSpec; - [x] cromwell.ScatterWorkflowSpec. Ready for review!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1371
https://github.com/broadinstitute/cromwell/pull/1371:6,Testability,test,test,6,Known test failures:; - [x] cromwell.SimpleWorkflowActorSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithBadMetadataSpec; - [x] cromwell.ArrayWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataSpec; - [x] cromwell.ArrayOfArrayCoercionSpec; - [x] cromwell.WdlFunctionsAtWorkflowLevelSpec; - [x] cromwell.MapWorkflowSpec; - [x] cromwell.WorkflowOutputsSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataOnFailureSpec; - [x] cromwell.WorkflowFailSlowSpec; - [x] cromwell.FilePassingWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorNormalSpec; - [x] cromwell.engine.WorkflowManagerActorSpec; - [x] cromwell.MultipleFilesWithSameNameWorkflowSpec; - [x] cromwell.CopyWorkflowOutputsSpec; - [x] cromwell.PostfixQuantifierWorkflowSpec; - [x] cromwell.ScatterWorkflowSpec. Ready for review!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1371
https://github.com/broadinstitute/cromwell/pull/1371:37,Usability,Simpl,SimpleWorkflowActorSpec,37,Known test failures:; - [x] cromwell.SimpleWorkflowActorSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithBadMetadataSpec; - [x] cromwell.ArrayWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataSpec; - [x] cromwell.ArrayOfArrayCoercionSpec; - [x] cromwell.WdlFunctionsAtWorkflowLevelSpec; - [x] cromwell.MapWorkflowSpec; - [x] cromwell.WorkflowOutputsSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataOnFailureSpec; - [x] cromwell.WorkflowFailSlowSpec; - [x] cromwell.FilePassingWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorNormalSpec; - [x] cromwell.engine.WorkflowManagerActorSpec; - [x] cromwell.MultipleFilesWithSameNameWorkflowSpec; - [x] cromwell.CopyWorkflowOutputsSpec; - [x] cromwell.PostfixQuantifierWorkflowSpec; - [x] cromwell.ScatterWorkflowSpec. Ready for review!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1371
https://github.com/broadinstitute/cromwell/issues/1373:31,Modifiability,Rewrite,Rewrite,31,Such complicated. Much states. Rewrite,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1373
https://github.com/broadinstitute/cromwell/issues/1374:252,Testability,log,logic,252,The EJEA is becoming quite the beast. It's starting to get too big to fit into a mental model so maybe we should decompose it a little. One obvious choice might be to extract the CC output fetch and copying phases into a single state and do actual the logic in a subactor?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1374
https://github.com/broadinstitute/cromwell/issues/1376:169,Integrability,message,message,169,in `when(WorkflowExecutionAbortingState)` there's a handler for `AbortedResponse` which appears to be doing some reasonable stuff but I can't find anything sending that message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1376
https://github.com/broadinstitute/cromwell/issues/1376:65,Safety,Abort,AbortedResponse,65,in `when(WorkflowExecutionAbortingState)` there's a handler for `AbortedResponse` which appears to be doing some reasonable stuff but I can't find anything sending that message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1376
https://github.com/broadinstitute/cromwell/issues/1377:409,Integrability,rout,router,409,"This is currently being built by all implementers of the BackendLifecycleActorFactory, as opposed to props like the trait normally does. @cjllanwarne explained that this is because we want system-wide throttling on this behavior. . At the moment it's being built as a direct descendent of the root, ie a sibling of CromwellRootActor, and this doesn't seem correct. Also, it appears that all instances of this router will have the same name which should be problematic. Make this more sane please",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1377
https://github.com/broadinstitute/cromwell/issues/1378:411,Performance,load,load,411,"As a user, I would like to be able to disable the metadata summary refresh. I want to do this because I'd like to be able to stand up a ""read only"" cromwell that that doesn't write to the database because I can use these to scale out horizontally for read api operations (e.g. status, metadata). Scaling out these operations is important because it allows my cromwell to remain responsive even under heavy read load. @geoffjentry -- pls expand/correct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378
https://github.com/broadinstitute/cromwell/issues/1378:378,Usability,responsiv,responsive,378,"As a user, I would like to be able to disable the metadata summary refresh. I want to do this because I'd like to be able to stand up a ""read only"" cromwell that that doesn't write to the database because I can use these to scale out horizontally for read api operations (e.g. status, metadata). Scaling out these operations is important because it allows my cromwell to remain responsive even under heavy read load. @geoffjentry -- pls expand/correct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378
https://github.com/broadinstitute/cromwell/pull/1379:22,Integrability,message,messages,22,See individual commit messages for more info.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379
https://github.com/broadinstitute/cromwell/issues/1380:74,Availability,failure,failure-mode,74,"User @gavvvr reports:. If i run cromwell with -Dworkflow-options.workflow-failure-mode=""ContinueWhilePossible"" it does not work. Also using -Dconfig.file=application.conf does not work. Only workflow_failure_mode option in JSON config works.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380
https://github.com/broadinstitute/cromwell/issues/1380:228,Modifiability,config,config,228,"User @gavvvr reports:. If i run cromwell with -Dworkflow-options.workflow-failure-mode=""ContinueWhilePossible"" it does not work. Also using -Dconfig.file=application.conf does not work. Only workflow_failure_mode option in JSON config works.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380
https://github.com/broadinstitute/cromwell/issues/1383:183,Availability,echo,echo,183,"The SharedFileSystem (SFS) backend that is the basis for the local and SGE backends currently has a race condition in the script:. ``` bash; #!/bin/sh; cd $cwd; $instantiatedCommand; echo $$? > rc; ```; 1. The `echo` needs to write to a `rc.tmp` and then use the atomic `mv rc.tmp rc`. Otherwise cromwell will (very very rarely?) pickup the existence of the file before bash has written, flushed, and closed the rc contents. This is not an issue on JES because the API is waiting for the script to exit, not the appearance of the `rc` file.; 2. The `rc` path should be absolute, whether inside or outside of docker. `echo $$? > $cwd/rc` may be enough? Not an issue on JES as it only writes to the dockerized rc path.; 3. The SFS (and probably JES for consistency, and backends in general?) should run the WDL command in a subshell, either with a (yet-another) call to bash, or using [bashisms](https://github.com/koalaman/shellcheck/wiki/SC2103#correct-code). This would better protect that the WDL from killing the current shell without writing an `rc`. A proper centaur test would contain _diabolical_ WDL such as:. ```; command {; exit; }; ```. ```; command {; mkdir newdir; cd newdir; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1383
https://github.com/broadinstitute/cromwell/issues/1383:211,Availability,echo,echo,211,"The SharedFileSystem (SFS) backend that is the basis for the local and SGE backends currently has a race condition in the script:. ``` bash; #!/bin/sh; cd $cwd; $instantiatedCommand; echo $$? > rc; ```; 1. The `echo` needs to write to a `rc.tmp` and then use the atomic `mv rc.tmp rc`. Otherwise cromwell will (very very rarely?) pickup the existence of the file before bash has written, flushed, and closed the rc contents. This is not an issue on JES because the API is waiting for the script to exit, not the appearance of the `rc` file.; 2. The `rc` path should be absolute, whether inside or outside of docker. `echo $$? > $cwd/rc` may be enough? Not an issue on JES as it only writes to the dockerized rc path.; 3. The SFS (and probably JES for consistency, and backends in general?) should run the WDL command in a subshell, either with a (yet-another) call to bash, or using [bashisms](https://github.com/koalaman/shellcheck/wiki/SC2103#correct-code). This would better protect that the WDL from killing the current shell without writing an `rc`. A proper centaur test would contain _diabolical_ WDL such as:. ```; command {; exit; }; ```. ```; command {; mkdir newdir; cd newdir; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1383
https://github.com/broadinstitute/cromwell/issues/1383:617,Availability,echo,echo,617,"The SharedFileSystem (SFS) backend that is the basis for the local and SGE backends currently has a race condition in the script:. ``` bash; #!/bin/sh; cd $cwd; $instantiatedCommand; echo $$? > rc; ```; 1. The `echo` needs to write to a `rc.tmp` and then use the atomic `mv rc.tmp rc`. Otherwise cromwell will (very very rarely?) pickup the existence of the file before bash has written, flushed, and closed the rc contents. This is not an issue on JES because the API is waiting for the script to exit, not the appearance of the `rc` file.; 2. The `rc` path should be absolute, whether inside or outside of docker. `echo $$? > $cwd/rc` may be enough? Not an issue on JES as it only writes to the dockerized rc path.; 3. The SFS (and probably JES for consistency, and backends in general?) should run the WDL command in a subshell, either with a (yet-another) call to bash, or using [bashisms](https://github.com/koalaman/shellcheck/wiki/SC2103#correct-code). This would better protect that the WDL from killing the current shell without writing an `rc`. A proper centaur test would contain _diabolical_ WDL such as:. ```; command {; exit; }; ```. ```; command {; mkdir newdir; cd newdir; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1383
https://github.com/broadinstitute/cromwell/issues/1383:100,Performance,race condition,race condition,100,"The SharedFileSystem (SFS) backend that is the basis for the local and SGE backends currently has a race condition in the script:. ``` bash; #!/bin/sh; cd $cwd; $instantiatedCommand; echo $$? > rc; ```; 1. The `echo` needs to write to a `rc.tmp` and then use the atomic `mv rc.tmp rc`. Otherwise cromwell will (very very rarely?) pickup the existence of the file before bash has written, flushed, and closed the rc contents. This is not an issue on JES because the API is waiting for the script to exit, not the appearance of the `rc` file.; 2. The `rc` path should be absolute, whether inside or outside of docker. `echo $$? > $cwd/rc` may be enough? Not an issue on JES as it only writes to the dockerized rc path.; 3. The SFS (and probably JES for consistency, and backends in general?) should run the WDL command in a subshell, either with a (yet-another) call to bash, or using [bashisms](https://github.com/koalaman/shellcheck/wiki/SC2103#correct-code). This would better protect that the WDL from killing the current shell without writing an `rc`. A proper centaur test would contain _diabolical_ WDL such as:. ```; command {; exit; }; ```. ```; command {; mkdir newdir; cd newdir; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1383
https://github.com/broadinstitute/cromwell/issues/1383:1072,Testability,test,test,1072,"The SharedFileSystem (SFS) backend that is the basis for the local and SGE backends currently has a race condition in the script:. ``` bash; #!/bin/sh; cd $cwd; $instantiatedCommand; echo $$? > rc; ```; 1. The `echo` needs to write to a `rc.tmp` and then use the atomic `mv rc.tmp rc`. Otherwise cromwell will (very very rarely?) pickup the existence of the file before bash has written, flushed, and closed the rc contents. This is not an issue on JES because the API is waiting for the script to exit, not the appearance of the `rc` file.; 2. The `rc` path should be absolute, whether inside or outside of docker. `echo $$? > $cwd/rc` may be enough? Not an issue on JES as it only writes to the dockerized rc path.; 3. The SFS (and probably JES for consistency, and backends in general?) should run the WDL command in a subshell, either with a (yet-another) call to bash, or using [bashisms](https://github.com/koalaman/shellcheck/wiki/SC2103#correct-code). This would better protect that the WDL from killing the current shell without writing an `rc`. A proper centaur test would contain _diabolical_ WDL such as:. ```; command {; exit; }; ```. ```; command {; mkdir newdir; cd newdir; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1383
https://github.com/broadinstitute/cromwell/issues/1388:126,Performance,cache,cached,126,"Something seems to be going wrong with how call caching copies data in Cromwell 0.19. For example shard 10 gets copied from a cached call, but all shards matching 10\* get copied along with it, e.g. shard 101, 1012, etc. So we end up with a directory like:; gs://bucket/workflow_id/shard-10/; gs://bucket/workflow_id/shard-10/output.10.txt; gs://bucket/workflow_id/shard-10/shard-101/output.101.txt; gs://bucket/workflow_id/shard-10/shard-1012/output.1012.txt. Where Cromwell should really only have copied output.10.txt.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1388
https://github.com/broadinstitute/cromwell/issues/1389:302,Deployability,configurat,configuration-files-for-jvm-apps,302,"NOTE: I haven't 100% confirmed that the application.conf isn't auto-included, but since we now have separated cromwell into artifacts, based on the HOCON docs it does not appear that we're using an ""application.conf"" as intended: https://github.com/typesafehub/config/blob/master/HOCON.md#conventional-configuration-files-for-jvm-apps. If someone needs to specify a backend via a new application.conf, they shouldn't need to do one of the two current workarounds:. 1) Know to include the _original_ application.conf.; 2) Copy/paste all of the previous application.conf, including things like akka dispatcher executors. The fix is to rename the core/src/main/resources/{application.conf => reference.conf}. When one then adjusts settings while running the fat-jar, one definitely doesn't need to re-include a reference.conf. The ticket says ""most of?"" because there may be some elements of the current core/.../application.conf that would be more appropriate for a fat-jar _only_, such as having the main.hsqldb and an included Local backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1389
https://github.com/broadinstitute/cromwell/issues/1389:261,Modifiability,config,config,261,"NOTE: I haven't 100% confirmed that the application.conf isn't auto-included, but since we now have separated cromwell into artifacts, based on the HOCON docs it does not appear that we're using an ""application.conf"" as intended: https://github.com/typesafehub/config/blob/master/HOCON.md#conventional-configuration-files-for-jvm-apps. If someone needs to specify a backend via a new application.conf, they shouldn't need to do one of the two current workarounds:. 1) Know to include the _original_ application.conf.; 2) Copy/paste all of the previous application.conf, including things like akka dispatcher executors. The fix is to rename the core/src/main/resources/{application.conf => reference.conf}. When one then adjusts settings while running the fat-jar, one definitely doesn't need to re-include a reference.conf. The ticket says ""most of?"" because there may be some elements of the current core/.../application.conf that would be more appropriate for a fat-jar _only_, such as having the main.hsqldb and an included Local backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1389
https://github.com/broadinstitute/cromwell/issues/1389:302,Modifiability,config,configuration-files-for-jvm-apps,302,"NOTE: I haven't 100% confirmed that the application.conf isn't auto-included, but since we now have separated cromwell into artifacts, based on the HOCON docs it does not appear that we're using an ""application.conf"" as intended: https://github.com/typesafehub/config/blob/master/HOCON.md#conventional-configuration-files-for-jvm-apps. If someone needs to specify a backend via a new application.conf, they shouldn't need to do one of the two current workarounds:. 1) Know to include the _original_ application.conf.; 2) Copy/paste all of the previous application.conf, including things like akka dispatcher executors. The fix is to rename the core/src/main/resources/{application.conf => reference.conf}. When one then adjusts settings while running the fat-jar, one definitely doesn't need to re-include a reference.conf. The ticket says ""most of?"" because there may be some elements of the current core/.../application.conf that would be more appropriate for a fat-jar _only_, such as having the main.hsqldb and an included Local backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1389
https://github.com/broadinstitute/cromwell/issues/1390:89,Modifiability,variab,variable,89,"This is even more vestigial than #1386, all the code was removed but for some reason the variable was left behind.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1390
https://github.com/broadinstitute/cromwell/pull/1394:95,Availability,error,error,95,"1. Looking specifically for feedback on what places JesCacheHitCopyingActor would require more error handling.; 2. Within the JesCacheHitCopyingActor, are there any messages not being sent to the metadata service that should be sent ?; 3. Currently, not re-saving the JobOutputs that JesCacheHitCopyingActor is copying, since we shouldn't require multiple copies of the same outputs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394
https://github.com/broadinstitute/cromwell/pull/1394:165,Integrability,message,messages,165,"1. Looking specifically for feedback on what places JesCacheHitCopyingActor would require more error handling.; 2. Within the JesCacheHitCopyingActor, are there any messages not being sent to the metadata service that should be sent ?; 3. Currently, not re-saving the JobOutputs that JesCacheHitCopyingActor is copying, since we shouldn't require multiple copies of the same outputs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394
https://github.com/broadinstitute/cromwell/pull/1394:28,Usability,feedback,feedback,28,"1. Looking specifically for feedback on what places JesCacheHitCopyingActor would require more error handling.; 2. Within the JesCacheHitCopyingActor, are there any messages not being sent to the metadata service that should be sent ?; 3. Currently, not re-saving the JobOutputs that JesCacheHitCopyingActor is copying, since we shouldn't require multiple copies of the same outputs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394
https://github.com/broadinstitute/cromwell/issues/1395:690,Availability,error,error,690,"Currently, to process a glob on JES, Cromwell does an `ls` of the google cloud storage location. The problem with this is that ls is eventually consistent, which leads to bugs like #843 . JES has added a feature (#28858407) where they now return the number of files that matched the glob as part of their metadata. These appear as events of the form. `{Description: ""copied 3 file(s) to \""gs://my-bucket/out/\"""",; StartTime: {Seconds: 1470063955,; Nanos: 748725437}},`. In Cromwell, when processing these globs, we should poll (with adjustable maximum timeout) for this number of files to appear via the ls. If they do not appear after the timeout, the task should fail with an appropriate error message. If we are processing globs on GCS and NOT using JES, the best we can do is just grab and go via the ls (as we are doing currently for JES).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395
https://github.com/broadinstitute/cromwell/issues/1395:696,Integrability,message,message,696,"Currently, to process a glob on JES, Cromwell does an `ls` of the google cloud storage location. The problem with this is that ls is eventually consistent, which leads to bugs like #843 . JES has added a feature (#28858407) where they now return the number of files that matched the glob as part of their metadata. These appear as events of the form. `{Description: ""copied 3 file(s) to \""gs://my-bucket/out/\"""",; StartTime: {Seconds: 1470063955,; Nanos: 748725437}},`. In Cromwell, when processing these globs, we should poll (with adjustable maximum timeout) for this number of files to appear via the ls. If they do not appear after the timeout, the task should fail with an appropriate error message. If we are processing globs on GCS and NOT using JES, the best we can do is just grab and go via the ls (as we are doing currently for JES).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395
https://github.com/broadinstitute/cromwell/issues/1395:552,Safety,timeout,timeout,552,"Currently, to process a glob on JES, Cromwell does an `ls` of the google cloud storage location. The problem with this is that ls is eventually consistent, which leads to bugs like #843 . JES has added a feature (#28858407) where they now return the number of files that matched the glob as part of their metadata. These appear as events of the form. `{Description: ""copied 3 file(s) to \""gs://my-bucket/out/\"""",; StartTime: {Seconds: 1470063955,; Nanos: 748725437}},`. In Cromwell, when processing these globs, we should poll (with adjustable maximum timeout) for this number of files to appear via the ls. If they do not appear after the timeout, the task should fail with an appropriate error message. If we are processing globs on GCS and NOT using JES, the best we can do is just grab and go via the ls (as we are doing currently for JES).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395
https://github.com/broadinstitute/cromwell/issues/1395:640,Safety,timeout,timeout,640,"Currently, to process a glob on JES, Cromwell does an `ls` of the google cloud storage location. The problem with this is that ls is eventually consistent, which leads to bugs like #843 . JES has added a feature (#28858407) where they now return the number of files that matched the glob as part of their metadata. These appear as events of the form. `{Description: ""copied 3 file(s) to \""gs://my-bucket/out/\"""",; StartTime: {Seconds: 1470063955,; Nanos: 748725437}},`. In Cromwell, when processing these globs, we should poll (with adjustable maximum timeout) for this number of files to appear via the ls. If they do not appear after the timeout, the task should fail with an appropriate error message. If we are processing globs on GCS and NOT using JES, the best we can do is just grab and go via the ls (as we are doing currently for JES).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1395
https://github.com/broadinstitute/cromwell/issues/1396:49,Safety,Abort,Abort,49,In the course of responding to a bug report with Abort @mcovarr quickly found several other related bugs in Abort which points to the fact that we should have better testing of the Abort endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396
https://github.com/broadinstitute/cromwell/issues/1396:108,Safety,Abort,Abort,108,In the course of responding to a bug report with Abort @mcovarr quickly found several other related bugs in Abort which points to the fact that we should have better testing of the Abort endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396
https://github.com/broadinstitute/cromwell/issues/1396:181,Safety,Abort,Abort,181,In the course of responding to a bug report with Abort @mcovarr quickly found several other related bugs in Abort which points to the fact that we should have better testing of the Abort endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396
https://github.com/broadinstitute/cromwell/issues/1396:166,Testability,test,testing,166,In the course of responding to a bug report with Abort @mcovarr quickly found several other related bugs in Abort which points to the fact that we should have better testing of the Abort endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1396
https://github.com/broadinstitute/cromwell/issues/1402:120,Performance,queue,queue,120,"HtCondor backend should be responsive to abort requests from the engine, and kill and remove the job from it's internal queue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1402
https://github.com/broadinstitute/cromwell/issues/1402:41,Safety,abort,abort,41,"HtCondor backend should be responsive to abort requests from the engine, and kill and remove the job from it's internal queue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1402
https://github.com/broadinstitute/cromwell/issues/1402:27,Usability,responsiv,responsive,27,"HtCondor backend should be responsive to abort requests from the engine, and kill and remove the job from it's internal queue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1402
https://github.com/broadinstitute/cromwell/pull/1403:151,Modifiability,config,configurable,151,"As a side effect to enable abort support in HtCondor, this PR makes the polling (for checking job status) asynchronous, and the polling interval to be configurable.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403
https://github.com/broadinstitute/cromwell/pull/1403:27,Safety,abort,abort,27,"As a side effect to enable abort support in HtCondor, this PR makes the polling (for checking job status) asynchronous, and the polling interval to be configurable.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403
https://github.com/broadinstitute/cromwell/issues/1406:2754,Availability,error,error,2754,"foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$work",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:2831,Availability,down,down,2831,"pp$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:2870,Availability,error,error,2870,"31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:380,Deployability,configurat,configuration,380,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:1984,Deployability,configurat,configuration,1984,"381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:2332,Deployability,pipeline,pipeline,2332,"); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6027,Deployability,configurat,configuration,6027,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:345,Modifiability,config,config,345,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:352,Modifiability,Config,ConfigException,352,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:380,Modifiability,config,configuration,380,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:451,Modifiability,config,config,451,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:530,Modifiability,config,config,530,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:606,Modifiability,config,config,606,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:676,Modifiability,config,config,676,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:746,Modifiability,config,config,746,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:819,Modifiability,config,config,819,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:912,Modifiability,config,config,912,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:1984,Modifiability,config,configuration,1984,"381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration file to. ```; backed {; defaultBackend = ""SGE""; backendsAllowed = [; ""Local"", ""SGE""; ]; providers { .... }; }; ```. However, this introduces another exception after a few seconds. . ```; [2016-09-13 17:39:24,467] [info] Slf4jLogger started; [2016-09-13 17:39:24,541] [info] RUN sub-command; [2016-09-13 17:39:24,542] [info] WDL file: pipeline.wdl; [2016-09-13 17:39:24,543] [info] Inputs: inputs.json; [2016-09-13 17:39:24,622] [info] SingleWorkflowRunnerActor: launching workflow; [2016-09-13 17:39:25,911] [info] Running with database db.url = jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:5992,Modifiability,config,config,5992,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:5999,Modifiability,Config,ConfigException,5999,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6027,Modifiability,config,configuration,6027,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6100,Modifiability,config,config,6100,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6179,Modifiability,config,config,6179,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6255,Modifiability,config,config,6255,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6325,Modifiability,config,config,6325,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6395,Modifiability,config,config,6395,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6470,Modifiability,config,config,6470,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6545,Modifiability,config,config,6545,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:5656,Performance,concurren,concurrent,5656,torActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(S,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:5729,Performance,concurren,concurrent,5729,tor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSys,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:5814,Performance,concurren,concurrent,5814,l.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:5891,Performance,concurren,concurrent,5891,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:3564,Security,Validat,ValidationFlatMap,3564,"a};shutdown=false;hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.Materia",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:3590,Security,Validat,Validation,3590,"hsqldb.tx=mvcc; [2016-09-13 17:39:33,39] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 8eab0d5a-925a-4e99-ae3b-f30dfadacb58; Uncaught error from thread [cromwell-system-akka.actor.default-dispatcher-2] shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; java.lang.ExceptionInInitializerError; at cromwell.engine.backend.local.SharedFileSystem$class.rootPath(SharedFileSystem.scala:113); at cromwell.engine.backend.sge.SgeBackend.rootPath(SgeBackend.scala:47); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescrip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:4277,Security,Validat,ValidationFlatMap,4277,$1.apply(MaterializeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(Materia,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:4303,Security,Validat,Validation,4303,zeWorkflowDescriptorActor.scala:87); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1$1.apply(MaterializeWorkflowDescriptorActor.scala:86); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$1(MaterializeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescrip,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:4760,Security,Validat,ValidationFlatMap,4760,alizeWorkflowDescriptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQue,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:4786,Security,Validat,Validation,4786,iptorActor.scala:86); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:110); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2$$anonfun$apply$8.apply(MaterializeWorkflowDescriptorActor.scala:109); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:109); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor$2.apply(MaterializeWorkflowDescriptorActor.scala:103); at scalaz.ValidationFlatMap.flatMap(Validation.scala:433); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:21,Usability,simpl,simple,21,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:463,Usability,Simpl,SimpleConfig,463,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:490,Usability,Simpl,SimpleConfig,490,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:542,Usability,Simpl,SimpleConfig,542,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:566,Usability,Simpl,SimpleConfig,566,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:618,Usability,Simpl,SimpleConfig,618,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:636,Usability,Simpl,SimpleConfig,636,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:688,Usability,Simpl,SimpleConfig,688,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:706,Usability,Simpl,SimpleConfig,706,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:758,Usability,Simpl,SimpleConfig,758,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:779,Usability,Simpl,SimpleConfig,779,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:831,Usability,Simpl,SimpleConfig,831,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:872,Usability,Simpl,SimpleConfig,872,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:924,Usability,Simpl,SimpleConfig,924,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:951,Usability,Simpl,SimpleConfig,951,"I am trying to run a simple WDL file under SGE. The documentation tells me to take the [application.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/core/src/main/resources/application.conf) file and uncomment the SGE section. . Cromwell will very quickly raise an exception:. ```; Exception in thread ""main"" com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'backendsAllowed'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:252); at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:323); at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:381); at cromwell.server.WorkflowManagerSystem$class.allowedBackends(WorkflowManagerSystem.scala:24); at cromwell.Main$$anon$1.allowedBackends(Main.scala:97); at cromwell.server.WorkflowManagerSystem$class.$init$(WorkflowManagerSystem.scala:27); at cromwell.Main$$anon$1.<init>(Main.scala:97); at cromwell.Main.<init>(Main.scala:97); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:49); at cromwell.Main$delayedInit$body.apply(Main.scala:31); at scala.Function0$class.apply$mcV$sp(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.App$$anonfun$main$1.apply(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:381); at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); at scala.App$class.main(App.scala:76); at cromwell.Main$.main(Main.scala:31); at cromwell.Main.main(Main.scala); ```. I managed to fix _this_ exception by changing the configuration fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6112,Usability,Simpl,SimpleConfig,6112,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6139,Usability,Simpl,SimpleConfig,6139,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6191,Usability,Simpl,SimpleConfig,6191,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6215,Usability,Simpl,SimpleConfig,6215,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6267,Usability,Simpl,SimpleConfig,6267,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6285,Usability,Simpl,SimpleConfig,6285,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6337,Usability,Simpl,SimpleConfig,6337,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6355,Usability,Simpl,SimpleConfig,6355,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6407,Usability,Simpl,SimpleConfig,6407,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6430,Usability,Simpl,SimpleConfig,6430,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6482,Usability,Simpl,SimpleConfig,6482,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6505,Usability,Simpl,SimpleConfig,6505,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6557,Usability,Simpl,SimpleConfig,6557,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/issues/1406:6580,Usability,Simpl,SimpleConfig,6580,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406
https://github.com/broadinstitute/cromwell/pull/1407:0,Testability,Test,Testing,0,"Testing the FSM state-by-state, testing what happens when transitions happen and then making assumptions about the starting conditions of subsequent states.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1407
https://github.com/broadinstitute/cromwell/pull/1407:32,Testability,test,testing,32,"Testing the FSM state-by-state, testing what happens when transitions happen and then making assumptions about the starting conditions of subsequent states.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1407
https://github.com/broadinstitute/cromwell/issues/1409:160,Safety,abort,abort,160,"Can you add more details? Is this at the REST endpoint level, or an internal thing? How would the user experience this issue?. Do you mean/ for example. If you abort a workflow/call running on JES via the REST endpoint, Cromwell will return a 200 OK indicating ""yes, I will try to abort this"" as opposed to 200 OK ""yes, I have aborted this"". If it's at the REST endpoint level the ""Request received"" sounds right to me. The user shouldn't have to wait for the abort to occur, and for some backends it's a best-efforts anyway (like JES' cancel operation command)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409
https://github.com/broadinstitute/cromwell/issues/1409:281,Safety,abort,abort,281,"Can you add more details? Is this at the REST endpoint level, or an internal thing? How would the user experience this issue?. Do you mean/ for example. If you abort a workflow/call running on JES via the REST endpoint, Cromwell will return a 200 OK indicating ""yes, I will try to abort this"" as opposed to 200 OK ""yes, I have aborted this"". If it's at the REST endpoint level the ""Request received"" sounds right to me. The user shouldn't have to wait for the abort to occur, and for some backends it's a best-efforts anyway (like JES' cancel operation command)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409
https://github.com/broadinstitute/cromwell/issues/1409:327,Safety,abort,aborted,327,"Can you add more details? Is this at the REST endpoint level, or an internal thing? How would the user experience this issue?. Do you mean/ for example. If you abort a workflow/call running on JES via the REST endpoint, Cromwell will return a 200 OK indicating ""yes, I will try to abort this"" as opposed to 200 OK ""yes, I have aborted this"". If it's at the REST endpoint level the ""Request received"" sounds right to me. The user shouldn't have to wait for the abort to occur, and for some backends it's a best-efforts anyway (like JES' cancel operation command)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409
https://github.com/broadinstitute/cromwell/issues/1409:460,Safety,abort,abort,460,"Can you add more details? Is this at the REST endpoint level, or an internal thing? How would the user experience this issue?. Do you mean/ for example. If you abort a workflow/call running on JES via the REST endpoint, Cromwell will return a 200 OK indicating ""yes, I will try to abort this"" as opposed to 200 OK ""yes, I have aborted this"". If it's at the REST endpoint level the ""Request received"" sounds right to me. The user shouldn't have to wait for the abort to occur, and for some backends it's a best-efforts anyway (like JES' cancel operation command)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409
https://github.com/broadinstitute/cromwell/issues/1409:98,Usability,user experience,user experience,98,"Can you add more details? Is this at the REST endpoint level, or an internal thing? How would the user experience this issue?. Do you mean/ for example. If you abort a workflow/call running on JES via the REST endpoint, Cromwell will return a 200 OK indicating ""yes, I will try to abort this"" as opposed to 200 OK ""yes, I have aborted this"". If it's at the REST endpoint level the ""Request received"" sounds right to me. The user shouldn't have to wait for the abort to occur, and for some backends it's a best-efforts anyway (like JES' cancel operation command)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1409
https://github.com/broadinstitute/cromwell/pull/1410:0,Availability,Failure,Failure,0,Failure mode 3 from #1253,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1410
https://github.com/broadinstitute/cromwell/issues/1412:69,Deployability,pipeline,pipelines,69,"Most of the genomic file types we work with in the variant discovery pipelines are typically accompanied by an index file with a conventionally predictable name (eg my_callset.vcf comes with my_callset.vcf.idx). Right now, as a WDL author, I have to supply these index files explicitly in my inputs json files and in several places in my workflows. This is very tedious, so it would be glorious to have Cromwell just automatically recognize when file inputs and outputs are one of a defined index-associated types, search for the corresponding indices based on given naming conventions, and implicitly co-localize the index files that it finds (but not fail if it doesn't find them, because sometimes we work without indices!).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412
https://github.com/broadinstitute/cromwell/issues/1412:144,Safety,predict,predictable,144,"Most of the genomic file types we work with in the variant discovery pipelines are typically accompanied by an index file with a conventionally predictable name (eg my_callset.vcf comes with my_callset.vcf.idx). Right now, as a WDL author, I have to supply these index files explicitly in my inputs json files and in several places in my workflows. This is very tedious, so it would be glorious to have Cromwell just automatically recognize when file inputs and outputs are one of a defined index-associated types, search for the corresponding indices based on given naming conventions, and implicitly co-localize the index files that it finds (but not fail if it doesn't find them, because sometimes we work without indices!).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1412
https://github.com/broadinstitute/cromwell/issues/1413:284,Integrability,message,messages,284,Amorphous ticket - will need to be explored & refined (likely into multiple tickets?) before being acted upon. With our new suite of actors we still have one spot which seems to be a central hub for all commerce and activity. Multiple people have opined that with all of the incoming messages & state transitions that there are almost certainly pieces which could be peeled out in order to streamline it a bit,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413
https://github.com/broadinstitute/cromwell/issues/1414:504,Availability,down,down,504,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414
https://github.com/broadinstitute/cromwell/issues/1414:721,Availability,failure,failure,721,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414
https://github.com/broadinstitute/cromwell/issues/1414:82,Safety,abort,abort,82,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414
https://github.com/broadinstitute/cromwell/issues/1414:274,Safety,abort,abort,274,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414
https://github.com/broadinstitute/cromwell/issues/1414:339,Safety,abort,aborting,339,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414
https://github.com/broadinstitute/cromwell/issues/1414:371,Safety,abort,aborted,371,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414
https://github.com/broadinstitute/cromwell/issues/1414:403,Safety,abort,abort,403,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414
https://github.com/broadinstitute/cromwell/issues/1414:422,Safety,abort,abort,422,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414
https://github.com/broadinstitute/cromwell/issues/1414:480,Testability,log,logical,480,"When we traced through the new actor model there was a lot of wonkiness in how an abort request works its way through the system. There were end runs being made, direct shortcuts to specific things, etc. . Our thoughts were:; - Make sure that the whole process is async. An abort request returns immediately and the workflow is set to an `aborting` state (as opposed to `aborted` and ??? if it fails to abort); - Have the abort request work its way through the workflow in a more logical manner, working down from the `WorkflowActor` to the `WorkflowExecutionActor` to the `EJEA`, etc; - Have each stage watch for all of its children (not necessarily in a supervision sense) to complete or not before sending the success/failure response back up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1414
https://github.com/broadinstitute/cromwell/pull/1415:775,Energy Efficiency,reduce,reduces,775,"- Commit 1: Creates 2 subdirectories in the call directory. ```; cromwell-executions/globbinginput/; └── 970c776f-3b9d-4c0e-a2ff-dbd76395b4ba; ├── call-globtask; │   ├── execution; │   │   ├── outputfile1.txt; │   │   ├── outputfile2.txt; │   │   ├── rc; │   │   ├── script; │   │   ├── script.background; │   │   ├── script.submit; │   │   ├── stderr; │   │   ├── stderr.background; │   │   ├── stdout; │   │   └── stdout.background; │   └── inputs; │   └── 90e8d77e2a99e99efa82c33e27365d71; │   └── somefile.txt; ```. This prevents globbing from matching input files as they're not in the same branch.; - Commit 2: Instead of recreating the whole directory structure underneath the call directory, hash the path and create a single directory named with this hash. ; - This reduces the depth of the call directory, and is hopefully a bit less confusing (people tend to be confused when they see the full path of their input file appended to the call directory); - The hash is only computed from the fullpath of the input's parent directory (not including the filename). This ensures that multiple files from the same directory will still end up in the same directory once localized). The hashing part is not necessary to fix this bug at all, and I remember the choice was made not to use hashes because it's unreadable and makes it hard to know where the inputs came from but I feel like recreating the full path underneath the call directory is even worse...; - Commit 3: Change the globbing pattern so it does NOT traverse recursively the hierarchy looking for matches by default. If that's the desired behaviour it should be reflected in the glob pattern in the wdl (with `**`). This is necessary to allow for this kind of usage https://github.com/broadinstitute/cromwell/issues/1245",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1415
https://github.com/broadinstitute/cromwell/pull/1415:700,Security,hash,hash,700,"- Commit 1: Creates 2 subdirectories in the call directory. ```; cromwell-executions/globbinginput/; └── 970c776f-3b9d-4c0e-a2ff-dbd76395b4ba; ├── call-globtask; │   ├── execution; │   │   ├── outputfile1.txt; │   │   ├── outputfile2.txt; │   │   ├── rc; │   │   ├── script; │   │   ├── script.background; │   │   ├── script.submit; │   │   ├── stderr; │   │   ├── stderr.background; │   │   ├── stdout; │   │   └── stdout.background; │   └── inputs; │   └── 90e8d77e2a99e99efa82c33e27365d71; │   └── somefile.txt; ```. This prevents globbing from matching input files as they're not in the same branch.; - Commit 2: Instead of recreating the whole directory structure underneath the call directory, hash the path and create a single directory named with this hash. ; - This reduces the depth of the call directory, and is hopefully a bit less confusing (people tend to be confused when they see the full path of their input file appended to the call directory); - The hash is only computed from the fullpath of the input's parent directory (not including the filename). This ensures that multiple files from the same directory will still end up in the same directory once localized). The hashing part is not necessary to fix this bug at all, and I remember the choice was made not to use hashes because it's unreadable and makes it hard to know where the inputs came from but I feel like recreating the full path underneath the call directory is even worse...; - Commit 3: Change the globbing pattern so it does NOT traverse recursively the hierarchy looking for matches by default. If that's the desired behaviour it should be reflected in the glob pattern in the wdl (with `**`). This is necessary to allow for this kind of usage https://github.com/broadinstitute/cromwell/issues/1245",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1415
https://github.com/broadinstitute/cromwell/pull/1415:760,Security,hash,hash,760,"- Commit 1: Creates 2 subdirectories in the call directory. ```; cromwell-executions/globbinginput/; └── 970c776f-3b9d-4c0e-a2ff-dbd76395b4ba; ├── call-globtask; │   ├── execution; │   │   ├── outputfile1.txt; │   │   ├── outputfile2.txt; │   │   ├── rc; │   │   ├── script; │   │   ├── script.background; │   │   ├── script.submit; │   │   ├── stderr; │   │   ├── stderr.background; │   │   ├── stdout; │   │   └── stdout.background; │   └── inputs; │   └── 90e8d77e2a99e99efa82c33e27365d71; │   └── somefile.txt; ```. This prevents globbing from matching input files as they're not in the same branch.; - Commit 2: Instead of recreating the whole directory structure underneath the call directory, hash the path and create a single directory named with this hash. ; - This reduces the depth of the call directory, and is hopefully a bit less confusing (people tend to be confused when they see the full path of their input file appended to the call directory); - The hash is only computed from the fullpath of the input's parent directory (not including the filename). This ensures that multiple files from the same directory will still end up in the same directory once localized). The hashing part is not necessary to fix this bug at all, and I remember the choice was made not to use hashes because it's unreadable and makes it hard to know where the inputs came from but I feel like recreating the full path underneath the call directory is even worse...; - Commit 3: Change the globbing pattern so it does NOT traverse recursively the hierarchy looking for matches by default. If that's the desired behaviour it should be reflected in the glob pattern in the wdl (with `**`). This is necessary to allow for this kind of usage https://github.com/broadinstitute/cromwell/issues/1245",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1415
https://github.com/broadinstitute/cromwell/pull/1415:969,Security,hash,hash,969,"- Commit 1: Creates 2 subdirectories in the call directory. ```; cromwell-executions/globbinginput/; └── 970c776f-3b9d-4c0e-a2ff-dbd76395b4ba; ├── call-globtask; │   ├── execution; │   │   ├── outputfile1.txt; │   │   ├── outputfile2.txt; │   │   ├── rc; │   │   ├── script; │   │   ├── script.background; │   │   ├── script.submit; │   │   ├── stderr; │   │   ├── stderr.background; │   │   ├── stdout; │   │   └── stdout.background; │   └── inputs; │   └── 90e8d77e2a99e99efa82c33e27365d71; │   └── somefile.txt; ```. This prevents globbing from matching input files as they're not in the same branch.; - Commit 2: Instead of recreating the whole directory structure underneath the call directory, hash the path and create a single directory named with this hash. ; - This reduces the depth of the call directory, and is hopefully a bit less confusing (people tend to be confused when they see the full path of their input file appended to the call directory); - The hash is only computed from the fullpath of the input's parent directory (not including the filename). This ensures that multiple files from the same directory will still end up in the same directory once localized). The hashing part is not necessary to fix this bug at all, and I remember the choice was made not to use hashes because it's unreadable and makes it hard to know where the inputs came from but I feel like recreating the full path underneath the call directory is even worse...; - Commit 3: Change the globbing pattern so it does NOT traverse recursively the hierarchy looking for matches by default. If that's the desired behaviour it should be reflected in the glob pattern in the wdl (with `**`). This is necessary to allow for this kind of usage https://github.com/broadinstitute/cromwell/issues/1245",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1415
https://github.com/broadinstitute/cromwell/pull/1415:1189,Security,hash,hashing,1189,"- Commit 1: Creates 2 subdirectories in the call directory. ```; cromwell-executions/globbinginput/; └── 970c776f-3b9d-4c0e-a2ff-dbd76395b4ba; ├── call-globtask; │   ├── execution; │   │   ├── outputfile1.txt; │   │   ├── outputfile2.txt; │   │   ├── rc; │   │   ├── script; │   │   ├── script.background; │   │   ├── script.submit; │   │   ├── stderr; │   │   ├── stderr.background; │   │   ├── stdout; │   │   └── stdout.background; │   └── inputs; │   └── 90e8d77e2a99e99efa82c33e27365d71; │   └── somefile.txt; ```. This prevents globbing from matching input files as they're not in the same branch.; - Commit 2: Instead of recreating the whole directory structure underneath the call directory, hash the path and create a single directory named with this hash. ; - This reduces the depth of the call directory, and is hopefully a bit less confusing (people tend to be confused when they see the full path of their input file appended to the call directory); - The hash is only computed from the fullpath of the input's parent directory (not including the filename). This ensures that multiple files from the same directory will still end up in the same directory once localized). The hashing part is not necessary to fix this bug at all, and I remember the choice was made not to use hashes because it's unreadable and makes it hard to know where the inputs came from but I feel like recreating the full path underneath the call directory is even worse...; - Commit 3: Change the globbing pattern so it does NOT traverse recursively the hierarchy looking for matches by default. If that's the desired behaviour it should be reflected in the glob pattern in the wdl (with `**`). This is necessary to allow for this kind of usage https://github.com/broadinstitute/cromwell/issues/1245",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1415
https://github.com/broadinstitute/cromwell/pull/1415:1289,Security,hash,hashes,1289,"- Commit 1: Creates 2 subdirectories in the call directory. ```; cromwell-executions/globbinginput/; └── 970c776f-3b9d-4c0e-a2ff-dbd76395b4ba; ├── call-globtask; │   ├── execution; │   │   ├── outputfile1.txt; │   │   ├── outputfile2.txt; │   │   ├── rc; │   │   ├── script; │   │   ├── script.background; │   │   ├── script.submit; │   │   ├── stderr; │   │   ├── stderr.background; │   │   ├── stdout; │   │   └── stdout.background; │   └── inputs; │   └── 90e8d77e2a99e99efa82c33e27365d71; │   └── somefile.txt; ```. This prevents globbing from matching input files as they're not in the same branch.; - Commit 2: Instead of recreating the whole directory structure underneath the call directory, hash the path and create a single directory named with this hash. ; - This reduces the depth of the call directory, and is hopefully a bit less confusing (people tend to be confused when they see the full path of their input file appended to the call directory); - The hash is only computed from the fullpath of the input's parent directory (not including the filename). This ensures that multiple files from the same directory will still end up in the same directory once localized). The hashing part is not necessary to fix this bug at all, and I remember the choice was made not to use hashes because it's unreadable and makes it hard to know where the inputs came from but I feel like recreating the full path underneath the call directory is even worse...; - Commit 3: Change the globbing pattern so it does NOT traverse recursively the hierarchy looking for matches by default. If that's the desired behaviour it should be reflected in the glob pattern in the wdl (with `**`). This is necessary to allow for this kind of usage https://github.com/broadinstitute/cromwell/issues/1245",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1415
https://github.com/broadinstitute/cromwell/issues/1423:63,Deployability,configurat,configuration,63,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/issues/1423:168,Deployability,configurat,configuration-in-active-record,168,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/issues/1423:494,Deployability,update,update,494,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/issues/1423:543,Deployability,Update,Update,543,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/issues/1423:902,Deployability,update,updates,902,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/issues/1423:63,Modifiability,config,configuration,63,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/issues/1423:168,Modifiability,config,configuration-in-active-record,168,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/issues/1423:695,Safety,avoid,avoiding,695,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/issues/1423:103,Usability,guid,guides,103,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/issues/1423:238,Usability,guid,guide,238,"The `database.sql` artifact should follow the ""convention over configuration"" nature of [Rails](http://guides.rubyonrails.org/active_record_basics.html#convention-over-configuration-in-active-record), [GORM](http://docs.grails.org/3.1.11/guide/GORM.html#domainClasses), [Hibernate](http://docs.jboss.org/hibernate/orm/5.2/quickstart/html_single/#tutorial_annotations), etc. Classes should be named after the table, and replacing under_scores with CapitalizedNames. We can either:; 1. Liquibase update the database to match the scala names; 2. Update the scala to match the database naming; 3. Ignore the issue and not have a naming convention. Updating the scala seems like the least intrusive, avoiding any possibly time consuming database migrations and index rebuilding at the moment. However, developers may have put more effort into naming the scala tables and less into the sql tables. Liquibase updates could follow as part of a future issue, if/when the schema needs other modifications.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1423
https://github.com/broadinstitute/cromwell/pull/1424:60,Performance,cache,cached,60,Functionality added to localize WdlFile / WdlArray[WdlFile] cached results in HtCondor so in that way cached results are link to the new task. i.e.:; 1. w0/task1 execution produces w0/result1.; 2. w0/result1 is stored in the cache.; 3. w1/task1 (w1/task1 is the same as w0/task1) is executed again.; 4. Cache result is hit.; 5. File cached results are used to generate new symlinks to point to them in current task.; 6. Result is generated using new paths based on symlinks and/or other cached results. This change is based on new security/permissions requirements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424
https://github.com/broadinstitute/cromwell/pull/1424:102,Performance,cache,cached,102,Functionality added to localize WdlFile / WdlArray[WdlFile] cached results in HtCondor so in that way cached results are link to the new task. i.e.:; 1. w0/task1 execution produces w0/result1.; 2. w0/result1 is stored in the cache.; 3. w1/task1 (w1/task1 is the same as w0/task1) is executed again.; 4. Cache result is hit.; 5. File cached results are used to generate new symlinks to point to them in current task.; 6. Result is generated using new paths based on symlinks and/or other cached results. This change is based on new security/permissions requirements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424
https://github.com/broadinstitute/cromwell/pull/1424:225,Performance,cache,cache,225,Functionality added to localize WdlFile / WdlArray[WdlFile] cached results in HtCondor so in that way cached results are link to the new task. i.e.:; 1. w0/task1 execution produces w0/result1.; 2. w0/result1 is stored in the cache.; 3. w1/task1 (w1/task1 is the same as w0/task1) is executed again.; 4. Cache result is hit.; 5. File cached results are used to generate new symlinks to point to them in current task.; 6. Result is generated using new paths based on symlinks and/or other cached results. This change is based on new security/permissions requirements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424
https://github.com/broadinstitute/cromwell/pull/1424:303,Performance,Cache,Cache,303,Functionality added to localize WdlFile / WdlArray[WdlFile] cached results in HtCondor so in that way cached results are link to the new task. i.e.:; 1. w0/task1 execution produces w0/result1.; 2. w0/result1 is stored in the cache.; 3. w1/task1 (w1/task1 is the same as w0/task1) is executed again.; 4. Cache result is hit.; 5. File cached results are used to generate new symlinks to point to them in current task.; 6. Result is generated using new paths based on symlinks and/or other cached results. This change is based on new security/permissions requirements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424
https://github.com/broadinstitute/cromwell/pull/1424:333,Performance,cache,cached,333,Functionality added to localize WdlFile / WdlArray[WdlFile] cached results in HtCondor so in that way cached results are link to the new task. i.e.:; 1. w0/task1 execution produces w0/result1.; 2. w0/result1 is stored in the cache.; 3. w1/task1 (w1/task1 is the same as w0/task1) is executed again.; 4. Cache result is hit.; 5. File cached results are used to generate new symlinks to point to them in current task.; 6. Result is generated using new paths based on symlinks and/or other cached results. This change is based on new security/permissions requirements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424
https://github.com/broadinstitute/cromwell/pull/1424:487,Performance,cache,cached,487,Functionality added to localize WdlFile / WdlArray[WdlFile] cached results in HtCondor so in that way cached results are link to the new task. i.e.:; 1. w0/task1 execution produces w0/result1.; 2. w0/result1 is stored in the cache.; 3. w1/task1 (w1/task1 is the same as w0/task1) is executed again.; 4. Cache result is hit.; 5. File cached results are used to generate new symlinks to point to them in current task.; 6. Result is generated using new paths based on symlinks and/or other cached results. This change is based on new security/permissions requirements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424
https://github.com/broadinstitute/cromwell/pull/1424:531,Security,secur,security,531,Functionality added to localize WdlFile / WdlArray[WdlFile] cached results in HtCondor so in that way cached results are link to the new task. i.e.:; 1. w0/task1 execution produces w0/result1.; 2. w0/result1 is stored in the cache.; 3. w1/task1 (w1/task1 is the same as w0/task1) is executed again.; 4. Cache result is hit.; 5. File cached results are used to generate new symlinks to point to them in current task.; 6. Result is generated using new paths based on symlinks and/or other cached results. This change is based on new security/permissions requirements.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1424
https://github.com/broadinstitute/cromwell/issues/1425:168,Performance,cache,cached,168,Based on new security/permissions requirements there is a need to add extra functionality to HtCondor backend. Basically the functionality should provide a way to link cached file / array of files outputs to the current workflow execution. All paths should point to current workflow execution dir.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1425
https://github.com/broadinstitute/cromwell/issues/1425:13,Security,secur,security,13,Based on new security/permissions requirements there is a need to add extra functionality to HtCondor backend. Basically the functionality should provide a way to link cached file / array of files outputs to the current workflow execution. All paths should point to current workflow execution dir.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1425
https://github.com/broadinstitute/cromwell/issues/1426:347,Safety,detect,detects,347,"The zeroth localizers checks to see if a file exists before re-localizing. The copy localizer should therefore copy-to-temp-then-rename. Current broken behavior:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam`; - Kill the job during localization; - Restart cromwell; - Cromwell detects the partial `<call_root>/input.bam` exists.; - The job continues without relocalizing. Better behavior:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam.tmp`; - Kill the job during localization; - Restart cromwell; - Cromwell doesn't detects the partial `<call_root>/input.bam` exists.; - The job continues with relocalizing. And when cromwell isn't killed:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam.tmp`, ensuring to overwrite previous results; - When copying finishes rename `<call_root>/input.bam.tmp` to `<call_root>/input.bam`; - The job continues. NOTE: Most people do not like copying inputs anyway, so this hasn't been a major issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426
https://github.com/broadinstitute/cromwell/issues/1426:656,Safety,detect,detects,656,"The zeroth localizers checks to see if a file exists before re-localizing. The copy localizer should therefore copy-to-temp-then-rename. Current broken behavior:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam`; - Kill the job during localization; - Restart cromwell; - Cromwell detects the partial `<call_root>/input.bam` exists.; - The job continues without relocalizing. Better behavior:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam.tmp`; - Kill the job during localization; - Restart cromwell; - Cromwell doesn't detects the partial `<call_root>/input.bam` exists.; - The job continues with relocalizing. And when cromwell isn't killed:; - Run SGE task with a large `input.bam` and copy localization; - Cromwell starts copying to `<call_root>/input.bam.tmp`, ensuring to overwrite previous results; - When copying finishes rename `<call_root>/input.bam.tmp` to `<call_root>/input.bam`; - The job continues. NOTE: Most people do not like copying inputs anyway, so this hasn't been a major issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1426
https://github.com/broadinstitute/cromwell/issues/1428:73,Availability,Error,Error,73,"Building docker with `docker build .` generates warnings like. `; [warn] Error extracting zip entry 'scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class' to '/cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class': java.io.FileNotFoundException: /cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class (File name too long); `. It appears this is because the max filename under docker is ~242 characters, but the sbt default for max generated class name is ~254/255. See https://github.com/docker/docker/issues/1413. The fix is to reduce this as described here . http://stackoverflow.com/questions/28565837/filename-too-long-sbt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1428
https://github.com/broadinstitute/cromwell/issues/1428:1574,Energy Efficiency,reduce,reduce,1574,"Building docker with `docker build .` generates warnings like. `; [warn] Error extracting zip entry 'scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class' to '/cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class': java.io.FileNotFoundException: /cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class (File name too long); `. It appears this is because the max filename under docker is ~242 characters, but the sbt default for max generated class name is ~254/255. See https://github.com/docker/docker/issues/1413. The fix is to reduce this as described here . http://stackoverflow.com/questions/28565837/filename-too-long-sbt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1428
https://github.com/broadinstitute/cromwell/pull/1429:144,Deployability,configurat,configuration,144,"Only the refresh-token mode requires new credentials to be created/validated for every workflow, all other modes can be validated only from the configuration. This ensures we don't re-create unnecessary auth modes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429
https://github.com/broadinstitute/cromwell/pull/1429:144,Modifiability,config,configuration,144,"Only the refresh-token mode requires new credentials to be created/validated for every workflow, all other modes can be validated only from the configuration. This ensures we don't re-create unnecessary auth modes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429
https://github.com/broadinstitute/cromwell/pull/1429:67,Security,validat,validated,67,"Only the refresh-token mode requires new credentials to be created/validated for every workflow, all other modes can be validated only from the configuration. This ensures we don't re-create unnecessary auth modes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429
https://github.com/broadinstitute/cromwell/pull/1429:120,Security,validat,validated,120,"Only the refresh-token mode requires new credentials to be created/validated for every workflow, all other modes can be validated only from the configuration. This ensures we don't re-create unnecessary auth modes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429
https://github.com/broadinstitute/cromwell/pull/1432:39,Modifiability,Config,Configurable,39,Minor changes in order to support:; 1. Configurable docker command in HtCondor.; 2. Allow the use of soft-links in dockerized jobs.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1432
https://github.com/broadinstitute/cromwell/issues/1433:32,Modifiability,Config,Configurable,32,There is a need to support:; 1. Configurable docker command in HtCondor.; 2. Allow the use of soft-links in dockerized jobs.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1433
https://github.com/broadinstitute/cromwell/issues/1435:411,Integrability,message,message,411,"We saw during JG testing that a stray non-`lazy` instance variable in `JesAsyncBackendJobExecutionActor` led to an exception being thrown during `<init>`. This exception wasn't handled by any supervisor nor by JABJEA itself, so the JABJEA (and possibly `JesJobExecutionActor` and other actors) simply crashed. The net result was the `WorkflowExecutionActor` stayed `Running` forever, waiting for a terminal job message that would never come.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1435
https://github.com/broadinstitute/cromwell/issues/1435:58,Modifiability,variab,variable,58,"We saw during JG testing that a stray non-`lazy` instance variable in `JesAsyncBackendJobExecutionActor` led to an exception being thrown during `<init>`. This exception wasn't handled by any supervisor nor by JABJEA itself, so the JABJEA (and possibly `JesJobExecutionActor` and other actors) simply crashed. The net result was the `WorkflowExecutionActor` stayed `Running` forever, waiting for a terminal job message that would never come.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1435
https://github.com/broadinstitute/cromwell/issues/1435:17,Testability,test,testing,17,"We saw during JG testing that a stray non-`lazy` instance variable in `JesAsyncBackendJobExecutionActor` led to an exception being thrown during `<init>`. This exception wasn't handled by any supervisor nor by JABJEA itself, so the JABJEA (and possibly `JesJobExecutionActor` and other actors) simply crashed. The net result was the `WorkflowExecutionActor` stayed `Running` forever, waiting for a terminal job message that would never come.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1435
https://github.com/broadinstitute/cromwell/issues/1435:294,Usability,simpl,simply,294,"We saw during JG testing that a stray non-`lazy` instance variable in `JesAsyncBackendJobExecutionActor` led to an exception being thrown during `<init>`. This exception wasn't handled by any supervisor nor by JABJEA itself, so the JABJEA (and possibly `JesJobExecutionActor` and other actors) simply crashed. The net result was the `WorkflowExecutionActor` stayed `Running` forever, waiting for a terminal job message that would never come.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1435
https://github.com/broadinstitute/cromwell/issues/1436:41,Testability,test,testing,41,@ruchim has the stack trace from 9/16 JG testing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436
https://github.com/broadinstitute/cromwell/issues/1437:93,Availability,redundant,redundantly,93,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437
https://github.com/broadinstitute/cromwell/issues/1437:261,Availability,error,error,261,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437
https://github.com/broadinstitute/cromwell/issues/1437:355,Availability,error,error,355,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437
https://github.com/broadinstitute/cromwell/issues/1437:15,Modifiability,extend,extends,15,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437
https://github.com/broadinstitute/cromwell/issues/1437:93,Safety,redund,redundantly,93,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437
https://github.com/broadinstitute/cromwell/issues/1437:243,Testability,test,testing,243,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437
https://github.com/broadinstitute/cromwell/issues/1437:511,Usability,simpl,simply,511,"`JesCallPaths` extends `JesWorkflowPaths` and appears to create a lot of workflow-level data redundantly. Besides wasting time and possibly money, this causes unnecessary calls to Google APIs which count against our QPS limits. During 9/16 JG testing we saw an error creating `storage` in `JesWorkflowPaths` (which should have been treated as a transient error per #1436), but it seems that storage should have been created as part of the `JesBackendInitializationData` by the `JesInitializationActor` and then simply passed to the job actors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1437
https://github.com/broadinstitute/cromwell/pull/1440:152,Deployability,configurat,configuration,152,Now setting config directly in the `database` stanza.; Deprecated `database.config`.; Database migration now just uses the key `database.migration` for configuration settings.; Moved database migration defaults into the reference.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1440
https://github.com/broadinstitute/cromwell/pull/1440:12,Modifiability,config,config,12,Now setting config directly in the `database` stanza.; Deprecated `database.config`.; Database migration now just uses the key `database.migration` for configuration settings.; Moved database migration defaults into the reference.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1440
https://github.com/broadinstitute/cromwell/pull/1440:76,Modifiability,config,config,76,Now setting config directly in the `database` stanza.; Deprecated `database.config`.; Database migration now just uses the key `database.migration` for configuration settings.; Moved database migration defaults into the reference.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1440
https://github.com/broadinstitute/cromwell/pull/1440:152,Modifiability,config,configuration,152,Now setting config directly in the `database` stanza.; Deprecated `database.config`.; Database migration now just uses the key `database.migration` for configuration settings.; Moved database migration defaults into the reference.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1440
https://github.com/broadinstitute/cromwell/issues/1441:483,Availability,recover,recoverable,483,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441
https://github.com/broadinstitute/cromwell/issues/1441:110,Deployability,release,release,110,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441
https://github.com/broadinstitute/cromwell/issues/1441:254,Performance,cache,cache,254,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441
https://github.com/broadinstitute/cromwell/issues/1441:483,Safety,recover,recoverable,483,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441
https://github.com/broadinstitute/cromwell/issues/1441:318,Security,hash,hash,318,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441
https://github.com/broadinstitute/cromwell/issues/1441:526,Security,hash,hash,526,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441
https://github.com/broadinstitute/cromwell/issues/1441:672,Security,hash,hashes,672,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441
https://github.com/broadinstitute/cromwell/pull/1442:68,Availability,failure,failure,68,"Replacing `awaitCond` with `eventually` we should also get a better failure message than ""timeout expired""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1442
https://github.com/broadinstitute/cromwell/pull/1442:76,Integrability,message,message,76,"Replacing `awaitCond` with `eventually` we should also get a better failure message than ""timeout expired""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1442
https://github.com/broadinstitute/cromwell/pull/1442:90,Safety,timeout,timeout,90,"Replacing `awaitCond` with `eventually` we should also get a better failure message than ""timeout expired""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1442
https://github.com/broadinstitute/cromwell/issues/1443:274,Availability,failure,failures,274,Bring in lots of Scott's wdl4s goodness. This should include; - [x] Individuals spend 1-2 hours reviewing the PR as homework; - [x] Team gathers for group discussion of the code; - [ ] Rebase wdl4s; - [ ] Test cromwell 0.21 with wdl4s (unit tests & centaur); - [ ] Fix test failures from above; - [ ] Merge!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1443
https://github.com/broadinstitute/cromwell/issues/1443:205,Testability,Test,Test,205,Bring in lots of Scott's wdl4s goodness. This should include; - [x] Individuals spend 1-2 hours reviewing the PR as homework; - [x] Team gathers for group discussion of the code; - [ ] Rebase wdl4s; - [ ] Test cromwell 0.21 with wdl4s (unit tests & centaur); - [ ] Fix test failures from above; - [ ] Merge!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1443
https://github.com/broadinstitute/cromwell/issues/1443:241,Testability,test,tests,241,Bring in lots of Scott's wdl4s goodness. This should include; - [x] Individuals spend 1-2 hours reviewing the PR as homework; - [x] Team gathers for group discussion of the code; - [ ] Rebase wdl4s; - [ ] Test cromwell 0.21 with wdl4s (unit tests & centaur); - [ ] Fix test failures from above; - [ ] Merge!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1443
https://github.com/broadinstitute/cromwell/issues/1443:269,Testability,test,test,269,Bring in lots of Scott's wdl4s goodness. This should include; - [x] Individuals spend 1-2 hours reviewing the PR as homework; - [x] Team gathers for group discussion of the code; - [ ] Rebase wdl4s; - [ ] Test cromwell 0.21 with wdl4s (unit tests & centaur); - [ ] Fix test failures from above; - [ ] Merge!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1443
https://github.com/broadinstitute/cromwell/issues/1444:95,Availability,error,errors,95,"When there are a lot (~1800 or as many as 3000) of cache hits in a workflow, timeouts or other errors talking to Google often occur (probably can all be fixed with added retries). Impact: High. Prevents us from using call caching for 20k sample sets, which in turn requires us to manually stitch together outputs from multiple workflows to gather all the successes together, which is prone to error. When it is feasible to use, this problem still often requires us to repeatedly relaunch the same workflow, duplicating the data many times. This can be tested by the same WDL used to verify #1185",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1444
https://github.com/broadinstitute/cromwell/issues/1444:393,Availability,error,error,393,"When there are a lot (~1800 or as many as 3000) of cache hits in a workflow, timeouts or other errors talking to Google often occur (probably can all be fixed with added retries). Impact: High. Prevents us from using call caching for 20k sample sets, which in turn requires us to manually stitch together outputs from multiple workflows to gather all the successes together, which is prone to error. When it is feasible to use, this problem still often requires us to repeatedly relaunch the same workflow, duplicating the data many times. This can be tested by the same WDL used to verify #1185",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1444
https://github.com/broadinstitute/cromwell/issues/1444:51,Performance,cache,cache,51,"When there are a lot (~1800 or as many as 3000) of cache hits in a workflow, timeouts or other errors talking to Google often occur (probably can all be fixed with added retries). Impact: High. Prevents us from using call caching for 20k sample sets, which in turn requires us to manually stitch together outputs from multiple workflows to gather all the successes together, which is prone to error. When it is feasible to use, this problem still often requires us to repeatedly relaunch the same workflow, duplicating the data many times. This can be tested by the same WDL used to verify #1185",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1444
https://github.com/broadinstitute/cromwell/issues/1444:77,Safety,timeout,timeouts,77,"When there are a lot (~1800 or as many as 3000) of cache hits in a workflow, timeouts or other errors talking to Google often occur (probably can all be fixed with added retries). Impact: High. Prevents us from using call caching for 20k sample sets, which in turn requires us to manually stitch together outputs from multiple workflows to gather all the successes together, which is prone to error. When it is feasible to use, this problem still often requires us to repeatedly relaunch the same workflow, duplicating the data many times. This can be tested by the same WDL used to verify #1185",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1444
https://github.com/broadinstitute/cromwell/issues/1444:552,Testability,test,tested,552,"When there are a lot (~1800 or as many as 3000) of cache hits in a workflow, timeouts or other errors talking to Google often occur (probably can all be fixed with added retries). Impact: High. Prevents us from using call caching for 20k sample sets, which in turn requires us to manually stitch together outputs from multiple workflows to gather all the successes together, which is prone to error. When it is feasible to use, this problem still often requires us to repeatedly relaunch the same workflow, duplicating the data many times. This can be tested by the same WDL used to verify #1185",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1444
https://github.com/broadinstitute/cromwell/issues/1445:255,Energy Efficiency,monitor,monitor,255,"Under load Cromwell seems to have a large number of engine dispatcher threads blocked with stack traces like the following:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-107"" #455 prio=5 os_prio=0 tid=0x00007fa5b8075000 nid=0x4598 waiting for monitor entry [0x00007fa7999d8000]; java.lang.Thread.State: BLOCKED (on object monitor); at cromwell.core.logging.WorkflowLogger$.makeFileLogger(WorkflowLogger.scala:26); - waiting to lock <0x00000000ca4a8b40> (a cromwell.core.logging.WorkflowLogger$); at cromwell.core.logging.WorkflowLogger.<init>(WorkflowLogger.scala:108); at cromwell.core.logging.WorkflowLogging$class.workflowLogger(WorkflowLogger.scala:21); at cromwell.engine.workflow.WorkflowActor.workflowLogger$lzycompute(WorkflowActor.scala:151); - locked <0x00000000eceaf5f0> (a cromwell.engine.workflow.WorkflowActor); at cromwell.engine.workflow.WorkflowActor.workflowLogger(WorkflowActor.scala:151); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:262); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:260); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:151); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:151); at akka.actor.FSM$class.processEvent(FSM.scala:668); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445
https://github.com/broadinstitute/cromwell/issues/1445:334,Energy Efficiency,monitor,monitor,334,"Under load Cromwell seems to have a large number of engine dispatcher threads blocked with stack traces like the following:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-107"" #455 prio=5 os_prio=0 tid=0x00007fa5b8075000 nid=0x4598 waiting for monitor entry [0x00007fa7999d8000]; java.lang.Thread.State: BLOCKED (on object monitor); at cromwell.core.logging.WorkflowLogger$.makeFileLogger(WorkflowLogger.scala:26); - waiting to lock <0x00000000ca4a8b40> (a cromwell.core.logging.WorkflowLogger$); at cromwell.core.logging.WorkflowLogger.<init>(WorkflowLogger.scala:108); at cromwell.core.logging.WorkflowLogging$class.workflowLogger(WorkflowLogger.scala:21); at cromwell.engine.workflow.WorkflowActor.workflowLogger$lzycompute(WorkflowActor.scala:151); - locked <0x00000000eceaf5f0> (a cromwell.engine.workflow.WorkflowActor); at cromwell.engine.workflow.WorkflowActor.workflowLogger(WorkflowActor.scala:151); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:262); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:260); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:151); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:151); at akka.actor.FSM$class.processEvent(FSM.scala:668); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445
https://github.com/broadinstitute/cromwell/issues/1445:6,Performance,load,load,6,"Under load Cromwell seems to have a large number of engine dispatcher threads blocked with stack traces like the following:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-107"" #455 prio=5 os_prio=0 tid=0x00007fa5b8075000 nid=0x4598 waiting for monitor entry [0x00007fa7999d8000]; java.lang.Thread.State: BLOCKED (on object monitor); at cromwell.core.logging.WorkflowLogger$.makeFileLogger(WorkflowLogger.scala:26); - waiting to lock <0x00000000ca4a8b40> (a cromwell.core.logging.WorkflowLogger$); at cromwell.core.logging.WorkflowLogger.<init>(WorkflowLogger.scala:108); at cromwell.core.logging.WorkflowLogging$class.workflowLogger(WorkflowLogger.scala:21); at cromwell.engine.workflow.WorkflowActor.workflowLogger$lzycompute(WorkflowActor.scala:151); - locked <0x00000000eceaf5f0> (a cromwell.engine.workflow.WorkflowActor); at cromwell.engine.workflow.WorkflowActor.workflowLogger(WorkflowActor.scala:151); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:262); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:260); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:151); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:151); at akka.actor.FSM$class.processEvent(FSM.scala:668); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445
https://github.com/broadinstitute/cromwell/issues/1445:361,Testability,log,logging,361,"Under load Cromwell seems to have a large number of engine dispatcher threads blocked with stack traces like the following:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-107"" #455 prio=5 os_prio=0 tid=0x00007fa5b8075000 nid=0x4598 waiting for monitor entry [0x00007fa7999d8000]; java.lang.Thread.State: BLOCKED (on object monitor); at cromwell.core.logging.WorkflowLogger$.makeFileLogger(WorkflowLogger.scala:26); - waiting to lock <0x00000000ca4a8b40> (a cromwell.core.logging.WorkflowLogger$); at cromwell.core.logging.WorkflowLogger.<init>(WorkflowLogger.scala:108); at cromwell.core.logging.WorkflowLogging$class.workflowLogger(WorkflowLogger.scala:21); at cromwell.engine.workflow.WorkflowActor.workflowLogger$lzycompute(WorkflowActor.scala:151); - locked <0x00000000eceaf5f0> (a cromwell.engine.workflow.WorkflowActor); at cromwell.engine.workflow.WorkflowActor.workflowLogger(WorkflowActor.scala:151); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:262); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:260); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:151); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:151); at akka.actor.FSM$class.processEvent(FSM.scala:668); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445
https://github.com/broadinstitute/cromwell/issues/1445:482,Testability,log,logging,482,"Under load Cromwell seems to have a large number of engine dispatcher threads blocked with stack traces like the following:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-107"" #455 prio=5 os_prio=0 tid=0x00007fa5b8075000 nid=0x4598 waiting for monitor entry [0x00007fa7999d8000]; java.lang.Thread.State: BLOCKED (on object monitor); at cromwell.core.logging.WorkflowLogger$.makeFileLogger(WorkflowLogger.scala:26); - waiting to lock <0x00000000ca4a8b40> (a cromwell.core.logging.WorkflowLogger$); at cromwell.core.logging.WorkflowLogger.<init>(WorkflowLogger.scala:108); at cromwell.core.logging.WorkflowLogging$class.workflowLogger(WorkflowLogger.scala:21); at cromwell.engine.workflow.WorkflowActor.workflowLogger$lzycompute(WorkflowActor.scala:151); - locked <0x00000000eceaf5f0> (a cromwell.engine.workflow.WorkflowActor); at cromwell.engine.workflow.WorkflowActor.workflowLogger(WorkflowActor.scala:151); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:262); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:260); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:151); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:151); at akka.actor.FSM$class.processEvent(FSM.scala:668); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445
https://github.com/broadinstitute/cromwell/issues/1445:525,Testability,log,logging,525,"Under load Cromwell seems to have a large number of engine dispatcher threads blocked with stack traces like the following:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-107"" #455 prio=5 os_prio=0 tid=0x00007fa5b8075000 nid=0x4598 waiting for monitor entry [0x00007fa7999d8000]; java.lang.Thread.State: BLOCKED (on object monitor); at cromwell.core.logging.WorkflowLogger$.makeFileLogger(WorkflowLogger.scala:26); - waiting to lock <0x00000000ca4a8b40> (a cromwell.core.logging.WorkflowLogger$); at cromwell.core.logging.WorkflowLogger.<init>(WorkflowLogger.scala:108); at cromwell.core.logging.WorkflowLogging$class.workflowLogger(WorkflowLogger.scala:21); at cromwell.engine.workflow.WorkflowActor.workflowLogger$lzycompute(WorkflowActor.scala:151); - locked <0x00000000eceaf5f0> (a cromwell.engine.workflow.WorkflowActor); at cromwell.engine.workflow.WorkflowActor.workflowLogger(WorkflowActor.scala:151); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:262); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:260); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:151); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:151); at akka.actor.FSM$class.processEvent(FSM.scala:668); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445
https://github.com/broadinstitute/cromwell/issues/1445:599,Testability,log,logging,599,"Under load Cromwell seems to have a large number of engine dispatcher threads blocked with stack traces like the following:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-107"" #455 prio=5 os_prio=0 tid=0x00007fa5b8075000 nid=0x4598 waiting for monitor entry [0x00007fa7999d8000]; java.lang.Thread.State: BLOCKED (on object monitor); at cromwell.core.logging.WorkflowLogger$.makeFileLogger(WorkflowLogger.scala:26); - waiting to lock <0x00000000ca4a8b40> (a cromwell.core.logging.WorkflowLogger$); at cromwell.core.logging.WorkflowLogger.<init>(WorkflowLogger.scala:108); at cromwell.core.logging.WorkflowLogging$class.workflowLogger(WorkflowLogger.scala:21); at cromwell.engine.workflow.WorkflowActor.workflowLogger$lzycompute(WorkflowActor.scala:151); - locked <0x00000000eceaf5f0> (a cromwell.engine.workflow.WorkflowActor); at cromwell.engine.workflow.WorkflowActor.workflowLogger(WorkflowActor.scala:151); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:262); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:260); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:151); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:151); at akka.actor.FSM$class.processEvent(FSM.scala:668); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445
https://github.com/broadinstitute/cromwell/issues/1446:193,Availability,avail,available,193,"It would be nice to have conda recipes for cromwell and its dependencies as part of the [conda-forge channel](https://conda-forge.github.io/). The package would be roughly analogous to the one available via Homebrew, except multi-platform.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1446
https://github.com/broadinstitute/cromwell/issues/1446:60,Integrability,depend,dependencies,60,"It would be nice to have conda recipes for cromwell and its dependencies as part of the [conda-forge channel](https://conda-forge.github.io/). The package would be roughly analogous to the one available via Homebrew, except multi-platform.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1446
https://github.com/broadinstitute/cromwell/pull/1447:50,Performance,Cache,Cache,50,"Instead of adding the MetaInfoId as a part of the Cache Hit Metadata, add the source worklow id, call name and job index using the Fetch Cached Results Actor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447
https://github.com/broadinstitute/cromwell/pull/1447:137,Performance,Cache,Cached,137,"Instead of adding the MetaInfoId as a part of the Cache Hit Metadata, add the source worklow id, call name and job index using the Fetch Cached Results Actor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1447
https://github.com/broadinstitute/cromwell/issues/1448:10,Testability,log,logs,10,"Right now logs are copied after the workflow finishes as opposed to the JES/stdout/stderr logs of yore, which do this periodically. The logs are useful to know what is going on _while_ a workflow is running. . Also when the workflow fails we want to make sure that the workflow logs appear as this is the most common time you'd need to look at them (debugging)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448
https://github.com/broadinstitute/cromwell/issues/1448:90,Testability,log,logs,90,"Right now logs are copied after the workflow finishes as opposed to the JES/stdout/stderr logs of yore, which do this periodically. The logs are useful to know what is going on _while_ a workflow is running. . Also when the workflow fails we want to make sure that the workflow logs appear as this is the most common time you'd need to look at them (debugging)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448
https://github.com/broadinstitute/cromwell/issues/1448:136,Testability,log,logs,136,"Right now logs are copied after the workflow finishes as opposed to the JES/stdout/stderr logs of yore, which do this periodically. The logs are useful to know what is going on _while_ a workflow is running. . Also when the workflow fails we want to make sure that the workflow logs appear as this is the most common time you'd need to look at them (debugging)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448
https://github.com/broadinstitute/cromwell/issues/1448:278,Testability,log,logs,278,"Right now logs are copied after the workflow finishes as opposed to the JES/stdout/stderr logs of yore, which do this periodically. The logs are useful to know what is going on _while_ a workflow is running. . Also when the workflow fails we want to make sure that the workflow logs appear as this is the most common time you'd need to look at them (debugging)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1448
https://github.com/broadinstitute/cromwell/issues/1449:214,Availability,failure,failure,214,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449
https://github.com/broadinstitute/cromwell/issues/1449:324,Availability,down,download,324,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449
https://github.com/broadinstitute/cromwell/issues/1449:399,Availability,error,error,399,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449
https://github.com/broadinstitute/cromwell/issues/1449:405,Integrability,message,message,405,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449
https://github.com/broadinstitute/cromwell/issues/1449:195,Safety,safe,safeguards,195,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449
https://github.com/broadinstitute/cromwell/issues/1450:152,Availability,error,error,152,"- cromwell pre-0.21 dev snapshot; - JES backend; - command line execution (single workflow) . Current behavior, which happens frequently:; - Mysterious error 500 appears on cromwell stdout. Apologies that I do not have example. ; - cromwell hangs. One time, cromwell was left running overnight and no progress was made.; - ctl-c which ends cromwell; - up arrow and return; - job completes successfully. Can cromwell detect these errors on JES and retry the jobs?. More observations:; - These were never seen on local backend. On JES, these were common.; - All jobs were self-contained. I.e. did not hit a web service nor make a HTTP request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450
https://github.com/broadinstitute/cromwell/issues/1450:429,Availability,error,errors,429,"- cromwell pre-0.21 dev snapshot; - JES backend; - command line execution (single workflow) . Current behavior, which happens frequently:; - Mysterious error 500 appears on cromwell stdout. Apologies that I do not have example. ; - cromwell hangs. One time, cromwell was left running overnight and no progress was made.; - ctl-c which ends cromwell; - up arrow and return; - job completes successfully. Can cromwell detect these errors on JES and retry the jobs?. More observations:; - These were never seen on local backend. On JES, these were common.; - All jobs were self-contained. I.e. did not hit a web service nor make a HTTP request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450
https://github.com/broadinstitute/cromwell/issues/1450:416,Safety,detect,detect,416,"- cromwell pre-0.21 dev snapshot; - JES backend; - command line execution (single workflow) . Current behavior, which happens frequently:; - Mysterious error 500 appears on cromwell stdout. Apologies that I do not have example. ; - cromwell hangs. One time, cromwell was left running overnight and no progress was made.; - ctl-c which ends cromwell; - up arrow and return; - job completes successfully. Can cromwell detect these errors on JES and retry the jobs?. More observations:; - These were never seen on local backend. On JES, these were common.; - All jobs were self-contained. I.e. did not hit a web service nor make a HTTP request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1450
https://github.com/broadinstitute/cromwell/pull/1456:61,Integrability,synchroniz,synchronization,61,Overriding `WorkflowLogger.getName`.; Lookup loggers without synchronization. Create loggers with synchronization.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1456
https://github.com/broadinstitute/cromwell/pull/1456:98,Integrability,synchroniz,synchronization,98,Overriding `WorkflowLogger.getName`.; Lookup loggers without synchronization. Create loggers with synchronization.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1456
https://github.com/broadinstitute/cromwell/pull/1456:45,Testability,log,loggers,45,Overriding `WorkflowLogger.getName`.; Lookup loggers without synchronization. Create loggers with synchronization.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1456
https://github.com/broadinstitute/cromwell/pull/1456:85,Testability,log,loggers,85,Overriding `WorkflowLogger.getName`.; Lookup loggers without synchronization. Create loggers with synchronization.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1456
https://github.com/broadinstitute/cromwell/issues/1457:859,Availability,down,down,859,"There are at least four spots in `WorkflowExecutionActor` where `==` is invoked on `Scope`s. This is not so good as `==` in Scala does a deep equals, and the `Scope` implementations are case classes with compiler-generated `equals` and `hashCode` implementations. These implementations invoke `equals`/`hashCode` on their constructor fields, including `Task` which contains a lot of hairy stuff including ASTs. This is extra bad in `WorkflowExecutionActor` since these `==`s are invoked synchronously with message processing to update the execution store, and all this computation is effectively single-threaded within a workflow. In practice, for a 20K wide scatter this causes the `WorkflowExecutionActor` to spend upwards of 20 minutes from the time work finishes in (mock) JES to actually succeeding the workflow. I _think_ this might be fixed by chasing down all the spots where `==` is used on `Scope`s and replacing that with call FQN + index + attempt comparisons instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457
https://github.com/broadinstitute/cromwell/issues/1457:528,Deployability,update,update,528,"There are at least four spots in `WorkflowExecutionActor` where `==` is invoked on `Scope`s. This is not so good as `==` in Scala does a deep equals, and the `Scope` implementations are case classes with compiler-generated `equals` and `hashCode` implementations. These implementations invoke `equals`/`hashCode` on their constructor fields, including `Task` which contains a lot of hairy stuff including ASTs. This is extra bad in `WorkflowExecutionActor` since these `==`s are invoked synchronously with message processing to update the execution store, and all this computation is effectively single-threaded within a workflow. In practice, for a 20K wide scatter this causes the `WorkflowExecutionActor` to spend upwards of 20 minutes from the time work finishes in (mock) JES to actually succeeding the workflow. I _think_ this might be fixed by chasing down all the spots where `==` is used on `Scope`s and replacing that with call FQN + index + attempt comparisons instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457
https://github.com/broadinstitute/cromwell/issues/1457:506,Integrability,message,message,506,"There are at least four spots in `WorkflowExecutionActor` where `==` is invoked on `Scope`s. This is not so good as `==` in Scala does a deep equals, and the `Scope` implementations are case classes with compiler-generated `equals` and `hashCode` implementations. These implementations invoke `equals`/`hashCode` on their constructor fields, including `Task` which contains a lot of hairy stuff including ASTs. This is extra bad in `WorkflowExecutionActor` since these `==`s are invoked synchronously with message processing to update the execution store, and all this computation is effectively single-threaded within a workflow. In practice, for a 20K wide scatter this causes the `WorkflowExecutionActor` to spend upwards of 20 minutes from the time work finishes in (mock) JES to actually succeeding the workflow. I _think_ this might be fixed by chasing down all the spots where `==` is used on `Scope`s and replacing that with call FQN + index + attempt comparisons instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457
https://github.com/broadinstitute/cromwell/issues/1457:237,Security,hash,hashCode,237,"There are at least four spots in `WorkflowExecutionActor` where `==` is invoked on `Scope`s. This is not so good as `==` in Scala does a deep equals, and the `Scope` implementations are case classes with compiler-generated `equals` and `hashCode` implementations. These implementations invoke `equals`/`hashCode` on their constructor fields, including `Task` which contains a lot of hairy stuff including ASTs. This is extra bad in `WorkflowExecutionActor` since these `==`s are invoked synchronously with message processing to update the execution store, and all this computation is effectively single-threaded within a workflow. In practice, for a 20K wide scatter this causes the `WorkflowExecutionActor` to spend upwards of 20 minutes from the time work finishes in (mock) JES to actually succeeding the workflow. I _think_ this might be fixed by chasing down all the spots where `==` is used on `Scope`s and replacing that with call FQN + index + attempt comparisons instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457
https://github.com/broadinstitute/cromwell/issues/1457:303,Security,hash,hashCode,303,"There are at least four spots in `WorkflowExecutionActor` where `==` is invoked on `Scope`s. This is not so good as `==` in Scala does a deep equals, and the `Scope` implementations are case classes with compiler-generated `equals` and `hashCode` implementations. These implementations invoke `equals`/`hashCode` on their constructor fields, including `Task` which contains a lot of hairy stuff including ASTs. This is extra bad in `WorkflowExecutionActor` since these `==`s are invoked synchronously with message processing to update the execution store, and all this computation is effectively single-threaded within a workflow. In practice, for a 20K wide scatter this causes the `WorkflowExecutionActor` to spend upwards of 20 minutes from the time work finishes in (mock) JES to actually succeeding the workflow. I _think_ this might be fixed by chasing down all the spots where `==` is used on `Scope`s and replacing that with call FQN + index + attempt comparisons instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457
https://github.com/broadinstitute/cromwell/issues/1457:771,Testability,mock,mock,771,"There are at least four spots in `WorkflowExecutionActor` where `==` is invoked on `Scope`s. This is not so good as `==` in Scala does a deep equals, and the `Scope` implementations are case classes with compiler-generated `equals` and `hashCode` implementations. These implementations invoke `equals`/`hashCode` on their constructor fields, including `Task` which contains a lot of hairy stuff including ASTs. This is extra bad in `WorkflowExecutionActor` since these `==`s are invoked synchronously with message processing to update the execution store, and all this computation is effectively single-threaded within a workflow. In practice, for a 20K wide scatter this causes the `WorkflowExecutionActor` to spend upwards of 20 minutes from the time work finishes in (mock) JES to actually succeeding the workflow. I _think_ this might be fixed by chasing down all the spots where `==` is used on `Scope`s and replacing that with call FQN + index + attempt comparisons instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457
https://github.com/broadinstitute/cromwell/pull/1458:60,Performance,cache,cache,60,"For example, we can see how long we spend checking the call cache, copying results, etc, for each call.; Before:; <img width=""619"" alt=""screen shot 2016-09-21 at 11 21 49 am"" src=""https://cloud.githubusercontent.com/assets/13006282/18717366/a9c6cf10-7fed-11e6-9075-18eef4fc4871.png"">. After:; <img width=""978"" alt=""screen shot 2016-09-21 at 11 19 18 am"" src=""https://cloud.githubusercontent.com/assets/13006282/18717376/b2dbbbb0-7fed-11e6-8ac2-69445de55ed3.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1458
https://github.com/broadinstitute/cromwell/pull/1459:68,Deployability,update,updated,68,"This is a branch off of ks_liquibase_updates (since it has the most updated metadata CallCacheSpec requires to pass). The last few commits are to testing, the remainder is Khalid's hard work. Key changes:; - altered travis cromwell config to enable call-caching; - altered centaur.wdl in a way to checkout specific centaur branch (needs to be undone)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1459
https://github.com/broadinstitute/cromwell/pull/1459:232,Modifiability,config,config,232,"This is a branch off of ks_liquibase_updates (since it has the most updated metadata CallCacheSpec requires to pass). The last few commits are to testing, the remainder is Khalid's hard work. Key changes:; - altered travis cromwell config to enable call-caching; - altered centaur.wdl in a way to checkout specific centaur branch (needs to be undone)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1459
https://github.com/broadinstitute/cromwell/pull/1459:146,Testability,test,testing,146,"This is a branch off of ks_liquibase_updates (since it has the most updated metadata CallCacheSpec requires to pass). The last few commits are to testing, the remainder is Khalid's hard work. Key changes:; - altered travis cromwell config to enable call-caching; - altered centaur.wdl in a way to checkout specific centaur branch (needs to be undone)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1459
https://github.com/broadinstitute/cromwell/pull/1459:343,Usability,undo,undone,343,"This is a branch off of ks_liquibase_updates (since it has the most updated metadata CallCacheSpec requires to pass). The last few commits are to testing, the remainder is Khalid's hard work. Key changes:; - altered travis cromwell config to enable call-caching; - altered centaur.wdl in a way to checkout specific centaur branch (needs to be undone)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1459
https://github.com/broadinstitute/cromwell/issues/1461:427,Availability,ERROR,ERROR,427,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461
https://github.com/broadinstitute/cromwell/issues/1461:810,Availability,echo,echo,810,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461
https://github.com/broadinstitute/cromwell/issues/1461:1959,Availability,echo,echo,1959,"""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s = read_string(stdout()) }; }. task mirror {; Array[String] s; command {}; output { Array[String] out = s }; }. workflow helloArray {; Array[Int] ints = range(100); scatter(i in ints) {; call helloWorld; }; call mirror { input: s = helloWorld.s }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461
https://github.com/broadinstitute/cromwell/issues/1461:118,Integrability,message,message,118,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461
https://github.com/broadinstitute/cromwell/issues/1461:74,Performance,cache,cache,74,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461
https://github.com/broadinstitute/cromwell/issues/1461:478,Performance,cache,cache,478,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461
https://github.com/broadinstitute/cromwell/issues/1461:585,Performance,cache,cache,585,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461
https://github.com/broadinstitute/cromwell/issues/1461:163,Usability,clear,clearly,163,"EDIT: The final (non-scattered) task didn't print out the `Failed copying cache results, falling back to running job` message but the timing diagram shows that it clearly transitioned quickly from BackendIsCopyingCallCacheOutputs to spend the same time ""RunningJob"" as everything else. Happened for all the scatters of a hello world workflow:. ```; 2016-09-20 18:53:47,051 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - helloArray.helloWorld:79:1: Failed copying cache results, falling back to running job: java.lang.RuntimeException: The call detritus files for source cache hit aren't found for call helloArray.helloWorld; 2016-09-20 18:53:47,052 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: `echo ""hello, world""`; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: executing: /bin/bash /Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script; 2016-09-20 18:53:47,053 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: command: ""/bin/bash"" ""/Users/chrisl/IdeaProjects/cromwell/cromwell-executions/helloArray/55d1e515-90fb-4d96-a025-b19a7decd1f4/call-helloWorld/shard-79/execution/script.submit""; 2016-09-20 18:53:47,059 cromwell-system-akka.dispatchers.backend-dispatcher-66 INFO - SharedFileSystemAsyncJobExecutionActor [UUID(55d1e515)helloArray.helloWorld:79:1]: job id: 89817; 2016-09-20 18:53:47,907 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - WorkflowExecutionActor-55d1e515-90fb-4d96-a025-b19a7decd1f4 [UUID(55d1e515)]: Job helloArray.helloWorld:79:1 succeeded!; ```. The workflow:. ```; task helloWorld {; command { echo ""hello, world"" }; output { String s =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1461
https://github.com/broadinstitute/cromwell/pull/1464:53,Performance,cache,cache,53,making sure the detritus callRootPath is present for cache hit calls,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1464
https://github.com/broadinstitute/cromwell/issues/1465:141,Availability,error,error,141,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:230,Availability,error,error,230,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:826,Availability,Error,Error,826,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:873,Availability,error,error,873,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:832,Integrability,message,message,832,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:3035,Modifiability,config,config,3035,"fun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.immutable.Map$Map3.foreach(Map.scala:161); ....snip....; ```. default_runtime:. ```; {; ""default_runtime_attributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""zones"": ""us-central1-a us-central1-b"",; ""disks"": ""local-disk 200 SSD"",; ""memory"": ""6G""; }; }; ```. Relevant snippet from local_application.conf:. ```; ....; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:3042,Modifiability,Config,ConfigBackendLifecycleActorFactory,3042,"zationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.immutable.Map$Map3.foreach(Map.scala:161); ....snip....; ```. default_runtime:. ```; {; ""default_runtime_attributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""zones"": ""us-central1-a us-central1-b"",; ""disks"": ""local-disk 200 SSD"",; ""memory"": ""6G""; }; }; ```. Relevant snippet from local_application.conf:. ```; ....; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; }; ......; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:3079,Modifiability,config,config,3079,"zationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.immutable.Map$Map3.foreach(Map.scala:161); ....snip....; ```. default_runtime:. ```; {; ""default_runtime_attributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""zones"": ""us-central1-a us-central1-b"",; ""disks"": ""local-disk 200 SSD"",; ""memory"": ""6G""; }; }; ```. Relevant snippet from local_application.conf:. ```; ....; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; }; ......; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:953,Performance,perform,perform,953,"fying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeVali",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:1297,Performance,perform,perform,1297,"java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializati",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:161,Security,validat,validates,161,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:1560,Security,validat,validation,1560," ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:1605,Security,validat,validateOptionalExpression,1605,"validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.immutable.Map$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:1692,Security,validat,validation,1692,"mes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.immutable.Map$Map3.foreach(Map.scala:161); ....snip....; ```. default_runtime:.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/issues/1465:1739,Security,validat,validateOptionalExpression,1739,"p_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:141); at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$badRuntimeAttrsForTask$1$2.apply(BackendWorkflowInitializationActor.scala:139); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245); at scala.collection.immutable.Map$Map3.foreach(Map.scala:161); ....snip....; ```. default_runtime:. ```; {; ""default_runtime_attributes"": {; ""docker"": ""broadinstitute/g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465
https://github.com/broadinstitute/cromwell/pull/1471:36,Modifiability,config,configurable,36,Goals of these changes:; 1. Support configurable default docker container working dir for cases where dockerWorkingDir is not defined in runtime attributes.; 2. Support configurable default docker container output dir for cases where dockerOutputDir is not defined in runtime attributes.; 3. Be able to mount working directories for cases where a tool creates intermediate results (big files) in order to produce final output in the node FS and this is not capable to handle those file sizes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1471
https://github.com/broadinstitute/cromwell/pull/1471:169,Modifiability,config,configurable,169,Goals of these changes:; 1. Support configurable default docker container working dir for cases where dockerWorkingDir is not defined in runtime attributes.; 2. Support configurable default docker container output dir for cases where dockerOutputDir is not defined in runtime attributes.; 3. Be able to mount working directories for cases where a tool creates intermediate results (big files) in order to produce final output in the node FS and this is not capable to handle those file sizes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1471
https://github.com/broadinstitute/cromwell/issues/1473:132,Modifiability,variab,variable,132,"As a part of supporting sub-workflows, workflow outputs need to behave similarly to task outputs. Task outputs are defined as typed variable declarations (e.g. File myout = ""${foo}.bam""). Currently workflow outputs just ""expose"" the outputs of tasks, and operate more like a whitelist or filter. You can not fabricate a workflow output based on a task output (like the about myout example). However, this should still be backwards compatible with the current definitions. For example you can write:. `output {; task.value; }`. However, we should be able to allow this as syntactic sugar by inferring the type of this from the task definition. For example if this was a File, the above would be the same as `File task.value = task.value`. `output {; File task.value = task.value; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1473
https://github.com/broadinstitute/cromwell/issues/1473:221,Security,expose,expose,221,"As a part of supporting sub-workflows, workflow outputs need to behave similarly to task outputs. Task outputs are defined as typed variable declarations (e.g. File myout = ""${foo}.bam""). Currently workflow outputs just ""expose"" the outputs of tasks, and operate more like a whitelist or filter. You can not fabricate a workflow output based on a task output (like the about myout example). However, this should still be backwards compatible with the current definitions. For example you can write:. `output {; task.value; }`. However, we should be able to allow this as syntactic sugar by inferring the type of this from the task definition. For example if this was a File, the above would be the same as `File task.value = task.value`. `output {; File task.value = task.value; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1473
https://github.com/broadinstitute/cromwell/issues/1475:502,Security,access,access,502,"As a WDL author, I would like to be able to write a sub workflow and call it from my main workflow as a subworkflow. I would like the main workflow to be backwards compatible, so I would like to use a keyword modifier to flag the _other_ workflows as being non-primary. . E.g. perhaps the keyword `sub` so I would write `sub workflow { //blah }`. Since these workflows would be callable when they are included via an import I'd prefer to not have a keyword like `private` since that implies visibility/access",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1475
https://github.com/broadinstitute/cromwell/issues/1479:279,Availability,failure,failures,279,"Looking at status message in the swagger GUI, I see that my workflow failed. However, the job it specifies is scattered and I do not know which shard is the issue.... If there is an easy way to find this, that I missed, then apologies. Example:. ```; ....; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479
https://github.com/broadinstitute/cromwell/issues/1479:18,Integrability,message,message,18,"Looking at status message in the swagger GUI, I see that my workflow failed. However, the job it specifies is scattered and I do not know which shard is the issue.... If there is an easy way to find this, that I missed, then apologies. Example:. ```; ....; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479
https://github.com/broadinstitute/cromwell/issues/1479:297,Integrability,message,message,297,"Looking at status message in the swagger GUI, I see that my workflow failed. However, the job it specifies is scattered and I do not know which shard is the issue.... If there is an easy way to find this, that I missed, then apologies. Example:. ```; ....; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479
https://github.com/broadinstitute/cromwell/issues/1480:14901,Availability,down,downstream,14901,"fasta} \; --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \; else touch ${entity_id}.coverage.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Calculate coverage on Whole Genome Sequence using Spark.; # This task automatically creates a target output file.; task WholeGenomeCoverage {; String entity_id; File coverage_file ; File target_file; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; String gatk_jar; Boolean isWGS; Int wgsBinSize; Int mem. # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --ta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:14999,Availability,down,downstream,14999,"fasta} \; --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \; else touch ${entity_id}.coverage.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Calculate coverage on Whole Genome Sequence using Spark.; # This task automatically creates a target output file.; task WholeGenomeCoverage {; String entity_id; File coverage_file ; File target_file; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; String gatk_jar; Boolean isWGS; Int wgsBinSize; Int mem. # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --ta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:15850,Availability,down,downstream,15850,"sks; # If not, coverage and target files (received from upstream) for WES are passed downstream; command {; if [ ${isWGS} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \; --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \; else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \; fi; }. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; File gatk_target_file = ""${entity_id}.coverage.tsv.targets.tsv""; }; }. # Add new columns to an existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else ln -s ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent norma",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:16479,Availability,down,downstream,16479,"existing target table with various targets; # Note that this task is optional ; task AnnotateTargets {; String entity_id; File target_file; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then an empty file gets passed downstream; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else ln -s ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normalization (noise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:37577,Availability,down,downstream,37577,"duplicatereads ${keep_duplicate_reads} --input ${input_bam} --reference ${ref_fasta} \\\n --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \\\n --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \\\n --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \\\n else touch ${entity_id}.coverage.tsv; \\\n fi\n }\n\n output {\n File gatk_coverage_file = \""${entity_id}.coverage.tsv\""\n }\n\n #runtime {\n # docker: \""gatk-protected/a1\""\n #}\n}\n\n# Calculate coverage on Whole Genome Sequence using Spark.\n# This task automatically creates a target output file.\ntask WholeGenomeCoverage {\n String entity_id\n File coverage_file \n File target_file\n File input_bam\n File bam_idx\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n String gatk_jar\n Boolean isWGS\n Int wgsBinSize\n Int mem\n\n # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks\n # If not, coverage and target files (received from upstream) for WES are passed downstream\n command {\n if [ ${isWGS} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \\\n --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \\\n else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \\\n fi\n }\n\n output {\n File gatk_coverage_file = \""${entity_id}.coverage.tsv\""\n File gatk_target_file = \""${entity_id}.coverage.tsv.targets.tsv\""\n }\n}\n\n# Add new columns to an existing target table with various targets\n# Note that this task is optional \ntask AnnotateTargets {\n String entity_id\n File target_file\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:37676,Availability,down,downstream,37676,"duplicatereads ${keep_duplicate_reads} --input ${input_bam} --reference ${ref_fasta} \\\n --disable_all_read_filters ${disable_all_read_filters} --interval_set_rule UNION --interval_padding 0 \\\n --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \\\n --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; \\\n else touch ${entity_id}.coverage.tsv; \\\n fi\n }\n\n output {\n File gatk_coverage_file = \""${entity_id}.coverage.tsv\""\n }\n\n #runtime {\n # docker: \""gatk-protected/a1\""\n #}\n}\n\n# Calculate coverage on Whole Genome Sequence using Spark.\n# This task automatically creates a target output file.\ntask WholeGenomeCoverage {\n String entity_id\n File coverage_file \n File target_file\n File input_bam\n File bam_idx\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n String gatk_jar\n Boolean isWGS\n Int wgsBinSize\n Int mem\n\n # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks\n # If not, coverage and target files (received from upstream) for WES are passed downstream\n command {\n if [ ${isWGS} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \\\n --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \\\n else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \\\n fi\n }\n\n output {\n File gatk_coverage_file = \""${entity_id}.coverage.tsv\""\n File gatk_target_file = \""${entity_id}.coverage.tsv.targets.tsv\""\n }\n}\n\n# Add new columns to an existing target table with various targets\n# Note that this task is optional \ntask AnnotateTargets {\n String entity_id\n File target_file\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:38561,Availability,down,downstream,38561," Int mem\n\n # If isWGS is set to true, the task produces WGS coverage and targets that are passed to downstream tasks\n # If not, coverage and target files (received from upstream) for WES are passed downstream\n command {\n if [ ${isWGS} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} SparkGenomeReadCounts --outputFile ${entity_id}.coverage.tsv \\\n --reference ${ref_fasta} --input ${input_bam} --sparkMaster local[1] --binsize ${wgsBinSize}; \\\n else ln -s ${coverage_file} ${entity_id}.coverage.tsv; ln -s ${target_file} ${entity_id}.coverage.tsv.targets.tsv; \\\n fi\n }\n\n output {\n File gatk_coverage_file = \""${entity_id}.coverage.tsv\""\n File gatk_target_file = \""${entity_id}.coverage.tsv.targets.tsv\""\n }\n}\n\n# Add new columns to an existing target table with various targets\n# Note that this task is optional \ntask AnnotateTargets {\n String entity_id\n File target_file\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then an empty file gets passed downstream\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \\\n else touch ${entity_id}.annotated.tsv; \\\n fi\n }\n\n output {\n File annotated_targets = \""${entity_id}.annotated.tsv\""\n }\n}\n\n# Correct coverage for sample-specific GC bias effects\n# Note that this task is optional \ntask CorrectGCBias {\n String entity_id\n File coverage_file\n File annotated_targets\n String gatk_jar\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then the coverage file gets passed downstream unchanged\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \\\n --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \\\n else ln -s $",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:39217,Availability,down,downstream,39217,"get_file = \""${entity_id}.coverage.tsv.targets.tsv\""\n }\n}\n\n# Add new columns to an existing target table with various targets\n# Note that this task is optional \ntask AnnotateTargets {\n String entity_id\n File target_file\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then an empty file gets passed downstream\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \\\n else touch ${entity_id}.annotated.tsv; \\\n fi\n }\n\n output {\n File annotated_targets = \""${entity_id}.annotated.tsv\""\n }\n}\n\n# Correct coverage for sample-specific GC bias effects\n# Note that this task is optional \ntask CorrectGCBias {\n String entity_id\n File coverage_file\n File annotated_targets\n String gatk_jar\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then the coverage file gets passed downstream unchanged\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \\\n --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \\\n else ln -s ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \\\n fi\n }\n\n output {\n File gatk_cnv_coverage_file_gcbias = \""${entity_id}.gc_corrected_coverage.tsv\""\n }\n}\n\n# Perform tangent normalization (noise reduction) on the proportional coverage file.\ntask NormalizeSomaticReadCounts {\n String entity_id\n File coverage_file\n File padded_target_file\n File pon\n String gatk_jar\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \\\n --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \\\n --betaHatsOutput ${",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:68651,Availability,failure,failures,68651,"butes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P4H.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P4H.bai"",; ""entity_id"": ""SM-74P4H"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""returnCode"": -1,; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""jobId"": ""28216"",; ""backend"": ""Local"",; ""end"": ""2016-09-23T13:53:28.554Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-14/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-14"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2016-09-23T13:53:25.040Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-23T13:53:25.122Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.122Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-23T13:53:25.164Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.164Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-23T13:53:25.291Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.291Z"",; ""description"": ""RunningJob"",; ""endT",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:82715,Availability,failure,failures,82715,"_cnloh_dir"": ""splits/"",; ""case_gatk_acnv_workflow.seg_param_eta"": 0.05,; ""case_gatk_acnv_workflow.seg_param_trim"": 0.025,; ""case_gatk_acnv_workflow.plots_dir"": ""plots/"",; ""case_gatk_acnv_workflow.seg_param_pmethod"": ""HYBRID"",; ""case_gatk_acnv_workflow.seg_param_nperm"": 10000,; ""case_gatk_acnv_workflow.PoN"": ""/data/ice_rcs_eval.v1.pd250.spark.pon"",; ""case_gatk_acnv_workflow.input_bam_list"": ""/home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_bam_list_local_paths.txt"",; ""case_gatk_acnv_workflow.ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""case_gatk_acnv_workflow.enable_gc_correction"": true,; ""case_gatk_acnv_workflow.wgsBinSize"": 10000,; ""case_gatk_acnv_workflow.seg_param_undoPrune"": 0.05,; ""case_gatk_acnv_workflow.gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:83447,Availability,down,down,83447,".gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true',",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84700,Availability,failure,failure,84700,"g API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85761,Availability,failure,failure-mode,85761,"ts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85988,Availability,failure,failure-mode,85988,"itted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:87710,Availability,avail,available,87710," of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies use",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84281,Deployability,configurat,configuration,84281,"elism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-worker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:86055,Deployability,configurat,configuration,86055,"kflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:410,Integrability,message,message,410,"Specific issue: `TumorWholeGenomeCoverage` does not appear to start after `TumorCalculateTargetCoverage` has completed successfully.; - local backend; - server mode; - call caching is enabled; - dev snapshot from develop taken Sept 23, 2016 ... `cromwell-0.20-e56c9e8-SNAPSHOT.jar`; - Running on google cloud VM -- 32 cores 104GB RAM; - WDL, json, metadata, conf, and options file are found at the end of this message.; - Some jobs are still running as I write this issue; - The workflow has been restarted two times; - clean MySQL database for first workflow run. Untouched since then.; - Timing diagram seems to be unavailable for latest workflow run; - URL: http://104.198.41.229:8080/swagger/index.html?url=/swagger/cromwell.yaml#!/Workflows/get_workflows_version_id_metadata; - In a previous snapshot (~2 weeks ago -- no call caching), running on JES, this worked fine. WDL has not changed.; - Attempted with three dev snapshots this week and all three had this issue. Other confounds made it difficult to isolate this issue, but those may be resolved.; - server running with sudo; - docker image already pulled locally. options file (default_runtimes):. ```; {; ""default_runtime_attributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""zones"": ""us-central1-a us-central1-b"",; ""disks"": ""local-disk 200 SSD"",; ""memory"": ""6G""; }; }; ```. json:. ``` json; {; ""case_gatk_acnv_workflow.input_bam_list"": ""/home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_bam_list_local_paths.txt"",; ""case_gatk_acnv_workflow.CalculateTargetCoverage.grouping"": ""SAMPLE"",; ""case_gatk_acnv_workflow.common_snp_list"": ""/data/allchr.1kg.phase3.v5a.snp.maf10.biallelic.recode.fixed.prune5.interval_list"",; ""case_gatk_acnv_workflow.ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv"",; ""case_gatk_acnv_workflow.gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:68669,Integrability,message,message,68669,"butes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P4H.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P4H.bai"",; ""entity_id"": ""SM-74P4H"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""returnCode"": -1,; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""jobId"": ""28216"",; ""backend"": ""Local"",; ""end"": ""2016-09-23T13:53:28.554Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-14/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-14"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2016-09-23T13:53:25.040Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-23T13:53:25.122Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.122Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-23T13:53:25.164Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.164Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-23T13:53:25.291Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.291Z"",; ""description"": ""RunningJob"",; ""endT",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:82733,Integrability,message,message,82733,"_cnloh_dir"": ""splits/"",; ""case_gatk_acnv_workflow.seg_param_eta"": 0.05,; ""case_gatk_acnv_workflow.seg_param_trim"": 0.025,; ""case_gatk_acnv_workflow.plots_dir"": ""plots/"",; ""case_gatk_acnv_workflow.seg_param_pmethod"": ""HYBRID"",; ""case_gatk_acnv_workflow.seg_param_nperm"": 10000,; ""case_gatk_acnv_workflow.PoN"": ""/data/ice_rcs_eval.v1.pd250.spark.pon"",; ""case_gatk_acnv_workflow.input_bam_list"": ""/home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_bam_list_local_paths.txt"",; ""case_gatk_acnv_workflow.ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""case_gatk_acnv_workflow.enable_gc_correction"": true,; ""case_gatk_acnv_workflow.wgsBinSize"": 10000,; ""case_gatk_acnv_workflow.seg_param_undoPrune"": 0.05,; ""case_gatk_acnv_workflow.gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:82968,Integrability,interface,interface,82968,"ase_gatk_acnv_workflow.seg_param_nperm"": 10000,; ""case_gatk_acnv_workflow.PoN"": ""/data/ice_rcs_eval.v1.pd250.spark.pon"",; ""case_gatk_acnv_workflow.input_bam_list"": ""/home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_bam_list_local_paths.txt"",; ""case_gatk_acnv_workflow.ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""case_gatk_acnv_workflow.enable_gc_correction"": true,; ""case_gatk_acnv_workflow.wgsBinSize"": 10000,; ""case_gatk_acnv_workflow.seg_param_undoPrune"": 0.05,; ""case_gatk_acnv_workflow.gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:4169,Modifiability,variab,variable,4169,"; ""case_gatk_acnv_workflow.seg_param_nmin"": ""200"",; ""case_gatk_acnv_workflow.seg_param_eta"": ""0.05"",; ""case_gatk_acnv_workflow.seg_param_trim"": ""0.025"",; ""case_gatk_acnv_workflow.seg_param_undoSplits"": ""NONE"",; ""case_gatk_acnv_workflow.seg_param_undoPrune"": ""0.05"",; ""case_gatk_acnv_workflow.seg_param_undoSD"": ""3""; }; ```. WDL:. ``` wdl; #; # Case sample workflow for a list of pairs of case-control samples. Includes GATK CNV and ACNV. Supports both WGS and WES samples. This was tested on a3c7368 commit of gatk-protected.; #; # Notes:; #; # - the input file(input_bam_list) must contain a list of tab separated values in the following format(one or more lines must be supplied):; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--first input; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--second input; # etc...; #; # - set isWGS variable to true or false to specify whether to run a WGS or WES workflow respectively; #; # - file names will use the entity ID specified, but inside the file, the bam SM tag will typically be used.; #; # - target file (which must be in tsv format) is only used with WES workflow, WGS workflow generates its own targets (so user can pass any string as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save; # your modified version with a different filename and do not commit to gatk-protected repo).; #; # - Some call inputs might seem out of place - consult with the comments in task definitions for details; #; #############. workflow case_gatk_acnv_workflow {; # Workflow input files; File target_file; File ref_fasta; File ref_fasta_dict; File ref_fasta_fai; File common_snp_list; File i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:26413,Modifiability,variab,variable,26413,"low.seg_param_nmin\"": \""200\"",\n \""case_gatk_acnv_workflow.seg_param_eta\"": \""0.05\"",\n \""case_gatk_acnv_workflow.seg_param_trim\"": \""0.025\"",\n \""case_gatk_acnv_workflow.seg_param_undoSplits\"": \""NONE\"",\n \""case_gatk_acnv_workflow.seg_param_undoPrune\"": \""0.05\"",\n \""case_gatk_acnv_workflow.seg_param_undoSD\"": \""3\""\n}"",; ""workflow"": ""#\n# Case sample workflow for a list of pairs of case-control samples. Includes GATK CNV and ACNV. Supports both WGS and WES samples. This was tested on a3c7368 commit of gatk-protected.\n#\n# Notes:\n#\n# - the input file(input_bam_list) must contain a list of tab separated values in the following format(one or more lines must be supplied):\n# tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--first input\n# tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--second input\n# etc...\n#\n# - set isWGS variable to true or false to specify whether to run a WGS or WES workflow respectively\n#\n# - file names will use the entity ID specified, but inside the file, the bam SM tag will typically be used.\n#\n# - target file (which must be in tsv format) is only used with WES workflow, WGS workflow generates its own targets (so user can pass any string as an argument)\n#\n# - the WGS PoN must be generated with WGS samples\n# \n# - THIS SCRIPT SHOULD BE CONSIDERED OF \""BETA\"" QUALITY\n#\n# - Example invocation:\n# java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json\n# - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save\n# your modified version with a different filename and do not commit to gatk-protected repo).\n#\n# - Some call inputs might seem out of place - consult with the comments in task definitions for details\n#\n#############\n\nworkflow case_gatk_acnv_workflow {\n # Workflow input files\n File target_file\n File ref_fasta\n File ref_fasta_dict\n File ref_fasta_fai\n File common_snp_li",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84281,Modifiability,config,configuration,84281,"elism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-worker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:86055,Modifiability,config,configuration,86055,"kflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:87342,Modifiability,variab,variables,87342,"ry DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:87891,Modifiability,config,config,87891,"ult""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:87898,Modifiability,Config,ConfigBackendLifecycleActorFactory,87898," = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; }; }; }; }; }; }. services {; KeyValue {; class = ""c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:87935,Modifiability,config,config,87935," = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; }; }; }; }; }; }. services {; KeyValue {; class = ""c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:89234,Modifiability,config,config,89234,"available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; }; }; }; }; }; }. services {; KeyValue {; class = ""cromwell.services.keyvalue.impl.SqlKeyValueServiceActor""; }; MetadataService {; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; }. database {; // This specifies which database to use; //config = main.hsqldb; config = main.mysql_localhost. main {; hsqldb {; driver = ""slick.driver.HsqldbDriver$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${slick.uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. mysql_localhost {; driver = ""slick.driver.MySQLDriver$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell""; user = ""root""; password = ""cromwell""; connectionTimeout = 5000; }; }; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:89256,Modifiability,config,config,89256,"available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; }; }; }; }; }; }. services {; KeyValue {; class = ""cromwell.services.keyvalue.impl.SqlKeyValueServiceActor""; }; MetadataService {; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; }. database {; // This specifies which database to use; //config = main.hsqldb; config = main.mysql_localhost. main {; hsqldb {; driver = ""slick.driver.HsqldbDriver$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${slick.uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. mysql_localhost {; driver = ""slick.driver.MySQLDriver$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell""; user = ""root""; password = ""cromwell""; connectionTimeout = 5000; }; }; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:5391,Performance,Perform,PerformSegmentation,5391,"g as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save; # your modified version with a different filename and do not commit to gatk-protected repo).; #; # - Some call inputs might seem out of place - consult with the comments in task definitions for details; #; #############. workflow case_gatk_acnv_workflow {; # Workflow input files; File target_file; File ref_fasta; File ref_fasta_dict; File ref_fasta_fai; File common_snp_list; File input_bam_list; Array[Array[String]] bam_list_array = read_tsv(input_bam_list); File PoN; String gatk_jar. # Input parameters of the PerformSegmentation tool; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD. # Workflow output directories and options; String plots_dir; String call_cnloh_dir; Boolean disable_sequence_dictionary_validation; Boolean enable_gc_correction; Boolean isWGS; Int wgsBinSize. call PadTargets {; input:; target_file=target_file,; gatk_jar=gatk_jar,; isWGS=isWGS,; mem=1; }. scatter (row in bam_list_array) {. call CalculateTargetCoverage as TumorCalculateTargetCoverage {; input:; entity_id=row[0],; padded_target_file=PadTargets.padded_target_file,; input_bam=row[1],; bam_idx=row[2],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; isWGS=isWGS,; mem=2; } . call WholeGenomeCoverage as TumorWholeGenomeCoverage {; input:; entity_id=row[0],; target_file=PadTargets.padded_target_file,; input_b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:7588,Performance,Perform,PerformSegmentation,7588,"f_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; isWGS=isWGS,; wgsBinSize=wgsBinSize,; mem=4; }. call AnnotateTargets as TumorAnnotateTargets {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; target_file=TumorWholeGenomeCoverage.gatk_target_file,; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; enable_gc_correction=enable_gc_correction,; mem=4; }. call CorrectGCBias as TumorCorrectGCBias {; input:; entity_id=row[0], ; gatk_jar=gatk_jar,; coverage_file=TumorWholeGenomeCoverage.gatk_coverage_file,; annotated_targets=TumorAnnotateTargets.annotated_targets,; enable_gc_correction=enable_gc_correction,; mem=4; }. call NormalizeSomaticReadCounts as TumorNormalizeSomaticReadCounts {; input:; entity_id=row[0], ; coverage_file=TumorCorrectGCBias.gatk_cnv_coverage_file_gcbias,; padded_target_file=TumorWholeGenomeCoverage.gatk_target_file,; pon=PoN,; gatk_jar=gatk_jar,; mem=2; }. call PerformSegmentation as TumorPerformSeg {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; tn_file=TumorNormalizeSomaticReadCounts.tn_file,; seg_param_alpha=seg_param_alpha,; seg_param_nperm=seg_param_nperm,; seg_param_pmethod=seg_param_pmethod,; seg_param_minWidth=seg_param_minWidth,; seg_param_kmax=seg_param_kmax,; seg_param_nmin=seg_param_nmin,; seg_param_eta=seg_param_eta,; seg_param_trim=seg_param_trim,; seg_param_undoSplits=seg_param_undoSplits,; seg_param_undoPrune=seg_param_undoPrune,; seg_param_undoSD=seg_param_undoSD,; mem=2; }. call Caller as TumorCaller {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; tn_file=TumorNormalizeSomaticReadCounts.tn_file,; seg_file=TumorPerformSeg.seg_file,; mem=2; }. call HetPulldown {; input:; entity_id_tumor=row[0],; entity_id_normal=row[3],; gatk_jar=gatk_jar,; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; tumor_bam=row[1],; tumor_bam_idx=row[2],; normal_bam=row[4],; normal_bam_idx=row[5],; common_snp_list=common_snp_list,; mem=4. }. call AllelicCNV {; input:; entity",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:11192,Performance,Perform,PerformSegmentation,11192,"a_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; isWGS=isWGS,; wgsBinSize=wgsBinSize,; mem=4; }. call AnnotateTargets as NormalAnnotateTargets{; input:; entity_id=row[3],; gatk_jar=gatk_jar,; target_file=NormalWholeGenomeCoverage.gatk_target_file,; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; enable_gc_correction=enable_gc_correction,; mem=4; }. call CorrectGCBias as NormalCorrectGCBias {; input:; entity_id=row[3],; gatk_jar=gatk_jar,; coverage_file=NormalWholeGenomeCoverage.gatk_coverage_file,; annotated_targets=NormalAnnotateTargets.annotated_targets,; enable_gc_correction=enable_gc_correction,; mem=4; }. call NormalizeSomaticReadCounts as NormalNormalizeSomaticReadCounts {; input:; entity_id=row[3],; coverage_file=NormalCorrectGCBias.gatk_cnv_coverage_file_gcbias,; padded_target_file=NormalWholeGenomeCoverage.gatk_target_file,; pon=PoN,; gatk_jar=gatk_jar,; mem=2; }. call PerformSegmentation as NormalPerformSeg {; input:; entity_id=row[3],; gatk_jar=gatk_jar,; tn_file=NormalNormalizeSomaticReadCounts.tn_file,; seg_param_alpha=seg_param_alpha,; seg_param_nperm=seg_param_nperm,; seg_param_pmethod=seg_param_pmethod,; seg_param_minWidth=seg_param_minWidth,; seg_param_kmax=seg_param_kmax,; seg_param_nmin=seg_param_nmin,; seg_param_eta=seg_param_eta,; seg_param_trim=seg_param_trim,; seg_param_undoSplits=seg_param_undoSplits,; seg_param_undoPrune=seg_param_undoPrune,; seg_param_undoSD=seg_param_undoSD,; mem=2; }. call Caller as NormalCaller {; input:; entity_id=row[3],; gatk_jar=gatk_jar,; tn_file=NormalNormalizeSomaticReadCounts.tn_file,; seg_file=NormalPerformSeg.seg_file,; mem=2; }; }; }. # Pad the target file. This was found to help sensitivity and specificity. This step should only be altered; # by advanced users. Note that by changing this, you need to have a PoN that also reflects the change.; task PadTargets {; File target_file; Int padding = 250; String gatk_jar; Boolean isWGS; Int mem. # Note that when isWGS is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:16893,Performance,Perform,Perform,16893,"em}g -jar ${gatk_jar} AnnotateTargets --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \; else touch ${entity_id}.annotated.tsv; \; fi; }. output {; File annotated_targets = ""${entity_id}.annotated.tsv""; }; }. # Correct coverage for sample-specific GC bias effects; # Note that this task is optional ; task CorrectGCBias {; String entity_id; File coverage_file; File annotated_targets; String gatk_jar; Boolean enable_gc_correction; Int mem. # If GC correction is disabled, then the coverage file gets passed downstream unchanged; command {; if [ ${enable_gc_correction} = true ]; \; then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \; --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \; else ln -s ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \; fi; }. output {; File gatk_cnv_coverage_file_gcbias = ""${entity_id}.gc_corrected_coverage.tsv""; }; }. # Perform tangent normalization (noise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:17762,Performance,Perform,PerformSegmentation,17762,"ise reduction) on the proportional coverage file.; task NormalizeSomaticReadCounts {; String entity_id; File coverage_file; File padded_target_file; File pon; String gatk_jar; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:18143,Performance,Perform,PerformSegmentation,18143,"r} NormalizeSomaticReadCounts --input ${coverage_file} \; --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \; --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:27652,Performance,Perform,PerformSegmentation,27652,"d with WGS samples\n# \n# - THIS SCRIPT SHOULD BE CONSIDERED OF \""BETA\"" QUALITY\n#\n# - Example invocation:\n# java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json\n# - See case_gatk_acnv_workflow_template.json for a template json file to modify with your own parameters (please save\n# your modified version with a different filename and do not commit to gatk-protected repo).\n#\n# - Some call inputs might seem out of place - consult with the comments in task definitions for details\n#\n#############\n\nworkflow case_gatk_acnv_workflow {\n # Workflow input files\n File target_file\n File ref_fasta\n File ref_fasta_dict\n File ref_fasta_fai\n File common_snp_list\n File input_bam_list\n Array[Array[String]] bam_list_array = read_tsv(input_bam_list)\n File PoN\n String gatk_jar\n\n # Input parameters of the PerformSegmentation tool\n Float seg_param_alpha\n Int seg_param_nperm\n String seg_param_pmethod\n Int seg_param_minWidth\n Int seg_param_kmax\n Int seg_param_nmin\n Float seg_param_eta\n Float seg_param_trim\n String seg_param_undoSplits\n Float seg_param_undoPrune\n Int seg_param_undoSD\n\n # Workflow output directories and options\n String plots_dir\n String call_cnloh_dir\n Boolean disable_sequence_dictionary_validation\n Boolean enable_gc_correction\n Boolean isWGS\n Int wgsBinSize\n\n call PadTargets {\n input:\n target_file=target_file,\n gatk_jar=gatk_jar,\n isWGS=isWGS,\n mem=1\n }\n\n scatter (row in bam_list_array) {\n\n call CalculateTargetCoverage as TumorCalculateTargetCoverage {\n input:\n entity_id=row[0],\n padded_target_file=PadTargets.padded_target_file,\n input_bam=row[1],\n bam_idx=row[2],\n ref_fasta=ref_fasta,\n ref_fasta_fai=ref_fasta_fai,\n ref_fasta_dict=ref_fasta_dict,\n gatk_jar=gatk_jar,\n disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,\n isWGS=isWGS,\n mem=2\n } \n\n call WholeGenomeCoverage as TumorWholeGenomeCoverage {\n input:\n entity_id=row[0],\n target_file=PadTargets.padded_tar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:29952,Performance,Perform,PerformSegmentation,29952," ref_fasta_dict=ref_fasta_dict,\n gatk_jar=gatk_jar,\n isWGS=isWGS,\n wgsBinSize=wgsBinSize,\n mem=4\n }\n\n call AnnotateTargets as TumorAnnotateTargets {\n input:\n entity_id=row[0],\n gatk_jar=gatk_jar,\n target_file=TumorWholeGenomeCoverage.gatk_target_file,\n ref_fasta=ref_fasta,\n ref_fasta_fai=ref_fasta_fai,\n ref_fasta_dict=ref_fasta_dict,\n enable_gc_correction=enable_gc_correction,\n mem=4\n }\n\n call CorrectGCBias as TumorCorrectGCBias {\n input:\n entity_id=row[0], \n gatk_jar=gatk_jar,\n coverage_file=TumorWholeGenomeCoverage.gatk_coverage_file,\n annotated_targets=TumorAnnotateTargets.annotated_targets,\n enable_gc_correction=enable_gc_correction,\n mem=4\n }\n\n call NormalizeSomaticReadCounts as TumorNormalizeSomaticReadCounts {\n input:\n entity_id=row[0], \n coverage_file=TumorCorrectGCBias.gatk_cnv_coverage_file_gcbias,\n padded_target_file=TumorWholeGenomeCoverage.gatk_target_file,\n pon=PoN,\n gatk_jar=gatk_jar,\n mem=2\n }\n\n call PerformSegmentation as TumorPerformSeg {\n input:\n entity_id=row[0],\n gatk_jar=gatk_jar,\n tn_file=TumorNormalizeSomaticReadCounts.tn_file,\n seg_param_alpha=seg_param_alpha,\n seg_param_nperm=seg_param_nperm,\n seg_param_pmethod=seg_param_pmethod,\n seg_param_minWidth=seg_param_minWidth,\n seg_param_kmax=seg_param_kmax,\n seg_param_nmin=seg_param_nmin,\n seg_param_eta=seg_param_eta,\n seg_param_trim=seg_param_trim,\n seg_param_undoSplits=seg_param_undoSplits,\n seg_param_undoPrune=seg_param_undoPrune,\n seg_param_undoSD=seg_param_undoSD,\n mem=2\n }\n\n call Caller as TumorCaller {\n input:\n entity_id=row[0],\n gatk_jar=gatk_jar,\n tn_file=TumorNormalizeSomaticReadCounts.tn_file,\n seg_file=TumorPerformSeg.seg_file,\n mem=2\n }\n\n call HetPulldown {\n input:\n entity_id_tumor=row[0],\n entity_id_normal=row[3],\n gatk_jar=gatk_jar,\n ref_fasta=ref_fasta,\n ref_fasta_fai=ref_fasta_fai,\n ref_fasta_dict=ref_fasta_dict,\n tumor_bam=row[1],\n tumor_bam_idx=row[2],\n normal_bam=row[4],\n normal_bam_idx=row[5],\n comm",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:33725,Performance,Perform,PerformSegmentation,33725,"asta_dict=ref_fasta_dict,\n gatk_jar=gatk_jar,\n isWGS=isWGS,\n wgsBinSize=wgsBinSize,\n mem=4\n }\n\n call AnnotateTargets as NormalAnnotateTargets{\n input:\n entity_id=row[3],\n gatk_jar=gatk_jar,\n target_file=NormalWholeGenomeCoverage.gatk_target_file,\n ref_fasta=ref_fasta,\n ref_fasta_fai=ref_fasta_fai,\n ref_fasta_dict=ref_fasta_dict,\n enable_gc_correction=enable_gc_correction,\n mem=4\n }\n\n call CorrectGCBias as NormalCorrectGCBias {\n input:\n entity_id=row[3],\n gatk_jar=gatk_jar,\n coverage_file=NormalWholeGenomeCoverage.gatk_coverage_file,\n annotated_targets=NormalAnnotateTargets.annotated_targets,\n enable_gc_correction=enable_gc_correction,\n mem=4\n }\n\n call NormalizeSomaticReadCounts as NormalNormalizeSomaticReadCounts {\n input:\n entity_id=row[3],\n coverage_file=NormalCorrectGCBias.gatk_cnv_coverage_file_gcbias,\n padded_target_file=NormalWholeGenomeCoverage.gatk_target_file,\n pon=PoN,\n gatk_jar=gatk_jar,\n mem=2\n }\n\n call PerformSegmentation as NormalPerformSeg {\n input:\n entity_id=row[3],\n gatk_jar=gatk_jar,\n tn_file=NormalNormalizeSomaticReadCounts.tn_file,\n seg_param_alpha=seg_param_alpha,\n seg_param_nperm=seg_param_nperm,\n seg_param_pmethod=seg_param_pmethod,\n seg_param_minWidth=seg_param_minWidth,\n seg_param_kmax=seg_param_kmax,\n seg_param_nmin=seg_param_nmin,\n seg_param_eta=seg_param_eta,\n seg_param_trim=seg_param_trim,\n seg_param_undoSplits=seg_param_undoSplits,\n seg_param_undoPrune=seg_param_undoPrune,\n seg_param_undoSD=seg_param_undoSD,\n mem=2\n }\n\n call Caller as NormalCaller {\n input:\n entity_id=row[3],\n gatk_jar=gatk_jar,\n tn_file=NormalNormalizeSomaticReadCounts.tn_file,\n seg_file=NormalPerformSeg.seg_file,\n mem=2\n }\n }\n}\n\n# Pad the target file. This was found to help sensitivity and specificity. This step should only be altered\n# by advanced users. Note that by changing this, you need to have a PoN that also reflects the change.\ntask PadTargets {\n File target_file\n Int padding = 250\n Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:39651,Performance,Perform,Perform,39651,"s --targets ${target_file} --reference ${ref_fasta} --output ${entity_id}.annotated.tsv; \\\n else touch ${entity_id}.annotated.tsv; \\\n fi\n }\n\n output {\n File annotated_targets = \""${entity_id}.annotated.tsv\""\n }\n}\n\n# Correct coverage for sample-specific GC bias effects\n# Note that this task is optional \ntask CorrectGCBias {\n String entity_id\n File coverage_file\n File annotated_targets\n String gatk_jar\n Boolean enable_gc_correction\n Int mem\n\n # If GC correction is disabled, then the coverage file gets passed downstream unchanged\n command {\n if [ ${enable_gc_correction} = true ]; \\\n then java -Xmx${mem}g -jar ${gatk_jar} CorrectGCBias --input ${coverage_file} \\\n --output ${entity_id}.gc_corrected_coverage.tsv --targets ${annotated_targets}; \\\n else ln -s ${coverage_file} ${entity_id}.gc_corrected_coverage.tsv; \\\n fi\n }\n\n output {\n File gatk_cnv_coverage_file_gcbias = \""${entity_id}.gc_corrected_coverage.tsv\""\n }\n}\n\n# Perform tangent normalization (noise reduction) on the proportional coverage file.\ntask NormalizeSomaticReadCounts {\n String entity_id\n File coverage_file\n File padded_target_file\n File pon\n String gatk_jar\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \\\n --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \\\n --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false\n }\n\n output {\n File tn_file = \""${entity_id}.tn.tsv\""\n File pre_tn_file = \""${entity_id}.preTN.tsv\""\n File betahats_file = \""${entity_id}.betaHats.tsv\""\n }\n #runtime {\n # docker: \""gatk-protected/a1\""\n #}\n}\n\n# Segment the tangent normalized coverage profile.\ntask PerformSegmentation {\n String entity_id\n Float seg_param_alpha\n Int seg_param_nperm\n String seg_param_pmethod\n Int seg_para",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:40555,Performance,Perform,PerformSegmentation,40555,"id\n File coverage_file\n File padded_target_file\n File pon\n String gatk_jar\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \\\n --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \\\n --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false\n }\n\n output {\n File tn_file = \""${entity_id}.tn.tsv\""\n File pre_tn_file = \""${entity_id}.preTN.tsv\""\n File betahats_file = \""${entity_id}.betaHats.tsv\""\n }\n #runtime {\n # docker: \""gatk-protected/a1\""\n #}\n}\n\n# Segment the tangent normalized coverage profile.\ntask PerformSegmentation {\n String entity_id\n Float seg_param_alpha\n Int seg_param_nperm\n String seg_param_pmethod\n Int seg_param_minWidth\n Int seg_param_kmax\n Int seg_param_nmin\n Float seg_param_eta\n Float seg_param_trim\n String seg_param_undoSplits\n Float seg_param_undoPrune\n Int seg_param_undoSD\n String gatk_jar\n File tn_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \\\n --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \\\n --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \\\n --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \\\n --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \\\n --verbosity INFO --QUIET false\n }\n\n output {\n File seg_file = \""${entity_id}.seg\""\n }\n}\n\n# Make calls (amp, neutral, or deleted) on each segment.\ntask Caller {\n String entity_id\n String gatk_jar\n File tn_file\n File seg_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \\\n --segments ${seg_file} --output ${entity",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:40955,Performance,Perform,PerformSegmentation,40955,"id\n File coverage_file\n File padded_target_file\n File pon\n String gatk_jar\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} NormalizeSomaticReadCounts --input ${coverage_file} \\\n --targets ${padded_target_file} --panelOfNormals ${pon} --factorNormalizedOutput ${entity_id}.fnt.tsv --tangentNormalized ${entity_id}.tn.tsv \\\n --betaHatsOutput ${entity_id}.betaHats.tsv --preTangentNormalized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false\n }\n\n output {\n File tn_file = \""${entity_id}.tn.tsv\""\n File pre_tn_file = \""${entity_id}.preTN.tsv\""\n File betahats_file = \""${entity_id}.betaHats.tsv\""\n }\n #runtime {\n # docker: \""gatk-protected/a1\""\n #}\n}\n\n# Segment the tangent normalized coverage profile.\ntask PerformSegmentation {\n String entity_id\n Float seg_param_alpha\n Int seg_param_nperm\n String seg_param_pmethod\n Int seg_param_minWidth\n Int seg_param_kmax\n Int seg_param_nmin\n Float seg_param_eta\n Float seg_param_trim\n String seg_param_undoSplits\n Float seg_param_undoPrune\n Int seg_param_undoSD\n String gatk_jar\n File tn_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \\\n --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \\\n --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \\\n --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \\\n --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \\\n --verbosity INFO --QUIET false\n }\n\n output {\n File seg_file = \""${entity_id}.seg\""\n }\n}\n\n# Make calls (amp, neutral, or deleted) on each segment.\ntask Caller {\n String entity_id\n String gatk_jar\n File tn_file\n File seg_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \\\n --segments ${seg_file} --output ${entity",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:46751,Performance,Cache,Cache,46751,"tput_dir}/${entity_id}-sim-final.acs.seg\"" \n File cnloh_final_cnb_called_segs=\""${output_dir}/${entity_id}-sim-final.cnb_called.seg\""\n File cnloh_final_cnv_segs=\""${output_dir}/${entity_id}-sim-final.cnv.seg\""\n File cnloh_final_titan_hets=\""${output_dir}/${entity_id}-sim-final.titan.het.tsv\""\n File cnloh_final_titan_tn=\""${output_dir}/${entity_id}-sim-final.titan.tn.tsv\""\n }\n}\n"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""docker\"": \""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e\"",\n \""zones\"": \""us-central1-a us-central1-b\"",\n\t \""disks\"": \""local-disk 200 SSD\"",\n\t \""memory\"": \""6G\""\n\t }\n}""; },; ""calls"": {; ""case_gatk_acnv_workflow.TumorCalculateTargetCoverage"": [; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-0/execution/stdout"",; ""shardIndex"": 0,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P2T.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P2T.bam.bai"",; ""entity_id"": ""SM-74P2T"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28218"",; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:47160,Performance,cache,cache,47160,"tput_dir}/${entity_id}-sim-final.acs.seg\"" \n File cnloh_final_cnb_called_segs=\""${output_dir}/${entity_id}-sim-final.cnb_called.seg\""\n File cnloh_final_cnv_segs=\""${output_dir}/${entity_id}-sim-final.cnv.seg\""\n File cnloh_final_titan_hets=\""${output_dir}/${entity_id}-sim-final.titan.het.tsv\""\n File cnloh_final_titan_tn=\""${output_dir}/${entity_id}-sim-final.titan.tn.tsv\""\n }\n}\n"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""docker\"": \""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e\"",\n \""zones\"": \""us-central1-a us-central1-b\"",\n\t \""disks\"": \""local-disk 200 SSD\"",\n\t \""memory\"": \""6G\""\n\t }\n}""; },; ""calls"": {; ""case_gatk_acnv_workflow.TumorCalculateTargetCoverage"": [; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-0/execution/stdout"",; ""shardIndex"": 0,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P2T.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P2T.bam.bai"",; ""entity_id"": ""SM-74P2T"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28218"",; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:48445,Performance,Cache,Cache,48445,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28218"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-0/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-0"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.068Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-1/execution/stdout"",; ""shardIndex"": 1,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P35.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P35.bam.bai"",; ""entity_id"": ""SM-74P35"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28224"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:48854,Performance,cache,cache,48854,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28218"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-0/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-0"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.068Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-1/execution/stdout"",; ""shardIndex"": 1,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P35.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P35.bam.bai"",; ""entity_id"": ""SM-74P35"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28224"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:50139,Performance,Cache,Cache,50139,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28224"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-1/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-1"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.033Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-2/execution/stdout"",; ""shardIndex"": 2,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P3J.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P3J.bam.bai"",; ""entity_id"": ""SM-74P3J"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28182"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:50548,Performance,cache,cache,50548,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28224"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-1/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-1"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.033Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-2/execution/stdout"",; ""shardIndex"": 2,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P3J.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P3J.bam.bai"",; ""entity_id"": ""SM-74P3J"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28182"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:51984,Performance,Cache,Cache,51984,"-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28182"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-2/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-2"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.038Z""; },; {; ""shardIndex"": 3,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.040Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-4/execution/stdout"",; ""shardIndex"": 4,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P3M.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P3M.bam.bai"",; ""entity_id"": ""SM-74P3M"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28178"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:52393,Performance,cache,cache,52393,"-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28182"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-2/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-2"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.038Z""; },; {; ""shardIndex"": 3,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.040Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-4/execution/stdout"",; ""shardIndex"": 4,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P3M.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P3M.bam.bai"",; ""entity_id"": ""SM-74P3M"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28178"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:53678,Performance,Cache,Cache,53678,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28178"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-4/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-4"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.037Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-5/execution/stdout"",; ""shardIndex"": 5,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P4M.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P4M.bam.bai"",; ""entity_id"": ""SM-74P4M"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28213"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:54087,Performance,cache,cache,54087,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28178"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-4/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-4"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.037Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-5/execution/stdout"",; ""shardIndex"": 5,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P4M.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P4M.bam.bai"",; ""entity_id"": ""SM-74P4M"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28213"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:55372,Performance,Cache,Cache,55372,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28213"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-5/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-5"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.033Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-6/execution/stdout"",; ""shardIndex"": 6,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P51.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P51.bam.bai"",; ""entity_id"": ""SM-74P51"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28181"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:55781,Performance,cache,cache,55781,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28213"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-5/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-5"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.033Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-6/execution/stdout"",; ""shardIndex"": 6,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P51.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P51.bam.bai"",; ""entity_id"": ""SM-74P51"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28181"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:57066,Performance,Cache,Cache,57066,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28181"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-6/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-6"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.041Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-7/execution/stdout"",; ""shardIndex"": 7,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P56.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P56.bam.bai"",; ""entity_id"": ""SM-74P56"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28185"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:57475,Performance,cache,cache,57475,"sform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28181"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-6/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-6"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.041Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-7/execution/stdout"",; ""shardIndex"": 7,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P56.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P56.bam.bai"",; ""entity_id"": ""SM-74P56"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28185"",; ""backend"": ""L",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:58911,Performance,Cache,Cache,58911,"-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28185"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-7/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-7"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.038Z""; },; {; ""shardIndex"": 8,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.069Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-9/execution/stdout"",; ""shardIndex"": 9,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P1Z.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P1Z.bai"",; ""entity_id"": ""SM-74P1Z"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28221"",; ""backend"": ""Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:59320,Performance,cache,cache,59320,"-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28185"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-7/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-7"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.038Z""; },; {; ""shardIndex"": 8,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.069Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-9/execution/stdout"",; ""shardIndex"": 9,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74P1Z.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74P1Z.bai"",; ""entity_id"": ""SM-74P1Z"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28221"",; ""backend"": ""Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:60601,Performance,Cache,Cache,60601,"form"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28221"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-9/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-9"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.040Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-10/execution/stdout"",; ""shardIndex"": 10,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74NF5.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74NF5.bai"",; ""entity_id"": ""SM-74NF5"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28215"",; ""backend"": ""Loca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:61012,Performance,cache,cache,61012,"form"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28221"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-9/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-9"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.040Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-10/execution/stdout"",; ""shardIndex"": 10,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74NF5.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74NF5.bai"",; ""entity_id"": ""SM-74NF5"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28215"",; ""backend"": ""Loca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:62295,Performance,Cache,Cache,62295,"rm"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28215"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-10/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-10"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.036Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-11/execution/stdout"",; ""shardIndex"": 11,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74ND9.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74ND9.bai"",; ""entity_id"": ""SM-74ND9"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28184"",; ""backend"": ""Loca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:62706,Performance,cache,cache,62706,"rm"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28215"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-10/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-10"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.036Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-11/execution/stdout"",; ""shardIndex"": 11,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/public/SM-74ND9.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/public/SM-74ND9.bai"",; ""entity_id"": ""SM-74ND9"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28184"",; ""backend"": ""Loca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:63989,Performance,Cache,Cache,63989,"m"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28184"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-11/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-11"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.034Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-12/execution/stdout"",; ""shardIndex"": 12,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P47.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P47.bai"",; ""entity_id"": ""SM-74P47"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28177"",; ""backend"": ""Lo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:64400,Performance,cache,cache,64400,"m"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28184"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-11/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-11"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.034Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-12/execution/stdout"",; ""shardIndex"": 12,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P47.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P47.bai"",; ""entity_id"": ""SM-74P47"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28177"",; ""backend"": ""Lo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:65685,Performance,Cache,Cache,65685,"m"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28177"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-12/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-12"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.037Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-13/execution/stdout"",; ""shardIndex"": 13,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P5I.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P5I.bai"",; ""entity_id"": ""SM-74P5I"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28189"",; ""backend"": ""Lo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:66096,Performance,cache,cache,66096,"m"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28177"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-12/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-12"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.037Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-13/execution/stdout"",; ""shardIndex"": 13,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P5I.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P5I.bai"",; ""entity_id"": ""SM-74P5I"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28189"",; ""backend"": ""Lo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:67381,Performance,Cache,Cache,67381,"""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28189"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-13/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-13"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.035Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-14/execution/stdout"",; ""shardIndex"": 14,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P4H.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P4H.bai"",; ""entity_id"": ""SM-74P4H"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""returnCode"": -1,; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:67819,Performance,cache,cache,67819,"""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28189"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-13/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-13"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.035Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-14/execution/stdout"",; ""shardIndex"": 14,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P4H.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P4H.bai"",; ""entity_id"": ""SM-74P4H"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""returnCode"": -1,; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:69903,Performance,Cache,Cache,69903,"teTargetCoverage/shard-14"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2016-09-23T13:53:25.040Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-23T13:53:25.122Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.122Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-23T13:53:25.164Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.164Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-23T13:53:25.291Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.291Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2016-09-23T13:53:28.539Z""; },; {; ""startTime"": ""2016-09-23T13:53:28.539Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2016-09-23T13:53:28.554Z""; }; ],; ""start"": ""2016-09-23T13:53:25.040Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-15/execution/stdout"",; ""shardIndex"": 15,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P3P.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P3P.bai"",; ""entity_id"": ""SM-74P3P"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28214"",; ""backend"": ""Lo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:70314,Performance,cache,cache,70314,"teTargetCoverage/shard-14"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2016-09-23T13:53:25.040Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-23T13:53:25.122Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.122Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-23T13:53:25.164Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.164Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-23T13:53:25.291Z""; },; {; ""startTime"": ""2016-09-23T13:53:25.291Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2016-09-23T13:53:28.539Z""; },; {; ""startTime"": ""2016-09-23T13:53:28.539Z"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2016-09-23T13:53:28.554Z""; }; ],; ""start"": ""2016-09-23T13:53:25.040Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-15/execution/stdout"",; ""shardIndex"": 15,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P3P.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P3P.bai"",; ""entity_id"": ""SM-74P3P"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28214"",; ""backend"": ""Lo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:71599,Performance,Cache,Cache,71599,"m"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28214"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-15/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-15"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.068Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-16/execution/stdout"",; ""shardIndex"": 16,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P44.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P44.bai"",; ""entity_id"": ""SM-74P44"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28226"",; ""backend"": ""Lo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:72010,Performance,cache,cache,72010,"m"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28214"",; ""backend"": ""Local"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-15/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-15"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:25.068Z""; },; {; ""Call caching read result"": ""Cache Miss"",; ""executionStatus"": ""Running"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-TumorCalculateTargetCoverage/shard-16/execution/stdout"",; ""shardIndex"": 16,; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""input_bam"": ""/data/private/SM-74P44.bam"",; ""ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""keep_duplicate_reads"": true,; ""grouping"": ""SAMPLE"",; ""disable_all_read_filters"": false,; ""ref_fasta_fai"": ""/data/ref/Homo_sapiens_assembly19.fasta.fai"",; ""bam_idx"": ""/data/private/SM-74P44.bai"",; ""entity_id"": ""SM-74P44"",; ""disable_sequence_dictionary_validation"": true,; ""mem"": 2,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""transform"": ""PCOV"",; ""isWGS"": false,; ""ref_fasta_dict"": ""/data/ref/Homo_sapiens_assembly19.dict"",; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""jobId"": ""28226"",; ""backend"": ""Lo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:75958,Performance,Cache,Cache,75958,"{; ""shardIndex"": 11,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.287Z""; },; {; ""shardIndex"": 12,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.287Z""; },; {; ""shardIndex"": 13,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.288Z""; },; {; ""shardIndex"": 14,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; },; {; ""shardIndex"": 15,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; },; {; ""shardIndex"": 16,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; }; ],; ""case_gatk_acnv_workflow.PadTargets"": [; {; ""Call caching read result"": ""Cache Hit: 6b52652e-a50d-4787-86b1-794e53958ada:case_gatk_acnv_workflow.PadTargets:-1"",; ""executionStatus"": ""Done"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/stdout"",; ""shardIndex"": -1,; ""outputs"": {; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""target_file"": ""/data/target/ice_targets.tsv"",; ""mem"": 1,; ""padding"": 250,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""isWGS"": false; },; ""returnCode"": 0,; ""backend"": ""Local"",; ""end"": ""2016-09-23T13:53:07.897Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:76608,Performance,cache,cache,76608,": 15,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; },; {; ""shardIndex"": 16,; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""backend"": ""Local"",; ""attempt"": 1,; ""start"": ""2016-09-23T13:53:07.289Z""; }; ],; ""case_gatk_acnv_workflow.PadTargets"": [; {; ""Call caching read result"": ""Cache Hit: 6b52652e-a50d-4787-86b1-794e53958ada:case_gatk_acnv_workflow.PadTargets:-1"",; ""executionStatus"": ""Done"",; ""stdout"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/stdout"",; ""shardIndex"": -1,; ""outputs"": {; ""padded_target_file"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/targets.padded.tsv""; },; ""runtimeAttributes"": {; ""docker"": ""broadinstitute/gatk-protected:24e6bdc0c058eaa9abe63e1987418d0c144fef8e"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""ReadAndWriteCache"",; ""inputs"": {; ""target_file"": ""/data/target/ice_targets.tsv"",; ""mem"": 1,; ""padding"": 250,; ""gatk_jar"": ""/root/gatk-protected.jar"",; ""isWGS"": false; },; ""returnCode"": 0,; ""backend"": ""Local"",; ""end"": ""2016-09-23T13:53:07.897Z"",; ""stderr"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets/execution/stderr"",; ""callRoot"": ""/home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/ecf3a99c-634e-42d6-9e25-1ebba542098c/call-PadTargets"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2016-09-23T13:53:07.203Z"",; ""description"": ""Pending"",; ""endTime"": ""2016-09-23T13:53:07.212Z""; },; {; ""startTime"": ""2016-09-23T13:53:07.212Z"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-09-23T13:53:07.235Z""; },; {; ""startTime"": ""2016-09-23T13:53:07.235Z"",; ""description"": ""CheckingCallCache"",; ""endTime"": ""2016-09-23",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:83254,Performance,tune,tune,83254,"ipts/crsp_validation/crsp_validation_bam_list_local_paths.txt"",; ""case_gatk_acnv_workflow.ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""case_gatk_acnv_workflow.enable_gc_correction"": true,; ""case_gatk_acnv_workflow.wgsBinSize"": 10000,; ""case_gatk_acnv_workflow.seg_param_undoPrune"": 0.05,; ""case_gatk_acnv_workflow.gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:83366,Performance,perform,performing,83366,".gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true',",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:83629,Performance,tune,tuned,83629,".gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true',",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:83746,Performance,load,load,83746," case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84037,Performance,perform,performance,84037,"ice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84926,Performance,concurren,concurrent-workflows,84926,"ur is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridde",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:87275,Performance,perform,perform,87275,"or example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and wr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:87945,Performance,concurren,concurrent-job-limit,87945," = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; }; }; }; }; }; }. services {; KeyValue {; class = ""c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84381,Safety,timeout,timeout,84381,"ts the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84417,Safety,timeout,timeout,84417,"ts the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84443,Safety,timeout,timeout,84443,"ts the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84533,Safety,abort,abort,84533," = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:84582,Safety,abort,abort-jobs-on-terminate,84582," = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85366,Security,encrypt,encrypted,85366,"r {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85405,Security,encrypt,encrypted-fields,85405,"r {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85452,Security,encrypt,encrypt,85452,"T will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85475,Security,encrypt,encrypted-fields,85475,"T will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85501,Security,encrypt,encryption-key,85501,"T will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:86249,Security,hash,hash,86249,"/ Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/fil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:86337,Security,hash,hash,86337,"lds`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:86547,Security,hash,hashes,86547,"lds`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:86763,Security,hash,hash,86763,"Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:86798,Security,hash,hash,86798,"to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; }; ]; }. engine {; // This instructs the engine which filesystems are at its disposal to perform any IO operation that it might need.; // For instance, WDL variables declared at the Workflow level will be evaluated using the filesystems declared here.; // If you intend to be able to run workflows with this kind of declarations:; // workflow {; // String str = read_string(""gs://bucket/my-file.txt""); // }; // You will need to provide the engine with a gcs filesystem; // Note that the default filesystem (local) is always available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:89646,Security,password,password,89646,"available.; //filesystems {; // gcs {; // auth = ""application-default""; // }; //}; }. backend {; default = ""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 28; run-in-background = true; runtime-attributes = ""String? docker""; submit = ""/bin/bash ${script}""; submit-docker = ""docker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". // Root directory where Cromwell writes job results. This directory must be; // visible and writeable by the Cromwell process as well as the jobs that Cromwell; // launches.; root: ""cromwell-executions"". filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }. local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; }; }; }; }; }; }. services {; KeyValue {; class = ""cromwell.services.keyvalue.impl.SqlKeyValueServiceActor""; }; MetadataService {; class = ""cromwell.services.metadata.impl.MetadataServiceActor""; }; }. database {; // This specifies which database to use; //config = main.hsqldb; config = main.mysql_localhost. main {; hsqldb {; driver = ""slick.driver.HsqldbDriver$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${slick.uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }; }. mysql_localhost {; driver = ""slick.driver.MySQLDriver$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell""; user = ""root""; password = ""cromwell""; connectionTimeout = 5000; }; }; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:3741,Testability,test,tested,3741,"tk_acnv_workflow.plots_dir"": ""plots/"",; ""case_gatk_acnv_workflow.call_cnloh_dir"": ""splits/"",; ""case_gatk_acnv_workflow.enable_gc_correction"": ""true"",; ""case_gatk_acnv_workflow.isWGS"": ""false"",; ""case_gatk_acnv_workflow.wgsBinSize"": ""10000"",; ""case_gatk_acnv_workflow.seg_param_alpha"": ""0.01"",; ""case_gatk_acnv_workflow.seg_param_nperm"": ""10000"",; ""case_gatk_acnv_workflow.seg_param_pmethod"": ""HYBRID"",; ""case_gatk_acnv_workflow.seg_param_minWidth"": ""2"",; ""case_gatk_acnv_workflow.seg_param_kmax"": ""25"",; ""case_gatk_acnv_workflow.seg_param_nmin"": ""200"",; ""case_gatk_acnv_workflow.seg_param_eta"": ""0.05"",; ""case_gatk_acnv_workflow.seg_param_trim"": ""0.025"",; ""case_gatk_acnv_workflow.seg_param_undoSplits"": ""NONE"",; ""case_gatk_acnv_workflow.seg_param_undoPrune"": ""0.05"",; ""case_gatk_acnv_workflow.seg_param_undoSD"": ""3""; }; ```. WDL:. ``` wdl; #; # Case sample workflow for a list of pairs of case-control samples. Includes GATK CNV and ACNV. Supports both WGS and WES samples. This was tested on a3c7368 commit of gatk-protected.; #; # Notes:; #; # - the input file(input_bam_list) must contain a list of tab separated values in the following format(one or more lines must be supplied):; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--first input; # tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--second input; # etc...; #; # - set isWGS variable to true or false to specify whether to run a WGS or WES workflow respectively; #; # - file names will use the entity ID specified, but inside the file, the bam SM tag will typically be used.; #; # - target file (which must be in tsv format) is only used with WES workflow, WGS workflow generates its own targets (so user can pass any string as an argument); #; # - the WGS PoN must be generated with WGS samples; # ; # - THIS SCRIPT SHOULD BE CONSIDERED OF ""BETA"" QUALITY; #; # - Example invocation:; # java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json; # -",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:25985,Testability,test,tested,25985,"l_cnloh_dir\"": \""splits/\"",\n \""case_gatk_acnv_workflow.enable_gc_correction\"": \""true\"",\n \""case_gatk_acnv_workflow.isWGS\"": \""false\"",\n \""case_gatk_acnv_workflow.wgsBinSize\"": \""10000\"",\n \""case_gatk_acnv_workflow.seg_param_alpha\"": \""0.01\"",\n \""case_gatk_acnv_workflow.seg_param_nperm\"": \""10000\"",\n \""case_gatk_acnv_workflow.seg_param_pmethod\"": \""HYBRID\"",\n \""case_gatk_acnv_workflow.seg_param_minWidth\"": \""2\"",\n \""case_gatk_acnv_workflow.seg_param_kmax\"": \""25\"",\n \""case_gatk_acnv_workflow.seg_param_nmin\"": \""200\"",\n \""case_gatk_acnv_workflow.seg_param_eta\"": \""0.05\"",\n \""case_gatk_acnv_workflow.seg_param_trim\"": \""0.025\"",\n \""case_gatk_acnv_workflow.seg_param_undoSplits\"": \""NONE\"",\n \""case_gatk_acnv_workflow.seg_param_undoPrune\"": \""0.05\"",\n \""case_gatk_acnv_workflow.seg_param_undoSD\"": \""3\""\n}"",; ""workflow"": ""#\n# Case sample workflow for a list of pairs of case-control samples. Includes GATK CNV and ACNV. Supports both WGS and WES samples. This was tested on a3c7368 commit of gatk-protected.\n#\n# Notes:\n#\n# - the input file(input_bam_list) must contain a list of tab separated values in the following format(one or more lines must be supplied):\n# tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--first input\n# tumor_entity tumor_bam tumor_bam_index normal_entity normal_bam normal_bam_index <--second input\n# etc...\n#\n# - set isWGS variable to true or false to specify whether to run a WGS or WES workflow respectively\n#\n# - file names will use the entity ID specified, but inside the file, the bam SM tag will typically be used.\n#\n# - target file (which must be in tsv format) is only used with WES workflow, WGS workflow generates its own targets (so user can pass any string as an argument)\n#\n# - the WGS PoN must be generated with WGS samples\n# \n# - THIS SCRIPT SHOULD BE CONSIDERED OF \""BETA\"" QUALITY\n#\n# - Example invocation:\n# java -jar cromwell.jar run case_gatk_acnv_workflow.wdl myParameters.json\n#",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:83029,Testability,log,loggers,83029,"oN"": ""/data/ice_rcs_eval.v1.pd250.spark.pon"",; ""case_gatk_acnv_workflow.input_bam_list"": ""/home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_bam_list_local_paths.txt"",; ""case_gatk_acnv_workflow.ref_fasta"": ""/data/ref/Homo_sapiens_assembly19.fasta"",; ""case_gatk_acnv_workflow.enable_gc_correction"": true,; ""case_gatk_acnv_workflow.wgsBinSize"": 10000,; ""case_gatk_acnv_workflow.seg_param_undoPrune"": 0.05,; ""case_gatk_acnv_workflow.gatk_jar"": ""/root/gatk-protected.jar"",; ""case_gatk_acnv_workflow.seg_param_nmin"": 200,; ""case_gatk_acnv_workflow.target_file"": ""/data/target/ice_targets.tsv""; },; ""submission"": ""2016-09-23T13:53:05.453Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85286,Testability,log,log-copy-workers,85286,"utor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a comp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85606,Testability,log,logs,85606,"alse. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85621,Testability,log,log-dir,85621,"alse. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85649,Testability,log,logs,85649,"alse. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85683,Testability,log,logs,85683,"ailure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external serv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:85728,Testability,log,log-temporary,85728,"ailure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external serv",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:18454,Usability,undo,undoSplits,18454,"alized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.; # Entity IDs can be the same value; task HetPulldown {; String entity_id_tumor; String entity_id_normal; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File tumor_bam; File tumor_bam_idx; File",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:18494,Usability,undo,undoPrune,18494,"alized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.; # Entity IDs can be the same value; task HetPulldown {; String entity_id_tumor; String entity_id_normal; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File tumor_bam; File tumor_bam_idx; File",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:18529,Usability,undo,undoSD,18529,"alized ${entity_id}.preTN.tsv --help false --version false --verbosity INFO --QUIET false; }. output {; File tn_file = ""${entity_id}.tn.tsv""; File pre_tn_file = ""${entity_id}.preTN.tsv""; File betahats_file = ""${entity_id}.betaHats.tsv""; }; #runtime {; # docker: ""gatk-protected/a1""; #}; }. # Segment the tangent normalized coverage profile.; task PerformSegmentation {; String entity_id; Float seg_param_alpha; Int seg_param_nperm; String seg_param_pmethod; Int seg_param_minWidth; Int seg_param_kmax; Int seg_param_nmin; Float seg_param_eta; Float seg_param_trim; String seg_param_undoSplits; Float seg_param_undoPrune; Int seg_param_undoSD; String gatk_jar; File tn_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \; --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \; --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \; --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \; --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \; --verbosity INFO --QUIET false; }. output {; File seg_file = ""${entity_id}.seg""; }; }. # Make calls (amp, neutral, or deleted) on each segment.; task Caller {; String entity_id; String gatk_jar; File tn_file; File seg_file; Int mem. command {; java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \; --segments ${seg_file} --output ${entity_id}.called --legacy false \; --help false --version false --verbosity INFO --QUIET false; }. output {; File called_file=""${entity_id}.called""; }; }. # Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.; # Entity IDs can be the same value; task HetPulldown {; String entity_id_tumor; String entity_id_normal; String gatk_jar; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File tumor_bam; File tumor_bam_idx; File",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:41272,Usability,undo,undoSplits,41272," INFO --QUIET false\n }\n\n output {\n File tn_file = \""${entity_id}.tn.tsv\""\n File pre_tn_file = \""${entity_id}.preTN.tsv\""\n File betahats_file = \""${entity_id}.betaHats.tsv\""\n }\n #runtime {\n # docker: \""gatk-protected/a1\""\n #}\n}\n\n# Segment the tangent normalized coverage profile.\ntask PerformSegmentation {\n String entity_id\n Float seg_param_alpha\n Int seg_param_nperm\n String seg_param_pmethod\n Int seg_param_minWidth\n Int seg_param_kmax\n Int seg_param_nmin\n Float seg_param_eta\n Float seg_param_trim\n String seg_param_undoSplits\n Float seg_param_undoPrune\n Int seg_param_undoSD\n String gatk_jar\n File tn_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \\\n --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \\\n --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \\\n --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \\\n --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \\\n --verbosity INFO --QUIET false\n }\n\n output {\n File seg_file = \""${entity_id}.seg\""\n }\n}\n\n# Make calls (amp, neutral, or deleted) on each segment.\ntask Caller {\n String entity_id\n String gatk_jar\n File tn_file\n File seg_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \\\n --segments ${seg_file} --output ${entity_id}.called --legacy false \\\n --help false --version false --verbosity INFO --QUIET false\n }\n\n output {\n File called_file=\""${entity_id}.called\""\n }\n}\n\n# Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.\n# Entity IDs can be the same value\ntask HetPulldown {\n String entity_id_tumor\n String entity_id_normal\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n File tumor_bam\n Fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:41314,Usability,undo,undoPrune,41314," INFO --QUIET false\n }\n\n output {\n File tn_file = \""${entity_id}.tn.tsv\""\n File pre_tn_file = \""${entity_id}.preTN.tsv\""\n File betahats_file = \""${entity_id}.betaHats.tsv\""\n }\n #runtime {\n # docker: \""gatk-protected/a1\""\n #}\n}\n\n# Segment the tangent normalized coverage profile.\ntask PerformSegmentation {\n String entity_id\n Float seg_param_alpha\n Int seg_param_nperm\n String seg_param_pmethod\n Int seg_param_minWidth\n Int seg_param_kmax\n Int seg_param_nmin\n Float seg_param_eta\n Float seg_param_trim\n String seg_param_undoSplits\n Float seg_param_undoPrune\n Int seg_param_undoSD\n String gatk_jar\n File tn_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \\\n --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \\\n --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \\\n --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \\\n --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \\\n --verbosity INFO --QUIET false\n }\n\n output {\n File seg_file = \""${entity_id}.seg\""\n }\n}\n\n# Make calls (amp, neutral, or deleted) on each segment.\ntask Caller {\n String entity_id\n String gatk_jar\n File tn_file\n File seg_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \\\n --segments ${seg_file} --output ${entity_id}.called --legacy false \\\n --help false --version false --verbosity INFO --QUIET false\n }\n\n output {\n File called_file=\""${entity_id}.called\""\n }\n}\n\n# Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.\n# Entity IDs can be the same value\ntask HetPulldown {\n String entity_id_tumor\n String entity_id_normal\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n File tumor_bam\n Fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:41349,Usability,undo,undoSD,41349," INFO --QUIET false\n }\n\n output {\n File tn_file = \""${entity_id}.tn.tsv\""\n File pre_tn_file = \""${entity_id}.preTN.tsv\""\n File betahats_file = \""${entity_id}.betaHats.tsv\""\n }\n #runtime {\n # docker: \""gatk-protected/a1\""\n #}\n}\n\n# Segment the tangent normalized coverage profile.\ntask PerformSegmentation {\n String entity_id\n Float seg_param_alpha\n Int seg_param_nperm\n String seg_param_pmethod\n Int seg_param_minWidth\n Int seg_param_kmax\n Int seg_param_nmin\n Float seg_param_eta\n Float seg_param_trim\n String seg_param_undoSplits\n Float seg_param_undoPrune\n Int seg_param_undoSD\n String gatk_jar\n File tn_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} PerformSegmentation --targets ${tn_file} \\\n --output ${entity_id}.seg --log2Input true --alpha ${seg_param_alpha} --nperm ${seg_param_nperm} \\\n --pmethod ${seg_param_pmethod} --minWidth ${seg_param_minWidth} --kmax ${seg_param_kmax} \\\n --nmin ${seg_param_nmin} --eta ${seg_param_eta} --trim ${seg_param_trim} --undoSplits ${seg_param_undoSplits} \\\n --undoPrune ${seg_param_undoPrune} --undoSD ${seg_param_undoSD} --help false --version false \\\n --verbosity INFO --QUIET false\n }\n\n output {\n File seg_file = \""${entity_id}.seg\""\n }\n}\n\n# Make calls (amp, neutral, or deleted) on each segment.\ntask Caller {\n String entity_id\n String gatk_jar\n File tn_file\n File seg_file\n Int mem\n\n command {\n java -Xmx${mem}g -jar ${gatk_jar} CallSegments --targets ${tn_file} \\\n --segments ${seg_file} --output ${entity_id}.called --legacy false \\\n --help false --version false --verbosity INFO --QUIET false\n }\n\n output {\n File called_file=\""${entity_id}.called\""\n }\n}\n\n# Call heterozygous SNPs in the normal and then count the reads in the tumor for each called position.\n# Entity IDs can be the same value\ntask HetPulldown {\n String entity_id_tumor\n String entity_id_normal\n String gatk_jar\n File ref_fasta\n File ref_fasta_fai\n File ref_fasta_dict\n File tumor_bam\n Fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1480:83717,Usability,responsiv,responsive,83717," case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ""end"": ""2016-09-23T13:53:29.816Z"",; ""start"": ""2016-09-23T13:53:06.277Z""; }; ```. local_application.conf. ```; webservice {; port = 8000; interface = 0.0.0.0; instance.name = ""reference""; }. akka {; loggers = [""akka.event.slf4j.Slf4jLogger""]; actor {; default-dispatcher {; fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; #parallelism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480
https://github.com/broadinstitute/cromwell/issues/1483:150,Performance,cache,cache,150,"- local backend. The slowness is probably driven by calculating a md5 hash on the bam input files. For jobs that do not have bam files as inputs, the cache determination is fast. Calculating a md5 on a bam file is too slow for use in call caching on a local backend. I can provide a time estimate, if necessary. Does SGE backend has the same issue?. Proposed solution:; - If md5 file is next to input file (e.g. sample1.bam is next to sample1.bam.md5) then read the md5 hash from the *.md5 file. Otherwise, use the file path. For bams, the file paths will tend to be static for most local backend environments anyway. ; - Make sure this convention is documented.; - Perhaps have a flag in the conf file so that users can choose which convention they prefer? IMHO, have the fast method (above) as the default.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1483
https://github.com/broadinstitute/cromwell/issues/1483:70,Security,hash,hash,70,"- local backend. The slowness is probably driven by calculating a md5 hash on the bam input files. For jobs that do not have bam files as inputs, the cache determination is fast. Calculating a md5 on a bam file is too slow for use in call caching on a local backend. I can provide a time estimate, if necessary. Does SGE backend has the same issue?. Proposed solution:; - If md5 file is next to input file (e.g. sample1.bam is next to sample1.bam.md5) then read the md5 hash from the *.md5 file. Otherwise, use the file path. For bams, the file paths will tend to be static for most local backend environments anyway. ; - Make sure this convention is documented.; - Perhaps have a flag in the conf file so that users can choose which convention they prefer? IMHO, have the fast method (above) as the default.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1483
https://github.com/broadinstitute/cromwell/issues/1483:470,Security,hash,hash,470,"- local backend. The slowness is probably driven by calculating a md5 hash on the bam input files. For jobs that do not have bam files as inputs, the cache determination is fast. Calculating a md5 on a bam file is too slow for use in call caching on a local backend. I can provide a time estimate, if necessary. Does SGE backend has the same issue?. Proposed solution:; - If md5 file is next to input file (e.g. sample1.bam is next to sample1.bam.md5) then read the md5 hash from the *.md5 file. Otherwise, use the file path. For bams, the file paths will tend to be static for most local backend environments anyway. ; - Make sure this convention is documented.; - Perhaps have a flag in the conf file so that users can choose which convention they prefer? IMHO, have the fast method (above) as the default.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1483
https://github.com/broadinstitute/cromwell/issues/1484:1478,Availability,down,down,1478,"I have caught cromwell red-handed! I did not realize that the cromwell timing was implemented in [TrumpScript](https://github.com/samshadwell/TrumpScript). **TL;DR** The timing diagram showed that my job took > 2 hours to run, even though half of that was spent on overhead in cromwell. ; ## Proof. Here is a snapshot of the timing diagram (see highlighted runtime below -- 2h 6m).; ![cromwell_snapshot_so_slow_lies](https://cloud.githubusercontent.com/assets/2152339/18798566/23db7738-81a1-11e6-8a39-43612a561aa7.png); Yet, when I check that job:. ```; $ head -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout. 17:06:18.839 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/root/gatk-protected/build/libs/gatk-protected-all-24e6bdc-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 17:06:19.327 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; 17:06:19.353 INFO CalculateTargetCoverage - Defaults.BUFFER_SIZE : 131072; 17:06:19.355 INFO CalculateTargetCoverage - Defaults.COMPRESSION_LEVEL : 5. $ tail -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout; 18:10:04.122 INFO CalculateTargetCoverage - Writing counts ...; 18:10:05.250 INFO CalculateTargetCoverage - Writing counts done.; 18:10:05.250 INFO CalculateTargetCoverage - Shutting down engine; ```. The job finished in about 64 minutes (please note that timezones are not concordant between timing and log messages).; This will likely (de facto) be addressed once the md5 issue is resolved in issue #1483",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1484
https://github.com/broadinstitute/cromwell/issues/1484:1603,Integrability,message,messages,1603,"I have caught cromwell red-handed! I did not realize that the cromwell timing was implemented in [TrumpScript](https://github.com/samshadwell/TrumpScript). **TL;DR** The timing diagram showed that my job took > 2 hours to run, even though half of that was spent on overhead in cromwell. ; ## Proof. Here is a snapshot of the timing diagram (see highlighted runtime below -- 2h 6m).; ![cromwell_snapshot_so_slow_lies](https://cloud.githubusercontent.com/assets/2152339/18798566/23db7738-81a1-11e6-8a39-43612a561aa7.png); Yet, when I check that job:. ```; $ head -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout. 17:06:18.839 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/root/gatk-protected/build/libs/gatk-protected-all-24e6bdc-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 17:06:19.327 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; 17:06:19.353 INFO CalculateTargetCoverage - Defaults.BUFFER_SIZE : 131072; 17:06:19.355 INFO CalculateTargetCoverage - Defaults.COMPRESSION_LEVEL : 5. $ tail -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout; 18:10:04.122 INFO CalculateTargetCoverage - Writing counts ...; 18:10:05.250 INFO CalculateTargetCoverage - Writing counts done.; 18:10:05.250 INFO CalculateTargetCoverage - Shutting down engine; ```. The job finished in about 64 minutes (please note that timezones are not concordant between timing and log messages).; This will likely (de facto) be addressed once the md5 issue is resolved in issue #1483",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1484
https://github.com/broadinstitute/cromwell/issues/1484:750,Performance,load,load,750,"I have caught cromwell red-handed! I did not realize that the cromwell timing was implemented in [TrumpScript](https://github.com/samshadwell/TrumpScript). **TL;DR** The timing diagram showed that my job took > 2 hours to run, even though half of that was spent on overhead in cromwell. ; ## Proof. Here is a snapshot of the timing diagram (see highlighted runtime below -- 2h 6m).; ![cromwell_snapshot_so_slow_lies](https://cloud.githubusercontent.com/assets/2152339/18798566/23db7738-81a1-11e6-8a39-43612a561aa7.png); Yet, when I check that job:. ```; $ head -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout. 17:06:18.839 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/root/gatk-protected/build/libs/gatk-protected-all-24e6bdc-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 17:06:19.327 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; 17:06:19.353 INFO CalculateTargetCoverage - Defaults.BUFFER_SIZE : 131072; 17:06:19.355 INFO CalculateTargetCoverage - Defaults.COMPRESSION_LEVEL : 5. $ tail -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout; 18:10:04.122 INFO CalculateTargetCoverage - Writing counts ...; 18:10:05.250 INFO CalculateTargetCoverage - Writing counts done.; 18:10:05.250 INFO CalculateTargetCoverage - Shutting down engine; ```. The job finished in about 64 minutes (please note that timezones are not concordant between timing and log messages).; This will likely (de facto) be addressed once the md5 issue is resolved in issue #1483",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1484
https://github.com/broadinstitute/cromwell/issues/1484:968,Performance,load,loaded,968,"I have caught cromwell red-handed! I did not realize that the cromwell timing was implemented in [TrumpScript](https://github.com/samshadwell/TrumpScript). **TL;DR** The timing diagram showed that my job took > 2 hours to run, even though half of that was spent on overhead in cromwell. ; ## Proof. Here is a snapshot of the timing diagram (see highlighted runtime below -- 2h 6m).; ![cromwell_snapshot_so_slow_lies](https://cloud.githubusercontent.com/assets/2152339/18798566/23db7738-81a1-11e6-8a39-43612a561aa7.png); Yet, when I check that job:. ```; $ head -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout. 17:06:18.839 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/root/gatk-protected/build/libs/gatk-protected-all-24e6bdc-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 17:06:19.327 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; 17:06:19.353 INFO CalculateTargetCoverage - Defaults.BUFFER_SIZE : 131072; 17:06:19.355 INFO CalculateTargetCoverage - Defaults.COMPRESSION_LEVEL : 5. $ tail -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout; 18:10:04.122 INFO CalculateTargetCoverage - Writing counts ...; 18:10:05.250 INFO CalculateTargetCoverage - Writing counts done.; 18:10:05.250 INFO CalculateTargetCoverage - Shutting down engine; ```. The job finished in about 64 minutes (please note that timezones are not concordant between timing and log messages).; This will likely (de facto) be addressed once the md5 issue is resolved in issue #1483",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1484
https://github.com/broadinstitute/cromwell/issues/1484:1599,Testability,log,log,1599,"I have caught cromwell red-handed! I did not realize that the cromwell timing was implemented in [TrumpScript](https://github.com/samshadwell/TrumpScript). **TL;DR** The timing diagram showed that my job took > 2 hours to run, even though half of that was spent on overhead in cromwell. ; ## Proof. Here is a snapshot of the timing diagram (see highlighted runtime below -- 2h 6m).; ![cromwell_snapshot_so_slow_lies](https://cloud.githubusercontent.com/assets/2152339/18798566/23db7738-81a1-11e6-8a39-43612a561aa7.png); Yet, when I check that job:. ```; $ head -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout. 17:06:18.839 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/root/gatk-protected/build/libs/gatk-protected-all-24e6bdc-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 17:06:19.327 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; 17:06:19.353 INFO CalculateTargetCoverage - Defaults.BUFFER_SIZE : 131072; 17:06:19.355 INFO CalculateTargetCoverage - Defaults.COMPRESSION_LEVEL : 5. $ tail -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout; 18:10:04.122 INFO CalculateTargetCoverage - Writing counts ...; 18:10:05.250 INFO CalculateTargetCoverage - Writing counts done.; 18:10:05.250 INFO CalculateTargetCoverage - Shutting down engine; ```. The job finished in about 64 minutes (please note that timezones are not concordant between timing and log messages).; This will likely (de facto) be addressed once the md5 issue is resolved in issue #1483",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1484
https://github.com/broadinstitute/cromwell/issues/1485:190,Energy Efficiency,reduce,reduce,190,"Rather than shoving this required, one-per-entry information as a runtime-required value in detritus, why not add a not-nullable column in the call caching metainfo?. This would also let us reduce the size of the detritus table by optionally dropping the constant callRootPath prefix from every path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1485
https://github.com/broadinstitute/cromwell/issues/1488:159,Performance,cache,cache,159,"- local backend with 16 cores and 104GB RAM; - cromwell-0.21-6da2d10-SNAPSHOT.jar (incl. file path hashing and local backend throttling). Whether the job is a cache hit or not, it seems that the cromwell final overhead takes 3-8 minutes, which is a long time. This happens even for jobs where the files generated (and input) are very small (and there are few files). We do not use `read_string` in the wdl. http://104.198.41.229:8080/api/workflows/v2/70a6e380-1dd7-473b-a852-4bd54b22ecdf/timing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488
https://github.com/broadinstitute/cromwell/issues/1488:99,Security,hash,hashing,99,"- local backend with 16 cores and 104GB RAM; - cromwell-0.21-6da2d10-SNAPSHOT.jar (incl. file path hashing and local backend throttling). Whether the job is a cache hit or not, it seems that the cromwell final overhead takes 3-8 minutes, which is a long time. This happens even for jobs where the files generated (and input) are very small (and there are few files). We do not use `read_string` in the wdl. http://104.198.41.229:8080/api/workflows/v2/70a6e380-1dd7-473b-a852-4bd54b22ecdf/timing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1488
https://github.com/broadinstitute/cromwell/issues/1491:348,Energy Efficiency,schedul,scheduling,348,"Submitting jobs to UGER should use an array:. > You are limited to 100 job in the cluster at once. This is to encourage the use of Task Arrays. Task arrays use the variable $SGE_TASK_ID which can be called inside the job script.""qsub -t 1-1000 /path/to/jobScript"". via: https://intranet.broadinstitute.org/bits/service-catalog/research-support/job-scheduling",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491
https://github.com/broadinstitute/cromwell/issues/1491:164,Modifiability,variab,variable,164,"Submitting jobs to UGER should use an array:. > You are limited to 100 job in the cluster at once. This is to encourage the use of Task Arrays. Task arrays use the variable $SGE_TASK_ID which can be called inside the job script.""qsub -t 1-1000 /path/to/jobScript"". via: https://intranet.broadinstitute.org/bits/service-catalog/research-support/job-scheduling",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491
https://github.com/broadinstitute/cromwell/issues/1492:34,Integrability,wrap,wrapper,34,"As a user, I would like to have a wrapper script / CLI which lets me run a workflow (with it's inputs, imports, workflow options) either in single workflow mode or in server mode. The script should check for the existence of all specified files, and provide a nice usage statement if run without any options. For single workflow mode, this is pretty simple. For server mode, this wraps the submit HTTP call. This should make life easier for users who are submitting workflows with multiple import.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492
https://github.com/broadinstitute/cromwell/issues/1492:380,Integrability,wrap,wraps,380,"As a user, I would like to have a wrapper script / CLI which lets me run a workflow (with it's inputs, imports, workflow options) either in single workflow mode or in server mode. The script should check for the existence of all specified files, and provide a nice usage statement if run without any options. For single workflow mode, this is pretty simple. For server mode, this wraps the submit HTTP call. This should make life easier for users who are submitting workflows with multiple import.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492
https://github.com/broadinstitute/cromwell/issues/1492:350,Usability,simpl,simple,350,"As a user, I would like to have a wrapper script / CLI which lets me run a workflow (with it's inputs, imports, workflow options) either in single workflow mode or in server mode. The script should check for the existence of all specified files, and provide a nice usage statement if run without any options. For single workflow mode, this is pretty simple. For server mode, this wraps the submit HTTP call. This should make life easier for users who are submitting workflows with multiple import.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492
https://github.com/broadinstitute/cromwell/issues/1494:211,Performance,cache,cache,211,"- local backend + docker; - dev snapshot with two merged branches. _There are three issues confounding this run, so please only look at the HetPulldown runs (at the top)._. All jobs in this run should have been cache hits. However, it seems as if two of the HetPulldown jobs were marked as cache misses, though nothing had changed. ![all_should_be_cache_hits](https://cloud.githubusercontent.com/assets/2152339/18922404/7e258c6e-8576-11e6-976e-4b95860384ac.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494
https://github.com/broadinstitute/cromwell/issues/1494:290,Performance,cache,cache,290,"- local backend + docker; - dev snapshot with two merged branches. _There are three issues confounding this run, so please only look at the HetPulldown runs (at the top)._. All jobs in this run should have been cache hits. However, it seems as if two of the HetPulldown jobs were marked as cache misses, though nothing had changed. ![all_should_be_cache_hits](https://cloud.githubusercontent.com/assets/2152339/18922404/7e258c6e-8576-11e6-976e-4b95860384ac.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494
https://github.com/broadinstitute/cromwell/issues/1495:35,Deployability,release,release,35,- local backend; - **cromwell-0.21 release**. Did nothing... had to resort to kill -9. I think it was calculating a lot of md5s at the time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495
https://github.com/broadinstitute/cromwell/issues/1496:33,Energy Efficiency,power,powerful,33,"The timing diagram would be more powerful if the workflow status was included at the top of the chart. As it stands now, I need to check two places to understand the detailed state of my workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1496
https://github.com/broadinstitute/cromwell/issues/1497:34,Performance,cache,cache,34,"I am running a workflow that will cache hit every job. Given the log dump on the command line, that seems to be happening. However, the timing diagram is not updating. All I see is the first job and the timing diagram has been in that state for over ten minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1497
https://github.com/broadinstitute/cromwell/issues/1497:65,Testability,log,log,65,"I am running a workflow that will cache hit every job. Given the log dump on the command line, that seems to be happening. However, the timing diagram is not updating. All I see is the first job and the timing diagram has been in that state for over ten minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1497
https://github.com/broadinstitute/cromwell/issues/1498:112,Modifiability,config,config,112,"Cromwell apparently behaves badly in single-workflow mode when two instances are run in parallel (with a shared config and persistent DB). Since this is an untested operational mode, we could simply prevent the second instance from starting up?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1498
https://github.com/broadinstitute/cromwell/issues/1498:192,Usability,simpl,simply,192,"Cromwell apparently behaves badly in single-workflow mode when two instances are run in parallel (with a shared config and persistent DB). Since this is an untested operational mode, we could simply prevent the second instance from starting up?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1498
https://github.com/broadinstitute/cromwell/issues/1499:452,Deployability,release,release,452,"This is an issue to track the same problem that @ffinfo has raised on Gitter and PR #1346. The scenario for us is an HPC cluster environment with a scheduler (SGE or PBS) that strictly enforces walltimes; having jobs be killed by the scheduler (without producing rc file) when they exceed the requested walltime is, if not exactly common, not that rare either. The hacky non-async solution I have been using (up until the nice configurable backends of release 0.21 changed everything in backend-land) was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to poll the scheduler itself: https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499
https://github.com/broadinstitute/cromwell/issues/1499:148,Energy Efficiency,schedul,scheduler,148,"This is an issue to track the same problem that @ffinfo has raised on Gitter and PR #1346. The scenario for us is an HPC cluster environment with a scheduler (SGE or PBS) that strictly enforces walltimes; having jobs be killed by the scheduler (without producing rc file) when they exceed the requested walltime is, if not exactly common, not that rare either. The hacky non-async solution I have been using (up until the nice configurable backends of release 0.21 changed everything in backend-land) was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to poll the scheduler itself: https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499
https://github.com/broadinstitute/cromwell/issues/1499:234,Energy Efficiency,schedul,scheduler,234,"This is an issue to track the same problem that @ffinfo has raised on Gitter and PR #1346. The scenario for us is an HPC cluster environment with a scheduler (SGE or PBS) that strictly enforces walltimes; having jobs be killed by the scheduler (without producing rc file) when they exceed the requested walltime is, if not exactly common, not that rare either. The hacky non-async solution I have been using (up until the nice configurable backends of release 0.21 changed everything in backend-land) was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to poll the scheduler itself: https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499
https://github.com/broadinstitute/cromwell/issues/1499:614,Energy Efficiency,schedul,scheduler,614,"This is an issue to track the same problem that @ffinfo has raised on Gitter and PR #1346. The scenario for us is an HPC cluster environment with a scheduler (SGE or PBS) that strictly enforces walltimes; having jobs be killed by the scheduler (without producing rc file) when they exceed the requested walltime is, if not exactly common, not that rare either. The hacky non-async solution I have been using (up until the nice configurable backends of release 0.21 changed everything in backend-land) was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to poll the scheduler itself: https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499
https://github.com/broadinstitute/cromwell/issues/1499:427,Modifiability,config,configurable,427,"This is an issue to track the same problem that @ffinfo has raised on Gitter and PR #1346. The scenario for us is an HPC cluster environment with a scheduler (SGE or PBS) that strictly enforces walltimes; having jobs be killed by the scheduler (without producing rc file) when they exceed the requested walltime is, if not exactly common, not that rare either. The hacky non-async solution I have been using (up until the nice configurable backends of release 0.21 changed everything in backend-land) was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to poll the scheduler itself: https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499
https://github.com/broadinstitute/cromwell/pull/1500:248,Modifiability,refactor,refactored,248,Suggested reading order:; - JobExecutionTokenDispenserActorSpec (to see how the TokenDispenserActor is intended to operate); - EJEA (to see how the token dispenser gets invoked); - Everything else (a lot of the more annoying wiring can probably be refactored once I add per-backend god-actors in my other ticket),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1500
https://github.com/broadinstitute/cromwell/issues/1501:86,Deployability,pipeline,pipeline,86,It looks like with .21 there is a an extra `resources` key with default values in the pipeline args section when you describe a jes job. When I ran the 50 workflow test with .21 none of my jobs seemed to get preempted which seems unusual and I want to make sure this extra `resources` stanza with default values isn't causing issues. from .19:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 208; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 10; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/exec.sh; input_bam-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bam; input_bam_index-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bai; interval_list-0: gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0018_of_50/scattered.interval_list; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17.log; outputs:; HaplotypeCaller-17-rc.txt: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17-rc.txt; NWD989972.vcf.gz: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/NWD989972.vcf.gz; NWD98,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:590,Deployability,pipeline,pipelineArgs,590,It looks like with .21 there is a an extra `resources` key with default values in the pipeline args section when you describe a jes job. When I ran the 50 workflow test with .21 none of my jobs seemed to get preempted which seems unusual and I want to make sure this extra `resources` stanza with default values isn't causing issues. from .19:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 208; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 10; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/exec.sh; input_bam-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bam; input_bam_index-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bai; interval_list-0: gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0018_of_50/scattered.interval_list; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17.log; outputs:; HaplotypeCaller-17-rc.txt: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17-rc.txt; NWD989972.vcf.gz: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/NWD989972.vcf.gz; NWD98,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:3220,Deployability,pipeline,pipelineArgs,3220,gotc-prod; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-15719508696139283496; instanceName: ggp-15719508696139283496; machineType: us-central1-c/n1-highmem-2; zone: us-central1-c; startTime: '2016-09-28T07:20:39Z'; name: operations/EIf5mP32Khio-K6o-ru6k9oBIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl; ```. from .21:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 200; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:3375,Security,Validat,ValidateReadGroupSamFile,3375,gotc-prod; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-15719508696139283496; instanceName: ggp-15719508696139283496; machineType: us-central1-c/n1-highmem-2; zone: us-central1-c; startTime: '2016-09-28T07:20:39Z'; name: operations/EIf5mP32Khio-K6o-ru6k9oBIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl; ```. from .21:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 200; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:3970,Security,Validat,ValidateReadGroupSamFile,3970,eDSoPcHJvZHVjdGlvblF1ZXVl; ```. from .21:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 200; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:4004,Security,Validat,ValidateReadGroupSamFile-,4004,eDSoPcHJvZHVjdGlvblF1ZXVl; ```. from .21:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 200; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:4201,Security,Validat,ValidateReadGroupSamFile,4201, preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-12146155240789544851; instanceName: ggp-12146155240789544851; machineType: us-central1-b/n1-standard-2;,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:4278,Security,Validat,ValidateReadGroupSamFile-,4278,cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-12146155240789544851; instanceName: ggp-12146155240789544851; machineType: us-central1-b/n1-standard-2; zone: us-central1-b; startTime: '2016-09-28T04:07:00Z'; name: operations/EPuKyPf2KhiTn4qQ6vrzx6gBINj5z_mlEioPcHJvZHVjdGlvblF1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:4425,Security,Validat,ValidateReadGroupSamFile,4425,execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-12146155240789544851; instanceName: ggp-12146155240789544851; machineType: us-central1-b/n1-standard-2; zone: us-central1-b; startTime: '2016-09-28T04:07:00Z'; name: operations/EPuKyPf2KhiTn4qQ6vrzx6gBINj5z_mlEioPcHJvZHVjdGlvblF1ZXVl; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:4459,Security,Validat,ValidateReadGroupSamFile-,4459,execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-12146155240789544851; instanceName: ggp-12146155240789544851; machineType: us-central1-b/n1-standard-2; zone: us-central1-b; startTime: '2016-09-28T04:07:00Z'; name: operations/EPuKyPf2KhiTn4qQ6vrzx6gBINj5z_mlEioPcHJvZHVjdGlvblF1ZXVl; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:164,Testability,test,test,164,It looks like with .21 there is a an extra `resources` key with default values in the pipeline args section when you describe a jes job. When I ran the 50 workflow test with .21 none of my jobs seemed to get preempted which seems unusual and I want to make sure this extra `resources` stanza with default values isn't causing issues. from .19:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 208; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 10; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/exec.sh; input_bam-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bam; input_bam_index-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bai; interval_list-0: gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0018_of_50/scattered.interval_list; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17.log; outputs:; HaplotypeCaller-17-rc.txt: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17-rc.txt; NWD989972.vcf.gz: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/NWD989972.vcf.gz; NWD98,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:1442,Testability,log,logging,1442,; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/exec.sh; input_bam-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bam; input_bam_index-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bai; interval_list-0: gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0018_of_50/scattered.interval_list; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17.log; outputs:; HaplotypeCaller-17-rc.txt: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17-rc.txt; NWD989972.vcf.gz: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/NWD989972.vcf.gz; NWD989972.vcf.gz.tbi: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/NWD989972.vcf.gz.tbi; projectId: broad-gotc-prod; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.goog,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:1617,Testability,log,log,1617,s://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/exec.sh; input_bam-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bam; input_bam_index-0: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-GatherBamFiles/NWD989972.bai; interval_list-0: gs://broad-references/hg38/v0/scattered_calling_intervals/temp_0018_of_50/scattered.interval_list; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17.log; outputs:; HaplotypeCaller-17-rc.txt: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/HaplotypeCaller-17-rc.txt; NWD989972.vcf.gz: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/NWD989972.vcf.gz; NWD989972.vcf.gz.tbi: gs://broad-gotc-prod-cromwell-execution/PairedEndSingleSampleWorkflow/0e160d60-4f48-4058-84c3-36351a7b2cbd/call-HaplotypeCaller/shard-17/NWD989972.vcf.gz.tbi; projectId: broad-gotc-prod; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleap,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:3840,Testability,log,logging,3840,eDSoPcHJvZHVjdGlvblF1ZXVl; ```. from .21:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 200; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1501:4032,Testability,log,log,4032,ntPoint: /cromwell_root; name: local-disk; sizeGb: 200; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/HTC3GCCXX.8.Pond-536132.validation_report; ValidateReadGroupSamFile-22-rc.txt: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22-rc.txt; projectId: broad-gotc-int; resources:; bootDiskSizeGb: 0; disks: []; minimumCpuCores: 0; minimumRamGb: 0; noAddress: false; preemptible: false; zones: []; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - l,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501
https://github.com/broadinstitute/cromwell/issues/1503:370,Performance,cache,cache,370,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503
https://github.com/broadinstitute/cromwell/issues/1503:386,Performance,cache,cache,386,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503
https://github.com/broadinstitute/cromwell/issues/1503:38,Safety,abort,aborted,38,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503
https://github.com/broadinstitute/cromwell/issues/1503:61,Security,hash,hashing,61,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503
https://github.com/broadinstitute/cromwell/issues/1503:127,Security,hash,hashing,127,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503
https://github.com/broadinstitute/cromwell/issues/1503:357,Security,hash,hashes,357,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503
https://github.com/broadinstitute/cromwell/issues/1503:450,Security,hash,hashing,450,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503
https://github.com/broadinstitute/cromwell/issues/1503:270,Usability,simpl,simply,270,"A few observations:; - If the EJEA is aborted, we could stop hashing the remaining files; - If the EJHA is done, it could stop hashing the remaining files; - Since the EJHA already blocks work into chunks of 100 (and BackendFileHashers tend to be synchronous), it could simply not send the next batch if it knows it doesn't need to; - If the set of initial hashes are a cache miss (and cache writing is disabled), we don't need to send the files for hashing in the first place",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1503
https://github.com/broadinstitute/cromwell/issues/1504:134,Availability,down,down,134,"Currently WorkflowActor aborts BJEAs directly. A whole bunch of much nicerness would occur if it aborted the EJEA and let that ripple down the abort to the BJEA (or something else, depending on its FSM state)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504
https://github.com/broadinstitute/cromwell/issues/1504:181,Integrability,depend,depending,181,"Currently WorkflowActor aborts BJEAs directly. A whole bunch of much nicerness would occur if it aborted the EJEA and let that ripple down the abort to the BJEA (or something else, depending on its FSM state)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504
https://github.com/broadinstitute/cromwell/issues/1504:24,Safety,abort,aborts,24,"Currently WorkflowActor aborts BJEAs directly. A whole bunch of much nicerness would occur if it aborted the EJEA and let that ripple down the abort to the BJEA (or something else, depending on its FSM state)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504
https://github.com/broadinstitute/cromwell/issues/1504:97,Safety,abort,aborted,97,"Currently WorkflowActor aborts BJEAs directly. A whole bunch of much nicerness would occur if it aborted the EJEA and let that ripple down the abort to the BJEA (or something else, depending on its FSM state)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504
https://github.com/broadinstitute/cromwell/issues/1504:143,Safety,abort,abort,143,"Currently WorkflowActor aborts BJEAs directly. A whole bunch of much nicerness would occur if it aborted the EJEA and let that ripple down the abort to the BJEA (or something else, depending on its FSM state)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1504
https://github.com/broadinstitute/cromwell/issues/1505:38,Integrability,message,messages,38,"Say you get 4500 ""Quota Exceeded"" log messages. You don't want to ignore them, but you don't want 4500 entries in the cromwell log either. Perhaps there's a middle ground where they get aggregated and you only get one summary message every 10 minutes?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1505
https://github.com/broadinstitute/cromwell/issues/1505:226,Integrability,message,message,226,"Say you get 4500 ""Quota Exceeded"" log messages. You don't want to ignore them, but you don't want 4500 entries in the cromwell log either. Perhaps there's a middle ground where they get aggregated and you only get one summary message every 10 minutes?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1505
https://github.com/broadinstitute/cromwell/issues/1505:34,Testability,log,log,34,"Say you get 4500 ""Quota Exceeded"" log messages. You don't want to ignore them, but you don't want 4500 entries in the cromwell log either. Perhaps there's a middle ground where they get aggregated and you only get one summary message every 10 minutes?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1505
https://github.com/broadinstitute/cromwell/issues/1505:127,Testability,log,log,127,"Say you get 4500 ""Quota Exceeded"" log messages. You don't want to ignore them, but you don't want 4500 entries in the cromwell log either. Perhaps there's a middle ground where they get aggregated and you only get one summary message every 10 minutes?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1505
https://github.com/broadinstitute/cromwell/pull/1507:262,Availability,error,errors,262,"In order to allow the Private IP flag, it needs to be set both at pipeline creation time and run time. Run time resources override create time resources. However mount point must be set at create time but cannot be re-set at runtime... . ```; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""disks must not have mount points at run time"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""disks must not have mount points at run time"",; ""status"" : ""INVALID_ARGUMENT""; }; ```. This sets mount points to null for run time only. The rest is strictly identical to create time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507
https://github.com/broadinstitute/cromwell/pull/1507:66,Deployability,pipeline,pipeline,66,"In order to allow the Private IP flag, it needs to be set both at pipeline creation time and run time. Run time resources override create time resources. However mount point must be set at create time but cannot be re-set at runtime... . ```; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""disks must not have mount points at run time"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""disks must not have mount points at run time"",; ""status"" : ""INVALID_ARGUMENT""; }; ```. This sets mount points to null for run time only. The rest is strictly identical to create time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507
https://github.com/broadinstitute/cromwell/pull/1507:300,Integrability,message,message,300,"In order to allow the Private IP flag, it needs to be set both at pipeline creation time and run time. Run time resources override create time resources. However mount point must be set at create time but cannot be re-set at runtime... . ```; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""disks must not have mount points at run time"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""disks must not have mount points at run time"",; ""status"" : ""INVALID_ARGUMENT""; }; ```. This sets mount points to null for run time only. The rest is strictly identical to create time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507
https://github.com/broadinstitute/cromwell/pull/1507:392,Integrability,message,message,392,"In order to allow the Private IP flag, it needs to be set both at pipeline creation time and run time. Run time resources override create time resources. However mount point must be set at create time but cannot be re-set at runtime... . ```; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""disks must not have mount points at run time"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""disks must not have mount points at run time"",; ""status"" : ""INVALID_ARGUMENT""; }; ```. This sets mount points to null for run time only. The rest is strictly identical to create time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507
https://github.com/broadinstitute/cromwell/issues/1510:19,Performance,cache,cache,19,"If a call hits the cache but fails to use the cached call for whatever reason (output / detritus file not found etc...) , the cached call isn't invalidated so every other similar call will always fail to use the cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1510
https://github.com/broadinstitute/cromwell/issues/1510:46,Performance,cache,cached,46,"If a call hits the cache but fails to use the cached call for whatever reason (output / detritus file not found etc...) , the cached call isn't invalidated so every other similar call will always fail to use the cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1510
https://github.com/broadinstitute/cromwell/issues/1510:126,Performance,cache,cached,126,"If a call hits the cache but fails to use the cached call for whatever reason (output / detritus file not found etc...) , the cached call isn't invalidated so every other similar call will always fail to use the cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1510
https://github.com/broadinstitute/cromwell/issues/1510:212,Performance,cache,cache,212,"If a call hits the cache but fails to use the cached call for whatever reason (output / detritus file not found etc...) , the cached call isn't invalidated so every other similar call will always fail to use the cache.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1510
https://github.com/broadinstitute/cromwell/issues/1512:70,Modifiability,variab,variable,70,"Often we call a bash command in a task, read in the stdout, and set a variable to the string read in. When a cromwell job is submitted to SGE, however, cromwell prints the SGE job id to the stdout (don't know when or how often). When we then read from the stdout, the job id gets read in addition to the real output we are interested in. . The example test wdl can be found in /humgen/gsa-hpprojects/dev/tsato/wdl/testSGE.wdl. The output of `GetBamFileName` starts with the job-id + newline character. We don't want them. The same workflow runs as expected on a GSAx server.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1512
https://github.com/broadinstitute/cromwell/issues/1512:352,Testability,test,test,352,"Often we call a bash command in a task, read in the stdout, and set a variable to the string read in. When a cromwell job is submitted to SGE, however, cromwell prints the SGE job id to the stdout (don't know when or how often). When we then read from the stdout, the job id gets read in addition to the real output we are interested in. . The example test wdl can be found in /humgen/gsa-hpprojects/dev/tsato/wdl/testSGE.wdl. The output of `GetBamFileName` starts with the job-id + newline character. We don't want them. The same workflow runs as expected on a GSAx server.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1512
https://github.com/broadinstitute/cromwell/issues/1512:414,Testability,test,testSGE,414,"Often we call a bash command in a task, read in the stdout, and set a variable to the string read in. When a cromwell job is submitted to SGE, however, cromwell prints the SGE job id to the stdout (don't know when or how often). When we then read from the stdout, the job id gets read in addition to the real output we are interested in. . The example test wdl can be found in /humgen/gsa-hpprojects/dev/tsato/wdl/testSGE.wdl. The output of `GetBamFileName` starts with the job-id + newline character. We don't want them. The same workflow runs as expected on a GSAx server.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1512
https://github.com/broadinstitute/cromwell/issues/1513:953,Availability,error,error,953,"It would be great to be able to declare variables based on output from tasks. In this example WDL I want to use an array of all the shards from TaskA as input to TaskB even though TaskB is in the same scatter. ```; task TaskA {; File single_input_file. command {; #Something happens that takes a single file and returns a single file; }; output {; File output_single_file = ""some.file""; }; }. task TaskB {; Array[File] array_of_files; File single_input_file. command {; #Something happens that takes an array of files and a single file; }; }. workflow inputsFromScatter {; Array[File] list_of_files = [""a.file"", ""b.file"", ""c.file""]. scatter(file in list_of_files) {; call TaskA {; input:; single_input_file = file; }. call TaskB {; input:; single_input_file = file,; array_of_files = variable_declared_outside_of_scatter; }; }. Array[File] variable_declared_outside_of_scatter = TaskA.output_single_file; }; ```. This workflow currently results in this error: `Workflow input processing failed.; Workflow has invalid declarations: Could not find a value for TaskA`. I may have oversimplified this example because TaskB could be in a separate scatter. If you'd like a real example with actual tasks I'd be happy to provide it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1513
https://github.com/broadinstitute/cromwell/issues/1513:40,Modifiability,variab,variables,40,"It would be great to be able to declare variables based on output from tasks. In this example WDL I want to use an array of all the shards from TaskA as input to TaskB even though TaskB is in the same scatter. ```; task TaskA {; File single_input_file. command {; #Something happens that takes a single file and returns a single file; }; output {; File output_single_file = ""some.file""; }; }. task TaskB {; Array[File] array_of_files; File single_input_file. command {; #Something happens that takes an array of files and a single file; }; }. workflow inputsFromScatter {; Array[File] list_of_files = [""a.file"", ""b.file"", ""c.file""]. scatter(file in list_of_files) {; call TaskA {; input:; single_input_file = file; }. call TaskB {; input:; single_input_file = file,; array_of_files = variable_declared_outside_of_scatter; }; }. Array[File] variable_declared_outside_of_scatter = TaskA.output_single_file; }; ```. This workflow currently results in this error: `Workflow input processing failed.; Workflow has invalid declarations: Could not find a value for TaskA`. I may have oversimplified this example because TaskB could be in a separate scatter. If you'd like a real example with actual tasks I'd be happy to provide it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1513
https://github.com/broadinstitute/cromwell/issues/1514:25,Availability,error,error,25,"REST API correctly gives error on unknown status value:. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=blah; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: blah""; }; ```. and correctly gives results for status with exact case-sensitive spelling (""Failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=Failed; {; ""results"": [{; ""id"": ""8b8ce542-9e4a-4549-8fd3-c72ea325f392"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:15:38.214+10:00"",; ""end"": ""2016-10-05T11:15:38.217+10:00""; }, {; ""id"": ""1784a44d-e623-42fd-bf7a-90b00d300017"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:16:38.273+10:00"",; ""end"": ""2016-10-05T11:16:38.275+10:00""; }]; }; ```. But returns neither error nor results when given a status value that is only case-insensitively correct (""failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=failed; {; ""results"": []; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1514
https://github.com/broadinstitute/cromwell/issues/1514:785,Availability,error,error,785,"REST API correctly gives error on unknown status value:. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=blah; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: blah""; }; ```. and correctly gives results for status with exact case-sensitive spelling (""Failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=Failed; {; ""results"": [{; ""id"": ""8b8ce542-9e4a-4549-8fd3-c72ea325f392"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:15:38.214+10:00"",; ""end"": ""2016-10-05T11:15:38.217+10:00""; }, {; ""id"": ""1784a44d-e623-42fd-bf7a-90b00d300017"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:16:38.273+10:00"",; ""end"": ""2016-10-05T11:16:38.275+10:00""; }]; }; ```. But returns neither error nor results when given a status value that is only case-insensitively correct (""failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=failed; {; ""results"": []; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1514
https://github.com/broadinstitute/cromwell/issues/1514:182,Integrability,message,message,182,"REST API correctly gives error on unknown status value:. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=blah; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: blah""; }; ```. and correctly gives results for status with exact case-sensitive spelling (""Failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=Failed; {; ""results"": [{; ""id"": ""8b8ce542-9e4a-4549-8fd3-c72ea325f392"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:15:38.214+10:00"",; ""end"": ""2016-10-05T11:15:38.217+10:00""; }, {; ""id"": ""1784a44d-e623-42fd-bf7a-90b00d300017"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:16:38.273+10:00"",; ""end"": ""2016-10-05T11:16:38.275+10:00""; }]; }; ```. But returns neither error nor results when given a status value that is only case-insensitively correct (""failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=failed; {; ""results"": []; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1514
https://github.com/broadinstitute/cromwell/issues/1515:138,Availability,error,error-handling,138,"Using release 0.21.; There seems to be a discrepancy between [what the docs say should happen](https://github.com/broadinstitute/cromwell#error-handling) if you submit badly-formed WDL inputs to REST API, and what actually happens: which is that the inputs are accepted (get back a response with ""Submitted"" status), but later fail. Or am I misunderstanding the docs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515
https://github.com/broadinstitute/cromwell/issues/1515:6,Deployability,release,release,6,"Using release 0.21.; There seems to be a discrepancy between [what the docs say should happen](https://github.com/broadinstitute/cromwell#error-handling) if you submit badly-formed WDL inputs to REST API, and what actually happens: which is that the inputs are accepted (get back a response with ""Submitted"" status), but later fail. Or am I misunderstanding the docs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515
https://github.com/broadinstitute/cromwell/issues/1521:6,Performance,race condition,race conditions,6,These race conditions can leave Cromwell in an inconsistent state:; - The `WEA` might receive an `AbortedResponse` outside of the `Aborting` state. It should do something appropriate; - The `WEA` might receive something other than `AbortedResponse` while in the `Aborting` state. It should do something appropriate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521
https://github.com/broadinstitute/cromwell/issues/1521:98,Safety,Abort,AbortedResponse,98,These race conditions can leave Cromwell in an inconsistent state:; - The `WEA` might receive an `AbortedResponse` outside of the `Aborting` state. It should do something appropriate; - The `WEA` might receive something other than `AbortedResponse` while in the `Aborting` state. It should do something appropriate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521
https://github.com/broadinstitute/cromwell/issues/1521:131,Safety,Abort,Aborting,131,These race conditions can leave Cromwell in an inconsistent state:; - The `WEA` might receive an `AbortedResponse` outside of the `Aborting` state. It should do something appropriate; - The `WEA` might receive something other than `AbortedResponse` while in the `Aborting` state. It should do something appropriate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521
https://github.com/broadinstitute/cromwell/issues/1521:232,Safety,Abort,AbortedResponse,232,These race conditions can leave Cromwell in an inconsistent state:; - The `WEA` might receive an `AbortedResponse` outside of the `Aborting` state. It should do something appropriate; - The `WEA` might receive something other than `AbortedResponse` while in the `Aborting` state. It should do something appropriate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521
https://github.com/broadinstitute/cromwell/issues/1521:263,Safety,Abort,Aborting,263,These race conditions can leave Cromwell in an inconsistent state:; - The `WEA` might receive an `AbortedResponse` outside of the `Aborting` state. It should do something appropriate; - The `WEA` might receive something other than `AbortedResponse` while in the `Aborting` state. It should do something appropriate,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1521
https://github.com/broadinstitute/cromwell/issues/1536:85,Performance,cache,cache,85,Goal: Run a 20K Wide Scatter with 1800 Files created in each job and be able to call cache it.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1536
https://github.com/broadinstitute/cromwell/pull/1545:284,Availability,down,down,284,"tl;dr I tried replacing existing `Future[Option[A]]`s with monad transformers as requested by #1212, but unfortunately found them not to be a good fit for our use cases. I wouldn't recommend merging this PR in its current state. Problems:. 1) Monad transformer examples seem to break down into a ""happy path"" or at least ""substantive path"" (in the case of `Option`). A value is either mapped/flatMapped out of the fused monadic context on the happy path, or processing short circuits on the unhappy path and eventually results in a one-size-fits-all error. But this doesn't really fit the patterns of our APIs. Most of the `Future[Option[A]]`s on database APIs break down into 3 distinct states in engine or API code, while for initialization actors a failed `Future` is the only error case; if the inner `Option` is `None` that's perfectly fine but is handled in deeper business logic. 2) There are a couple of spots included in this PR where I could apply monad transformers. But IMHO the net effect of these changes is to ""Waldo"" these two APIs to be inconsistent with the others for reasons that are not obvious or particularly compelling. Also by the point above, this will restrict the way in which callers can use these APIs going forward.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1545
https://github.com/broadinstitute/cromwell/pull/1545:550,Availability,error,error,550,"tl;dr I tried replacing existing `Future[Option[A]]`s with monad transformers as requested by #1212, but unfortunately found them not to be a good fit for our use cases. I wouldn't recommend merging this PR in its current state. Problems:. 1) Monad transformer examples seem to break down into a ""happy path"" or at least ""substantive path"" (in the case of `Option`). A value is either mapped/flatMapped out of the fused monadic context on the happy path, or processing short circuits on the unhappy path and eventually results in a one-size-fits-all error. But this doesn't really fit the patterns of our APIs. Most of the `Future[Option[A]]`s on database APIs break down into 3 distinct states in engine or API code, while for initialization actors a failed `Future` is the only error case; if the inner `Option` is `None` that's perfectly fine but is handled in deeper business logic. 2) There are a couple of spots included in this PR where I could apply monad transformers. But IMHO the net effect of these changes is to ""Waldo"" these two APIs to be inconsistent with the others for reasons that are not obvious or particularly compelling. Also by the point above, this will restrict the way in which callers can use these APIs going forward.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1545
https://github.com/broadinstitute/cromwell/pull/1545:667,Availability,down,down,667,"tl;dr I tried replacing existing `Future[Option[A]]`s with monad transformers as requested by #1212, but unfortunately found them not to be a good fit for our use cases. I wouldn't recommend merging this PR in its current state. Problems:. 1) Monad transformer examples seem to break down into a ""happy path"" or at least ""substantive path"" (in the case of `Option`). A value is either mapped/flatMapped out of the fused monadic context on the happy path, or processing short circuits on the unhappy path and eventually results in a one-size-fits-all error. But this doesn't really fit the patterns of our APIs. Most of the `Future[Option[A]]`s on database APIs break down into 3 distinct states in engine or API code, while for initialization actors a failed `Future` is the only error case; if the inner `Option` is `None` that's perfectly fine but is handled in deeper business logic. 2) There are a couple of spots included in this PR where I could apply monad transformers. But IMHO the net effect of these changes is to ""Waldo"" these two APIs to be inconsistent with the others for reasons that are not obvious or particularly compelling. Also by the point above, this will restrict the way in which callers can use these APIs going forward.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1545
https://github.com/broadinstitute/cromwell/pull/1545:780,Availability,error,error,780,"tl;dr I tried replacing existing `Future[Option[A]]`s with monad transformers as requested by #1212, but unfortunately found them not to be a good fit for our use cases. I wouldn't recommend merging this PR in its current state. Problems:. 1) Monad transformer examples seem to break down into a ""happy path"" or at least ""substantive path"" (in the case of `Option`). A value is either mapped/flatMapped out of the fused monadic context on the happy path, or processing short circuits on the unhappy path and eventually results in a one-size-fits-all error. But this doesn't really fit the patterns of our APIs. Most of the `Future[Option[A]]`s on database APIs break down into 3 distinct states in engine or API code, while for initialization actors a failed `Future` is the only error case; if the inner `Option` is `None` that's perfectly fine but is handled in deeper business logic. 2) There are a couple of spots included in this PR where I could apply monad transformers. But IMHO the net effect of these changes is to ""Waldo"" these two APIs to be inconsistent with the others for reasons that are not obvious or particularly compelling. Also by the point above, this will restrict the way in which callers can use these APIs going forward.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1545
https://github.com/broadinstitute/cromwell/pull/1545:880,Testability,log,logic,880,"tl;dr I tried replacing existing `Future[Option[A]]`s with monad transformers as requested by #1212, but unfortunately found them not to be a good fit for our use cases. I wouldn't recommend merging this PR in its current state. Problems:. 1) Monad transformer examples seem to break down into a ""happy path"" or at least ""substantive path"" (in the case of `Option`). A value is either mapped/flatMapped out of the fused monadic context on the happy path, or processing short circuits on the unhappy path and eventually results in a one-size-fits-all error. But this doesn't really fit the patterns of our APIs. Most of the `Future[Option[A]]`s on database APIs break down into 3 distinct states in engine or API code, while for initialization actors a failed `Future` is the only error case; if the inner `Option` is `None` that's perfectly fine but is handled in deeper business logic. 2) There are a couple of spots included in this PR where I could apply monad transformers. But IMHO the net effect of these changes is to ""Waldo"" these two APIs to be inconsistent with the others for reasons that are not obvious or particularly compelling. Also by the point above, this will restrict the way in which callers can use these APIs going forward.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1545
https://github.com/broadinstitute/cromwell/issues/1555:486,Availability,ERROR,ERROR,486,"I was trying to run the [PublicPairedSingleSampleWf](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.wdl) using Cromwell 0.19.3 and it was consistently failing with the following:. ```; 2016-10-06 21:32:38,239 cromwell-system-akka.actor.default-dispatcher-41 INFO - JES Run [UUID(65d59b8b):SamToFastqAndBwaMem:1]: Status change from Running to Success; 2016-10-06 21:32:38,586 cromwell-system-akka.actor.default-dispatcher-4 ERROR -WorkflowActor [UUID(65d59b8b)]: Completion work failed for call SamToFastqAndBwaMem:1.; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell.jar:0.19]; <snip>; ```. I had run the previous version of the PublicPairedSingleSampleWf successfully. I finally realized that it had nothing to do with the workflow, but rather because I changed the GCS path for Cromwell's `baseExecutionBucket` to something much longer. Instead of my previous:. ```; gs://my-bucket/gatk-test/; ```. I had changed to:. ```; gs://my-bucket/broad_pipelines/PublicPairedSingleSampleWf_160927/; ```. When I went back and shortened the path, everything worked again. Along the way, I ran a similar test where I changed the task name for my test [vcf_chr_count.wdl](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/workflows/vcf_chr_count/vcf_chr_count.wdl) from:. ```; task vcf_split; ```. to . ```; task vcf_split_what_if_i_have_a_really_long_task_is_that_the_problem; ```. This also generated the same exception:. ```; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555
https://github.com/broadinstitute/cromwell/issues/1555:1658,Deployability,pipeline,pipelines-api-examples,1658,"airedSingleSampleWf](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.wdl) using Cromwell 0.19.3 and it was consistently failing with the following:. ```; 2016-10-06 21:32:38,239 cromwell-system-akka.actor.default-dispatcher-41 INFO - JES Run [UUID(65d59b8b):SamToFastqAndBwaMem:1]: Status change from Running to Success; 2016-10-06 21:32:38,586 cromwell-system-akka.actor.default-dispatcher-4 ERROR -WorkflowActor [UUID(65d59b8b)]: Completion work failed for call SamToFastqAndBwaMem:1.; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell.jar:0.19]; <snip>; ```. I had run the previous version of the PublicPairedSingleSampleWf successfully. I finally realized that it had nothing to do with the workflow, but rather because I changed the GCS path for Cromwell's `baseExecutionBucket` to something much longer. Instead of my previous:. ```; gs://my-bucket/gatk-test/; ```. I had changed to:. ```; gs://my-bucket/broad_pipelines/PublicPairedSingleSampleWf_160927/; ```. When I went back and shortened the path, everything worked again. Along the way, I ran a similar test where I changed the task name for my test [vcf_chr_count.wdl](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/workflows/vcf_chr_count/vcf_chr_count.wdl) from:. ```; task vcf_split; ```. to . ```; task vcf_split_what_if_i_have_a_really_long_task_is_that_the_problem; ```. This also generated the same exception:. ```; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555
https://github.com/broadinstitute/cromwell/issues/1555:1352,Testability,test,test,1352,"airedSingleSampleWf](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.wdl) using Cromwell 0.19.3 and it was consistently failing with the following:. ```; 2016-10-06 21:32:38,239 cromwell-system-akka.actor.default-dispatcher-41 INFO - JES Run [UUID(65d59b8b):SamToFastqAndBwaMem:1]: Status change from Running to Success; 2016-10-06 21:32:38,586 cromwell-system-akka.actor.default-dispatcher-4 ERROR -WorkflowActor [UUID(65d59b8b)]: Completion work failed for call SamToFastqAndBwaMem:1.; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell.jar:0.19]; <snip>; ```. I had run the previous version of the PublicPairedSingleSampleWf successfully. I finally realized that it had nothing to do with the workflow, but rather because I changed the GCS path for Cromwell's `baseExecutionBucket` to something much longer. Instead of my previous:. ```; gs://my-bucket/gatk-test/; ```. I had changed to:. ```; gs://my-bucket/broad_pipelines/PublicPairedSingleSampleWf_160927/; ```. When I went back and shortened the path, everything worked again. Along the way, I ran a similar test where I changed the task name for my test [vcf_chr_count.wdl](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/workflows/vcf_chr_count/vcf_chr_count.wdl) from:. ```; task vcf_split; ```. to . ```; task vcf_split_what_if_i_have_a_really_long_task_is_that_the_problem; ```. This also generated the same exception:. ```; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555
https://github.com/broadinstitute/cromwell/issues/1555:1557,Testability,test,test,1557,"airedSingleSampleWf](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.wdl) using Cromwell 0.19.3 and it was consistently failing with the following:. ```; 2016-10-06 21:32:38,239 cromwell-system-akka.actor.default-dispatcher-41 INFO - JES Run [UUID(65d59b8b):SamToFastqAndBwaMem:1]: Status change from Running to Success; 2016-10-06 21:32:38,586 cromwell-system-akka.actor.default-dispatcher-4 ERROR -WorkflowActor [UUID(65d59b8b)]: Completion work failed for call SamToFastqAndBwaMem:1.; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell.jar:0.19]; <snip>; ```. I had run the previous version of the PublicPairedSingleSampleWf successfully. I finally realized that it had nothing to do with the workflow, but rather because I changed the GCS path for Cromwell's `baseExecutionBucket` to something much longer. Instead of my previous:. ```; gs://my-bucket/gatk-test/; ```. I had changed to:. ```; gs://my-bucket/broad_pipelines/PublicPairedSingleSampleWf_160927/; ```. When I went back and shortened the path, everything worked again. Along the way, I ran a similar test where I changed the task name for my test [vcf_chr_count.wdl](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/workflows/vcf_chr_count/vcf_chr_count.wdl) from:. ```; task vcf_split; ```. to . ```; task vcf_split_what_if_i_have_a_really_long_task_is_that_the_problem; ```. This also generated the same exception:. ```; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555
https://github.com/broadinstitute/cromwell/issues/1555:1599,Testability,test,test,1599,"airedSingleSampleWf](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.wdl) using Cromwell 0.19.3 and it was consistently failing with the following:. ```; 2016-10-06 21:32:38,239 cromwell-system-akka.actor.default-dispatcher-41 INFO - JES Run [UUID(65d59b8b):SamToFastqAndBwaMem:1]: Status change from Running to Success; 2016-10-06 21:32:38,586 cromwell-system-akka.actor.default-dispatcher-4 ERROR -WorkflowActor [UUID(65d59b8b)]: Completion work failed for call SamToFastqAndBwaMem:1.; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell.jar:0.19]; <snip>; ```. I had run the previous version of the PublicPairedSingleSampleWf successfully. I finally realized that it had nothing to do with the workflow, but rather because I changed the GCS path for Cromwell's `baseExecutionBucket` to something much longer. Instead of my previous:. ```; gs://my-bucket/gatk-test/; ```. I had changed to:. ```; gs://my-bucket/broad_pipelines/PublicPairedSingleSampleWf_160927/; ```. When I went back and shortened the path, everything worked again. Along the way, I ran a similar test where I changed the task name for my test [vcf_chr_count.wdl](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/workflows/vcf_chr_count/vcf_chr_count.wdl) from:. ```; task vcf_split; ```. to . ```; task vcf_split_what_if_i_have_a_really_long_task_is_that_the_problem; ```. This also generated the same exception:. ```; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555
https://github.com/broadinstitute/cromwell/issues/1556:865,Availability,error,error,865,"Cromwell version 0.21; Running on Openstack + ubuntu 16.04 instance; Local backend with a docker container. I was working on creating a simple WDL for this workflow: https://github.com/ICGC-TCGA-PanCancer/pcawg_delly_workflow. Which looks a little like this:. ```; workflow PcawgDelly {; call SeqwareWorkflow; }. task SeqwareWorkflow {. String run_id; File reference_gc; File tumor_bam; File normal_bam; File reference_gz; String delly_id = ""embl-delly_1-3-0-preFilter.20150318"". command {; perl /usr/bin/run_seqware_workflow.pl \; --run-id ${run_id} \; --reference-gc ${reference_gc} \; --tumor-bam ${tumor_bam} \; --normal-bam ${normal_bam} \; --reference-gz ${reference_gz}; }. runtime {; docker: ""delly-docker-root""; }; }; ```. and I'm leaving out some details, but you get the idea, it's very simple. I would frequently get a failed Cromwell workflow, with an error in the logs like:. ```; mv: cannot stat/root/PcawgDelly/e173fd52-3c15-4b87-bfec-087c7cf0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently wit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556
https://github.com/broadinstitute/cromwell/issues/1556:1100,Availability,error,error,1100,"ting a simple WDL for this workflow: https://github.com/ICGC-TCGA-PanCancer/pcawg_delly_workflow. Which looks a little like this:. ```; workflow PcawgDelly {; call SeqwareWorkflow; }. task SeqwareWorkflow {. String run_id; File reference_gc; File tumor_bam; File normal_bam; File reference_gz; String delly_id = ""embl-delly_1-3-0-preFilter.20150318"". command {; perl /usr/bin/run_seqware_workflow.pl \; --run-id ${run_id} \; --reference-gc ${reference_gc} \; --tumor-bam ${tumor_bam} \; --normal-bam ${normal_bam} \; --reference-gz ${reference_gz}; }. runtime {; docker: ""delly-docker-root""; }; }; ```. and I'm leaving out some details, but you get the idea, it's very simple. I would frequently get a failed Cromwell workflow, with an error in the logs like:. ```; mv: cannot stat/root/PcawgDelly/e173fd52-3c15-4b87-bfec-087c7cf0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently without error. The new script.submit looks like:. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556
https://github.com/broadinstitute/cromwell/issues/1556:2006,Availability,error,error,2006,"f0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently without error. The new script.submit looks like:. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data-alex/cromwell-executions/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow:/root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow -i delly-docker-root /bin/bash /root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow/execution/script; ```. Basically, it runs the script from inside the docker container. I cannot, unfortunately, describe why the bug happened in this seemingly rare case, other than pointing at the dangers of shell commands being interpreted from pipes/stdin and waving my hands a lot. But, I do think avoiding input redirection of commands into bash is probably a good thing, in this case. Am I missing some case where it's necessary?. Luckily I think the fix is simple, and probably just involves updating the core conf. here:; core/src/main/resources",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556
https://github.com/broadinstitute/cromwell/issues/1556:3003,Modifiability,config,config,3003,"e the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently without error. The new script.submit looks like:. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data-alex/cromwell-executions/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow:/root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow -i delly-docker-root /bin/bash /root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow/execution/script; ```. Basically, it runs the script from inside the docker container. I cannot, unfortunately, describe why the bug happened in this seemingly rare case, other than pointing at the dangers of shell commands being interpreted from pipes/stdin and waving my hands a lot. But, I do think avoiding input redirection of commands into bash is probably a good thing, in this case. Am I missing some case where it's necessary?. Luckily I think the fix is simple, and probably just involves updating the core conf. here:; core/src/main/resources/reference.conf. I was able to provide my own config file overrides for now as a workaround. Thanks! Sorry for the super long bug report.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556
https://github.com/broadinstitute/cromwell/issues/1556:2706,Safety,avoid,avoiding,2706,"e the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently without error. The new script.submit looks like:. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data-alex/cromwell-executions/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow:/root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow -i delly-docker-root /bin/bash /root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow/execution/script; ```. Basically, it runs the script from inside the docker container. I cannot, unfortunately, describe why the bug happened in this seemingly rare case, other than pointing at the dangers of shell commands being interpreted from pipes/stdin and waving my hands a lot. But, I do think avoiding input redirection of commands into bash is probably a good thing, in this case. Am I missing some case where it's necessary?. Luckily I think the fix is simple, and probably just involves updating the core conf. here:; core/src/main/resources/reference.conf. I was able to provide my own config file overrides for now as a workaround. Thanks! Sorry for the super long bug report.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556
https://github.com/broadinstitute/cromwell/issues/1556:878,Testability,log,logs,878,"Cromwell version 0.21; Running on Openstack + ubuntu 16.04 instance; Local backend with a docker container. I was working on creating a simple WDL for this workflow: https://github.com/ICGC-TCGA-PanCancer/pcawg_delly_workflow. Which looks a little like this:. ```; workflow PcawgDelly {; call SeqwareWorkflow; }. task SeqwareWorkflow {. String run_id; File reference_gc; File tumor_bam; File normal_bam; File reference_gz; String delly_id = ""embl-delly_1-3-0-preFilter.20150318"". command {; perl /usr/bin/run_seqware_workflow.pl \; --run-id ${run_id} \; --reference-gc ${reference_gc} \; --tumor-bam ${tumor_bam} \; --normal-bam ${normal_bam} \; --reference-gz ${reference_gz}; }. runtime {; docker: ""delly-docker-root""; }; }; ```. and I'm leaving out some details, but you get the idea, it's very simple. I would frequently get a failed Cromwell workflow, with an error in the logs like:. ```; mv: cannot stat/root/PcawgDelly/e173fd52-3c15-4b87-bfec-087c7cf0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently wit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556
https://github.com/broadinstitute/cromwell/issues/1556:136,Usability,simpl,simple,136,"Cromwell version 0.21; Running on Openstack + ubuntu 16.04 instance; Local backend with a docker container. I was working on creating a simple WDL for this workflow: https://github.com/ICGC-TCGA-PanCancer/pcawg_delly_workflow. Which looks a little like this:. ```; workflow PcawgDelly {; call SeqwareWorkflow; }. task SeqwareWorkflow {. String run_id; File reference_gc; File tumor_bam; File normal_bam; File reference_gz; String delly_id = ""embl-delly_1-3-0-preFilter.20150318"". command {; perl /usr/bin/run_seqware_workflow.pl \; --run-id ${run_id} \; --reference-gc ${reference_gc} \; --tumor-bam ${tumor_bam} \; --normal-bam ${normal_bam} \; --reference-gz ${reference_gz}; }. runtime {; docker: ""delly-docker-root""; }; }; ```. and I'm leaving out some details, but you get the idea, it's very simple. I would frequently get a failed Cromwell workflow, with an error in the logs like:. ```; mv: cannot stat/root/PcawgDelly/e173fd52-3c15-4b87-bfec-087c7cf0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently wit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556
https://github.com/broadinstitute/cromwell/issues/1556:798,Usability,simpl,simple,798,"Cromwell version 0.21; Running on Openstack + ubuntu 16.04 instance; Local backend with a docker container. I was working on creating a simple WDL for this workflow: https://github.com/ICGC-TCGA-PanCancer/pcawg_delly_workflow. Which looks a little like this:. ```; workflow PcawgDelly {; call SeqwareWorkflow; }. task SeqwareWorkflow {. String run_id; File reference_gc; File tumor_bam; File normal_bam; File reference_gz; String delly_id = ""embl-delly_1-3-0-preFilter.20150318"". command {; perl /usr/bin/run_seqware_workflow.pl \; --run-id ${run_id} \; --reference-gc ${reference_gc} \; --tumor-bam ${tumor_bam} \; --normal-bam ${normal_bam} \; --reference-gz ${reference_gz}; }. runtime {; docker: ""delly-docker-root""; }; }; ```. and I'm leaving out some details, but you get the idea, it's very simple. I would frequently get a failed Cromwell workflow, with an error in the logs like:. ```; mv: cannot stat/root/PcawgDelly/e173fd52-3c15-4b87-bfec-087c7cf0a4ac/call-SeqwareWorkflow/execution/rc.tmp': No such file or directory`; ```. I tried to come up with a minimal WDL that would reproduce the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently wit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556
https://github.com/broadinstitute/cromwell/issues/1556:2868,Usability,simpl,simple,2868,"e the error, but so far I can only get it from this workflow, possibly because of running time, IO, perl, seqware, I don't know. After much headdesking, I am nearly certain I have fixed the issue by changing the way cromwell executes a call. As you know, Cromwell generates a script.submit file which looks like this (in this case):. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow:/root/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow -i delly-docker-root /bin/bash < /home/ubuntu/projects/debug-small-data/cromwell-executions/PcawgDelly/ebce533d-9f49-4e6d-8512-db7a182d3365/call-SeqwareWorkflow/execution/script; ```. By removing input redirection into bash (i.e. removing the ""<"" character and changing the paths) I can get this workflow to run consistently without error. The new script.submit looks like:. ``` bash; #!/bin/bash; docker run --rm -v /home/ubuntu/projects/debug-small-data-alex/cromwell-executions/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow:/root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow -i delly-docker-root /bin/bash /root/PcawgDelly/bdc567a9-b1f7-4530-853a-5263b890ca6a/call-SeqwareWorkflow/execution/script; ```. Basically, it runs the script from inside the docker container. I cannot, unfortunately, describe why the bug happened in this seemingly rare case, other than pointing at the dangers of shell commands being interpreted from pipes/stdin and waving my hands a lot. But, I do think avoiding input redirection of commands into bash is probably a good thing, in this case. Am I missing some case where it's necessary?. Luckily I think the fix is simple, and probably just involves updating the core conf. here:; core/src/main/resources/reference.conf. I was able to provide my own config file overrides for now as a workaround. Thanks! Sorry for the super long bug report.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1556
https://github.com/broadinstitute/cromwell/pull/1557:25,Testability,Test,Testing,25,...to Enable Cromwell 21 Testing!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1557
https://github.com/broadinstitute/cromwell/pull/1558:52,Modifiability,config,config,52,Some builds were failing to pullapprove because the config contains a lost collaborator :(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1558
https://github.com/broadinstitute/cromwell/pull/1559:44,Performance,cache,cache,44,"When a backend fails to copy outputs from a cache hit, invalidates that cache hit for the future - and try potential other hits found. Otherwise run the job normally.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1559
https://github.com/broadinstitute/cromwell/pull/1559:72,Performance,cache,cache,72,"When a backend fails to copy outputs from a cache hit, invalidates that cache hit for the future - and try potential other hits found. Otherwise run the job normally.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1559
https://github.com/broadinstitute/cromwell/issues/1561:57,Availability,error,error,57,"Tried to pass string to wdl input of type ""File"", got no error and workflow failed. . I ran this in firecloud, and my expression for ""id"" evaluated as a string. Here's my wdl:. ```; task get_mutations_across_intervals {; File id; File orignal_maf; Array[File] mafs. command {; python /opt/make_intervals_v2.py ${id} ${orignal_maf} ${sep="" "" mafs}; }. runtime {; docker: ""gcr.io/broad-firecloud-itools/pysam""; }. output {; File interval_maf = ""${id}.forecallready.maf""; }; }. workflow get_mutations_across_intervals_wkfl {; call get_mutations_across_intervals; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1561
https://github.com/broadinstitute/cromwell/pull/1563:12,Safety,avoid,avoid,12,Intended to avoid multiple people iterating on this having to regenerate the same set of placeholders:; - Project definition in sbt; - Basic ecs backend factory; - Better defaults in the BackendLifecycleActorFactory trait; - Fixed a few typos in nearby files... 😳,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1563
https://github.com/broadinstitute/cromwell/pull/1565:22,Deployability,update,updated,22,Candidate branch with updated versions and release notes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1565
https://github.com/broadinstitute/cromwell/pull/1565:43,Deployability,release,release,43,Candidate branch with updated versions and release notes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1565
https://github.com/broadinstitute/cromwell/issues/1566:791,Deployability,PATCH,PATCH,791,"As a high volume production user, I'm glad that Cromwell now has a decoupled mechanism for submitting workflows versus running them. I often will submit more workflows than my server or google quotas can handle and so I keep the maximum running workflows set to a smaller number of workflows than I submit. Previously I would have to keep my own queue and dribble workflows into Cromwell to get the same effect. However, one thing I have lost is the ability to prioritize those workflows! I need to be able to prioritize those submitted workflows that have not started running. This feature has several components:; - ability to specify a priority (integer) when submitting a workflow; - ability to set a priority for a submitted workflow (ok to not check it's status) [new REST endpoint to PATCH a workflow]; - change the query for the polling mechanism that picks up workflows to sort by descending priority (e.g. 1 is highest priority) when selecting workflows. While users can change the priority of a running workflow, this has no effect. It is the priority to start a workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566
https://github.com/broadinstitute/cromwell/issues/1566:346,Performance,queue,queue,346,"As a high volume production user, I'm glad that Cromwell now has a decoupled mechanism for submitting workflows versus running them. I often will submit more workflows than my server or google quotas can handle and so I keep the maximum running workflows set to a smaller number of workflows than I submit. Previously I would have to keep my own queue and dribble workflows into Cromwell to get the same effect. However, one thing I have lost is the ability to prioritize those workflows! I need to be able to prioritize those submitted workflows that have not started running. This feature has several components:; - ability to specify a priority (integer) when submitting a workflow; - ability to set a priority for a submitted workflow (ok to not check it's status) [new REST endpoint to PATCH a workflow]; - change the query for the polling mechanism that picks up workflows to sort by descending priority (e.g. 1 is highest priority) when selecting workflows. While users can change the priority of a running workflow, this has no effect. It is the priority to start a workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566
https://github.com/broadinstitute/cromwell/issues/1569:231,Availability,echo,echo,231,"To wire up the File output, either by running the file delocalisation in the execution script, or by copying the required files to a known mounted volume. Success criteria: we can run a WDL such as:. ```; task fileOut {; command { echo ""hello, amazon"" > myFile }; output { File outFile = ""myFile"" }; }. workflow {; call fileOut; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1569
https://github.com/broadinstitute/cromwell/issues/1571:49,Testability,mock,mock-jes,49,Recent changes in Cromwell has made it such that mock-jes no longer works. . https://www.youtube.com/watch?v=DtRNg5uSKQ0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1571
https://github.com/broadinstitute/cromwell/issues/1573:147,Testability,log,log,147,"The AWS needs to be able to correctly fill in the `JobExecutionResult` on completion. This isn't really a good enough long-term answer:. ```; [51] log.info(""AWS task completed!\n{}"", taskDescription.toString); [52] SucceededResponse(jobDescriptor.key, Option(0), Map.empty, None, Seq.empty); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1573
https://github.com/broadinstitute/cromwell/issues/1577:264,Availability,down,downstreams,264,"Cromwell keeps track of the status of the DAG with an internal map of `Scope`s.; This map should probably now reference `GraphNodes` which are a subset of the `Scope`s and represent more accurately a node in the graph, with associated traversal methods (upstream, downstreams etc...). `Declaration`s are now `Scope`s (and even (`GraphNode`s). This should allow WDL like. ```; ...; workflow wf {; call A; String dec = A.out; call B { input: b_input = dec }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1577
https://github.com/broadinstitute/cromwell/issues/1581:197,Testability,test,test,197,"When switching to the last version of WDL4s, one noticeable change is that a task cannot share its name with the workflow name. I realized that there are close to a dozen ""hello world"" WDLs in the test suite scattered across projects but also duplicated within the same project. This probable is somehow close to the idea of having a canonical test suite for backends that we talked about before.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581
https://github.com/broadinstitute/cromwell/issues/1581:344,Testability,test,test,344,"When switching to the last version of WDL4s, one noticeable change is that a task cannot share its name with the workflow name. I realized that there are close to a dozen ""hello world"" WDLs in the test suite scattered across projects but also duplicated within the same project. This probable is somehow close to the idea of having a canonical test suite for backends that we talked about before.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581
https://github.com/broadinstitute/cromwell/issues/1587:148,Performance,cache,cache,148,"As FireCloud, I would like to enable call caching. Because this is a multi-tenant system, and Cromwell knows nothing about users, I may try to call cache to a call where I don't have permission to read the files. Currently (due to #1510) this would cause that cache hit to be ejected from the cache, and thus unavailable to users who do have the right file permissions. I would like to have a option in cromwell which would allow me to disable this invalidation step. When I attempt to call cache, and the cache hit fails due to a permission problem I simply attempt to cache using the next hit (if there are any). @abaumann -- please comment with any changes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587
https://github.com/broadinstitute/cromwell/issues/1587:260,Performance,cache,cache,260,"As FireCloud, I would like to enable call caching. Because this is a multi-tenant system, and Cromwell knows nothing about users, I may try to call cache to a call where I don't have permission to read the files. Currently (due to #1510) this would cause that cache hit to be ejected from the cache, and thus unavailable to users who do have the right file permissions. I would like to have a option in cromwell which would allow me to disable this invalidation step. When I attempt to call cache, and the cache hit fails due to a permission problem I simply attempt to cache using the next hit (if there are any). @abaumann -- please comment with any changes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587
https://github.com/broadinstitute/cromwell/issues/1587:293,Performance,cache,cache,293,"As FireCloud, I would like to enable call caching. Because this is a multi-tenant system, and Cromwell knows nothing about users, I may try to call cache to a call where I don't have permission to read the files. Currently (due to #1510) this would cause that cache hit to be ejected from the cache, and thus unavailable to users who do have the right file permissions. I would like to have a option in cromwell which would allow me to disable this invalidation step. When I attempt to call cache, and the cache hit fails due to a permission problem I simply attempt to cache using the next hit (if there are any). @abaumann -- please comment with any changes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587
https://github.com/broadinstitute/cromwell/issues/1587:491,Performance,cache,cache,491,"As FireCloud, I would like to enable call caching. Because this is a multi-tenant system, and Cromwell knows nothing about users, I may try to call cache to a call where I don't have permission to read the files. Currently (due to #1510) this would cause that cache hit to be ejected from the cache, and thus unavailable to users who do have the right file permissions. I would like to have a option in cromwell which would allow me to disable this invalidation step. When I attempt to call cache, and the cache hit fails due to a permission problem I simply attempt to cache using the next hit (if there are any). @abaumann -- please comment with any changes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587
https://github.com/broadinstitute/cromwell/issues/1587:506,Performance,cache,cache,506,"As FireCloud, I would like to enable call caching. Because this is a multi-tenant system, and Cromwell knows nothing about users, I may try to call cache to a call where I don't have permission to read the files. Currently (due to #1510) this would cause that cache hit to be ejected from the cache, and thus unavailable to users who do have the right file permissions. I would like to have a option in cromwell which would allow me to disable this invalidation step. When I attempt to call cache, and the cache hit fails due to a permission problem I simply attempt to cache using the next hit (if there are any). @abaumann -- please comment with any changes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587
https://github.com/broadinstitute/cromwell/issues/1587:570,Performance,cache,cache,570,"As FireCloud, I would like to enable call caching. Because this is a multi-tenant system, and Cromwell knows nothing about users, I may try to call cache to a call where I don't have permission to read the files. Currently (due to #1510) this would cause that cache hit to be ejected from the cache, and thus unavailable to users who do have the right file permissions. I would like to have a option in cromwell which would allow me to disable this invalidation step. When I attempt to call cache, and the cache hit fails due to a permission problem I simply attempt to cache using the next hit (if there are any). @abaumann -- please comment with any changes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587
https://github.com/broadinstitute/cromwell/issues/1587:552,Usability,simpl,simply,552,"As FireCloud, I would like to enable call caching. Because this is a multi-tenant system, and Cromwell knows nothing about users, I may try to call cache to a call where I don't have permission to read the files. Currently (due to #1510) this would cause that cache hit to be ejected from the cache, and thus unavailable to users who do have the right file permissions. I would like to have a option in cromwell which would allow me to disable this invalidation step. When I attempt to call cache, and the cache hit fails due to a permission problem I simply attempt to cache using the next hit (if there are any). @abaumann -- please comment with any changes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587
https://github.com/broadinstitute/cromwell/pull/1589:505,Deployability,update,updates,505,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589
https://github.com/broadinstitute/cromwell/pull/1589:136,Integrability,depend,dependencies,136,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589
https://github.com/broadinstitute/cromwell/pull/1589:184,Integrability,depend,dependency,184,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589
https://github.com/broadinstitute/cromwell/pull/1589:258,Integrability,depend,dependencies,258,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589
https://github.com/broadinstitute/cromwell/pull/1589:356,Safety,timeout,timeout,356,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589
https://github.com/broadinstitute/cromwell/pull/1589:95,Testability,mock,mock,95,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589
https://github.com/broadinstitute/cromwell/issues/1590:50,Deployability,configurat,configuration,50,Anyone trying to set up cromwell in a non-default configuration (i.e. one that requires a custom `application.conf`) has to find all relevant stanzas within the documentation (and these are not all together). We should provide a default template that works with the latest release. Then a user can simply modify that default template and get up-and-running.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590
https://github.com/broadinstitute/cromwell/issues/1590:273,Deployability,release,release,273,Anyone trying to set up cromwell in a non-default configuration (i.e. one that requires a custom `application.conf`) has to find all relevant stanzas within the documentation (and these are not all together). We should provide a default template that works with the latest release. Then a user can simply modify that default template and get up-and-running.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590
https://github.com/broadinstitute/cromwell/issues/1590:50,Modifiability,config,configuration,50,Anyone trying to set up cromwell in a non-default configuration (i.e. one that requires a custom `application.conf`) has to find all relevant stanzas within the documentation (and these are not all together). We should provide a default template that works with the latest release. Then a user can simply modify that default template and get up-and-running.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590
https://github.com/broadinstitute/cromwell/issues/1590:298,Usability,simpl,simply,298,Anyone trying to set up cromwell in a non-default configuration (i.e. one that requires a custom `application.conf`) has to find all relevant stanzas within the documentation (and these are not all together). We should provide a default template that works with the latest release. Then a user can simply modify that default template and get up-and-running.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590
https://github.com/broadinstitute/cromwell/issues/1591:124,Deployability,configurat,configuration,124,"When using MySQL for call caching, I am getting peppered with the message below. I realize that this is technically a MySQL configuration issue, but since it will happen to most cromwell users by default, can we include some documentation to get rid of it?. ```; Tue Oct 18 14:03:44 UTC 2016 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591
https://github.com/broadinstitute/cromwell/issues/1591:66,Integrability,message,message,66,"When using MySQL for call caching, I am getting peppered with the message below. I realize that this is technically a MySQL configuration issue, but since it will happen to most cromwell users by default, can we include some documentation to get rid of it?. ```; Tue Oct 18 14:03:44 UTC 2016 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591
https://github.com/broadinstitute/cromwell/issues/1591:124,Modifiability,config,configuration,124,"When using MySQL for call caching, I am getting peppered with the message below. I realize that this is technically a MySQL configuration issue, but since it will happen to most cromwell users by default, can we include some documentation to get rid of it?. ```; Tue Oct 18 14:03:44 UTC 2016 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591
https://github.com/broadinstitute/cromwell/issues/1591:753,Security,certificate,certificate,753,"When using MySQL for call caching, I am getting peppered with the message below. I realize that this is technically a MySQL configuration issue, but since it will happen to most cromwell users by default, can we include some documentation to get rid of it?. ```; Tue Oct 18 14:03:44 UTC 2016 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1591
https://github.com/broadinstitute/cromwell/issues/1593:317,Performance,Queue,Queue,317,"When looking at the output of cromwell (0.22) single workflow mode (local backend), it would be useful to have output that listed the number of completed jobs and remaining jobs. Currently, it is not possible to determine whether the jobs are finishing, hanging, etc in single workflow mode. The behavior in GATK 3.x Queue can serve as a model.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1593
https://github.com/broadinstitute/cromwell/issues/1594:911,Integrability,message,messages,911,"- Single workflow mode; - Local backend (using throttling in custom application.conf); - 0.22; - using docker images; - call-caching enabled (localhost mysql instance); - all data is on local filesystem (not even shared filesystem); - N=2; - One time took 6 minutes before I did ctl-C. The second time it was left overnight and never completed. I did notice that (before I hit Ctl-C) cromwell got the shutdown signal and was aborting running jobs, even though there were none. If this was desired behavior, is there a flag to disable?. What other information can I provide? WDL? application.conf is attached. No other `-D` command line parameters were used. [local_application.conf.txt](https://github.com/broadinstitute/cromwell/files/539083/local_application.conf.txt). I am attempting to run cromwell as part of a larger shell script and I am positive that cromwell is not exiting (I still see MySQL warning messages). The workflow results appear to be there and no jobs are running (according to `top -c`)...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594
https://github.com/broadinstitute/cromwell/issues/1594:425,Safety,abort,aborting,425,"- Single workflow mode; - Local backend (using throttling in custom application.conf); - 0.22; - using docker images; - call-caching enabled (localhost mysql instance); - all data is on local filesystem (not even shared filesystem); - N=2; - One time took 6 minutes before I did ctl-C. The second time it was left overnight and never completed. I did notice that (before I hit Ctl-C) cromwell got the shutdown signal and was aborting running jobs, even though there were none. If this was desired behavior, is there a flag to disable?. What other information can I provide? WDL? application.conf is attached. No other `-D` command line parameters were used. [local_application.conf.txt](https://github.com/broadinstitute/cromwell/files/539083/local_application.conf.txt). I am attempting to run cromwell as part of a larger shell script and I am positive that cromwell is not exiting (I still see MySQL warning messages). The workflow results appear to be there and no jobs are running (according to `top -c`)...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594
https://github.com/broadinstitute/cromwell/issues/1595:515,Performance,concurren,concurrent,515,"Seen while trying to recreate an issue for @LeeTL1220. Using docker on mac (full version info at the bottom of this ticket). After making around 50 docker calls locally, the docker service becomes unresponsive. Hence, Cromwell waits forever for a workflow that never ends. . At this stage I cannot tell whether this is:; - A bug in docker we must work around; - A bug in how Cromwell is calling docker; - Something else. Regardless, this is preventing me from running a lot of staggered jobs locally (even with the concurrent job limits). Docker version info:. ```; chrisl@wmf3e-823 [develop] cromwell $ docker version; Client:; Version: 1.12.1; API version: 1.24; Go version: go1.7.1; Git commit: 6f9534c; Built: Thu Sep 8 10:31:18 2016; OS/Arch: darwin/amd64. Server:; Version: 1.12.1; API version: 1.24; Go version: go1.6.3; Git commit: 23cf638; Built: Thu Aug 18 17:52:38 2016; OS/Arch: linux/amd64; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1595
https://github.com/broadinstitute/cromwell/issues/1597:1049,Deployability,update,updateDigest,1049,"e/cromwell/files/539674/local_application.conf.txt). conf file attached in case I got something wrong... Stack trace where a lot of time seems to be spent... ```; ""cromwell-system-akka.actor.default-dispatcher-16"" #50 prio=5 os_prio=0 tid=0x00007f2fb0054800 nid=0x956 runnable [0x00007f301befc000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.FileDispatcherImpl.read0(Native Method); at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleAct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1341,Modifiability,config,config,1341,07f301befc000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.FileDispatcherImpl.read0(Native Method); at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1387,Modifiability,Config,ConfigHashingStrategy,1387,t sun.nio.ch.FileDispatcherImpl.read0(Native Method); at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingAc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1449,Modifiability,config,config,1449,sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1495,Modifiability,Config,ConfigHashingStrategy,1495,pl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1768,Modifiability,config,config,1768,(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1797,Modifiability,Config,ConfigHashingStrategy,1797,un.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCel,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1859,Modifiability,config,config,1859,:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1866,Modifiability,Config,ConfigHashingStrategy,1866,io.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1896,Modifiability,Config,ConfigHashingStrategy,1896,ead(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.proce,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1958,Modifiability,config,config,1958,2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1965,Modifiability,Config,ConfigBackendLifecycleActorFactory,1965,m); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:2037,Modifiability,Config,ConfigBackendLifecycleActorFactory,2037,dateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:2112,Modifiability,config,config,2112,gest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.co,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:2119,Modifiability,Config,ConfigBackendLifecycleActorFactory,2119,ava:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:2191,Modifiability,Config,ConfigBackendLifecycleActorFactory,2191,ls.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurr,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:3041,Performance,concurren,concurrent,3041,".impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Snippet from attached conf file:. ```; ...snip...; local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; hashing {; strategy: ""path"" ; check-sibling-md5: true; } ; }; ...snip... ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:3114,Performance,concurren,concurrent,3114,".impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Snippet from attached conf file:. ```; ...snip...; local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; hashing {; strategy: ""path"" ; check-sibling-md5: true; } ; }; ...snip... ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:3199,Performance,concurren,concurrent,3199,".impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Snippet from attached conf file:. ```; ...snip...; local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; hashing {; strategy: ""path"" ; check-sibling-md5: true; } ; }; ...snip... ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:3276,Performance,concurren,concurrent,3276,".impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Snippet from attached conf file:. ```; ...snip...; local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; hashing {; strategy: ""path"" ; check-sibling-md5: true; } ; }; ...snip... ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1348,Security,Hash,HashFileStrategy,1348,.lang.Thread.State: RUNNABLE; at sun.nio.ch.FileDispatcherImpl.read0(Native Method); at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.bac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1374,Security,hash,hash,1374,.lang.Thread.State: RUNNABLE; at sun.nio.ch.FileDispatcherImpl.read0(Native Method); at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.bac,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1456,Security,Hash,HashFileStrategy,1456,tcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.O,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1482,Security,hash,hash,1482,tcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.O,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1775,Security,Hash,HashFileStrategy,1775,tStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:1792,Security,hash,hash,1792,un.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCel,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1597:3872,Security,hash,hashing,3872,".impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleActorFactory.scala:45); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1$$anonfun$1.apply(FileHashingActor.scala:21); at scala.Option.map(Option.scala:146); at cromwell.backend.callcaching.FileHashingActor$$anonfun$receive$1.applyOrElse(FileHashingActor.scala:21); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.callcaching.FileHashingActor.aroundReceive(FileHashingActor.scala:16); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. Snippet from attached conf file:. ```; ...snip...; local {; // Cromwell makes a link to your input files within <root>/<workflow UUID>/workflow-inputs; // The following are strategies used to make those links. They are ordered. If one fails; // The next one is tried:; //; // hard-link: attempt to create a hard-link to the file; // copy: copy the file; // soft-link: create a symbolic link to the file; //; // NOTE: soft-link will be skipped for Docker jobs; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; hashing {; strategy: ""path"" ; check-sibling-md5: true; } ; }; ...snip... ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597
https://github.com/broadinstitute/cromwell/issues/1598:43,Security,hash,hashing,43,"An incorrect key was being used to specify hashing strategy, but Cromwell didn't complain and the user wrongly thought their value was accepted.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598
https://github.com/broadinstitute/cromwell/issues/1600:117,Availability,down,down,117,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:985,Integrability,message,message,985,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:3970,Integrability,message,message,3970,"IET false --VALIDATION_STRINGENCY LENIENT; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: executing: docker run --rm -v /home/lichtens/test_eval/cromwell-ex; ecutions/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12:/root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12; -i broadinstitute/gatk-protected:3c44b2f93e29e360af41ba403465df02931f8e86 /bin/bash < /home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160; b0/call-HetPulldown/shard-12/execution/script; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: command: ""/bin/bash"" ""/home/lichtens/test_eval/cromwell-executions; /case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/execution/script.submit""; [2016-10-19 18:29:50,54] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: job id: 9788; [2016-10-19 18:30:17,05] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:4:; 1; [2016-10-19 18:30:17,10] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:7:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-ba; e7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:7:1/51ee236f-c31; a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:7:1#132070105 ; .... snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:210,Safety,Abort,Aborting,210,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:298,Safety,Abort,Aborting,298,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:389,Safety,Abort,Abort,389,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:405,Safety,Abort,Aborting,405,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:505,Safety,abort,abort,505,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:609,Safety,abort,abort,609,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:773,Safety,abort,aborted,773,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1600:3758,Safety,abort,aborted,3758,"IET false --VALIDATION_STRINGENCY LENIENT; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: executing: docker run --rm -v /home/lichtens/test_eval/cromwell-ex; ecutions/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12:/root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12; -i broadinstitute/gatk-protected:3c44b2f93e29e360af41ba403465df02931f8e86 /bin/bash < /home/lichtens/test_eval/cromwell-executions/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160; b0/call-HetPulldown/shard-12/execution/script; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: command: ""/bin/bash"" ""/home/lichtens/test_eval/cromwell-executions; /case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/execution/script.submit""; [2016-10-19 18:29:50,54] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: job id: 9788; [2016-10-19 18:30:17,05] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:4:; 1; [2016-10-19 18:30:17,10] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:7:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-ba; e7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:7:1/51ee236f-c31; a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:7:1#132070105 ; .... snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600
https://github.com/broadinstitute/cromwell/issues/1603:1318,Testability,log,logs,1318,"Given a single workflow and a specific task, the time spent in the ""Final Cromwell Overhead"" state increases everytime the workflow is set to scatter wider. Anything wider than 50 jobs ends up with a lot of leftover jobs in ""Running"" state and Cromwell just hangs. This is all being run on the SGE backend, no UGER. For example:; Given a 50 wide scatter, the average amount of time spent ""RunningJob"" is ~30 seconds and the amount of time spent in ""Final Cromwell Overhead"" is about ~300 seconds; Given a 250 wide scatter, average amount of time spent in ""Final Cromwell Overhead"" is +1.5 hours; Given a 350 wide scatter, average amount of time spent in ""Final Cromwell Overhead"" is +2 hours. <img width=""942"" alt=""250-wide-timing"" src=""https://cloud.githubusercontent.com/assets/14941133/19537736/10b406f6-961f-11e6-9fa0-03ac5275981e.png"">; <img width=""1202"" alt=""50-wide-timing"" src=""https://cloud.githubusercontent.com/assets/14941133/19537737/10b60f32-961f-11e6-8246-c42168d9189e.png"">; <img width=""1314"" alt=""350-wide-timing"" src=""https://cloud.githubusercontent.com/assets/14941133/19537735/10b1d39a-961f-11e6-8d24-fcb2cfe3459a.png"">; [Uploading mh-serverLogs_22.txt…](). Attaching the thread dump. ; [megan_thread_dump.txt](https://github.com/broadinstitute/cromwell/files/540239/megan_thread_dump.txt). Server logs can be found here: /humgen/gsa-hpprojects/dev/mshand/palantir/Analysis/398_FatPandaAutomation/serverlogs22.txt /users/rmunshi/mh-serverLogs_22.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1603
https://github.com/broadinstitute/cromwell/issues/1605:485,Performance,optimiz,optimize,485,"A user had the below features request, which I agreed to pass on to you here. . > It would be nice to have a setting where the user could choose to automatically split an interval file into contigs, or an arbitrary number, for the scatter gather process. I'm sure there are more optimal ways to split interval files, I think I saw in your example script on your github that you chose to split your interval list into 50 pieces due to your local server setup? Not every user is able to optimize an individual analysis, so having the option to automatically split the interval list into contigs is probably better than running it serially. We will probably set up a bash script for it, but having it built in could be usable for others too.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605
https://github.com/broadinstitute/cromwell/issues/1605:716,Usability,usab,usable,716,"A user had the below features request, which I agreed to pass on to you here. . > It would be nice to have a setting where the user could choose to automatically split an interval file into contigs, or an arbitrary number, for the scatter gather process. I'm sure there are more optimal ways to split interval files, I think I saw in your example script on your github that you chose to split your interval list into 50 pieces due to your local server setup? Not every user is able to optimize an individual analysis, so having the option to automatically split the interval list into contigs is probably better than running it serially. We will probably set up a bash script for it, but having it built in could be usable for others too.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1605
https://github.com/broadinstitute/cromwell/issues/1608:167,Usability,simpl,simple,167,"Currently to extract the name of a file from a WdlFile value, one needs to use the sub method with the right regexp.; This seems unnecessary complicated for a case as simple and frequent as getting the name of a file.; If not a function it could at least be an example of how to use the sub function in the docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1608
https://github.com/broadinstitute/cromwell/pull/1610:163,Energy Efficiency,green,green,163,This PR will need . ~~https://github.com/broadinstitute/centaur/pull/112~~; ~~and https://github.com/broadinstitute/wdl4s/pull/36~~. ~~merged before travis can go green~~,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1610
https://github.com/broadinstitute/cromwell/issues/1612:35,Availability,error,error,35,"_This issue may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:191,Availability,error,errors,191,"_This issue may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:315,Availability,error,error,315,"_This issue may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:388,Availability,error,error,388,"_This issue may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:441,Availability,error,error,441,"_This issue may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:887,Availability,error,error,887," may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:1318,Availability,error,error,1318,"rror] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.specificity_run_create_seg_gt_table:NA:1#2099383368] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:1762,Availability,error,error,1762,"[error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.specificity_run_create_seg_gt_table:NA:1#2099383368] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:2199,Availability,error,error,2199,"] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.specificity_run_create_seg_gt_table:NA:1#2099383368] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJob",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:2630,Availability,error,error,2630,"34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.specificity_run_create_seg_gt_table:NA:1#2099383368] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:7:1#142554972] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:3072,Availability,error,error,3072,"23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:7:1#142554972] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility2_run_create_seg_gt_table:NA:1#1336176467] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,67] [error] WorkflowManagerActor Workflow 54e13b6c-33e4-4777-a4bd-f7b2876c2df5 failed (during ExecutingWorkflowState): java.lang.Exception: Call crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:3:1: return code was -1. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:3502,Availability,error,error,3502,"23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:7:1#142554972] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility2_run_create_seg_gt_table:NA:1#1336176467] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,67] [error] WorkflowManagerActor Workflow 54e13b6c-33e4-4777-a4bd-f7b2876c2df5 failed (during ExecutingWorkflowState): java.lang.Exception: Call crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:3:1: return code was -1. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:3944,Availability,error,error,3944,"23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:1:1#1292909365] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility1_run_create_seg_gt_table:NA:1#1772150264] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:7:1#142554972] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.reproducibility2_run_create_seg_gt_table:NA:1#1336176467] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,67] [error] WorkflowManagerActor Workflow 54e13b6c-33e4-4777-a4bd-f7b2876c2df5 failed (during ExecutingWorkflowState): java.lang.Exception: Call crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:3:1: return code was -1. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1612:321,Integrability,message,message,321,"_This issue may just be around the error reporting, rather than an actual bug_; - develop branch (post-0.22); - local backend; - yes docker; - single workflow mode. What do the Job Execution errors mean? I _think_ this happens when there is an issue with the docker image, but I have not confirmed. Regardless, the error message is not very useful to an end user. And it makes the actual error harder to find. ```; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1#-1129669881] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.purity_run_create_seg_gt_table:4:1#1335133828] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2df5-EngineJobExecutionActor-crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:2:1#276570369] stopped without returning its Job Execution Token. Reclaiming it!; [2016-10-23 01:54:34,66] [error] Actor Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/WorkflowExecutionActor-54e13b6c-33e4-4777-a4bd-f7b2876c2df5/54e13b6c-33e4-4777-a4bd-f7b2876c2d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1612
https://github.com/broadinstitute/cromwell/issues/1614:228,Integrability,message,message,228,"In single workflow mode, the output of cromwell gives a lot of details, but does not seem to list where output is being deposited. Apologies if I just missed it, but can you make it a bit more prominent?. Proposed solution is a message (below is example) in single workflow mode:. ```; [2016-10-24 13:35:50,84] [info] RUN sub-command; [2016-10-24 13:35:50,84] [info] WDL file: /home/lichtens/test_eval/case_gatk_acnv_workflow.final.wdl; [2016-10-24 13:35:50,84] [info] Inputs: /home/lichtens/test_eval/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json; [2016-10-24 13:35:50,84] [info] Workflow Options: /home/lichtens/test_eval/default_runtime; [2016-10-24 13:35:50,84] [info] Workflow Metadata Output: /home/lichtens/test_eval/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; --->[2016-10-24 13:35:50,84] [info] Workflow Output: /home/lichtens/test_eval/eval-gatk-protected/cromwell-executions/5fff-ffff-ffff-ffff/case_gatk_acnv_workflow/ <---- HERE IS EXAMPLE; [2016-10-24 13:35:50,87] [info] SingleWorkflowRunnerActor: Submitting workflow. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1614
https://github.com/broadinstitute/cromwell/issues/1615:207,Availability,error,error,207,"- develop branch from 0.22; - local backend; - single workflow. I think one of the tasks failed, but got the message below. cromwell did not exit and I believe it should have. ```; [2016-10-24 14:44:19,47] [error] head of empty list; java.util.NoSuchElementException: head of empty list; at scala.collection.immutable.Nil$.head(List.scala:420); at scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615
https://github.com/broadinstitute/cromwell/issues/1615:109,Integrability,message,message,109,"- develop branch from 0.22; - local backend; - single workflow. I think one of the tasks failed, but got the message below. cromwell did not exit and I believe it should have. ```; [2016-10-24 14:44:19,47] [error] head of empty list; java.util.NoSuchElementException: head of empty list; at scala.collection.immutable.Nil$.head(List.scala:420); at scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615
https://github.com/broadinstitute/cromwell/issues/1615:2032,Performance,concurren,concurrent,2032, scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615
https://github.com/broadinstitute/cromwell/issues/1615:2105,Performance,concurren,concurrent,2105, scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615
https://github.com/broadinstitute/cromwell/issues/1615:2190,Performance,concurren,concurrent,2190, scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615
https://github.com/broadinstitute/cromwell/issues/1615:2267,Performance,concurren,concurrent,2267, scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615
https://github.com/broadinstitute/cromwell/issues/1615:1248,Testability,Log,LoggingFSM,1248,d of empty list; at scala.collection.immutable.Nil$.head(List.scala:420); at scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.conc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615
https://github.com/broadinstitute/cromwell/issues/1615:1330,Testability,Log,LoggingFSM,1330,:420); at scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615
https://github.com/broadinstitute/cromwell/issues/1617:102,Safety,avoid,avoid,102,"When a user specifies a Docker image of `whoever/myimage:latest`, they almost certainly don't want to avoid to a version of the job that was latest two years ago. This is the current behaviour. Please instead resolve the string to a Docker image hash and use that for call caching instead. The final Docker hash used should end up in call-level metadata so we can know what it is later. Tagging @abaumann so he knows I made this and can chime in to agree with me if there are any questions. Further Refinement from Office Hours:; - Supported Docker Image Repository; -- must have: DockerHub, GCR; -- should have: ECR if it doesn't delay, otherwise a separate ticket; -- bonus points to verify this works against Quay ; - Only API v2 supported ; - Support for both public and private images",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617
https://github.com/broadinstitute/cromwell/issues/1617:246,Security,hash,hash,246,"When a user specifies a Docker image of `whoever/myimage:latest`, they almost certainly don't want to avoid to a version of the job that was latest two years ago. This is the current behaviour. Please instead resolve the string to a Docker image hash and use that for call caching instead. The final Docker hash used should end up in call-level metadata so we can know what it is later. Tagging @abaumann so he knows I made this and can chime in to agree with me if there are any questions. Further Refinement from Office Hours:; - Supported Docker Image Repository; -- must have: DockerHub, GCR; -- should have: ECR if it doesn't delay, otherwise a separate ticket; -- bonus points to verify this works against Quay ; - Only API v2 supported ; - Support for both public and private images",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617
https://github.com/broadinstitute/cromwell/issues/1617:307,Security,hash,hash,307,"When a user specifies a Docker image of `whoever/myimage:latest`, they almost certainly don't want to avoid to a version of the job that was latest two years ago. This is the current behaviour. Please instead resolve the string to a Docker image hash and use that for call caching instead. The final Docker hash used should end up in call-level metadata so we can know what it is later. Tagging @abaumann so he knows I made this and can chime in to agree with me if there are any questions. Further Refinement from Office Hours:; - Supported Docker Image Repository; -- must have: DockerHub, GCR; -- should have: ECR if it doesn't delay, otherwise a separate ticket; -- bonus points to verify this works against Quay ; - Only API v2 supported ; - Support for both public and private images",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617
https://github.com/broadinstitute/cromwell/issues/1618:954,Availability,down,down,954,"Another in a long line of @geoffjentry written placeholder issues. While exploring WDL->CWL conversions as well as the potential to replace a strictly WDL object model with something more generic in the Cromwell engine I keep coming back to how to handle WDL expressions. Another ""problem"" (in quotes as it hasn't actually bitten us .... yet) is that expression evaluation is currently happening in an uncontrolled fashion within the engine - it is conceivable that if enough of these triggered at once that it could cause a lot of problems. This has been a nagging concern in the back of my mind for a long time now. Idea: Replace how we evaluate expressions by replacing them with a task upstream of the expression-ed task. These expression tasks shall run on the local backend allowing it to both be fast but also managed by our process throttling. As an example, a JES based `read_string` could involve a `gsutil` call in its `command` block to suck down the file or something similar for a `size` expression. This seems like it'd imply that there needs to be some universal primitives in Cromwell to effectively `read_XYZ` from the local filesystem. I'll admit to not having thought this all the way through. IMO this will improve the robustness/stability of the engine while making a big stride towards de-WDLing the underbelly of the engine. @kcibul note that I'm not asking for this to happen tomorrow but I do think it's an idea we should kick the tires on in the near term to see if it makes enough sense to pursue more deeply at a later point.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618
https://github.com/broadinstitute/cromwell/issues/1618:1240,Availability,robust,robustness,1240,"Another in a long line of @geoffjentry written placeholder issues. While exploring WDL->CWL conversions as well as the potential to replace a strictly WDL object model with something more generic in the Cromwell engine I keep coming back to how to handle WDL expressions. Another ""problem"" (in quotes as it hasn't actually bitten us .... yet) is that expression evaluation is currently happening in an uncontrolled fashion within the engine - it is conceivable that if enough of these triggered at once that it could cause a lot of problems. This has been a nagging concern in the back of my mind for a long time now. Idea: Replace how we evaluate expressions by replacing them with a task upstream of the expression-ed task. These expression tasks shall run on the local backend allowing it to both be fast but also managed by our process throttling. As an example, a JES based `read_string` could involve a `gsutil` call in its `command` block to suck down the file or something similar for a `size` expression. This seems like it'd imply that there needs to be some universal primitives in Cromwell to effectively `read_XYZ` from the local filesystem. I'll admit to not having thought this all the way through. IMO this will improve the robustness/stability of the engine while making a big stride towards de-WDLing the underbelly of the engine. @kcibul note that I'm not asking for this to happen tomorrow but I do think it's an idea we should kick the tires on in the near term to see if it makes enough sense to pursue more deeply at a later point.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1618
https://github.com/broadinstitute/cromwell/issues/1621:23,Testability,log,logs,23,"Right now per-workflow logs are not (ever?) copied when the workflow fails. We need these logs to help debug issues in execution, so they are most helpful when the workflow fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1621
https://github.com/broadinstitute/cromwell/issues/1621:90,Testability,log,logs,90,"Right now per-workflow logs are not (ever?) copied when the workflow fails. We need these logs to help debug issues in execution, so they are most helpful when the workflow fails.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1621
https://github.com/broadinstitute/cromwell/issues/1622:122,Modifiability,config,configurability,122,"My assumption is that by using log4j + logback for our logging infrastructure, we actually offer end users quite a bit of configurability in both what is logged as well as to where (e.g. appenders). However, without example and docs users don't know how to do this. For example, rather than using a log4j rotating log appender, Henry instead piped stdout into a standalone log appender. This means he had to build his own docker rather than use our published one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622
https://github.com/broadinstitute/cromwell/issues/1622:39,Testability,log,logback,39,"My assumption is that by using log4j + logback for our logging infrastructure, we actually offer end users quite a bit of configurability in both what is logged as well as to where (e.g. appenders). However, without example and docs users don't know how to do this. For example, rather than using a log4j rotating log appender, Henry instead piped stdout into a standalone log appender. This means he had to build his own docker rather than use our published one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622
https://github.com/broadinstitute/cromwell/issues/1622:55,Testability,log,logging,55,"My assumption is that by using log4j + logback for our logging infrastructure, we actually offer end users quite a bit of configurability in both what is logged as well as to where (e.g. appenders). However, without example and docs users don't know how to do this. For example, rather than using a log4j rotating log appender, Henry instead piped stdout into a standalone log appender. This means he had to build his own docker rather than use our published one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622
https://github.com/broadinstitute/cromwell/issues/1622:154,Testability,log,logged,154,"My assumption is that by using log4j + logback for our logging infrastructure, we actually offer end users quite a bit of configurability in both what is logged as well as to where (e.g. appenders). However, without example and docs users don't know how to do this. For example, rather than using a log4j rotating log appender, Henry instead piped stdout into a standalone log appender. This means he had to build his own docker rather than use our published one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622
https://github.com/broadinstitute/cromwell/issues/1622:314,Testability,log,log,314,"My assumption is that by using log4j + logback for our logging infrastructure, we actually offer end users quite a bit of configurability in both what is logged as well as to where (e.g. appenders). However, without example and docs users don't know how to do this. For example, rather than using a log4j rotating log appender, Henry instead piped stdout into a standalone log appender. This means he had to build his own docker rather than use our published one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622
https://github.com/broadinstitute/cromwell/issues/1622:373,Testability,log,log,373,"My assumption is that by using log4j + logback for our logging infrastructure, we actually offer end users quite a bit of configurability in both what is logged as well as to where (e.g. appenders). However, without example and docs users don't know how to do this. For example, rather than using a log4j rotating log appender, Henry instead piped stdout into a standalone log appender. This means he had to build his own docker rather than use our published one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1622
https://github.com/broadinstitute/cromwell/issues/1623:189,Security,hash,hashing,189,"From @jsotobroad. Even though getting MD5 from GCS is fast for call caching, when you do 20k of these it does take a bunch of time. Would it be possible to adopt a strategy like the ""File"" hashing strategy if the user wanted?. from Kris: especially for interior (e.g. from another task) inputs where we can be confident it hasn't been fiddled with. from @cjllanwarne : question: since call caching actually copies files, it's the MD5 we really need because we copy outputs around",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1623
https://github.com/broadinstitute/cromwell/issues/1624:606,Availability,down,downstream,606,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624
https://github.com/broadinstitute/cromwell/issues/1624:43,Deployability,pipeline,pipeline,43,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624
https://github.com/broadinstitute/cromwell/issues/1624:108,Deployability,pipeline,pipeline,108,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624
https://github.com/broadinstitute/cromwell/issues/1624:304,Deployability,pipeline,pipeline,304,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624
https://github.com/broadinstitute/cromwell/issues/1624:395,Deployability,pipeline,pipelines,395,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624
https://github.com/broadinstitute/cromwell/issues/1624:488,Deployability,pipeline,pipelines,488,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624
https://github.com/broadinstitute/cromwell/issues/1624:146,Security,expose,exposed,146,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624
https://github.com/broadinstitute/cromwell/pull/1627:151,Testability,test,tests,151,Not actually much change required in Cromwell - mostly just bringing in the changes in https://github.com/broadinstitute/wdl4s/pull/42. NB: Won't pass tests until the wdl4s PR merges,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1627
https://github.com/broadinstitute/cromwell/issues/1628:943,Integrability,Synchroniz,Synchronizing,943,"This is a continuation of a discussion from the Forum (http://gatkforums.broadinstitute.org/wdl/discussion/8454/feedback-on-initial-version-of-bcbio-wdl-converted-from-cwl) with @kcibul and @geoffjentry. It would be useful to have functionality in the standard library to be able to read outputs from a file generated by a task. This is based on transitioning from CWL, where your task can generate an output that looks like:. ```; {; ""region"": ""chrM:0-1000"", ; ""vrn_file_region"": {; ""class"": ""File"", ; ""path"": ""/var/spool/cwl/gatk-haplotype/chrM/Test2-chrM_0_1000.vcf.gz"", ; ""secondaryFiles"": [; {; ""class"": ""File"", ; ""path"": ""/var/spool/cwl/gatk-haplotype/chrM/Test2-chrM_0_1000.vcf.gz.tbi""; }; ]; }; }; ```. then WDL could ideally define a way to read the outputs from this file:. ```; output {; File vrn_file_region = read_cwl_json('cwl.output.json', 'vrn_file_region'); String region = read_cwl_json('cwl.output.json', 'region'); }; ```. Synchronizing with CWL would help with CWL/WDL interoperability but other ways of supporting this that are easier/more workable with WDL would also work great. Thanks for considering this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1628
https://github.com/broadinstitute/cromwell/issues/1628:990,Integrability,interoperab,interoperability,990,"This is a continuation of a discussion from the Forum (http://gatkforums.broadinstitute.org/wdl/discussion/8454/feedback-on-initial-version-of-bcbio-wdl-converted-from-cwl) with @kcibul and @geoffjentry. It would be useful to have functionality in the standard library to be able to read outputs from a file generated by a task. This is based on transitioning from CWL, where your task can generate an output that looks like:. ```; {; ""region"": ""chrM:0-1000"", ; ""vrn_file_region"": {; ""class"": ""File"", ; ""path"": ""/var/spool/cwl/gatk-haplotype/chrM/Test2-chrM_0_1000.vcf.gz"", ; ""secondaryFiles"": [; {; ""class"": ""File"", ; ""path"": ""/var/spool/cwl/gatk-haplotype/chrM/Test2-chrM_0_1000.vcf.gz.tbi""; }; ]; }; }; ```. then WDL could ideally define a way to read the outputs from this file:. ```; output {; File vrn_file_region = read_cwl_json('cwl.output.json', 'vrn_file_region'); String region = read_cwl_json('cwl.output.json', 'region'); }; ```. Synchronizing with CWL would help with CWL/WDL interoperability but other ways of supporting this that are easier/more workable with WDL would also work great. Thanks for considering this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1628
https://github.com/broadinstitute/cromwell/issues/1628:112,Usability,feedback,feedback-on-initial-version-of-bcbio-wdl-converted-from-cwl,112,"This is a continuation of a discussion from the Forum (http://gatkforums.broadinstitute.org/wdl/discussion/8454/feedback-on-initial-version-of-bcbio-wdl-converted-from-cwl) with @kcibul and @geoffjentry. It would be useful to have functionality in the standard library to be able to read outputs from a file generated by a task. This is based on transitioning from CWL, where your task can generate an output that looks like:. ```; {; ""region"": ""chrM:0-1000"", ; ""vrn_file_region"": {; ""class"": ""File"", ; ""path"": ""/var/spool/cwl/gatk-haplotype/chrM/Test2-chrM_0_1000.vcf.gz"", ; ""secondaryFiles"": [; {; ""class"": ""File"", ; ""path"": ""/var/spool/cwl/gatk-haplotype/chrM/Test2-chrM_0_1000.vcf.gz.tbi""; }; ]; }; }; ```. then WDL could ideally define a way to read the outputs from this file:. ```; output {; File vrn_file_region = read_cwl_json('cwl.output.json', 'vrn_file_region'); String region = read_cwl_json('cwl.output.json', 'region'); }; ```. Synchronizing with CWL would help with CWL/WDL interoperability but other ways of supporting this that are easier/more workable with WDL would also work great. Thanks for considering this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1628
https://github.com/broadinstitute/cromwell/issues/1629:340,Availability,Ping,Ping,340,"For FC provenance tracking we need to keep an eye on the file hashes of workflow inputs. My understanding is that call-caching stores the CRC32c of each _call_ input, but it's difficult to trace those hashes back to workflow inputs. Could you store the hashes of workflow inputs at job submit time and throw them in the workflow metadata?. Ping @abaumann for prio but I don't think it's super urgent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629
https://github.com/broadinstitute/cromwell/issues/1629:62,Security,hash,hashes,62,"For FC provenance tracking we need to keep an eye on the file hashes of workflow inputs. My understanding is that call-caching stores the CRC32c of each _call_ input, but it's difficult to trace those hashes back to workflow inputs. Could you store the hashes of workflow inputs at job submit time and throw them in the workflow metadata?. Ping @abaumann for prio but I don't think it's super urgent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629
https://github.com/broadinstitute/cromwell/issues/1629:201,Security,hash,hashes,201,"For FC provenance tracking we need to keep an eye on the file hashes of workflow inputs. My understanding is that call-caching stores the CRC32c of each _call_ input, but it's difficult to trace those hashes back to workflow inputs. Could you store the hashes of workflow inputs at job submit time and throw them in the workflow metadata?. Ping @abaumann for prio but I don't think it's super urgent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629
https://github.com/broadinstitute/cromwell/issues/1629:253,Security,hash,hashes,253,"For FC provenance tracking we need to keep an eye on the file hashes of workflow inputs. My understanding is that call-caching stores the CRC32c of each _call_ input, but it's difficult to trace those hashes back to workflow inputs. Could you store the hashes of workflow inputs at job submit time and throw them in the workflow metadata?. Ping @abaumann for prio but I don't think it's super urgent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1629
https://github.com/broadinstitute/cromwell/issues/1631:1542,Availability,error,error,1542,"eSomaticReadCounts -> JES, case_gatk_acnv_workflow.NormalWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.NormalAnnotateTargets -> JES, case_gatk_acnv_workflow.CNLoHAndSplitsCaller -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PadTargets -> JES; [2016-10-27 13:10:07,81] [info] JES [6f995b2d]: Creating authentication file for workflow 6f995b2d-cf39-4be1-adfb-b6d0a961bd9c at; gs://my-cromwell-workflows-bucket/case_gatk_acnv_workflow/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c_auth.json; [2016-10-27 13:10:08,17] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:1699,Availability,error,errors,1699,"tsCaller -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PadTargets -> JES; [2016-10-27 13:10:07,81] [info] JES [6f995b2d]: Creating authentication file for workflow 6f995b2d-cf39-4be1-adfb-b6d0a961bd9c at; gs://my-cromwell-workflows-bucket/case_gatk_acnv_workflow/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c_auth.json; [2016-10-27 13:10:08,17] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:3298,Availability,error,error,3298,"(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:3455,Availability,error,errors,3455,"gle.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5632,Availability,error,error,5632,"rator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5789,Availability,error,errors,5789,"$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7733,Availability,error,error,7733,"l$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7890,Availability,error,errors,7890,"spatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9972,Availability,error,error,9972,"patch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:10129,Availability,error,errors,10129,".forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12487,Availability,error,error,12487,"ue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12644,Availability,error,errors,12644,"inWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14519,Availability,error,error,14519,":469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14676,Availability,error,errors,14676,"(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:17036,Availability,error,error,17036,".runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:17193,Availability,error,errors,17193,"WorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:19344,Availability,error,error,19344,"ch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:19501,Availability,error,errors,19501,"rent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21626,Availability,error,error,21626,"run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21783,Availability,error,errors,21783,"java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:24279,Availability,error,error,24279,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:1737,Integrability,message,message,1737,"tsCaller -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PadTargets -> JES; [2016-10-27 13:10:07,81] [info] JES [6f995b2d]: Creating authentication file for workflow 6f995b2d-cf39-4be1-adfb-b6d0a961bd9c at; gs://my-cromwell-workflows-bucket/case_gatk_acnv_workflow/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c_auth.json; [2016-10-27 13:10:08,17] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:1792,Integrability,message,message,1792,"tsCaller -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PadTargets -> JES; [2016-10-27 13:10:07,81] [info] JES [6f995b2d]: Creating authentication file for workflow 6f995b2d-cf39-4be1-adfb-b6d0a961bd9c at; gs://my-cromwell-workflows-bucket/case_gatk_acnv_workflow/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c_auth.json; [2016-10-27 13:10:08,17] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:3493,Integrability,message,message,3493,"gle.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:3548,Integrability,message,message,3548,"gle.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5827,Integrability,message,message,5827,"$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5882,Integrability,message,message,5882,"$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7928,Integrability,message,message,7928,"spatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7983,Integrability,message,message,7983,"spatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:10167,Integrability,message,message,10167,".forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:10222,Integrability,message,message,10222,".forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12682,Integrability,message,message,12682,"inWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12737,Integrability,message,message,12737,"inWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14714,Integrability,message,message,14714,"(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14769,Integrability,message,message,14769,"(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:17231,Integrability,message,message,17231,"WorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:17286,Integrability,message,message,17286,"WorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:19539,Integrability,message,message,19539,"rent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:19594,Integrability,message,message,19594,"rent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21821,Integrability,message,message,21821,"java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21876,Integrability,message,message,21876,"java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:2738,Performance,concurren,concurrent,2738,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:2960,Performance,concurren,concurrent,2960,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:3033,Performance,concurren,concurrent,3033,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleCli",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:3118,Performance,concurren,concurrent,3118,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:3195,Performance,concurren,concurrent,3195,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:12,13] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:4494,Performance,concurren,concurrent,4494,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Wait",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:4716,Performance,concurren,concurrent,4716,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:4789,Performance,concurren,concurrent,4789,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:4874,Performance,concurren,concurrent,4874,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:4951,Performance,concurren,concurrent,4951,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:6828,Performance,concurren,concurrent,6828,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7050,Performance,concurren,concurrent,7050,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.Googl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7123,Performance,concurren,concurrent,7123,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7208,Performance,concurren,concurrent,7208,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExcept",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7285,Performance,concurren,concurrent,7285,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.clien",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:8929,Performance,concurren,concurrent,8929,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9151,Performance,concurren,concurrent,9151,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9224,Performance,concurren,concurrent,9224,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9309,Performance,concurren,concurrent,9309,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseExcepti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9386,Performance,concurren,concurrent,9386,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:11168,Performance,concurren,concurrent,11168,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:11390,Performance,concurren,concurrent,11390,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:11463,Performance,concurren,concurrent,11463,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:11548,Performance,concurren,concurrent,11548,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:11625,Performance,concurren,concurrent,11625,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:13683,Performance,concurren,concurrent,13683,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:13905,Performance,concurren,concurrent,13905,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:13978,Performance,concurren,concurrent,13978,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientR",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14063,Performance,concurren,concurrent,14063,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14140,Performance,concurren,concurrent,14140,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExcepti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:15715,Performance,concurren,concurrent,15715,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:15937,Performance,concurren,concurrent,15937,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16010,Performance,concurren,concurrent,16010,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16095,Performance,concurren,concurrent,16095,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.googl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16172,Performance,concurren,concurrent,16172,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:18232,Performance,concurren,concurrent,18232,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:18454,Performance,concurren,concurrent,18454,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseExce",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:18527,Performance,concurren,concurrent,18527,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""globa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:18612,Performance,concurren,concurrent,18612,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:18689,Performance,concurren,concurrent,18689,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:20540,Performance,concurren,concurrent,20540,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:20762,Performance,concurren,concurrent,20762,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:20835,Performance,concurren,concurrent,20835,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Foun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:20920,Performance,concurren,concurrent,20920,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.clien",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:20997,Performance,concurren,concurrent,20997,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseExceptio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:22822,Performance,concurren,concurrent,22822,"e"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23044,Performance,concurren,concurrent,23044,"is.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23117,Performance,concurren,concurrent,23117,"ractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23202,Performance,concurren,concurrent,23202,"son.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23279,Performance,concurren,concurrent,23279,"entRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5170,Safety,abort,abort,5170,"ervices.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5233,Safety,Abort,Aborting,5233,".client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5316,Safety,abort,abort,5316,".client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5385,Safety,abort,abort,5385,"om.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Ab",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5454,Safety,abort,abort,5454,"ration.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5523,Safety,abort,abort,5523,"oncurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:5596,Safety,abort,abort,5596,"ocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7421,Safety,abort,abort,7421,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7490,Safety,abort,abort,7490,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7559,Safety,abort,abort,7559,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7628,Safety,abort,abort,7628,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:7697,Safety,abort,abort,7697,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:19,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:20,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:21,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:22,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:23,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:24,84] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9522,Safety,abort,abort,9522,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9591,Safety,abort,abort,9591,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9660,Safety,abort,abort,9660,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9729,Safety,abort,abort,9729,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9798,Safety,abort,abort,9798,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9867,Safety,abort,abort,9867,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:9936,Safety,abort,abort,9936,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:11761,Safety,abort,abort,11761,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:11830,Safety,abort,abort,11830,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.jso",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:11899,Safety,abort,abort,11899,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:11968,Safety,abort,abort,11968,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12037,Safety,abort,abort,12037,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12106,Safety,abort,abort,12106,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12175,Safety,abort,abort,12175,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12244,Safety,abort,abort,12244,"at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12313,Safety,abort,abort,12313,"akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12382,Safety,abort,abort,12382,"ractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:12451,Safety,abort,abort,12451,"doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:31,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:32,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:33,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:34,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:35,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:36,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:37,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:38,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:39,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:40,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:41,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:42,05] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14276,Safety,abort,abort,14276,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14345,Safety,abort,abort,14345,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14414,Safety,abort,abort,14414,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:14483,Safety,abort,abort,14483,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16308,Safety,abort,abort,16308,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16377,Safety,abort,abort,16377,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16446,Safety,abort,abort,16446,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16515,Safety,abort,abort,16515,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16584,Safety,abort,abort,16584,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientReque",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16655,Safety,abort,abort,16655,"riteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16724,Safety,abort,abort,16724,"357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16793,Safety,abort,abort,16793," akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16862,Safety,abort,abort,16862,"ka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:16931,Safety,abort,abort,16931,"ctDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:17000,Safety,abort,abort,17000,"Exec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:46,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:47,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:48,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:49,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:50,94] [info] Waiting for 1 workflows to abort...; ^C[2016-10-27 13:10:51,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:52,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:53,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:54,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:55,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:56,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:57,16] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:18825,Safety,abort,abort,18825,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractG",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:18894,Safety,abort,abort,18894,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:18963,Safety,abort,abort,18963,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:19032,Safety,abort,abort,19032,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClient",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:19101,Safety,abort,abort,19101,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.Abstra",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:19170,Safety,abort,abort,19170,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:19239,Safety,abort,abort,19239,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleCl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:19308,Safety,abort,abort,19308,"at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:57,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:58,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:59,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:00,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:01,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:02,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:03,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:04,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:05,57] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21133,Safety,abort,abort,21133,"teUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.new",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21202,Safety,abort,abort,21202,"t.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21271,Safety,abort,abort,21271,"tractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.n",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21340,Safety,abort,abort,21340,"is.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21409,Safety,abort,abort,21409,"equest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21478,Safety,abort,abort,21478,"WriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:21547,Safety,abort,abort,21547,"a:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:05,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:06,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:07,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:08,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:09,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:10,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:11,95] [info] Waiting for 1 workflows to abort...; Killed; lichtens@lichtens-big:~/test_eval$ [2016-10-27 13:11:12,80] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparse",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23415,Safety,abort,abort,23415,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23484,Safety,abort,abort,23484,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23553,Safety,abort,abort,23553,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23622,Safety,abort,abort,23622,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23691,Safety,abort,abort,23691,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23760,Safety,abort,abort,23760,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23829,Safety,abort,abort,23829,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23898,Safety,abort,abort,23898,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:23967,Safety,abort,abort,23967,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:24036,Safety,abort,abort,24036,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:24105,Safety,abort,abort,24105,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:24174,Safety,abort,abort,24174,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:24243,Safety,abort,abort,24243,"bstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:11:12,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:13,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:14,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:15,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:16,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:17,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:18,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:19,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:20,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:21,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:22,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:23,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:24,95] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:11:25,41] [error] Exception not convertible into handled response; ...snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1631:1299,Security,authenticat,authentication,1299,"S, case_gatk_acnv_workflow.TumorWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.NormalCaller -> JES, case_gatk_acnv_workflow.PlotACNVResults -> JES, case_gatk_acnv_workflow.TumorNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.NormalWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.NormalAnnotateTargets -> JES, case_gatk_acnv_workflow.CNLoHAndSplitsCaller -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PadTargets -> JES; [2016-10-27 13:10:07,81] [info] JES [6f995b2d]: Creating authentication file for workflow 6f995b2d-cf39-4be1-adfb-b6d0a961bd9c at; gs://my-cromwell-workflows-bucket/case_gatk_acnv_workflow/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c/6f995b2d-cf39-4be1-adfb-b6d0a961bd9c_auth.json; [2016-10-27 13:10:08,17] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631
https://github.com/broadinstitute/cromwell/issues/1632:15,Deployability,release,release,15,"Using the 0.22 release jar. The default value of a Boolean variable is always used regardless of what is passed in the inputs file. . WDL:. ```; task test {; File testFile; Boolean long = false. command {; ls \; ${true='-l' false='' long} \; ${testFile}; }. output {; File foo = stdout(); }. runtime {; docker: 'ubuntu:14.04'; }; }. workflow run {; call test; }; ```. inputs:. ```; {; ""run.test.testFile"": ""/tmp/tmpfile"",; ""run.test.long"": true; }; ```. Results in the command:; `ls /tmp/tmpfile` rather than `ls -l /tmp/tmpfile` as expected. Additionally, if I try to specify a default value for a Boolean `${true='-l' false='' default=true long}`, this translates to 'true' being passed in to the command line rather than '-l' as expected. It also doesn't seem to matter if I pass the default as a string or boolean.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632
https://github.com/broadinstitute/cromwell/issues/1632:59,Modifiability,variab,variable,59,"Using the 0.22 release jar. The default value of a Boolean variable is always used regardless of what is passed in the inputs file. . WDL:. ```; task test {; File testFile; Boolean long = false. command {; ls \; ${true='-l' false='' long} \; ${testFile}; }. output {; File foo = stdout(); }. runtime {; docker: 'ubuntu:14.04'; }; }. workflow run {; call test; }; ```. inputs:. ```; {; ""run.test.testFile"": ""/tmp/tmpfile"",; ""run.test.long"": true; }; ```. Results in the command:; `ls /tmp/tmpfile` rather than `ls -l /tmp/tmpfile` as expected. Additionally, if I try to specify a default value for a Boolean `${true='-l' false='' default=true long}`, this translates to 'true' being passed in to the command line rather than '-l' as expected. It also doesn't seem to matter if I pass the default as a string or boolean.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632
https://github.com/broadinstitute/cromwell/issues/1632:150,Testability,test,test,150,"Using the 0.22 release jar. The default value of a Boolean variable is always used regardless of what is passed in the inputs file. . WDL:. ```; task test {; File testFile; Boolean long = false. command {; ls \; ${true='-l' false='' long} \; ${testFile}; }. output {; File foo = stdout(); }. runtime {; docker: 'ubuntu:14.04'; }; }. workflow run {; call test; }; ```. inputs:. ```; {; ""run.test.testFile"": ""/tmp/tmpfile"",; ""run.test.long"": true; }; ```. Results in the command:; `ls /tmp/tmpfile` rather than `ls -l /tmp/tmpfile` as expected. Additionally, if I try to specify a default value for a Boolean `${true='-l' false='' default=true long}`, this translates to 'true' being passed in to the command line rather than '-l' as expected. It also doesn't seem to matter if I pass the default as a string or boolean.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632
https://github.com/broadinstitute/cromwell/issues/1632:163,Testability,test,testFile,163,"Using the 0.22 release jar. The default value of a Boolean variable is always used regardless of what is passed in the inputs file. . WDL:. ```; task test {; File testFile; Boolean long = false. command {; ls \; ${true='-l' false='' long} \; ${testFile}; }. output {; File foo = stdout(); }. runtime {; docker: 'ubuntu:14.04'; }; }. workflow run {; call test; }; ```. inputs:. ```; {; ""run.test.testFile"": ""/tmp/tmpfile"",; ""run.test.long"": true; }; ```. Results in the command:; `ls /tmp/tmpfile` rather than `ls -l /tmp/tmpfile` as expected. Additionally, if I try to specify a default value for a Boolean `${true='-l' false='' default=true long}`, this translates to 'true' being passed in to the command line rather than '-l' as expected. It also doesn't seem to matter if I pass the default as a string or boolean.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632
https://github.com/broadinstitute/cromwell/issues/1632:244,Testability,test,testFile,244,"Using the 0.22 release jar. The default value of a Boolean variable is always used regardless of what is passed in the inputs file. . WDL:. ```; task test {; File testFile; Boolean long = false. command {; ls \; ${true='-l' false='' long} \; ${testFile}; }. output {; File foo = stdout(); }. runtime {; docker: 'ubuntu:14.04'; }; }. workflow run {; call test; }; ```. inputs:. ```; {; ""run.test.testFile"": ""/tmp/tmpfile"",; ""run.test.long"": true; }; ```. Results in the command:; `ls /tmp/tmpfile` rather than `ls -l /tmp/tmpfile` as expected. Additionally, if I try to specify a default value for a Boolean `${true='-l' false='' default=true long}`, this translates to 'true' being passed in to the command line rather than '-l' as expected. It also doesn't seem to matter if I pass the default as a string or boolean.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632
https://github.com/broadinstitute/cromwell/issues/1632:354,Testability,test,test,354,"Using the 0.22 release jar. The default value of a Boolean variable is always used regardless of what is passed in the inputs file. . WDL:. ```; task test {; File testFile; Boolean long = false. command {; ls \; ${true='-l' false='' long} \; ${testFile}; }. output {; File foo = stdout(); }. runtime {; docker: 'ubuntu:14.04'; }; }. workflow run {; call test; }; ```. inputs:. ```; {; ""run.test.testFile"": ""/tmp/tmpfile"",; ""run.test.long"": true; }; ```. Results in the command:; `ls /tmp/tmpfile` rather than `ls -l /tmp/tmpfile` as expected. Additionally, if I try to specify a default value for a Boolean `${true='-l' false='' default=true long}`, this translates to 'true' being passed in to the command line rather than '-l' as expected. It also doesn't seem to matter if I pass the default as a string or boolean.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632
https://github.com/broadinstitute/cromwell/issues/1632:390,Testability,test,test,390,"Using the 0.22 release jar. The default value of a Boolean variable is always used regardless of what is passed in the inputs file. . WDL:. ```; task test {; File testFile; Boolean long = false. command {; ls \; ${true='-l' false='' long} \; ${testFile}; }. output {; File foo = stdout(); }. runtime {; docker: 'ubuntu:14.04'; }; }. workflow run {; call test; }; ```. inputs:. ```; {; ""run.test.testFile"": ""/tmp/tmpfile"",; ""run.test.long"": true; }; ```. Results in the command:; `ls /tmp/tmpfile` rather than `ls -l /tmp/tmpfile` as expected. Additionally, if I try to specify a default value for a Boolean `${true='-l' false='' default=true long}`, this translates to 'true' being passed in to the command line rather than '-l' as expected. It also doesn't seem to matter if I pass the default as a string or boolean.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632
https://github.com/broadinstitute/cromwell/issues/1632:395,Testability,test,testFile,395,"Using the 0.22 release jar. The default value of a Boolean variable is always used regardless of what is passed in the inputs file. . WDL:. ```; task test {; File testFile; Boolean long = false. command {; ls \; ${true='-l' false='' long} \; ${testFile}; }. output {; File foo = stdout(); }. runtime {; docker: 'ubuntu:14.04'; }; }. workflow run {; call test; }; ```. inputs:. ```; {; ""run.test.testFile"": ""/tmp/tmpfile"",; ""run.test.long"": true; }; ```. Results in the command:; `ls /tmp/tmpfile` rather than `ls -l /tmp/tmpfile` as expected. Additionally, if I try to specify a default value for a Boolean `${true='-l' false='' default=true long}`, this translates to 'true' being passed in to the command line rather than '-l' as expected. It also doesn't seem to matter if I pass the default as a string or boolean.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632
https://github.com/broadinstitute/cromwell/issues/1632:428,Testability,test,test,428,"Using the 0.22 release jar. The default value of a Boolean variable is always used regardless of what is passed in the inputs file. . WDL:. ```; task test {; File testFile; Boolean long = false. command {; ls \; ${true='-l' false='' long} \; ${testFile}; }. output {; File foo = stdout(); }. runtime {; docker: 'ubuntu:14.04'; }; }. workflow run {; call test; }; ```. inputs:. ```; {; ""run.test.testFile"": ""/tmp/tmpfile"",; ""run.test.long"": true; }; ```. Results in the command:; `ls /tmp/tmpfile` rather than `ls -l /tmp/tmpfile` as expected. Additionally, if I try to specify a default value for a Boolean `${true='-l' false='' default=true long}`, this translates to 'true' being passed in to the command line rather than '-l' as expected. It also doesn't seem to matter if I pass the default as a string or boolean.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1632
https://github.com/broadinstitute/cromwell/issues/1633:135,Availability,down,down,135,"I have a few cases in Firecloud (using cromwell .19) where it appears like some scatter jobs are not being launched. I have tracked it down to a problem with outputs from the prepare call not being registered as actual outputs. According to the logs and the gcs bucket all the outputs from the prepare step are right, but according to cromwell the output array is missing a couple entries. Looking in [workflow metadata](https://github.com/broadinstitute/cromwell/files/557271/metadata.txt) at the CallingGroup_Workflow.mone_prepare call, the outputs for split_base.06.interval_list and split_base.22.interval_list are missing. According to the [google bucket listing of the glob dir](https://github.com/broadinstitute/cromwell/files/557270/glob_listing.txt), the files are there. The JES logs for the prepare step show the upload happens successfully. To summarize, the call CallingGroup_Workflow.mone_prepare successfully generates and upload 25 interval files but the output section of the metadata shows only 23 (6 and 22 are missing). This means that only 23 downstream calls run which is bad. [workflow log](https://github.com/broadinstitute/cromwell/files/557272/workflow.9b9e6b19-aac4-451f-bf24-a9132c0e5408.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1633
https://github.com/broadinstitute/cromwell/issues/1633:1064,Availability,down,downstream,1064,"I have a few cases in Firecloud (using cromwell .19) where it appears like some scatter jobs are not being launched. I have tracked it down to a problem with outputs from the prepare call not being registered as actual outputs. According to the logs and the gcs bucket all the outputs from the prepare step are right, but according to cromwell the output array is missing a couple entries. Looking in [workflow metadata](https://github.com/broadinstitute/cromwell/files/557271/metadata.txt) at the CallingGroup_Workflow.mone_prepare call, the outputs for split_base.06.interval_list and split_base.22.interval_list are missing. According to the [google bucket listing of the glob dir](https://github.com/broadinstitute/cromwell/files/557270/glob_listing.txt), the files are there. The JES logs for the prepare step show the upload happens successfully. To summarize, the call CallingGroup_Workflow.mone_prepare successfully generates and upload 25 interval files but the output section of the metadata shows only 23 (6 and 22 are missing). This means that only 23 downstream calls run which is bad. [workflow log](https://github.com/broadinstitute/cromwell/files/557272/workflow.9b9e6b19-aac4-451f-bf24-a9132c0e5408.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1633
https://github.com/broadinstitute/cromwell/issues/1633:245,Testability,log,logs,245,"I have a few cases in Firecloud (using cromwell .19) where it appears like some scatter jobs are not being launched. I have tracked it down to a problem with outputs from the prepare call not being registered as actual outputs. According to the logs and the gcs bucket all the outputs from the prepare step are right, but according to cromwell the output array is missing a couple entries. Looking in [workflow metadata](https://github.com/broadinstitute/cromwell/files/557271/metadata.txt) at the CallingGroup_Workflow.mone_prepare call, the outputs for split_base.06.interval_list and split_base.22.interval_list are missing. According to the [google bucket listing of the glob dir](https://github.com/broadinstitute/cromwell/files/557270/glob_listing.txt), the files are there. The JES logs for the prepare step show the upload happens successfully. To summarize, the call CallingGroup_Workflow.mone_prepare successfully generates and upload 25 interval files but the output section of the metadata shows only 23 (6 and 22 are missing). This means that only 23 downstream calls run which is bad. [workflow log](https://github.com/broadinstitute/cromwell/files/557272/workflow.9b9e6b19-aac4-451f-bf24-a9132c0e5408.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1633
https://github.com/broadinstitute/cromwell/issues/1633:789,Testability,log,logs,789,"I have a few cases in Firecloud (using cromwell .19) where it appears like some scatter jobs are not being launched. I have tracked it down to a problem with outputs from the prepare call not being registered as actual outputs. According to the logs and the gcs bucket all the outputs from the prepare step are right, but according to cromwell the output array is missing a couple entries. Looking in [workflow metadata](https://github.com/broadinstitute/cromwell/files/557271/metadata.txt) at the CallingGroup_Workflow.mone_prepare call, the outputs for split_base.06.interval_list and split_base.22.interval_list are missing. According to the [google bucket listing of the glob dir](https://github.com/broadinstitute/cromwell/files/557270/glob_listing.txt), the files are there. The JES logs for the prepare step show the upload happens successfully. To summarize, the call CallingGroup_Workflow.mone_prepare successfully generates and upload 25 interval files but the output section of the metadata shows only 23 (6 and 22 are missing). This means that only 23 downstream calls run which is bad. [workflow log](https://github.com/broadinstitute/cromwell/files/557272/workflow.9b9e6b19-aac4-451f-bf24-a9132c0e5408.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1633
https://github.com/broadinstitute/cromwell/issues/1633:1109,Testability,log,log,1109,"I have a few cases in Firecloud (using cromwell .19) where it appears like some scatter jobs are not being launched. I have tracked it down to a problem with outputs from the prepare call not being registered as actual outputs. According to the logs and the gcs bucket all the outputs from the prepare step are right, but according to cromwell the output array is missing a couple entries. Looking in [workflow metadata](https://github.com/broadinstitute/cromwell/files/557271/metadata.txt) at the CallingGroup_Workflow.mone_prepare call, the outputs for split_base.06.interval_list and split_base.22.interval_list are missing. According to the [google bucket listing of the glob dir](https://github.com/broadinstitute/cromwell/files/557270/glob_listing.txt), the files are there. The JES logs for the prepare step show the upload happens successfully. To summarize, the call CallingGroup_Workflow.mone_prepare successfully generates and upload 25 interval files but the output section of the metadata shows only 23 (6 and 22 are missing). This means that only 23 downstream calls run which is bad. [workflow log](https://github.com/broadinstitute/cromwell/files/557272/workflow.9b9e6b19-aac4-451f-bf24-a9132c0e5408.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1633
https://github.com/broadinstitute/cromwell/issues/1634:58,Testability,test,tested,58,"As a user of Cromwell, I would like to know what is being tested via Centaur. This is important so that I don't overtest things that are being tested and also don't stress out about things because I don't know if they are being tested. I would like a document that lists each test (WDL) being run and a sentence about what it is doing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634
https://github.com/broadinstitute/cromwell/issues/1634:143,Testability,test,tested,143,"As a user of Cromwell, I would like to know what is being tested via Centaur. This is important so that I don't overtest things that are being tested and also don't stress out about things because I don't know if they are being tested. I would like a document that lists each test (WDL) being run and a sentence about what it is doing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634
https://github.com/broadinstitute/cromwell/issues/1634:228,Testability,test,tested,228,"As a user of Cromwell, I would like to know what is being tested via Centaur. This is important so that I don't overtest things that are being tested and also don't stress out about things because I don't know if they are being tested. I would like a document that lists each test (WDL) being run and a sentence about what it is doing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634
https://github.com/broadinstitute/cromwell/issues/1634:276,Testability,test,test,276,"As a user of Cromwell, I would like to know what is being tested via Centaur. This is important so that I don't overtest things that are being tested and also don't stress out about things because I don't know if they are being tested. I would like a document that lists each test (WDL) being run and a sentence about what it is doing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1634
https://github.com/broadinstitute/cromwell/issues/1637:11,Availability,error,error,11,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:232,Availability,error,error,232,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:328,Availability,error,error,328,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:487,Availability,error,errors,487,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:525,Integrability,message,message,525,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:580,Integrability,message,message,580,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:1948,Performance,concurren,concurrent,1948,"_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:2021,Performance,concurren,concurrent,2021,"_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:2106,Performance,concurren,concurrent,2106,"_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:2183,Performance,concurren,concurrent,2183,"_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:323,Security,Hash,Hash,323,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:437,Security,hash,hash,437,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:1095,Testability,Log,LoggingFSM,1095,"like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1637:1173,Testability,Log,LoggingFSM,1173,"ly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637
https://github.com/broadinstitute/cromwell/issues/1638:103,Security,encrypt,encryption,103,"The unprocessed workflow options json is being stored in the database without being passed through our encryption code. The ""cleared"" workflow options should be stored in metadata, and the encrypted version stored during the workflow run in the workflow store. The workflow store will continue to be emptied once the workflow finishes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1638
https://github.com/broadinstitute/cromwell/issues/1638:189,Security,encrypt,encrypted,189,"The unprocessed workflow options json is being stored in the database without being passed through our encryption code. The ""cleared"" workflow options should be stored in metadata, and the encrypted version stored during the workflow run in the workflow store. The workflow store will continue to be emptied once the workflow finishes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1638
https://github.com/broadinstitute/cromwell/issues/1638:125,Usability,clear,cleared,125,"The unprocessed workflow options json is being stored in the database without being passed through our encryption code. The ""cleared"" workflow options should be stored in metadata, and the encrypted version stored during the workflow run in the workflow store. The workflow store will continue to be emptied once the workflow finishes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1638
https://github.com/broadinstitute/cromwell/pull/1639:68,Deployability,configurat,configuration,68,The idea of this change is to allow the user to add custom htcondor configuration to the submit file.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1639
https://github.com/broadinstitute/cromwell/pull/1639:68,Modifiability,config,configuration,68,The idea of this change is to allow the user to add custom htcondor configuration to the submit file.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1639
https://github.com/broadinstitute/cromwell/issues/1641:40,Deployability,pipeline,pipeline,40,"It would be helpful, for our production pipeline, if Cromwell's copying of workflow outputs and logs had an option to ""flatten"" the outputs by writing all to a single directory, instead of including the directory hierarchy when copying. We are careful to avoid file name collisions in our output and logs so handling that case wouldn't be an issue for us -- but might for other users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641
https://github.com/broadinstitute/cromwell/issues/1641:255,Safety,avoid,avoid,255,"It would be helpful, for our production pipeline, if Cromwell's copying of workflow outputs and logs had an option to ""flatten"" the outputs by writing all to a single directory, instead of including the directory hierarchy when copying. We are careful to avoid file name collisions in our output and logs so handling that case wouldn't be an issue for us -- but might for other users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641
https://github.com/broadinstitute/cromwell/issues/1641:96,Testability,log,logs,96,"It would be helpful, for our production pipeline, if Cromwell's copying of workflow outputs and logs had an option to ""flatten"" the outputs by writing all to a single directory, instead of including the directory hierarchy when copying. We are careful to avoid file name collisions in our output and logs so handling that case wouldn't be an issue for us -- but might for other users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641
https://github.com/broadinstitute/cromwell/issues/1641:300,Testability,log,logs,300,"It would be helpful, for our production pipeline, if Cromwell's copying of workflow outputs and logs had an option to ""flatten"" the outputs by writing all to a single directory, instead of including the directory hierarchy when copying. We are careful to avoid file name collisions in our output and logs so handling that case wouldn't be an issue for us -- but might for other users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641
https://github.com/broadinstitute/cromwell/issues/1642:29,Deployability,pipeline,pipelines,29,"We would like one of our WDL pipelines to be able to deposit its workflow outputs in two different locations. For our purposes, all the output files go to one location or the other and none go to both. It would be helpful if we could provide Cromwell's copy outputs function with a map of what outputs go where. Depending on how its implemented, this could also be the solution for https://github.com/broadinstitute/cromwell/issues/1641.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1642
https://github.com/broadinstitute/cromwell/issues/1642:312,Integrability,Depend,Depending,312,"We would like one of our WDL pipelines to be able to deposit its workflow outputs in two different locations. For our purposes, all the output files go to one location or the other and none go to both. It would be helpful if we could provide Cromwell's copy outputs function with a map of what outputs go where. Depending on how its implemented, this could also be the solution for https://github.com/broadinstitute/cromwell/issues/1641.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1642
https://github.com/broadinstitute/cromwell/pull/1643:167,Availability,failure,failures,167,"There is a requirement from CCC to be able to identify through logs why localization functionality fallback to an specific strategy. A simple solution would be to log failures in each strategy invoke but I'm open to any other alternative. OPEN: I made use of StrictLogging but if you think I should use a different implementation, just let me know and I will change it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1643
https://github.com/broadinstitute/cromwell/pull/1643:63,Testability,log,logs,63,"There is a requirement from CCC to be able to identify through logs why localization functionality fallback to an specific strategy. A simple solution would be to log failures in each strategy invoke but I'm open to any other alternative. OPEN: I made use of StrictLogging but if you think I should use a different implementation, just let me know and I will change it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1643
https://github.com/broadinstitute/cromwell/pull/1643:163,Testability,log,log,163,"There is a requirement from CCC to be able to identify through logs why localization functionality fallback to an specific strategy. A simple solution would be to log failures in each strategy invoke but I'm open to any other alternative. OPEN: I made use of StrictLogging but if you think I should use a different implementation, just let me know and I will change it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1643
https://github.com/broadinstitute/cromwell/pull/1643:135,Usability,simpl,simple,135,"There is a requirement from CCC to be able to identify through logs why localization functionality fallback to an specific strategy. A simple solution would be to log failures in each strategy invoke but I'm open to any other alternative. OPEN: I made use of StrictLogging but if you think I should use a different implementation, just let me know and I will change it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1643
https://github.com/broadinstitute/cromwell/issues/1644:1573,Security,authenticat,authentication,1573,"- single workflow; - JES. Been at this point for ~15 minutes. Please note that the workflow directory in the bucket does not exist. ```; ...snip....; [2016-11-02 13:27:06,88] [info] WorkflowManagerActor Successfully started WorkflowActor-35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16; [2016-11-02 13:27:06,88] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2016-11-02 13:27:07,21] [info] MaterializeWorkflowDescriptorActor [35b2e3c9]: Call-to-Backend assignments: case_gatk_acnv_workflow.PlotACNVResults -> JES, case_gatk_acnv_workflow.NormalNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.NormalWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.TumorCaller -> JES, case_gatk_acnv_workflow.TumorCorrectGCBias -> JES, case_gatk_acnv_workflow.NormalPerformSeg -> JES, case_gatk_acnv_workflow.TumorWholeGenomeCoverage -> JES, case_gatk_acnv_workflow.CNLoHAndSplitsCaller -> JES, case_gatk_acnv_workflow.TumorCalculateTargetCoverage -> JES, case_gatk_acnv_workflow.NormalCaller -> JES, case_gatk_acnv_workflow.TumorNormalizeSomaticReadCounts -> JES, case_gatk_acnv_workflow.HetPulldown -> JES, case_gatk_acnv_workflow.PlotSegmentedCopyRatio -> JES, case_gatk_acnv_workflow.TumorAnnotateTargets -> JES, case_gatk_acnv_workflow.NormalCorrectGCBias -> JES, case_gatk_acnv_workflow.TumorPerformSeg -> JES, case_gatk_acnv_workflow.NormalCalculateTargetCoverage -> JES, case_gatk_acnv_workflow.PadTargets -> JES, case_gatk_acnv_workflow.NormalAnnotateTargets -> JES, case_gatk_acnv_workflow.AllelicCNV -> JES; [2016-11-02 13:27:07,49] [info] JES [35b2e3c9]: Creating authentication file for workflow 35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16 at; gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/case_gatk_acnv_workflow/35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16/35b2e3c9-0aba-4f96-a7cf-9f1fbf3ced16_auth.json; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1644
https://github.com/broadinstitute/cromwell/issues/1646:436,Availability,Error,Error,436,"- cromwell-23-79f6e12-SNAPSHOT.jar; - SGE backend; - Broad Internal filesystem location: ``/dsde/working/lichtens/test_pon_cromwell/cromwell-executions/pon_gatk_workflow/3c28c49b-c243-4371-b80b-d14fb5286c43/``; - no docker; - Being run on Broad VM. The first task takes in a bam file and creates an entity_id, which is passed into the second task. This works fine on local backend. Feel free to contact me if you need more information. Error message:. ```; [ERROR] [11/03/2016 10:37:24.334] [cromwell-system-akka.dispatchers.engine-dispatcher-19] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 3c28c49b-c243-4371-b80b-d14fb5286c43 failed (during ExecutingWorkflowState): wdl4s.util.AggregatedException: Input evaluation for Call pon_gatk_workflow.CalculateTargetCoverage failedFailed to find index Success(WdlInteger(1)) on array:. Success([""/seq/picard_aggregation/C1850/GTEX-1A3MW-0004/current/GTEX-1A3MW-0004.bam""]). 1. ```. Relevant WDL:. ```; ...snip... scatter (row in bam_file_names) {. call GetBamFileName {; input:; input_bam=row[0]; }. call CalculateTargetCoverage {; input:; entity_id=GetBamFileName.name,; padded_target_file=PadTargets.padded_target_file,; input_bam=row[0],; bam_idx=row[1],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; disable_all_read_filters=disable_all_read_filters,; keep_duplicate_reads=keep_duplicate_reads,; transform=transform,; grouping=grouping,; isWGS=isWGS,; mem=calculate_target_coverage_memory; }; ...snip... # Helper task to get the name of the given bam file; task GetBamFileName {; File input_bam. command <<<; echo $(basename ""${input_bam}"" .bam); >>>. output {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean kee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646
https://github.com/broadinstitute/cromwell/issues/1646:458,Availability,ERROR,ERROR,458,"- cromwell-23-79f6e12-SNAPSHOT.jar; - SGE backend; - Broad Internal filesystem location: ``/dsde/working/lichtens/test_pon_cromwell/cromwell-executions/pon_gatk_workflow/3c28c49b-c243-4371-b80b-d14fb5286c43/``; - no docker; - Being run on Broad VM. The first task takes in a bam file and creates an entity_id, which is passed into the second task. This works fine on local backend. Feel free to contact me if you need more information. Error message:. ```; [ERROR] [11/03/2016 10:37:24.334] [cromwell-system-akka.dispatchers.engine-dispatcher-19] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 3c28c49b-c243-4371-b80b-d14fb5286c43 failed (during ExecutingWorkflowState): wdl4s.util.AggregatedException: Input evaluation for Call pon_gatk_workflow.CalculateTargetCoverage failedFailed to find index Success(WdlInteger(1)) on array:. Success([""/seq/picard_aggregation/C1850/GTEX-1A3MW-0004/current/GTEX-1A3MW-0004.bam""]). 1. ```. Relevant WDL:. ```; ...snip... scatter (row in bam_file_names) {. call GetBamFileName {; input:; input_bam=row[0]; }. call CalculateTargetCoverage {; input:; entity_id=GetBamFileName.name,; padded_target_file=PadTargets.padded_target_file,; input_bam=row[0],; bam_idx=row[1],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; disable_all_read_filters=disable_all_read_filters,; keep_duplicate_reads=keep_duplicate_reads,; transform=transform,; grouping=grouping,; isWGS=isWGS,; mem=calculate_target_coverage_memory; }; ...snip... # Helper task to get the name of the given bam file; task GetBamFileName {; File input_bam. command <<<; echo $(basename ""${input_bam}"" .bam); >>>. output {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean kee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646
https://github.com/broadinstitute/cromwell/issues/1646:1758,Availability,echo,echo,1758,"t evaluation for Call pon_gatk_workflow.CalculateTargetCoverage failedFailed to find index Success(WdlInteger(1)) on array:. Success([""/seq/picard_aggregation/C1850/GTEX-1A3MW-0004/current/GTEX-1A3MW-0004.bam""]). 1. ```. Relevant WDL:. ```; ...snip... scatter (row in bam_file_names) {. call GetBamFileName {; input:; input_bam=row[0]; }. call CalculateTargetCoverage {; input:; entity_id=GetBamFileName.name,; padded_target_file=PadTargets.padded_target_file,; input_bam=row[0],; bam_idx=row[1],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; disable_all_read_filters=disable_all_read_filters,; keep_duplicate_reads=keep_duplicate_reads,; transform=transform,; grouping=grouping,; isWGS=isWGS,; mem=calculate_target_coverage_memory; }; ...snip... # Helper task to get the name of the given bam file; task GetBamFileName {; File input_bam. command <<<; echo $(basename ""${input_bam}"" .bam); >>>. output {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean keep_duplicate_reads; Boolean disable_all_read_filters; String transform; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File gatk_jar; Boolean disable_sequence_dictionary_validation; Boolean isWGS; Int mem. # Note that when isWGS is true, this task is still called by the workflow.; # In that case, an empty coverage file is created and passed to the WholeGenomeCoverage ; # task to satisfy input and output requirements.; command <<<; if [ ${isWGS} = false ]; then; java -Xmx${mem}g -jar ${gatk_jar} CalculateTargetCoverage --output ${entity_id}.coverage.tsv \; --groupBy ${grouping} --transform ${transform} --targets ${padded_target_file} --targetInformationColumns FULL \; --input ${input_bam} --reference ${ref_fasta} --dis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646
https://github.com/broadinstitute/cromwell/issues/1646:2863,Availability,echo,echo,2863,"put {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean keep_duplicate_reads; Boolean disable_all_read_filters; String transform; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File gatk_jar; Boolean disable_sequence_dictionary_validation; Boolean isWGS; Int mem. # Note that when isWGS is true, this task is still called by the workflow.; # In that case, an empty coverage file is created and passed to the WholeGenomeCoverage ; # task to satisfy input and output requirements.; command <<<; if [ ${isWGS} = false ]; then; java -Xmx${mem}g -jar ${gatk_jar} CalculateTargetCoverage --output ${entity_id}.coverage.tsv \; --groupBy ${grouping} --transform ${transform} --targets ${padded_target_file} --targetInformationColumns FULL \; --input ${input_bam} --reference ${ref_fasta} --disableAllReadFilters ${disable_all_read_filters} \; $(if [ ${keep_duplicate_reads} = true ]; then echo "" --disableReadFilter NOT_DUPLICATE ""; else echo """"; fi) \; --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; else; touch ${entity_id}.coverage.tsv; fi; >>>. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }; ....snip....; ```. [sge_application.conf.txt](https://github.com/broadinstitute/cromwell/files/569269/sge_application.conf.txt). Shell command:; ```bash; #!/bin/bash -l; . /broad/tools/scripts/useuse; reuse -q GridEngine8; use .hdfview-2.9; use Java-1.8; use .r-3.1.3-gatk-only. ###############. java -Xmx6G -Dconfig.file=${PWD}/sge_application.conf -jar \; cromwell-23-79f6e12-SNAPSHOT.jar \; run pon_gatk_workflow.wdl \; pon_gatk_workflow.json \; sg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646
https://github.com/broadinstitute/cromwell/issues/1646:2912,Availability,echo,echo,2912,"put {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean keep_duplicate_reads; Boolean disable_all_read_filters; String transform; File input_bam; File bam_idx; File ref_fasta; File ref_fasta_fai; File ref_fasta_dict; File gatk_jar; Boolean disable_sequence_dictionary_validation; Boolean isWGS; Int mem. # Note that when isWGS is true, this task is still called by the workflow.; # In that case, an empty coverage file is created and passed to the WholeGenomeCoverage ; # task to satisfy input and output requirements.; command <<<; if [ ${isWGS} = false ]; then; java -Xmx${mem}g -jar ${gatk_jar} CalculateTargetCoverage --output ${entity_id}.coverage.tsv \; --groupBy ${grouping} --transform ${transform} --targets ${padded_target_file} --targetInformationColumns FULL \; --input ${input_bam} --reference ${ref_fasta} --disableAllReadFilters ${disable_all_read_filters} \; $(if [ ${keep_duplicate_reads} = true ]; then echo "" --disableReadFilter NOT_DUPLICATE ""; else echo """"; fi) \; --interval_set_rule UNION --interval_padding 0 \; --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation ${disable_sequence_dictionary_validation} \; --createOutputBamIndex true --help false --version false --verbosity INFO --QUIET false; else; touch ${entity_id}.coverage.tsv; fi; >>>. output {; File gatk_coverage_file = ""${entity_id}.coverage.tsv""; }. #runtime {; # docker: ""gatk-protected/a1""; #}; }; ....snip....; ```. [sge_application.conf.txt](https://github.com/broadinstitute/cromwell/files/569269/sge_application.conf.txt). Shell command:; ```bash; #!/bin/bash -l; . /broad/tools/scripts/useuse; reuse -q GridEngine8; use .hdfview-2.9; use Java-1.8; use .r-3.1.3-gatk-only. ###############. java -Xmx6G -Dconfig.file=${PWD}/sge_application.conf -jar \; cromwell-23-79f6e12-SNAPSHOT.jar \; run pon_gatk_workflow.wdl \; pon_gatk_workflow.json \; sg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646
https://github.com/broadinstitute/cromwell/issues/1646:442,Integrability,message,message,442,"- cromwell-23-79f6e12-SNAPSHOT.jar; - SGE backend; - Broad Internal filesystem location: ``/dsde/working/lichtens/test_pon_cromwell/cromwell-executions/pon_gatk_workflow/3c28c49b-c243-4371-b80b-d14fb5286c43/``; - no docker; - Being run on Broad VM. The first task takes in a bam file and creates an entity_id, which is passed into the second task. This works fine on local backend. Feel free to contact me if you need more information. Error message:. ```; [ERROR] [11/03/2016 10:37:24.334] [cromwell-system-akka.dispatchers.engine-dispatcher-19] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 3c28c49b-c243-4371-b80b-d14fb5286c43 failed (during ExecutingWorkflowState): wdl4s.util.AggregatedException: Input evaluation for Call pon_gatk_workflow.CalculateTargetCoverage failedFailed to find index Success(WdlInteger(1)) on array:. Success([""/seq/picard_aggregation/C1850/GTEX-1A3MW-0004/current/GTEX-1A3MW-0004.bam""]). 1. ```. Relevant WDL:. ```; ...snip... scatter (row in bam_file_names) {. call GetBamFileName {; input:; input_bam=row[0]; }. call CalculateTargetCoverage {; input:; entity_id=GetBamFileName.name,; padded_target_file=PadTargets.padded_target_file,; input_bam=row[0],; bam_idx=row[1],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; disable_all_read_filters=disable_all_read_filters,; keep_duplicate_reads=keep_duplicate_reads,; transform=transform,; grouping=grouping,; isWGS=isWGS,; mem=calculate_target_coverage_memory; }; ...snip... # Helper task to get the name of the given bam file; task GetBamFileName {; File input_bam. command <<<; echo $(basename ""${input_bam}"" .bam); >>>. output {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean kee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646
https://github.com/broadinstitute/cromwell/pull/1648:7,Availability,Robust,Robust,7,Bonus: Robust METADATA_VALUE embiggening.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1648
https://github.com/broadinstitute/cromwell/issues/1649:456,Safety,abort,abort,456,"- JES backend. ```; ...snip....; [2016-11-03 19:36:22,19] [info] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [2016-11-03 19:36:22,30] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; [2016-11-03 19:36:22,30] [info] WorkflowManagerActor: Received shutdown signal.; [2016-11-03 19:36:22,30] [info] Waiting for 1 workflows to abort...; ....15 minutes gone by....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649
https://github.com/broadinstitute/cromwell/issues/1654:587,Deployability,update,updated,587,"Standardize docker builds with git commits. Automatically build docker images and push to the cromwell docker hub when certain commits occur on certain git branches. A suggested tagging scheme in docker could be:; <Cromwell-version>-<git-commit-hash>. For example: 0.21-5273abb This not only informs you what major cromwell version it is but also the specific git hash. And this tag would pretty much not ever move. Additionally a moving tag could be built with just the <cromwell-version> (ex. 0.21) This will always be the most recent commit for that version. And of course it will be updated each time. Tasks. - [x] change docker image labeling to be <version>-<git-commit-hash> in Settings.scala; - [x] change .travis.yml to run 'sbt dockerBuildAndPush' as part of the after_success step (confirm that is the right place; - [x] manually publish the docker image for the 0.21 latest release (ie checkout, sbt dockerBuildAndPush). ; - [x] also tag ^^ with 0.21; - [x] at this tagging step to our release process document; - [x] tell @hjfbynara and @abaumann about ^^",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654
https://github.com/broadinstitute/cromwell/issues/1654:886,Deployability,release,release,886,"Standardize docker builds with git commits. Automatically build docker images and push to the cromwell docker hub when certain commits occur on certain git branches. A suggested tagging scheme in docker could be:; <Cromwell-version>-<git-commit-hash>. For example: 0.21-5273abb This not only informs you what major cromwell version it is but also the specific git hash. And this tag would pretty much not ever move. Additionally a moving tag could be built with just the <cromwell-version> (ex. 0.21) This will always be the most recent commit for that version. And of course it will be updated each time. Tasks. - [x] change docker image labeling to be <version>-<git-commit-hash> in Settings.scala; - [x] change .travis.yml to run 'sbt dockerBuildAndPush' as part of the after_success step (confirm that is the right place; - [x] manually publish the docker image for the 0.21 latest release (ie checkout, sbt dockerBuildAndPush). ; - [x] also tag ^^ with 0.21; - [x] at this tagging step to our release process document; - [x] tell @hjfbynara and @abaumann about ^^",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654
https://github.com/broadinstitute/cromwell/issues/1654:998,Deployability,release,release,998,"Standardize docker builds with git commits. Automatically build docker images and push to the cromwell docker hub when certain commits occur on certain git branches. A suggested tagging scheme in docker could be:; <Cromwell-version>-<git-commit-hash>. For example: 0.21-5273abb This not only informs you what major cromwell version it is but also the specific git hash. And this tag would pretty much not ever move. Additionally a moving tag could be built with just the <cromwell-version> (ex. 0.21) This will always be the most recent commit for that version. And of course it will be updated each time. Tasks. - [x] change docker image labeling to be <version>-<git-commit-hash> in Settings.scala; - [x] change .travis.yml to run 'sbt dockerBuildAndPush' as part of the after_success step (confirm that is the right place; - [x] manually publish the docker image for the 0.21 latest release (ie checkout, sbt dockerBuildAndPush). ; - [x] also tag ^^ with 0.21; - [x] at this tagging step to our release process document; - [x] tell @hjfbynara and @abaumann about ^^",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654
https://github.com/broadinstitute/cromwell/issues/1654:245,Security,hash,hash,245,"Standardize docker builds with git commits. Automatically build docker images and push to the cromwell docker hub when certain commits occur on certain git branches. A suggested tagging scheme in docker could be:; <Cromwell-version>-<git-commit-hash>. For example: 0.21-5273abb This not only informs you what major cromwell version it is but also the specific git hash. And this tag would pretty much not ever move. Additionally a moving tag could be built with just the <cromwell-version> (ex. 0.21) This will always be the most recent commit for that version. And of course it will be updated each time. Tasks. - [x] change docker image labeling to be <version>-<git-commit-hash> in Settings.scala; - [x] change .travis.yml to run 'sbt dockerBuildAndPush' as part of the after_success step (confirm that is the right place; - [x] manually publish the docker image for the 0.21 latest release (ie checkout, sbt dockerBuildAndPush). ; - [x] also tag ^^ with 0.21; - [x] at this tagging step to our release process document; - [x] tell @hjfbynara and @abaumann about ^^",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654
https://github.com/broadinstitute/cromwell/issues/1654:364,Security,hash,hash,364,"Standardize docker builds with git commits. Automatically build docker images and push to the cromwell docker hub when certain commits occur on certain git branches. A suggested tagging scheme in docker could be:; <Cromwell-version>-<git-commit-hash>. For example: 0.21-5273abb This not only informs you what major cromwell version it is but also the specific git hash. And this tag would pretty much not ever move. Additionally a moving tag could be built with just the <cromwell-version> (ex. 0.21) This will always be the most recent commit for that version. And of course it will be updated each time. Tasks. - [x] change docker image labeling to be <version>-<git-commit-hash> in Settings.scala; - [x] change .travis.yml to run 'sbt dockerBuildAndPush' as part of the after_success step (confirm that is the right place; - [x] manually publish the docker image for the 0.21 latest release (ie checkout, sbt dockerBuildAndPush). ; - [x] also tag ^^ with 0.21; - [x] at this tagging step to our release process document; - [x] tell @hjfbynara and @abaumann about ^^",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654
https://github.com/broadinstitute/cromwell/issues/1654:676,Security,hash,hash,676,"Standardize docker builds with git commits. Automatically build docker images and push to the cromwell docker hub when certain commits occur on certain git branches. A suggested tagging scheme in docker could be:; <Cromwell-version>-<git-commit-hash>. For example: 0.21-5273abb This not only informs you what major cromwell version it is but also the specific git hash. And this tag would pretty much not ever move. Additionally a moving tag could be built with just the <cromwell-version> (ex. 0.21) This will always be the most recent commit for that version. And of course it will be updated each time. Tasks. - [x] change docker image labeling to be <version>-<git-commit-hash> in Settings.scala; - [x] change .travis.yml to run 'sbt dockerBuildAndPush' as part of the after_success step (confirm that is the right place; - [x] manually publish the docker image for the 0.21 latest release (ie checkout, sbt dockerBuildAndPush). ; - [x] also tag ^^ with 0.21; - [x] at this tagging step to our release process document; - [x] tell @hjfbynara and @abaumann about ^^",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1654
https://github.com/broadinstitute/cromwell/issues/1656:246,Deployability,Pipeline,Pipelines,246,"Initiated by conversation on http://gatkforums.broadinstitute.org/wdl/discussion/8546/using-different-service-account-on-compute-instance#latest . Currently, we are supplying 'default' to the RunPipelineArgs service_account email. This means the Pipelines API node will spin up using the default compute service account for the project. However (a) that account can be removed and (b) users may want to use a different service account. This should be configurable at both the cromwell server level (ie applies to all workflows) as well as by the workflow options (so it can vary by workflow). This new capability should also be documented in the README (or whever @katevoss says!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1656
https://github.com/broadinstitute/cromwell/issues/1656:451,Modifiability,config,configurable,451,"Initiated by conversation on http://gatkforums.broadinstitute.org/wdl/discussion/8546/using-different-service-account-on-compute-instance#latest . Currently, we are supplying 'default' to the RunPipelineArgs service_account email. This means the Pipelines API node will spin up using the default compute service account for the project. However (a) that account can be removed and (b) users may want to use a different service account. This should be configurable at both the cromwell server level (ie applies to all workflows) as well as by the workflow options (so it can vary by workflow). This new capability should also be documented in the README (or whever @katevoss says!)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1656
https://github.com/broadinstitute/cromwell/issues/1662:201,Availability,error,errors,201,"I ran the WDL at the bottom of this issue using mock-jes. Initially it ran everything but then got stuck in a state with 2 jobs left and the 1 wf still ""running"". I looked and those 2 jobs had had 500 errors associated with them. . At a later point I restarted Cromwell, for a long (> 10 mins) period of time the stats endpoint was unresponsive although other endpoints were fine. After some period of time (< 1.5 hours) I came back and those 2 jobs had completed but the WF was still stuck as Running. task snooze {; command {; sleep 10 && ps > myfile.txt; }; output {; File procs = ""myfile.txt""; }; runtime {; docker: ""ubuntu:14.04""; preemptible: 3; cpu: 10; }; }. workflow one_step {; Array[Int] integers = range(20000); scatter(i in integers) {; call snooze; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662
https://github.com/broadinstitute/cromwell/issues/1662:48,Testability,mock,mock-jes,48,"I ran the WDL at the bottom of this issue using mock-jes. Initially it ran everything but then got stuck in a state with 2 jobs left and the 1 wf still ""running"". I looked and those 2 jobs had had 500 errors associated with them. . At a later point I restarted Cromwell, for a long (> 10 mins) period of time the stats endpoint was unresponsive although other endpoints were fine. After some period of time (< 1.5 hours) I came back and those 2 jobs had completed but the WF was still stuck as Running. task snooze {; command {; sleep 10 && ps > myfile.txt; }; output {; File procs = ""myfile.txt""; }; runtime {; docker: ""ubuntu:14.04""; preemptible: 3; cpu: 10; }; }. workflow one_step {; Array[Int] integers = range(20000); scatter(i in integers) {; call snooze; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662
https://github.com/broadinstitute/cromwell/issues/1663:192,Availability,robust,robust,192,How does Cromwell deal with rate limits on JES? I have some largish batch jobs that seem to result in triggering JES (google genomics api) rate limits when submitted via Cromwell. Is Cromwell robust to this?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1663
https://github.com/broadinstitute/cromwell/issues/1665:457,Availability,ERROR,ERROR,457,"I was running Cromwell with a JES backend. I got through about 300 out of 1000 workflows that were submitted as a batch; the rest are still listed as `Running`. Oddly, the genomics API operations report that they are still running though there are no instances still up. I am not sure if this is a Cromwell issue or a Google one, but here is what I am seeing from Cromwell. Cromwell has been working well for smaller batches. Any thoughts? . Thanks!. ```; [ERROR] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] The JES polling actor Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$8#-1275882726] unexpectedly terminated while conducting 11 polls. Making a new one...; [INFO] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$9#-1852863496]; [WARN] [11/12/2016 02:22:12.171] [cromwell-system-akka.dispatchers.backend-dispatcher-2573] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/WorkflowExecutionActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-EngineJobExecutionActor-salmonRun.salmonQuant:NA:1/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-BackendJobExecutionActor-7f0d7c16:salmonRun.salmonQuant:-1:1/JesAsyncBackendJobExecutionActor] JesAsyncBackendJobExecutionActor [UUID(7f0d7c16)salmonRun.salmonQuant:NA:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; 	at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665
https://github.com/broadinstitute/cromwell/issues/1665:2323,Performance,concurren,concurrent,2323,"ystem/user/cromwell-service/$b/$a/$8#-1275882726] unexpectedly terminated while conducting 11 polls. Making a new one...; [INFO] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$9#-1852863496]; [WARN] [11/12/2016 02:22:12.171] [cromwell-system-akka.dispatchers.backend-dispatcher-2573] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/WorkflowExecutionActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-EngineJobExecutionActor-salmonRun.salmonQuant:NA:1/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-BackendJobExecutionActor-7f0d7c16:salmonRun.salmonQuant:-1:1/JesAsyncBackendJobExecutionActor] JesAsyncBackendJobExecutionActor [UUID(7f0d7c16)salmonRun.salmonQuant:NA:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; 	at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665
https://github.com/broadinstitute/cromwell/issues/1665:2397,Performance,concurren,concurrent,2397,"ystem/user/cromwell-service/$b/$a/$8#-1275882726] unexpectedly terminated while conducting 11 polls. Making a new one...; [INFO] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$9#-1852863496]; [WARN] [11/12/2016 02:22:12.171] [cromwell-system-akka.dispatchers.backend-dispatcher-2573] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/WorkflowExecutionActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-EngineJobExecutionActor-salmonRun.salmonQuant:NA:1/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-BackendJobExecutionActor-7f0d7c16:salmonRun.salmonQuant:-1:1/JesAsyncBackendJobExecutionActor] JesAsyncBackendJobExecutionActor [UUID(7f0d7c16)salmonRun.salmonQuant:NA:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; 	at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665
https://github.com/broadinstitute/cromwell/issues/1665:2483,Performance,concurren,concurrent,2483,"ystem/user/cromwell-service/$b/$a/$8#-1275882726] unexpectedly terminated while conducting 11 polls. Making a new one...; [INFO] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$9#-1852863496]; [WARN] [11/12/2016 02:22:12.171] [cromwell-system-akka.dispatchers.backend-dispatcher-2573] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/WorkflowExecutionActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-EngineJobExecutionActor-salmonRun.salmonQuant:NA:1/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-BackendJobExecutionActor-7f0d7c16:salmonRun.salmonQuant:-1:1/JesAsyncBackendJobExecutionActor] JesAsyncBackendJobExecutionActor [UUID(7f0d7c16)salmonRun.salmonQuant:NA:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; 	at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665
https://github.com/broadinstitute/cromwell/issues/1665:2561,Performance,concurren,concurrent,2561,"ystem/user/cromwell-service/$b/$a/$8#-1275882726] unexpectedly terminated while conducting 11 polls. Making a new one...; [INFO] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$9#-1852863496]; [WARN] [11/12/2016 02:22:12.171] [cromwell-system-akka.dispatchers.backend-dispatcher-2573] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/WorkflowExecutionActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-EngineJobExecutionActor-salmonRun.salmonQuant:NA:1/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-BackendJobExecutionActor-7f0d7c16:salmonRun.salmonQuant:-1:1/JesAsyncBackendJobExecutionActor] JesAsyncBackendJobExecutionActor [UUID(7f0d7c16)salmonRun.salmonQuant:NA:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; 	at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665
https://github.com/broadinstitute/cromwell/issues/1666:220,Testability,log,logical,220,"Just a feature request (not a roadblock to anything we are doing), but it would be useful to be able to track a batch ID when submitting batch jobs. Batch jobs are often a set of related jobs that can be thought of as a logical unit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1666
https://github.com/broadinstitute/cromwell/issues/1667:74,Deployability,hotfix,hotfix,74,"Hi, can someone please clarify on LSF support?; It was introduced in 0.19 hotfix,... then it came back again.; But it does not seem to be in the main branch e.g. in the current development branch.; Apologies if I've missed something obvious.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1667
https://github.com/broadinstitute/cromwell/issues/1669:71,Availability,error,error,71,"This sucked up a few hours of time with the debugger, we need a better error message/handling. If you misconfigure Cromwell (in my case using a service account) whereby the auth specified in JES.filesystems.gcs.auth is unable to write to the bucket specified in JES.config.root. Specifically, I found that the uploadCommandScript was dying silently in the JABJEA and my workflow just stopped running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1669
https://github.com/broadinstitute/cromwell/issues/1669:77,Integrability,message,message,77,"This sucked up a few hours of time with the debugger, we need a better error message/handling. If you misconfigure Cromwell (in my case using a service account) whereby the auth specified in JES.filesystems.gcs.auth is unable to write to the bucket specified in JES.config.root. Specifically, I found that the uploadCommandScript was dying silently in the JABJEA and my workflow just stopped running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1669
https://github.com/broadinstitute/cromwell/issues/1669:266,Modifiability,config,config,266,"This sucked up a few hours of time with the debugger, we need a better error message/handling. If you misconfigure Cromwell (in my case using a service account) whereby the auth specified in JES.filesystems.gcs.auth is unable to write to the bucket specified in JES.config.root. Specifically, I found that the uploadCommandScript was dying silently in the JABJEA and my workflow just stopped running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1669
https://github.com/broadinstitute/cromwell/issues/1670:67,Performance,cache,cached,67,I would like to be able to tell Cromwell to not use a workflow for cached results without mucking around the database. ; ![628b747f8ccdfb757062f36a27eedecfc2295f515c0586e05fbfb0620c0571a2](https://cloud.githubusercontent.com/assets/961771/20325706/cb81eed2-ab53-11e6-90a5-802160419fa5.jpg),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1670
https://github.com/broadinstitute/cromwell/issues/1671:171,Availability,robust,robust,171,Reported by @fleharty - it appears that when call caching in symlink mode if the real file no long exists the caching will not fail. . Regardless of the mode we should be robust to cases where we're attempting to call cache but the underlying data is not there,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1671
https://github.com/broadinstitute/cromwell/issues/1671:218,Performance,cache,cache,218,Reported by @fleharty - it appears that when call caching in symlink mode if the real file no long exists the caching will not fail. . Regardless of the mode we should be robust to cases where we're attempting to call cache but the underlying data is not there,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1671
https://github.com/broadinstitute/cromwell/issues/1673:104,Deployability,Release,Release,104,"Source: https://broadinstitute.atlassian.net/wiki/pages/viewpage.action?spaceKey=DSDEEPB&title=Cromwell+Release. There are specific portions in the Cromwell Release checklist that are common among all the repos and can become reusable/composable step in automating the Cromwell release. Turn those steps into an executable script. repos = [wdltool, lenthall, wdl4s, cromwell]. - [ ] Run tests locally on develop to ensure tests still pass; - [ ] Ensure the build.sbt file has the correct version. If it does not, adjust it, commit and push to develop.; - [ ] Merge develop into master.; - [ ] Tag the resulting merge commit on master with the version number (e.g. 0.10,1.2.0); - [ ] git push origin master; - [ ] git push --tags; - [ ] This should kick off the travis build of the tag. Ensure that it completes successfully. This job should also produce a JAR in artifactory: https://artifactory.broadinstitute.org/. Ensure that this JAR exists. For example for wdl4s it would be in: libs-release-local/org/broadinstitute/wdl4s_2.11; - [ ] On develop, update build.sbt with the next version number",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1673
https://github.com/broadinstitute/cromwell/issues/1673:157,Deployability,Release,Release,157,"Source: https://broadinstitute.atlassian.net/wiki/pages/viewpage.action?spaceKey=DSDEEPB&title=Cromwell+Release. There are specific portions in the Cromwell Release checklist that are common among all the repos and can become reusable/composable step in automating the Cromwell release. Turn those steps into an executable script. repos = [wdltool, lenthall, wdl4s, cromwell]. - [ ] Run tests locally on develop to ensure tests still pass; - [ ] Ensure the build.sbt file has the correct version. If it does not, adjust it, commit and push to develop.; - [ ] Merge develop into master.; - [ ] Tag the resulting merge commit on master with the version number (e.g. 0.10,1.2.0); - [ ] git push origin master; - [ ] git push --tags; - [ ] This should kick off the travis build of the tag. Ensure that it completes successfully. This job should also produce a JAR in artifactory: https://artifactory.broadinstitute.org/. Ensure that this JAR exists. For example for wdl4s it would be in: libs-release-local/org/broadinstitute/wdl4s_2.11; - [ ] On develop, update build.sbt with the next version number",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1673
https://github.com/broadinstitute/cromwell/issues/1673:278,Deployability,release,release,278,"Source: https://broadinstitute.atlassian.net/wiki/pages/viewpage.action?spaceKey=DSDEEPB&title=Cromwell+Release. There are specific portions in the Cromwell Release checklist that are common among all the repos and can become reusable/composable step in automating the Cromwell release. Turn those steps into an executable script. repos = [wdltool, lenthall, wdl4s, cromwell]. - [ ] Run tests locally on develop to ensure tests still pass; - [ ] Ensure the build.sbt file has the correct version. If it does not, adjust it, commit and push to develop.; - [ ] Merge develop into master.; - [ ] Tag the resulting merge commit on master with the version number (e.g. 0.10,1.2.0); - [ ] git push origin master; - [ ] git push --tags; - [ ] This should kick off the travis build of the tag. Ensure that it completes successfully. This job should also produce a JAR in artifactory: https://artifactory.broadinstitute.org/. Ensure that this JAR exists. For example for wdl4s it would be in: libs-release-local/org/broadinstitute/wdl4s_2.11; - [ ] On develop, update build.sbt with the next version number",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1673
https://github.com/broadinstitute/cromwell/issues/1673:989,Deployability,release,release-local,989,"Source: https://broadinstitute.atlassian.net/wiki/pages/viewpage.action?spaceKey=DSDEEPB&title=Cromwell+Release. There are specific portions in the Cromwell Release checklist that are common among all the repos and can become reusable/composable step in automating the Cromwell release. Turn those steps into an executable script. repos = [wdltool, lenthall, wdl4s, cromwell]. - [ ] Run tests locally on develop to ensure tests still pass; - [ ] Ensure the build.sbt file has the correct version. If it does not, adjust it, commit and push to develop.; - [ ] Merge develop into master.; - [ ] Tag the resulting merge commit on master with the version number (e.g. 0.10,1.2.0); - [ ] git push origin master; - [ ] git push --tags; - [ ] This should kick off the travis build of the tag. Ensure that it completes successfully. This job should also produce a JAR in artifactory: https://artifactory.broadinstitute.org/. Ensure that this JAR exists. For example for wdl4s it would be in: libs-release-local/org/broadinstitute/wdl4s_2.11; - [ ] On develop, update build.sbt with the next version number",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1673
https://github.com/broadinstitute/cromwell/issues/1673:1052,Deployability,update,update,1052,"Source: https://broadinstitute.atlassian.net/wiki/pages/viewpage.action?spaceKey=DSDEEPB&title=Cromwell+Release. There are specific portions in the Cromwell Release checklist that are common among all the repos and can become reusable/composable step in automating the Cromwell release. Turn those steps into an executable script. repos = [wdltool, lenthall, wdl4s, cromwell]. - [ ] Run tests locally on develop to ensure tests still pass; - [ ] Ensure the build.sbt file has the correct version. If it does not, adjust it, commit and push to develop.; - [ ] Merge develop into master.; - [ ] Tag the resulting merge commit on master with the version number (e.g. 0.10,1.2.0); - [ ] git push origin master; - [ ] git push --tags; - [ ] This should kick off the travis build of the tag. Ensure that it completes successfully. This job should also produce a JAR in artifactory: https://artifactory.broadinstitute.org/. Ensure that this JAR exists. For example for wdl4s it would be in: libs-release-local/org/broadinstitute/wdl4s_2.11; - [ ] On develop, update build.sbt with the next version number",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1673
https://github.com/broadinstitute/cromwell/issues/1673:387,Testability,test,tests,387,"Source: https://broadinstitute.atlassian.net/wiki/pages/viewpage.action?spaceKey=DSDEEPB&title=Cromwell+Release. There are specific portions in the Cromwell Release checklist that are common among all the repos and can become reusable/composable step in automating the Cromwell release. Turn those steps into an executable script. repos = [wdltool, lenthall, wdl4s, cromwell]. - [ ] Run tests locally on develop to ensure tests still pass; - [ ] Ensure the build.sbt file has the correct version. If it does not, adjust it, commit and push to develop.; - [ ] Merge develop into master.; - [ ] Tag the resulting merge commit on master with the version number (e.g. 0.10,1.2.0); - [ ] git push origin master; - [ ] git push --tags; - [ ] This should kick off the travis build of the tag. Ensure that it completes successfully. This job should also produce a JAR in artifactory: https://artifactory.broadinstitute.org/. Ensure that this JAR exists. For example for wdl4s it would be in: libs-release-local/org/broadinstitute/wdl4s_2.11; - [ ] On develop, update build.sbt with the next version number",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1673
https://github.com/broadinstitute/cromwell/issues/1673:422,Testability,test,tests,422,"Source: https://broadinstitute.atlassian.net/wiki/pages/viewpage.action?spaceKey=DSDEEPB&title=Cromwell+Release. There are specific portions in the Cromwell Release checklist that are common among all the repos and can become reusable/composable step in automating the Cromwell release. Turn those steps into an executable script. repos = [wdltool, lenthall, wdl4s, cromwell]. - [ ] Run tests locally on develop to ensure tests still pass; - [ ] Ensure the build.sbt file has the correct version. If it does not, adjust it, commit and push to develop.; - [ ] Merge develop into master.; - [ ] Tag the resulting merge commit on master with the version number (e.g. 0.10,1.2.0); - [ ] git push origin master; - [ ] git push --tags; - [ ] This should kick off the travis build of the tag. Ensure that it completes successfully. This job should also produce a JAR in artifactory: https://artifactory.broadinstitute.org/. Ensure that this JAR exists. For example for wdl4s it would be in: libs-release-local/org/broadinstitute/wdl4s_2.11; - [ ] On develop, update build.sbt with the next version number",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1673
https://github.com/broadinstitute/cromwell/pull/1676:0,Integrability,Depend,Depends,0,Depends on https://github.com/broadinstitute/wdl4s/pull/49,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1676
https://github.com/broadinstitute/cromwell/issues/1678:116,Availability,failure,failure,116,Sometimes users are interested in a particular workflow and would like to know when changes to some terminal state: failure or success. Allowing multiple users to register as interested in a workflow and emailing them when the workflow is reaches one of these states (maybe also registering the set of desired states to be notified about) would be useful.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678
https://github.com/broadinstitute/cromwell/pull/1682:777,Availability,avail,available,777,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682
https://github.com/broadinstitute/cromwell/pull/1682:289,Integrability,inject,injected,289,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682
https://github.com/broadinstitute/cromwell/pull/1682:348,Safety,Abort,Aborts,348,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682
https://github.com/broadinstitute/cromwell/pull/1682:386,Safety,abort,abort,386,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682
https://github.com/broadinstitute/cromwell/pull/1682:289,Security,inject,injected,289,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682
https://github.com/broadinstitute/cromwell/pull/1682:569,Testability,log,logs,569,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682
https://github.com/broadinstitute/cromwell/issues/1683:30,Modifiability,config,config,30,"When Cromwell is pointed to a config referencing a pluggable backend class that's not on the classpath, this is the result:. ```; [me@computer ~]$ java8 -Dconfig.file=$PWD/cromwell.conf -jar cromwell.jar server; Exception in thread ""main"" java.lang.ClassNotFoundException: cromwell.backend.impl.aws.AwsBackendActorFactory; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:11); 	at cromwell.engine.backend.CromwellBackends$$anonfun$1.apply(CromwellBackends.scala:12); 	at cromwell.engine.backend.CromwellBackends$$anonfun$1.apply(CromwellBackends.scala:12); 	at scala.collection.immutable.List.map(List.scala:277); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:12); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:40); 	at cromwell.server.CromwellSystem$class.$init$(CromwellSystem.scala:21); 	at cromwell.Main$$anon$1.<init>(Main.scala:20); 	at cromwell.Main$.CromwellSystem$lzycompute(Main.scala:20); 	at cromwell.Main$.CromwellSystem(Main.scala:20); 	at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:24); 	at cromwell.Main$delayedInit$body.apply(Main.scala:15); 	at scala.Function0$class.apply$mcV$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App$$anonfun$main$1.apply(App.scala:76); 	at scala.App$$anonfun$main$1.apply(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); 	at scala.App$class.main(App.scala:76); 	at cromwell.Main$.main",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683
https://github.com/broadinstitute/cromwell/issues/1683:413,Performance,load,loadClass,413,"When Cromwell is pointed to a config referencing a pluggable backend class that's not on the classpath, this is the result:. ```; [me@computer ~]$ java8 -Dconfig.file=$PWD/cromwell.conf -jar cromwell.jar server; Exception in thread ""main"" java.lang.ClassNotFoundException: cromwell.backend.impl.aws.AwsBackendActorFactory; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:11); 	at cromwell.engine.backend.CromwellBackends$$anonfun$1.apply(CromwellBackends.scala:12); 	at cromwell.engine.backend.CromwellBackends$$anonfun$1.apply(CromwellBackends.scala:12); 	at scala.collection.immutable.List.map(List.scala:277); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:12); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:40); 	at cromwell.server.CromwellSystem$class.$init$(CromwellSystem.scala:21); 	at cromwell.Main$$anon$1.<init>(Main.scala:20); 	at cromwell.Main$.CromwellSystem$lzycompute(Main.scala:20); 	at cromwell.Main$.CromwellSystem(Main.scala:20); 	at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:24); 	at cromwell.Main$delayedInit$body.apply(Main.scala:15); 	at scala.Function0$class.apply$mcV$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App$$anonfun$main$1.apply(App.scala:76); 	at scala.App$$anonfun$main$1.apply(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); 	at scala.App$class.main(App.scala:76); 	at cromwell.Main$.main",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683
https://github.com/broadinstitute/cromwell/issues/1683:483,Performance,load,loadClass,483,"When Cromwell is pointed to a config referencing a pluggable backend class that's not on the classpath, this is the result:. ```; [me@computer ~]$ java8 -Dconfig.file=$PWD/cromwell.conf -jar cromwell.jar server; Exception in thread ""main"" java.lang.ClassNotFoundException: cromwell.backend.impl.aws.AwsBackendActorFactory; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:11); 	at cromwell.engine.backend.CromwellBackends$$anonfun$1.apply(CromwellBackends.scala:12); 	at cromwell.engine.backend.CromwellBackends$$anonfun$1.apply(CromwellBackends.scala:12); 	at scala.collection.immutable.List.map(List.scala:277); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:12); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:40); 	at cromwell.server.CromwellSystem$class.$init$(CromwellSystem.scala:21); 	at cromwell.Main$$anon$1.<init>(Main.scala:20); 	at cromwell.Main$.CromwellSystem$lzycompute(Main.scala:20); 	at cromwell.Main$.CromwellSystem(Main.scala:20); 	at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:24); 	at cromwell.Main$delayedInit$body.apply(Main.scala:15); 	at scala.Function0$class.apply$mcV$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App$$anonfun$main$1.apply(App.scala:76); 	at scala.App$$anonfun$main$1.apply(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); 	at scala.App$class.main(App.scala:76); 	at cromwell.Main$.main",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683
https://github.com/broadinstitute/cromwell/issues/1683:539,Performance,load,loadClass,539,"When Cromwell is pointed to a config referencing a pluggable backend class that's not on the classpath, this is the result:. ```; [me@computer ~]$ java8 -Dconfig.file=$PWD/cromwell.conf -jar cromwell.jar server; Exception in thread ""main"" java.lang.ClassNotFoundException: cromwell.backend.impl.aws.AwsBackendActorFactory; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:264); 	at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:11); 	at cromwell.engine.backend.CromwellBackends$$anonfun$1.apply(CromwellBackends.scala:12); 	at cromwell.engine.backend.CromwellBackends$$anonfun$1.apply(CromwellBackends.scala:12); 	at scala.collection.immutable.List.map(List.scala:277); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:12); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:40); 	at cromwell.server.CromwellSystem$class.$init$(CromwellSystem.scala:21); 	at cromwell.Main$$anon$1.<init>(Main.scala:20); 	at cromwell.Main$.CromwellSystem$lzycompute(Main.scala:20); 	at cromwell.Main$.CromwellSystem(Main.scala:20); 	at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:24); 	at cromwell.Main$delayedInit$body.apply(Main.scala:15); 	at scala.Function0$class.apply$mcV$sp(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App$$anonfun$main$1.apply(App.scala:76); 	at scala.App$$anonfun$main$1.apply(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35); 	at scala.App$class.main(App.scala:76); 	at cromwell.Main$.main",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1683
https://github.com/broadinstitute/cromwell/issues/1684:325,Availability,reliab,reliably,325,"Currently Workflow outputs can be copied at the end of a workflow, and this is done by then engine WorkflowFinalizationActor.; All the information this actor has is file paths as `String`s. To be able to copy those files out it needs to create a `Path` from them with the right filesystem / auth, which it currently can't do reliably since it doesn't have any information about which backend produced this output or with which auth. A possible fix to that would be to make `wdl4s.WdlFile` wrap `java.nio.File` instead of `String`, so the filesystem / auth information is not lost.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1684
https://github.com/broadinstitute/cromwell/issues/1684:489,Integrability,wrap,wrap,489,"Currently Workflow outputs can be copied at the end of a workflow, and this is done by then engine WorkflowFinalizationActor.; All the information this actor has is file paths as `String`s. To be able to copy those files out it needs to create a `Path` from them with the right filesystem / auth, which it currently can't do reliably since it doesn't have any information about which backend produced this output or with which auth. A possible fix to that would be to make `wdl4s.WdlFile` wrap `java.nio.File` instead of `String`, so the filesystem / auth information is not lost.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1684
https://github.com/broadinstitute/cromwell/issues/1687:7,Deployability,update,update,7,Either update it or remove it:; http://broadinstitute.github.io/cromwell/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1687
https://github.com/broadinstitute/cromwell/issues/1689:21,Modifiability,variab,variables,21,"Workflow-wide global variables would be useful for input parameters to the workflow that are constant across tasks e.g. reference file, reference index, gatk jar. Way things are now we have to give them as input to each task individually, which clutters code.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1689
https://github.com/broadinstitute/cromwell/issues/1690:660,Availability,echo,echo,660,"When running some of our pipelines with the latest release version (0.22), I am not able to get the pipelines to actually complete if they run into a preemption during their scatter operation. The scatter block will successfully complete for all cases, however the execution stalls and does not progress any further. To make sure this had nothing to do with a slow process I let the job sit overnight, and still it did not complete, or even move past the ""Success"" of the final scatter operation to complete. I used the wdl code below to replicate the issue several times locally, without having to run our entire workflow. ```; task c {; Int num; command {; 	echo ${num}; #Wait 5 minutes, and hope for a preemption. make longer to force preemption; 	sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; Int out = read_int(stdout()); }; }; task d {; Array[Int] num; command {; 	echo ${sep="" "" num}; sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; String out=stdout(); }; }; workflow wf {; Array[Int] first = [1,2,3,4,5]. scatter (x in first){; 	call c {input: num=x}; 	call c as a {input: num=c.out}; }. call d {input: num=a.out}; output {; d.*; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1690
https://github.com/broadinstitute/cromwell/issues/1690:897,Availability,echo,echo,897,"When running some of our pipelines with the latest release version (0.22), I am not able to get the pipelines to actually complete if they run into a preemption during their scatter operation. The scatter block will successfully complete for all cases, however the execution stalls and does not progress any further. To make sure this had nothing to do with a slow process I let the job sit overnight, and still it did not complete, or even move past the ""Success"" of the final scatter operation to complete. I used the wdl code below to replicate the issue several times locally, without having to run our entire workflow. ```; task c {; Int num; command {; 	echo ${num}; #Wait 5 minutes, and hope for a preemption. make longer to force preemption; 	sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; Int out = read_int(stdout()); }; }; task d {; Array[Int] num; command {; 	echo ${sep="" "" num}; sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; String out=stdout(); }; }; workflow wf {; Array[Int] first = [1,2,3,4,5]. scatter (x in first){; 	call c {input: num=x}; 	call c as a {input: num=c.out}; }. call d {input: num=a.out}; output {; d.*; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1690
https://github.com/broadinstitute/cromwell/issues/1690:25,Deployability,pipeline,pipelines,25,"When running some of our pipelines with the latest release version (0.22), I am not able to get the pipelines to actually complete if they run into a preemption during their scatter operation. The scatter block will successfully complete for all cases, however the execution stalls and does not progress any further. To make sure this had nothing to do with a slow process I let the job sit overnight, and still it did not complete, or even move past the ""Success"" of the final scatter operation to complete. I used the wdl code below to replicate the issue several times locally, without having to run our entire workflow. ```; task c {; Int num; command {; 	echo ${num}; #Wait 5 minutes, and hope for a preemption. make longer to force preemption; 	sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; Int out = read_int(stdout()); }; }; task d {; Array[Int] num; command {; 	echo ${sep="" "" num}; sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; String out=stdout(); }; }; workflow wf {; Array[Int] first = [1,2,3,4,5]. scatter (x in first){; 	call c {input: num=x}; 	call c as a {input: num=c.out}; }. call d {input: num=a.out}; output {; d.*; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1690
https://github.com/broadinstitute/cromwell/issues/1690:51,Deployability,release,release,51,"When running some of our pipelines with the latest release version (0.22), I am not able to get the pipelines to actually complete if they run into a preemption during their scatter operation. The scatter block will successfully complete for all cases, however the execution stalls and does not progress any further. To make sure this had nothing to do with a slow process I let the job sit overnight, and still it did not complete, or even move past the ""Success"" of the final scatter operation to complete. I used the wdl code below to replicate the issue several times locally, without having to run our entire workflow. ```; task c {; Int num; command {; 	echo ${num}; #Wait 5 minutes, and hope for a preemption. make longer to force preemption; 	sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; Int out = read_int(stdout()); }; }; task d {; Array[Int] num; command {; 	echo ${sep="" "" num}; sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; String out=stdout(); }; }; workflow wf {; Array[Int] first = [1,2,3,4,5]. scatter (x in first){; 	call c {input: num=x}; 	call c as a {input: num=c.out}; }. call d {input: num=a.out}; output {; d.*; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1690
https://github.com/broadinstitute/cromwell/issues/1690:100,Deployability,pipeline,pipelines,100,"When running some of our pipelines with the latest release version (0.22), I am not able to get the pipelines to actually complete if they run into a preemption during their scatter operation. The scatter block will successfully complete for all cases, however the execution stalls and does not progress any further. To make sure this had nothing to do with a slow process I let the job sit overnight, and still it did not complete, or even move past the ""Success"" of the final scatter operation to complete. I used the wdl code below to replicate the issue several times locally, without having to run our entire workflow. ```; task c {; Int num; command {; 	echo ${num}; #Wait 5 minutes, and hope for a preemption. make longer to force preemption; 	sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; Int out = read_int(stdout()); }; }; task d {; Array[Int] num; command {; 	echo ${sep="" "" num}; sleep 300; }; runtime {; docker: ""ubuntu""; preemptible: 4; }; output {; String out=stdout(); }; }; workflow wf {; Array[Int] first = [1,2,3,4,5]. scatter (x in first){; 	call c {input: num=x}; 	call c as a {input: num=c.out}; }. call d {input: num=a.out}; output {; d.*; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1690
https://github.com/broadinstitute/cromwell/issues/1691:4,Availability,error,error,4,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:107,Availability,error,error,107,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:167,Availability,error,errors,167,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:525,Deployability,PATCH,PATCH,525,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:10,Integrability,message,messages,10,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:117,Integrability,message,message,117,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:213,Integrability,message,message,213,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:279,Integrability,message,message,279,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:442,Security,Access,Access-Control-Max-Age,442,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:480,Security,Access,Access-Control-Allow-Methods,480,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:607,Security,Access,Access-Control-Allow-Origin,607,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:644,Security,Access,Access-Control-Expose-Headers,644,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:710,Security,Access,Access-Control-Allow-Headers,710,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/issues/1691:742,Security,authoriz,authorization,742,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691
https://github.com/broadinstitute/cromwell/pull/1692:173,Deployability,update,update,173,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:692,Deployability,configurat,configurationFile,692,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1163,Integrability,message,messages,1163,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1253,Integrability,message,messages,1253,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1319,Integrability,message,message,1319,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1380,Integrability,message,message,1380,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1488,Integrability,message,message,1488,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:692,Modifiability,config,configurationFile,692,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:132,Testability,test,test,132,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:467,Testability,log,logback,467,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:524,Testability,log,logs,524,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:647,Testability,log,logs,647,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:719,Testability,log,logback,719,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:957,Testability,log,logs,957,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1097,Testability,log,logs,1097,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1159,Testability,log,log,1159,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1176,Testability,log,logfile,1176,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1229,Testability,log,logfile,1229,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1376,Testability,log,log,1376,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1423,Testability,log,logfile,1423,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/pull/1692:1484,Testability,log,log,1484,"I labeled ""do not merge"" since I would like discussion first and there are one or two items that I need to figure out a good way to test. Once merged and in image one would update JAVA_OPTS similar to below to enable and modify the defaults. JAVA_OPTS=""-DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"". To try it out prior a full build I did the following. 1. Copy the logback.xml from this branch into some directory. make a logs subdirectory; 2. from that directory run the following. docker run -it --rm -p 8000:8000 -v ${PWD}:/working -v ${PWD}/logs:/local/cromwell -e JAVA_OPTS=""-Dlogback.configurationFile=/working/logback.xml -DLOG_MODE=FILEROLLER -DFILEROLLER_NAMEPATTERN=%d{yyyyMMddHHmm} -DFILEROLLER_NAME=crom -DFILEROLLER_DIR=/local/cromwell -DFILEROLLER_MAXHISTORY=3"" broadinstitute/cromwell:0.22-881e7b7 server. after running for several minutes logs directory should end up looking like:. 201611182054-crom; 201611182056-crom; crom. NOTE: FileRoller rotation seems to only happen when logs are generated. So IF your cromwell is not generating any log messages the logfile won't be rotated. So in my example above the logfile ""crom"" may have messages from several minutes ago (in my case). But when the next message comes in - crom will be rotated prior to the new log message being written. I think the rotated logfile will get the timestamp based on the date of the last log message in that file. Which is why in the list above there is a gap.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692
https://github.com/broadinstitute/cromwell/issues/1694:252,Availability,avail,available,252,"As a user I would like:. - [ ] an endpoint (/version) which returns metadata about the current version of cromwell. Currently just the version of the server, but if/when we have WDL versioning it should include that as well; - [ ] the same information available from a command line option (version). --- Original -- ; That returns the versions of Cromwell / WDL / etc. Ultimately we want to bubble this up to users in FC.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1694
https://github.com/broadinstitute/cromwell/issues/1695:369,Performance,cache,cache,369,"Currently we have the ability to turn caching on/off at a cromwell level, or a workflow level, but sometimes there are specific tasks in a workflow that should either participate or not in call caching. Similar to the workflow options, a task-level runtime set of options should exist to expose this behavior:; write_to_cache: don't add the results of this call to the cache; read_from_cache: don't attempt to use the cache for this call. Requested by Lee as they migrate off Queue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695
https://github.com/broadinstitute/cromwell/issues/1695:418,Performance,cache,cache,418,"Currently we have the ability to turn caching on/off at a cromwell level, or a workflow level, but sometimes there are specific tasks in a workflow that should either participate or not in call caching. Similar to the workflow options, a task-level runtime set of options should exist to expose this behavior:; write_to_cache: don't add the results of this call to the cache; read_from_cache: don't attempt to use the cache for this call. Requested by Lee as they migrate off Queue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695
https://github.com/broadinstitute/cromwell/issues/1695:476,Performance,Queue,Queue,476,"Currently we have the ability to turn caching on/off at a cromwell level, or a workflow level, but sometimes there are specific tasks in a workflow that should either participate or not in call caching. Similar to the workflow options, a task-level runtime set of options should exist to expose this behavior:; write_to_cache: don't add the results of this call to the cache; read_from_cache: don't attempt to use the cache for this call. Requested by Lee as they migrate off Queue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695
https://github.com/broadinstitute/cromwell/issues/1695:288,Security,expose,expose,288,"Currently we have the ability to turn caching on/off at a cromwell level, or a workflow level, but sometimes there are specific tasks in a workflow that should either participate or not in call caching. Similar to the workflow options, a task-level runtime set of options should exist to expose this behavior:; write_to_cache: don't add the results of this call to the cache; read_from_cache: don't attempt to use the cache for this call. Requested by Lee as they migrate off Queue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695
https://github.com/broadinstitute/cromwell/pull/1699:121,Availability,echo,echo,121,"Will need https://github.com/broadinstitute/wdl4s/pull/52 merged. Enables WDL like . ```; task t {; String i; command {; echo ""lol""; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t {input: i = ""hello""}; String a = t.o # This currently would not work; call t as u {input: i = a }; String b = u.o; ; output {; String wo = b; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1699
https://github.com/broadinstitute/cromwell/issues/1702:238,Availability,echo,echo,238,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702
https://github.com/broadinstitute/cromwell/issues/1702:592,Availability,ERROR,ERROR,592,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702
https://github.com/broadinstitute/cromwell/issues/1702:386,Modifiability,config,configurable,386,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702
https://github.com/broadinstitute/cromwell/issues/1702:1063,Modifiability,config,config,1063,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702
https://github.com/broadinstitute/cromwell/issues/1702:1070,Modifiability,Config,ConfigBackendLifecycleActorFactory,1070,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702
https://github.com/broadinstitute/cromwell/issues/1702:1107,Modifiability,config,config,1107,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702
https://github.com/broadinstitute/cromwell/issues/1702:1697,Security,validat,validated,1697,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702
https://github.com/broadinstitute/cromwell/issues/1702:579,Testability,log,logs,579,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702
https://github.com/broadinstitute/cromwell/issues/1702:1362,Testability,test,tested,1362,"Following the docs at https://github.com/broadinstitute/cromwell#runtime-attributes, I'd like to be able to pass runtime attributes as the inputs to a task, for example:; ```; task iRun {; String runtimeMemory; Int runtimeCpu; command {; echo ""so far away""; }; output {; String out = read_string(stdout()); }; runtime {; memory: runtimeMemory; #cpu: runtimeCpu; }; }; ```; When using a configurable backend, I can confirm this works for the String type attribute `memory` but not the Int type `cpu`: running the above with the cpu runtime attribute uncommented I get this in the logs:; ```; [ERROR] [11/24/2016 10:49:13.299] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow beb03899-5f22-4f2c-8a85-d619a2d8a969 failed (during InitializingWorkflowState): java.lang.IllegalArgumentException: Task iRun has an invalid runtime attribute cpu = runtimeCpu; ```. My custom backend application.conf section: ; ```; PBS { ; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config { ; runtime-attributes = """"""; Int cpu = 1 ; Int memory_mb = 1000; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; ...; }; }; ```. I thought it might be because of the special nature of the `cpu` runtime attribute, but I tested with a different custom runtime attribute `Int pbs_cpu` and got the same result, so my guess is that it's the Int type that is the problem. I am working around this by defining `String pbs_cpu` which is then interpreted as an expression in the runtime block, as documented, but it feels wrong because the value should really be validated as an Int.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1702
https://github.com/broadinstitute/cromwell/issues/1703:831,Availability,echo,echo,831,"@Horneth in https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairString",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:1615,Availability,echo,echo,1615,"= [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c504",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:1748,Availability,echo,echo,1748,"ing {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d64752",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:1881,Availability,echo,echo,1881,"info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:4636,Availability,error,error,4636,"syncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(WdlValueJsonFormatter.scala:10); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:4517,Deployability,configurat,configuration,4517,"tion/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(WdlValueJsonFormatter.scala:10); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(I",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:4103,Integrability,Message,Message,4103,"5:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(W",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:4517,Modifiability,config,configuration,4517,"tion/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(WdlValueJsonFormatter.scala:10); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(I",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:9402,Performance,concurren,concurrent,9402,"cala:248); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:286); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:102); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:80); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:31); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-11-24 15:22:46,81] [info] WorkflowManagerActor WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049 is in a terminal state: WorkflowFailedState. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:9476,Performance,concurren,concurrent,9476,"cala:248); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:286); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:102); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:80); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:31); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-11-24 15:22:46,81] [info] WorkflowManagerActor WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049 is in a terminal state: WorkflowFailedState. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:9562,Performance,concurren,concurrent,9562,"cala:248); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:286); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:102); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:80); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:31); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-11-24 15:22:46,81] [info] WorkflowManagerActor WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049 is in a terminal state: WorkflowFailedState. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:9640,Performance,concurren,concurrent,9640,"cala:248); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:286); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:102); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:80); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:31); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-11-24 15:22:46,81] [info] WorkflowManagerActor WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049 is in a terminal state: WorkflowFailedState. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:244,Testability,log,logs,244,"@Horneth in https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairString",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:429,Testability,test,testMe,429,"@Horneth in https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairString",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:444,Testability,test,testZippedOutput,444,"@Horneth in https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairString",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:480,Testability,test,testZippedOutput,480,"@Horneth in https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairString",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:569,Testability,test,testZippedOutput,569,"@Horneth in https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairString",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:980,Testability,test,testZippedOutput,980,"#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:1060,Testability,test,testMe,1060,"#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:1109,Testability,test,testZippedOutput,1109,"#issuecomment-261096132 you have an example workflow with zipped outputs that you suggest should work, but it doesn't work for me using latest cromwell-23-a763495-SNAP.jar - from the logs it looks like workflow executes but then dies in some serialization step. My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-exec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:1239,Testability,test,testZippedOutput,1239," My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:1398,Testability,test,testMe,1398," My example workflow, slightly different than yours but same idea and dies in the same way:; ```; workflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:1432,Testability,test,testMe,1432,"rkflow testMe {. call testZippedOutput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:1466,Testability,test,testMe,1466,"tput. scatter ( pair in testZippedOutput.zipped ) {; call printPairStringString { input: pair=pair }; }. }. task testZippedOutput {; Array[String] foo = [""foo1"", ""foo2"", ""foo3""]; Array[String] bar = [""bar1"", ""bar2"", ""bar3""]; command {; }; output {; Array[Pair[String, String]] zipped = zip(foo, bar); }; }. task printPairStringString {; Pair[String, String] pair; command {; echo ""${pair.left} ${pair.right}""; }; }; ```. outputs: ; ```; [2016-11-24 15:22:45,17] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-testZippedOutput/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:2069,Testability,test,testMe,2069,"Output/execution/script.submit""; [2016-11-24 15:22:45,18] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.testZippedOutput:NA:1]: job id: 26744; [2016-11-24 15:22:45,21] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: executing: /bin/bash",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:2337,Testability,test,testMe,2337,"f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: testMe.printPairStringString:0:1, testMe.printPairStringString:1:1, testMe.printPairStringString:2:1; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: echo ""foo1 bar1""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.prin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:2612,Testability,test,testMe,2612,"o] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: echo ""foo2 bar2""; [2016-11-24 15:22:45,22] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: echo ""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:2880,Testability,test,testMe,2880,"""foo3 bar3""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairSt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:3155,Testability,test,testMe,3155,"ileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-0/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[ak",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:3423,Testability,test,testMe,3423,"ipt.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-1/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:4474,Testability,log,logging,4474,"tion/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(WdlValueJsonFormatter.scala:10); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(I",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:4546,Testability,log,log-dead-letters,4546,"SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(WdlValueJsonFormatter.scala:10); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIter",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:4574,Testability,log,log-dead-letters-during-shutdown,4574,"onActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(WdlValueJsonFormatter.scala:10); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:133",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:8556,Testability,Log,LoggingFSM,8556,xecutionActor.scala:248); 	at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleWorkflowSuccessful(WorkflowExecutionActor.scala:248); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:286); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:102); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:80); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:31); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/issues/1703:8636,Testability,Log,LoggingFSM,8636,ly(Try.scala:237); 	at scala.util.Try$.apply(Try.scala:192); 	at scala.util.Success.map(Try.scala:237); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.handleWorkflowSuccessful(WorkflowExecutionActor.scala:248); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$handleCallSuccessful(WorkflowExecutionActor.scala:286); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:102); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$3.applyOrElse(WorkflowExecutionActor.scala:80); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:31); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:31); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703
https://github.com/broadinstitute/cromwell/pull/1704:184,Modifiability,extend,extend,184,"JsArray is the simplest... but do you think JsObject with keys ""left"" and ""right"", or ""0"" and ""1"" would be better?; ""left"" and ""right"" would be consistent with the wdl4s code, but not extend very well to larger tuple types if you ever decide to go there.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704
https://github.com/broadinstitute/cromwell/pull/1704:15,Usability,simpl,simplest,15,"JsArray is the simplest... but do you think JsObject with keys ""left"" and ""right"", or ""0"" and ""1"" would be better?; ""left"" and ""right"" would be consistent with the wdl4s code, but not extend very well to larger tuple types if you ever decide to go there.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1704
https://github.com/broadinstitute/cromwell/issues/1705:176,Availability,down,down,176,"NOTE: Not a request for immediate work. I'm not even sure that the suggestions are technically sound. This is something I've been mulling over for a while and wanted to get it down as a place holder and also in case anyone else is interested in experimenting. In particular the source and sink I'm not sure about. Currently when a workflow is submitted a record is dropped into the workflow store. At a regular interval the WMA will pull up to N submitted workflows and for each one will create a WorkflowActor which will both create its MaterializeWorkflowDescriptorActor and start running. Instead of a now and then batch system convert this to a streaming system controlled by backpressure. My thinking here is to have the source be the database table of submitted workflows (can a source be a perpetual query?) and then the stream could materialize the descriptor, create a workflow actor, the WA could register itself w/ the WMA and then start. I was picturing using Sink.actorRefWithAck on the WMA to provide back pressure (again, not sure it works like this). . In theory this would allow us to handle submitted workflows as rapidly as the WMA can handle w/o being overwhelmed. . Beyond the two key ""I don't know if it works like that"" points another is if there's another chokepoint which would better serve as the key backpressure signal and if so if it'd still make sense to wire this up as a stream - e.g. would the backpressure signaling point be too deep into the system to be practical?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1705
https://github.com/broadinstitute/cromwell/pull/1710:41,Deployability,configurat,configuration,41,* added support for a FileRoller logback configuration; * made both logback.xml files in repo the same; * rearrange to single logback.xml,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1710
https://github.com/broadinstitute/cromwell/pull/1710:41,Modifiability,config,configuration,41,* added support for a FileRoller logback configuration; * made both logback.xml files in repo the same; * rearrange to single logback.xml,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1710
https://github.com/broadinstitute/cromwell/pull/1710:33,Testability,log,logback,33,* added support for a FileRoller logback configuration; * made both logback.xml files in repo the same; * rearrange to single logback.xml,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1710
https://github.com/broadinstitute/cromwell/pull/1710:68,Testability,log,logback,68,* added support for a FileRoller logback configuration; * made both logback.xml files in repo the same; * rearrange to single logback.xml,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1710
https://github.com/broadinstitute/cromwell/pull/1710:126,Testability,log,logback,126,* added support for a FileRoller logback configuration; * made both logback.xml files in repo the same; * rearrange to single logback.xml,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1710
https://github.com/broadinstitute/cromwell/issues/1717:270,Modifiability,variab,variable,270,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/issues/1717:350,Modifiability,variab,variables,350,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/issues/1717:381,Modifiability,Variab,Variables,381,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/issues/1717:436,Modifiability,variab,variables,436,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/issues/1717:44,Security,access,access,44,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/issues/1717:100,Security,encrypt,encryption,100,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/issues/1717:419,Security,access,access,419,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/issues/1717:426,Security,encrypt,encrypted,426,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/issues/1717:236,Testability,test,test,236,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/issues/1717:526,Testability,test,tests,526,"Currently when an external user attempts to access our cloud accounts, Travis does not hand out the encryption keys. This is desired behavior, as we do not want to run up bills on unknown code from outside users. However, currently our test scripts are not checking the variable [`TRAVIS_SECURE_ENV_VARS`](https://docs.travis-ci.com/user/environment-variables/#Default-Environment-Variables), and are instead trying to access encrypted variables that are not and never will be present. This leads to false negatives where the tests are marked as failed, when instead they should best case be marked as ignored, worst case be marked as implicitly passed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1717
https://github.com/broadinstitute/cromwell/pull/1719:95,Availability,echo,echo,95,This affects to HtCondor backend. Given:; Following WDL task. ``` ; task Example {; command {; echo foo;; mkdir testFolder; }; } ; ```. When:; Runs workflow using HtCondor backend. Then:; It creates testFolder in docker host machine. Expected result:; Whole command executed within the container.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1719
https://github.com/broadinstitute/cromwell/pull/1719:112,Testability,test,testFolder,112,This affects to HtCondor backend. Given:; Following WDL task. ``` ; task Example {; command {; echo foo;; mkdir testFolder; }; } ; ```. When:; Runs workflow using HtCondor backend. Then:; It creates testFolder in docker host machine. Expected result:; Whole command executed within the container.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1719
https://github.com/broadinstitute/cromwell/pull/1719:199,Testability,test,testFolder,199,This affects to HtCondor backend. Given:; Following WDL task. ``` ; task Example {; command {; echo foo;; mkdir testFolder; }; } ; ```. When:; Runs workflow using HtCondor backend. Then:; It creates testFolder in docker host machine. Expected result:; Whole command executed within the container.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1719
https://github.com/broadinstitute/cromwell/pull/1721:235,Availability,down,downsized,235,"Apparently there weren't any ""real"" workflow names > 100 chars, only one artificially long one created for test purposes, and Cloud SQL did not take kindly to the attempt to have a 500-char field as part of an index. The test data was downsized and the embiggening commit reverted and the migration ran successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1721
https://github.com/broadinstitute/cromwell/pull/1721:107,Testability,test,test,107,"Apparently there weren't any ""real"" workflow names > 100 chars, only one artificially long one created for test purposes, and Cloud SQL did not take kindly to the attempt to have a 500-char field as part of an index. The test data was downsized and the embiggening commit reverted and the migration ran successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1721
https://github.com/broadinstitute/cromwell/pull/1721:221,Testability,test,test,221,"Apparently there weren't any ""real"" workflow names > 100 chars, only one artificially long one created for test purposes, and Cloud SQL did not take kindly to the attempt to have a 500-char field as part of an index. The test data was downsized and the embiggening commit reverted and the migration ran successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1721
https://github.com/broadinstitute/cromwell/pull/1724:34,Testability,test,test,34,"For some reason the local centaur test on travis can't localize files with hard link, so just stop trying.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1724
https://github.com/broadinstitute/cromwell/issues/1726:60,Integrability,protocol,protocols,60,There is nothing in the current Cromwell or Centaur testing protocols that checks the compatibility of our code with the Cloud SQL instances used in GOTC or FireCloud. Currently any incompatibility issues are discovered only during acceptance testing by those customers.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726
https://github.com/broadinstitute/cromwell/issues/1726:52,Testability,test,testing,52,There is nothing in the current Cromwell or Centaur testing protocols that checks the compatibility of our code with the Cloud SQL instances used in GOTC or FireCloud. Currently any incompatibility issues are discovered only during acceptance testing by those customers.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726
https://github.com/broadinstitute/cromwell/issues/1726:243,Testability,test,testing,243,There is nothing in the current Cromwell or Centaur testing protocols that checks the compatibility of our code with the Cloud SQL instances used in GOTC or FireCloud. Currently any incompatibility issues are discovered only during acceptance testing by those customers.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1726
https://github.com/broadinstitute/cromwell/pull/1727:57,Testability,log,log,57,This change is because we're seeing in Travis:; ```; The log length has exceeded the limit of 4 MB (this usually means that the test suite is raising the same exception over and over). The job has been terminated; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1727
https://github.com/broadinstitute/cromwell/pull/1727:128,Testability,test,test,128,This change is because we're seeing in Travis:; ```; The log length has exceeded the limit of 4 MB (this usually means that the test suite is raising the same exception over and over). The job has been terminated; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1727
https://github.com/broadinstitute/cromwell/issues/1729:37,Deployability,pipeline,pipelines,37,"Aberrant data sometimes requires our pipelines to run with much more memory than is required in the normal case. It would be helpful to have some mechanism to support automatically re-running a task with more memory (or other resources) if it fails for certain reasons -- e.g. if we knew that the task failed for OOM, we would want to try again with double the memory, perhaps more than once (up to some set threshold of attempts or total memory).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1729
https://github.com/broadinstitute/cromwell/issues/1730:124,Deployability,configurat,configuration,124,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730
https://github.com/broadinstitute/cromwell/issues/1730:236,Deployability,update,updated,236,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730
https://github.com/broadinstitute/cromwell/issues/1730:124,Modifiability,config,configuration,124,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730
https://github.com/broadinstitute/cromwell/issues/1730:206,Modifiability,config,config,206,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730
https://github.com/broadinstitute/cromwell/issues/1730:258,Modifiability,config,config,258,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730
https://github.com/broadinstitute/cromwell/issues/1730:298,Modifiability,config,config,298,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730
https://github.com/broadinstitute/cromwell/issues/1730:406,Security,password,password,406,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730
https://github.com/broadinstitute/cromwell/issues/1730:602,Testability,test,test,602,"If you use the example in the docs at https://github.com/broadinstitute/cromwell#database. you'll get a warning:. `Move the configuration directly under the 'database' element, and remove the key 'database.config'.`. The docs should be updated. Below is the config given in the readme. database {; config = main.mysql. main {; mysql {; db.url = ""jdbc:mysql://localhost:3306/cromwell""; db.user = ""root""; db.password = """"; db.driver = ""com.mysql.jdbc.Driver""; db.connectionTimeout = 5000 # NOTE: The default 1000ms is often too short for production mysql use; driver = ""slick.driver.MySQLDriver$""; }; }. test {; ...; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1730
https://github.com/broadinstitute/cromwell/issues/1737:487,Availability,ERROR,ERROR,487,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:14,Modifiability,config,config,14,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:98,Modifiability,config,config,98,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:105,Modifiability,Config,ConfigBackendLifecycleActorFactory,105,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:142,Modifiability,config,config,142,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:785,Modifiability,config,config,785,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:892,Modifiability,config,config,892,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:989,Modifiability,config,config,989,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:1098,Modifiability,config,config,1098,"fig.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:1224,Modifiability,config,config,1224,"g? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationVal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:1937,Modifiability,config,config,1937,ngType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2047,Modifiability,config,config,2047,dation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2054,Modifiability,Config,ConfigInitializationActor,2054, 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.ba,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2114,Modifiability,Config,ConfigInitializationActor,2114,rationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2181,Modifiability,config,config,2181,cala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitialization,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2188,Modifiability,Config,ConfigInitializationActor,2188,mwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2237,Modifiability,Config,ConfigInitializationActor,2237,tionValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflow,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2304,Modifiability,config,config,2304,n.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(B,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2311,Modifiability,Config,ConfigInitializationActor,2311,scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowIn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2373,Modifiability,Config,ConfigInitializationActor,2373,p$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.b,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2440,Modifiability,config,config,2440,$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycle,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2447,Modifiability,Config,ConfigInitializationActor,2447,ply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2498,Modifiability,Config,ConfigInitializationActor,2498,ala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSys,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:3403,Performance,perform,performActionThenRespond,3403,kend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.performActionThenRespond(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:163); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.aroundReceive(SharedFileSystemInitializationActor.scala:37); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:3522,Performance,perform,performActionThenRespond,3522,end.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.performActionThenRespond(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:163); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.aroundReceive(SharedFileSystemInitializationActor.scala:37); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I guess it's something to do with 85,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:4197,Performance,concurren,concurrent,4197,ctor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.performActionThenRespond(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:163); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.aroundReceive(SharedFileSystemInitializationActor.scala:37); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I guess it's something to do with 85216837ce62399b33c2b4d51c314d0bd98fe89a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:4271,Performance,concurren,concurrent,4271,ctor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.performActionThenRespond(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:163); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.aroundReceive(SharedFileSystemInitializationActor.scala:37); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I guess it's something to do with 85216837ce62399b33c2b4d51c314d0bd98fe89a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:4357,Performance,concurren,concurrent,4357,ctor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.performActionThenRespond(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:163); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.aroundReceive(SharedFileSystemInitializationActor.scala:37); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I guess it's something to do with 85216837ce62399b33c2b4d51c314d0bd98fe89a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:4435,Performance,concurren,concurrent,4435,ctor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.performActionThenRespond(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:163); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.aroundReceive(SharedFileSystemInitializationActor.scala:37); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I guess it's something to do with 85216837ce62399b33c2b4d51c314d0bd98fe89a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:2736,Security,validat,validateRuntimeAttributes,2736, scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations$lzycompute(ConfigInitializationActor.scala:38); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.declarationValidations(ConfigInitializationActor.scala:37); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder$lzycompute(ConfigInitializationActor.scala:47); 	at cromwell.backend.impl.sfs.config.ConfigInitializationActor.runtimeAttributesBuilder(ConfigInitializationActor.scala:46); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.coerceDefaultRuntimeAttributes(SharedFileSystemInitializationActor.scala:90); 	at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:138); 	at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1$$anonfun$applyOrElse$4.apply(BackendWorkflowInitializationActor.scala:163); 	at cromwell.backend.BackendLifecycleActor$class.performActionThenRespond(BackendLifecycleActor.scala:44); 	at cromwell.backend.sfs.SharedFileSystemInitializationActor.performActionThenRespond(SharedFileSystemInitializationActor.scala:37); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$receive$1.applyOrElse(BackendWorkflowInitializationActor.scala:163); 	at akka.actor.Actor$class.aroundReceiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1737:474,Testability,log,logs,474,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737
https://github.com/broadinstitute/cromwell/issues/1740:7,Testability,test,testing,7,"During testing I completely botched making my inputs file. Rather than a nice JSON map, I created a file with just the value: no key and no enclosing curlies. Cromwell did not take this well and never issued a response to the client. ```; spray.json.JsonParser$ParsingException: Unexpected character 'm' at input index 0 (line 1, position 1), expected JSON Value:; mcovarr; ^. spray.json.JsonParser$ParsingException: Unexpected character 'm' at input index 0 (line 1, position 1), expected JSON Value:; mcovarr; ^. 	at spray.json.JsonParser.fail(JsonParser.scala:213); 	at spray.json.JsonParser.value(JsonParser.scala:64); 	at spray.json.JsonParser.parseJsValue(JsonParser.scala:43); 	at spray.json.JsonParser$.apply(JsonParser.scala:28); 	at spray.json.PimpedString.parseJson(package.scala:45); 	at cromwell.webservice.CromwellApiService$PartialWorkflowSources$.cromwell$webservice$CromwellApiService$PartialWorkflowSources$$workflowInputs(CromwellApiService.scala:134); 	at cromwell.webservice.CromwellApiService$PartialWorkflowSources$$anonfun$6$$anonfun$apply$12.apply(CromwellApiService.scala:161); 	at cromwell.webservice.CromwellApiService$PartialWorkflowSources$$anonfun$6$$anonfun$apply$12.apply(CromwellApiService.scala:157); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1740
https://github.com/broadinstitute/cromwell/issues/1747:27,Energy Efficiency,monitor,monitoring,27,"I've been complaining that monitoring doesn't provide any insight into dispatchers. I was getting ready to inquire about this and noticed this in the docs:; https://developer.lightbend.com/docs/monitoring/latest/instrumentations/akka/dispatchers.html. I believe this is (relatively) new, see what's going on here",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1747
https://github.com/broadinstitute/cromwell/issues/1747:194,Energy Efficiency,monitor,monitoring,194,"I've been complaining that monitoring doesn't provide any insight into dispatchers. I was getting ready to inquire about this and noticed this in the docs:; https://developer.lightbend.com/docs/monitoring/latest/instrumentations/akka/dispatchers.html. I believe this is (relatively) new, see what's going on here",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1747
https://github.com/broadinstitute/cromwell/issues/1748:505,Availability,error,error,505,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:123,Deployability,configurat,configuration,123,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:521,Deployability,configurat,configuration,521,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:1828,Deployability,Pipeline,Pipelines,1828,"ed if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default""; ; # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; ; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }. #AWS {; ...snip...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:123,Modifiability,config,configuration,123,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:521,Modifiability,config,configuration,521,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:638,Modifiability,config,config,638,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:981,Performance,throughput,throughput,981,"- single workflow mode; - JES backend; - Google VM; - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with servi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:1071,Performance,perform,performance,1071," - 0.23. ```; ....snip...; [2016-12-06 01:52:49,82] [warn] Unrecognized configuration key(s) for Jes: genomics-api-queries-per-100-seconds, dockerhub.token, dockerhub.account, genomics.compute-service-account; ....snip....; ```. As far as I can tell, I am using the same keys as in the reference conf file. Worked in previous dev builds with same structure (though fewer keys). @kcibul This is important, though I would not be surprised if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce accoun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:1583,Security,access,access,1583,"ed if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default""; ; # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; ; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }. #AWS {; ...snip...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1748:2012,Security,authoriz,authorization,2012,"ed if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default""; ; # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; ; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }. #AWS {; ...snip...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748
https://github.com/broadinstitute/cromwell/issues/1751:444,Deployability,configurat,configuration,444,"Quoth the CHANGELOG.md:. >You can now limit the number of concurrent jobs for a backend by specifying the following option in the backend's config stanza:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. But this is not generally true. The implementation in `BackendLifecycleActorFactory` defaults to an infinitely large job limit and does not look for this `concurrent-job-limit` configuration setting. `ConfigBackendLifecycleActorFactory` overrides this behavior to be consistent with CHANGELOG.md, but no other backend factory does.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751
https://github.com/broadinstitute/cromwell/issues/1751:140,Modifiability,config,config,140,"Quoth the CHANGELOG.md:. >You can now limit the number of concurrent jobs for a backend by specifying the following option in the backend's config stanza:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. But this is not generally true. The implementation in `BackendLifecycleActorFactory` defaults to an infinitely large job limit and does not look for this `concurrent-job-limit` configuration setting. `ConfigBackendLifecycleActorFactory` overrides this behavior to be consistent with CHANGELOG.md, but no other backend factory does.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751
https://github.com/broadinstitute/cromwell/issues/1751:226,Modifiability,config,config,226,"Quoth the CHANGELOG.md:. >You can now limit the number of concurrent jobs for a backend by specifying the following option in the backend's config stanza:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. But this is not generally true. The implementation in `BackendLifecycleActorFactory` defaults to an infinitely large job limit and does not look for this `concurrent-job-limit` configuration setting. `ConfigBackendLifecycleActorFactory` overrides this behavior to be consistent with CHANGELOG.md, but no other backend factory does.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751
https://github.com/broadinstitute/cromwell/issues/1751:444,Modifiability,config,configuration,444,"Quoth the CHANGELOG.md:. >You can now limit the number of concurrent jobs for a backend by specifying the following option in the backend's config stanza:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. But this is not generally true. The implementation in `BackendLifecycleActorFactory` defaults to an infinitely large job limit and does not look for this `concurrent-job-limit` configuration setting. `ConfigBackendLifecycleActorFactory` overrides this behavior to be consistent with CHANGELOG.md, but no other backend factory does.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751
https://github.com/broadinstitute/cromwell/issues/1751:468,Modifiability,Config,ConfigBackendLifecycleActorFactory,468,"Quoth the CHANGELOG.md:. >You can now limit the number of concurrent jobs for a backend by specifying the following option in the backend's config stanza:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. But this is not generally true. The implementation in `BackendLifecycleActorFactory` defaults to an infinitely large job limit and does not look for this `concurrent-job-limit` configuration setting. `ConfigBackendLifecycleActorFactory` overrides this behavior to be consistent with CHANGELOG.md, but no other backend factory does.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751
https://github.com/broadinstitute/cromwell/issues/1751:58,Performance,concurren,concurrent,58,"Quoth the CHANGELOG.md:. >You can now limit the number of concurrent jobs for a backend by specifying the following option in the backend's config stanza:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. But this is not generally true. The implementation in `BackendLifecycleActorFactory` defaults to an infinitely large job limit and does not look for this `concurrent-job-limit` configuration setting. `ConfigBackendLifecycleActorFactory` overrides this behavior to be consistent with CHANGELOG.md, but no other backend factory does.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751
https://github.com/broadinstitute/cromwell/issues/1751:236,Performance,concurren,concurrent-job-limit,236,"Quoth the CHANGELOG.md:. >You can now limit the number of concurrent jobs for a backend by specifying the following option in the backend's config stanza:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. But this is not generally true. The implementation in `BackendLifecycleActorFactory` defaults to an infinitely large job limit and does not look for this `concurrent-job-limit` configuration setting. `ConfigBackendLifecycleActorFactory` overrides this behavior to be consistent with CHANGELOG.md, but no other backend factory does.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751
https://github.com/broadinstitute/cromwell/issues/1751:422,Performance,concurren,concurrent-job-limit,422,"Quoth the CHANGELOG.md:. >You can now limit the number of concurrent jobs for a backend by specifying the following option in the backend's config stanza:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. But this is not generally true. The implementation in `BackendLifecycleActorFactory` defaults to an infinitely large job limit and does not look for this `concurrent-job-limit` configuration setting. `ConfigBackendLifecycleActorFactory` overrides this behavior to be consistent with CHANGELOG.md, but no other backend factory does.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1751
https://github.com/broadinstitute/cromwell/issues/1754:474,Availability,error,error,474,"- JES backend; - 0.23; - single workflow. This workflow used to complete successfully (though cromwell did not exit), but with release 0.23, the workflow itself fails; Looks like cromwell can no longer handle spaces in the output file name. I believe that @kshakir had a similar issue in one of the develop builds. Did the fix make it into release 0.23? . ```; ...snip...; java.lang.RuntimeException: Task 5d13ddf0-dcf9-4b99-bd13-40b4321a954a:aggregate_results_html failed: error code 5. Message: 9: Failed to localize files: failed to copy; the following files: ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity; _series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png -> /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-gatk-protect; ed/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small Amplificati; ons.png (cp failed: gsutil -q -m cp gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-r; un_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-g; atk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small; Amplifications.png, command failed: CommandException: No URLs matched: gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0; -dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png\nCommandException: 1 file/; object could not be transferred.\n); gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754
https://github.com/broadinstitute/cromwell/issues/1754:127,Deployability,release,release,127,"- JES backend; - 0.23; - single workflow. This workflow used to complete successfully (though cromwell did not exit), but with release 0.23, the workflow itself fails; Looks like cromwell can no longer handle spaces in the output file name. I believe that @kshakir had a similar issue in one of the develop builds. Did the fix make it into release 0.23? . ```; ...snip...; java.lang.RuntimeException: Task 5d13ddf0-dcf9-4b99-bd13-40b4321a954a:aggregate_results_html failed: error code 5. Message: 9: Failed to localize files: failed to copy; the following files: ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity; _series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png -> /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-gatk-protect; ed/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small Amplificati; ons.png (cp failed: gsutil -q -m cp gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-r; un_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-g; atk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small; Amplifications.png, command failed: CommandException: No URLs matched: gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0; -dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png\nCommandException: 1 file/; object could not be transferred.\n); gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754
https://github.com/broadinstitute/cromwell/issues/1754:340,Deployability,release,release,340,"- JES backend; - 0.23; - single workflow. This workflow used to complete successfully (though cromwell did not exit), but with release 0.23, the workflow itself fails; Looks like cromwell can no longer handle spaces in the output file name. I believe that @kshakir had a similar issue in one of the develop builds. Did the fix make it into release 0.23? . ```; ...snip...; java.lang.RuntimeException: Task 5d13ddf0-dcf9-4b99-bd13-40b4321a954a:aggregate_results_html failed: error code 5. Message: 9: Failed to localize files: failed to copy; the following files: ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity; _series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png -> /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-gatk-protect; ed/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small Amplificati; ons.png (cp failed: gsutil -q -m cp gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-r; un_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-g; atk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small; Amplifications.png, command failed: CommandException: No URLs matched: gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0; -dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png\nCommandException: 1 file/; object could not be transferred.\n); gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754
https://github.com/broadinstitute/cromwell/issues/1754:488,Integrability,Message,Message,488,"- JES backend; - 0.23; - single workflow. This workflow used to complete successfully (though cromwell did not exit), but with release 0.23, the workflow itself fails; Looks like cromwell can no longer handle spaces in the output file name. I believe that @kshakir had a similar issue in one of the develop builds. Did the fix make it into release 0.23? . ```; ...snip...; java.lang.RuntimeException: Task 5d13ddf0-dcf9-4b99-bd13-40b4321a954a:aggregate_results_html failed: error code 5. Message: 9: Failed to localize files: failed to copy; the following files: ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity; _series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png -> /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-gatk-protect; ed/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small Amplificati; ons.png (cp failed: gsutil -q -m cp gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-r; un_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-g; atk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small; Amplifications.png, command failed: CommandException: No URLs matched: gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0; -dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png\nCommandException: 1 file/; object could not be transferred.\n); gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754
https://github.com/broadinstitute/cromwell/issues/1754:3760,Performance,concurren,concurrent,3760,"plifications.png\nCommandException: 1 file/; object could not be transferred.\n); gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-; run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Deletions.png -> /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-ga; tk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small D; eletions.png (cp failed: gsutil -q -m cp gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/c; all-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Deletions.png /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-g; atk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small; Deletions.png, command failed: CommandException: No URLs matched: gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9; -4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Deletions.png\nCommandException: 1 file/object cou; ld not be transferred.\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$handleFailure(JesAsyncBackendJobExecut; ionActor.scala:530); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:560); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$executionResult$1.apply(JesAsyncBackendJobExecutionActor.scala:537); at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24). ...snip...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754
https://github.com/broadinstitute/cromwell/issues/1761:197,Availability,error,error,197,"From `IntToIntArray` in the JG workflow, this was working on 23 but not 24. I chatted with @Horneth and he said it wasn't intentional so let's fix it. Presumably it's a coercion issue but unclear. error was:. `to evaluate outputs.: RuntimeException: Could not evaluate IntToIntArray.array = read_lines(stdout()); wdl4s.util.AggregatedException: Failed to evaluate outputs.: RuntimeException: Could not evaluate IntToIntArray.array = read_lines(stdout())`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1761
https://github.com/broadinstitute/cromwell/issues/1762:243,Availability,failure,failure,243,"**Finalized Ticket**. For the `read_x()` functions, limit the size of data which can be read in. Check the file size prior to attempting to read as reading can also incur network charges. Any attempt to read an oversized file will result in a failure. See notes from Geraldine down below about how to phrase the error. `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB. **Original text for posterity**; Note to @kcibul - this is both a request for the WDL spec and Cromwell's implementation, not just the latter. Our read_X() functions (e.g. read_int, read_string, read_lines) have a flaw in that they'll dutifully read in the entire file. The problem with this is that a malicious and/or less than careful user could take down Cromwell or another WDL implementing engine (unless it was written in Erlang!) by reading in an enormous file. For instance a careless user might `read_string(SomeEnormousBam)`. The point of these functions are more of a convenience, if users are trying to sling around huge chunks of data they should be passing files around. In particular things like `read_boolean()` are particularly egregious as it will only interpret `true` or `false`. Similarly it seems unlikely that someone would have a valid use case to read in the first 9 billion digits of Pi into a `Float`. . I propose that we place a cap on how much data we will read to some reasonable amount of data (e.g. CWL uses 64 KiB, which seems a little excessively small to me).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762
https://github.com/broadinstitute/cromwell/issues/1762:277,Availability,down,down,277,"**Finalized Ticket**. For the `read_x()` functions, limit the size of data which can be read in. Check the file size prior to attempting to read as reading can also incur network charges. Any attempt to read an oversized file will result in a failure. See notes from Geraldine down below about how to phrase the error. `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB. **Original text for posterity**; Note to @kcibul - this is both a request for the WDL spec and Cromwell's implementation, not just the latter. Our read_X() functions (e.g. read_int, read_string, read_lines) have a flaw in that they'll dutifully read in the entire file. The problem with this is that a malicious and/or less than careful user could take down Cromwell or another WDL implementing engine (unless it was written in Erlang!) by reading in an enormous file. For instance a careless user might `read_string(SomeEnormousBam)`. The point of these functions are more of a convenience, if users are trying to sling around huge chunks of data they should be passing files around. In particular things like `read_boolean()` are particularly egregious as it will only interpret `true` or `false`. Similarly it seems unlikely that someone would have a valid use case to read in the first 9 billion digits of Pi into a `Float`. . I propose that we place a cap on how much data we will read to some reasonable amount of data (e.g. CWL uses 64 KiB, which seems a little excessively small to me).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762
https://github.com/broadinstitute/cromwell/issues/1762:312,Availability,error,error,312,"**Finalized Ticket**. For the `read_x()` functions, limit the size of data which can be read in. Check the file size prior to attempting to read as reading can also incur network charges. Any attempt to read an oversized file will result in a failure. See notes from Geraldine down below about how to phrase the error. `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB. **Original text for posterity**; Note to @kcibul - this is both a request for the WDL spec and Cromwell's implementation, not just the latter. Our read_X() functions (e.g. read_int, read_string, read_lines) have a flaw in that they'll dutifully read in the entire file. The problem with this is that a malicious and/or less than careful user could take down Cromwell or another WDL implementing engine (unless it was written in Erlang!) by reading in an enormous file. For instance a careless user might `read_string(SomeEnormousBam)`. The point of these functions are more of a convenience, if users are trying to sling around huge chunks of data they should be passing files around. In particular things like `read_boolean()` are particularly egregious as it will only interpret `true` or `false`. Similarly it seems unlikely that someone would have a valid use case to read in the first 9 billion digits of Pi into a `Float`. . I propose that we place a cap on how much data we will read to some reasonable amount of data (e.g. CWL uses 64 KiB, which seems a little excessively small to me).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762
https://github.com/broadinstitute/cromwell/issues/1762:851,Availability,down,down,851,"**Finalized Ticket**. For the `read_x()` functions, limit the size of data which can be read in. Check the file size prior to attempting to read as reading can also incur network charges. Any attempt to read an oversized file will result in a failure. See notes from Geraldine down below about how to phrase the error. `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB. **Original text for posterity**; Note to @kcibul - this is both a request for the WDL spec and Cromwell's implementation, not just the latter. Our read_X() functions (e.g. read_int, read_string, read_lines) have a flaw in that they'll dutifully read in the entire file. The problem with this is that a malicious and/or less than careful user could take down Cromwell or another WDL implementing engine (unless it was written in Erlang!) by reading in an enormous file. For instance a careless user might `read_string(SomeEnormousBam)`. The point of these functions are more of a convenience, if users are trying to sling around huge chunks of data they should be passing files around. In particular things like `read_boolean()` are particularly egregious as it will only interpret `true` or `false`. Similarly it seems unlikely that someone would have a valid use case to read in the first 9 billion digits of Pi into a `Float`. . I propose that we place a cap on how much data we will read to some reasonable amount of data (e.g. CWL uses 64 KiB, which seems a little excessively small to me).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762
https://github.com/broadinstitute/cromwell/issues/1762:179,Energy Efficiency,charge,charges,179,"**Finalized Ticket**. For the `read_x()` functions, limit the size of data which can be read in. Check the file size prior to attempting to read as reading can also incur network charges. Any attempt to read an oversized file will result in a failure. See notes from Geraldine down below about how to phrase the error. `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB. **Original text for posterity**; Note to @kcibul - this is both a request for the WDL spec and Cromwell's implementation, not just the latter. Our read_X() functions (e.g. read_int, read_string, read_lines) have a flaw in that they'll dutifully read in the entire file. The problem with this is that a malicious and/or less than careful user could take down Cromwell or another WDL implementing engine (unless it was written in Erlang!) by reading in an enormous file. For instance a careless user might `read_string(SomeEnormousBam)`. The point of these functions are more of a convenience, if users are trying to sling around huge chunks of data they should be passing files around. In particular things like `read_boolean()` are particularly egregious as it will only interpret `true` or `false`. Similarly it seems unlikely that someone would have a valid use case to read in the first 9 billion digits of Pi into a `Float`. . I propose that we place a cap on how much data we will read to some reasonable amount of data (e.g. CWL uses 64 KiB, which seems a little excessively small to me).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762
https://github.com/broadinstitute/cromwell/issues/1763:158,Availability,error,error,158,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:230,Availability,error,errors,230,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:852,Availability,error,error,852,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:943,Availability,error,error,943,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:44,Deployability,Pipeline,Pipelines,44,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:172,Integrability,message,message,172,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:276,Integrability,message,message,276,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:512,Integrability,message,message,512,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:384,Modifiability,sandbox,sandbox,384,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:620,Modifiability,sandbox,sandbox,620,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
https://github.com/broadinstitute/cromwell/issues/1763:21,Testability,test,test,21,"GOTC was running the test for staging PAPI (Pipelines API). This test is launching 50 Single Sample workflows at once and 4 of our workflows failed with this error.; ```; ""message"": ""429 Too Many Requests\n{\n \""code\"" : 429,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""reason\"" : \""rateLimitExceeded\""\n } ],\n \""message\"" : \""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'.\"",\n \""status\"" : \""RESOURCE_EXHAUSTED\""\n}""; ```. All 4 of the jobs that failed were non-premptible whereas there are preemptible jobs that ran into this error and just went from attempt 1 to attempt 2 or w/e. . I don't think we would want this error to count towards our attempt count and we definitely don't want it to fail non preemptible tasks. @kcibul for prioritization",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763
