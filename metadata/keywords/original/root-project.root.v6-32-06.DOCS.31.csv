id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:16751,Security,hash,hash,16751,,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:16833,Security,hash,hash,16833,,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:16915,Security,hash,hash,16915,,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:16997,Security,hash,hash,16997,,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17079,Security,hash,hash,17079,,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17161,Security,hash,hash,17161,d7125dc5 \; --hash=sha256:062582fca9fabdd2c8b54a3ef1c978d786e0f6b3a1510e0ac93ef59e0ddae2bc \; --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \; --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \; --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \; --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \; --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \; --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \; --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \; --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17243,Security,hash,hash,17243,0ddae2bc \; --hash=sha256:0d3304d8c0adc42be59c5f8a4d9e3d7379e6955ad754aa9d6ab7a398b59dd1df \; --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \; --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \; --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \; --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \; --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \; --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \; --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c8,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17325,Security,hash,hash,17325,b59dd1df \; --hash=sha256:1635fd110e8d85d55237ab316b5b011de701ea0f29d07611174a1b42f1444741 \; --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \; --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \; --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \; --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \; --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \; --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17407,Security,hash,hash,17407,f1444741 \; --hash=sha256:184c5108a2aca3c5b3d3bf9395d50893a7ab82a38004c8f61c258d4428e80206 \; --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \; --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \; --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \; --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \; --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17489,Security,hash,hash,17489,28e80206 \; --hash=sha256:18aeb1bf9a78867dc38b259769503436b7c72f7a1f1f4c93ff9a17de54319b27 \; --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \; --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \; --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \; --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17571,Security,hash,hash,17571,54319b27 \; --hash=sha256:1d4c7e777c441b20e32f52bd377e0c409713e8bb1386e1099c2415f26e479595 \; --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \; --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \; --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17653,Security,hash,hash,17653,6e479595 \; --hash=sha256:1e2722cc9fbb45d9b87631ac70924c11d3a401b2d7f410cc0e3bbf249f2dca62 \; --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \; --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17735,Security,hash,hash,17735,9f2dca62 \; --hash=sha256:1fe35611261b29bd1de0070f0b2f47cb6ff71fa6595c077e42bd0c419fa27b98 \; --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eec,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17817,Security,hash,hash,17817,9fa27b98 \; --hash=sha256:28c119d996beec18c05208a8bd78cbe4007878c6dd15091efb73a30e90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e3,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17899,Security,hash,hash,17899,90539696 \; --hash=sha256:326c013efe8048858a6d312ddd31d56e468118ad4cdeda36c719bf5bb6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:17981,Security,hash,hash,17981,b6192290 \; --hash=sha256:40df9b996c2b73138957fe23a16a4f0ba614f4c0efce1e9406a184b6d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf29,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18063,Security,hash,hash,18063,d07fa3a9 \; --hash=sha256:42f8152b8dbc4fe7d96729ec2b99c7097d656dc1213a3229ca5383f973a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af73,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18145,Security,hash,hash,18145,73a5ed6d \; --hash=sha256:49a183be227561de579b4a36efbb21b3eab9651dd81b1858589f796549873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18227,Security,hash,hash,18227,49873dd6 \; --hash=sha256:4fb147e7a67ef577a588a0e2c17b6db51dda102c71de36f8549b6816a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18309,Security,hash,hash,18309,a96e1867 \; --hash=sha256:50550eb667afee136e9a77d6dc71ae76a44df8b3e51e41b77f6de2932bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098dd,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18391,Security,hash,hash,18391,2bfe0f47 \; --hash=sha256:510c9deebc5c0225e8c96813043e62b680ba2f9c50a08d3724c7f28a747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aa,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18473,Security,hash,hash,18473,747d1486 \; --hash=sha256:5773183b6446b2c99bb77e77595dd486303b4faab2b086e7b17bc6bef28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2c,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18555,Security,hash,hash,18555,f28865f6 \; --hash=sha256:596106435fa6ad000c2991a98fa58eeb8656ef2325d7e158344fb33864ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa5,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18637,Security,hash,hash,18637,64ed87e3 \; --hash=sha256:6965a7bc3cf88e5a1c3bd2e0b5c22f8d677dc88a455344035f03399034eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d7368,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18719,Security,hash,hash,18719,34eb3007 \; --hash=sha256:69b023b2b4daa7548bcfbd4aa3da05b3a74b772db9e23b982788168117739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18801,Security,hash,hash,18801,17739938 \; --hash=sha256:6c22bec3fbe2524cde73d7ada88f6566758a8f7227bfbf93a408a9d86bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18883,Security,hash,hash,18883,6bcc12a0 \; --hash=sha256:704219a11b772aea0d8ecd7058d0082713c3562b4e271b849ad7dc4a5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bd,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:18965,Security,hash,hash,18965,5c90c13c \; --hash=sha256:7e07cbde391ba96ab58e532ff4803f79c4129397514e1413a7dc761ccd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19047,Security,hash,hash,19047,cd755735 \; --hash=sha256:81e0b275a9ecc9c0c0c07b4b90ba548307583c125f54d5b6946cfee6360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19129,Security,hash,hash,19129,360c733d \; --hash=sha256:855fb52b0dc35af121542a76b9a84f8d1cd886ea97c84703eaa6d88e37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19211,Security,hash,hash,19211,37a2ad28 \; --hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; reco,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19293,Security,hash,hash,19293,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19375,Security,hash,hash,19375,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19457,Security,hash,hash,19457,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19539,Security,hash,hash,19539,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19621,Security,hash,hash,19621,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19703,Security,hash,hash,19703,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19785,Security,hash,hash,19785,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19867,Security,hash,hash,19867,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:19949,Security,hash,hash,19949,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20031,Security,hash,hash,20031,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20113,Security,hash,hash,20113,--hash=sha256:8d4e9c88387b0f5c7d5f281e55304de64cf7f9c0021a3525bd3b1c542da3b0e4 \; --hash=sha256:9046c58c4395dff28dd494285c82ba00b546adfc7ef001486fbf0324bc174fba \; --hash=sha256:9eb6caa9a297fc2c2fb8862bc5370d0303ddba53ba97e71f08023b6cd73d16a8 \; --hash=sha256:a0cd17c15d3bb3fa06978b4e8958dcdc6e0174ccea823003a106c7d4d7899ac5 \; --hash=sha256:afd7e57eddb1a54f0f1a974bc4391af8bcce0b444685d936840f125cf046d5bd \; --hash=sha256:b1275ad35a5d18c62a7220633c913e1b42d44b46ee12554e5fd39c70a243d6a3 \; --hash=sha256:b786eecbdf8499b9ca1d697215862083bd6d2a99965554781d0d8d1ad31e13a0 \; --hash=sha256:ba336e390cd8e4d1739f42dfe9bb83a3cc2e80f567d8805e11b46f4a943f5515 \; --hash=sha256:baa90d3f661d43131ca170712d903e6295d1f7a0f595074f151c0aed377c9b9c \; --hash=sha256:bc1bf2925a1ecd43da378f4db9e4f799775d6367bdb94671027b73b393a7c42c \; --hash=sha256:bd4af7373a854424dabd882decdc5579653d7868b8fb26dc7d0e99f823aa5924 \; --hash=sha256:bf07ee2fef7014951eeb99f56f39c9bb4af143d8aa3c21b1677805985307da34 \; --hash=sha256:bfdf460b1736c775f2ba9f6a92bca30bc2095067b8a9d77876d1fad6cc3b4a43 \; --hash=sha256:c8098ddcc2a85b61647b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20235,Security,hash,hash,20235,47b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontr,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20317,Security,hash,hash,20317,47b2590f825f3db38891662cfc2fc776415143f599bb859 \; --hash=sha256:d2b04aac4d386b172d5b9692e2d2da8de7bfb6c387fa4f801fbf6fb2e6ba4673 \; --hash=sha256:d483d2cdf104e7c9fa60c544d92981f12ad66a457afae824d146093b8c294c54 \; --hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontr,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20444,Security,hash,hash,20444,--hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=s,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20526,Security,hash,hash,20526,--hash=sha256:d858aa552c999bc8a8d57426ed01e40bef403cd8ccdd0fc5f6f04a00414cac2a \; --hash=sha256:e7d73685e87afe9f3b36c799222440d6cf362062f78be1013661b00c5c6f678b \; --hash=sha256:f003ed9ad21d6a4713f0a9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=s,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20646,Security,hash,hash,20646,9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea22684,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20728,Security,hash,hash,20728,9b5a7a0a79e08dd0f221aff4525a2be4c346ee60aab \; --hash=sha256:f22ac1c3cac4dbc50079e965eba2c1058622631e526bd9afd45fedd49ba781fa \; --hash=sha256:faca3bdcf85b2fc05d06ff3fbc1f83e1391b3e724afa3feba7d13eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea22684,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20840,Security,hash,hash,20840,eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:20922,Security,hash,hash,20922,eeab355484c \; --hash=sha256:fca0e3a251908a499833aa292323f32437106001d436eca0e6e7833256674585 \; --hash=sha256:fd1592b3fdf65fff2ad0004b5e363300ef59ced41c2e6b3a99d4089fa8c5435d \; --hash=sha256:fd66fc5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:21041,Security,hash,hash,21041,c5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd12030,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:21123,Security,hash,hash,21123,c5d0da6d9815ba2cebeb4205f95818ff4b79c3ebe268e75d961704af52f; # via myst-parser; recommonmark==0.7.1 \; --hash=sha256:1b1db69af0231efce3fa21b94ff627ea33dee7079a01dd0a7f8482c3da148b3f \; --hash=sha256:bdb4db649f2222dcd8d2d844f0006b958d627f732415d399791ee436a3686d67; # via -r requirements.txt; requests==2.31.0 \; --hash=sha256:58cd2187c01e70e6e26505bca751777aa9f2ee0b7f4300988b709f44e013003f \; --hash=sha256:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd12030,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:21449,Security,hash,hash,21449,6:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devh,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:21531,Security,hash,hash,21531,6:942c5a758f98d790eaed1a29cb6eefc7ffb0d1cf7af05c3d2791656dbd6ad1e1; # via sphinx; snowballstemmer==2.2.0 \; --hash=sha256:09b16deb8547d3412ad7b590689584cd0fe25ec8db3be37788be3810cbf19cb1 \; --hash=sha256:c8e1716e83cc398ae16824e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devh,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:21666,Security,hash,hash,21666,24e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; -,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:21748,Security,hash,hash,21748,24e5572ae04e0d9fc2c6b985fb0f900f5f0c96ecba1a; # via sphinx; soupsieve==2.5 \; --hash=sha256:5663d5a7b3bfaeee0bc4372e7fc48f9cff4940b3eec54a6451cc5299f1097690 \; --hash=sha256:eaa337ff55a1579b6549dc679565eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; -,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:21873,Security,hash,hash,21873,65eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:21955,Security,hash,hash,21955,65eac1e3d000563bcb1c8ab0d0fefbc0c2cdc7; # via beautifulsoup4; sphinx==7.1.2 \; --hash=sha256:780f4d32f1d7d1126576e0e5ecc19dc32ab76cd24e950228dcf7b1f6d3d9e22f \; --hash=sha256:d170a81825b2fcacb6dfd5a0d7f578a053e45d3f2b153fecc948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:22096,Security,hash,hash,22096,c948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sh,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:22178,Security,hash,hash,22178,c948c37344eb4cbe; # via; # -r requirements.txt; # furo; # myst-parser; # recommonmark; # sphinx-automodapi; # sphinx-basic-ng; # sphinxcontrib-devhelp; # sphinxcontrib-htmlhelp; # sphinxcontrib-qthelp; # sphinxcontrib-serializinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sh,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:22319,Security,hash,hash,22319,zinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:22401,Security,hash,hash,22401,zinghtml; sphinx-automodapi==0.16.0 \; --hash=sha256:68fc47064804604b90aa27c047016e86aaf970981d90a0082d5b5dd2e9d38afd \; --hash=sha256:6c673ef93066408e5ad3e2fa3533044d432a47fe6a826212b9ebf5f52a872554; # via -r requirements.txt; sphinx-basic-ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:22553,Security,hash,hash,22553,ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:22635,Security,hash,hash,22635,ng==1.0.0b2 \; --hash=sha256:9ec55a47c90c8c002b5960c57492ec3021f5193cb26cebc2dc4ea226848651c9 \; --hash=sha256:eb09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:22762,Security,hash,hash,22762,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:22844,Security,hash,hash,22844,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:22969,Security,hash,hash,22969,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:23051,Security,hash,hash,23051,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:23176,Security,hash,hash,23176,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:23258,Security,hash,hash,23258,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:23392,Security,hash,hash,23392,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:23474,Security,hash,hash,23474,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:23586,Security,hash,hash,23586,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt:23668,Security,hash,hash,23668,09aedbabfb650607e9b4b68c9d240b90b1e1be221d6ad71d61c52e29f7932b; # via furo; sphinx-bootstrap-theme==0.8.1 \; --hash=sha256:683e3b735448dadd0149f76edecf95ff4bd9157787e9e77e0d048ca6f1d680df \; --hash=sha256:6ef36206c211846ea6cbdb45bc85645578e7c62d0a883361181708f8b6ea743b; # via -r requirements.txt; sphinx-markdown-tables==0.0.17 \; --hash=sha256:2bd0c30779653e4dd120300cbd9ca412c480738cc2241f6dea477a883f299e04 \; --hash=sha256:6bc6d3d400eaccfeebd288446bc08dd83083367c58b85d40fe6c12d77ef592f1; # via -r requirements.txt; sphinxcontrib-applehelp==1.0.4 \; --hash=sha256:29d341f67fb0f6f586b23ad80e072c8e6ad0b48417db2bde114a4c9746feb228 \; --hash=sha256:828f867945bbe39817c210a1abfd1bc4895c8b73fcaade56d45357a348a07d7e; # via; # -r requirements.txt; # sphinx; sphinxcontrib-devhelp==1.0.5 \; --hash=sha256:63b41e0d38207ca40ebbeabcf4d8e51f76c03e78cd61abe118cf4435c73d4212 \; --hash=sha256:fe8009aed765188f08fcaadbb3ea0d90ce8ae2d76710b7e29ea7d047177dae2f; # via sphinx; sphinxcontrib-htmlhelp==2.0.4 \; --hash=sha256:6c26a118a05b76000738429b724a0568dbde5b72391a688577da08f11891092a \; --hash=sha256:8001661c077a73c29beaf4a79968d0726103c5605e27db92b9ebed8bab1359e9; # via sphinx; sphinxcontrib-jsmath==1.0.1 \; --hash=sha256:2ec2eaebfb78f3f2078e73666b1415417a116cc848b72e5172e596c871103178 \; --hash=sha256:a9925e4a4587247ed2191a22df5f6970656cb8ca2bd6284309578f2153e0c4b8; # via sphinx; sphinxcontrib-qthelp==1.0.6 \; --hash=sha256:62b9d1a186ab7f5ee3356d906f648cacb7a6bdb94d201ee7adf26db55092982d \; --hash=sha256:bf76886ee7470b934e363da7a954ea2825650013d367728588732c7350f49ea4; # via sphinx; sphinxcontrib-serializinghtml==1.1.9 \; --hash=sha256:0c64ff898339e1fac29abd2bf5f11078f3ec413cfe9c046d3120d7ca65530b54 \; --hash=sha256:9b36e503703ff04f20e9675771df105e58aa029cfcbc23b8ed716019b7416ae1; # via sphinx; urllib3==2.1.0 \; --hash=sha256:55901e917a5896a349ff771be919f8bd99aff50b79fe58fec595eb37bbc56bb3 \; --hash=sha256:df7aa8afb0148fa78488e7899b2c59b5f4ffcfa82e6c54ccb9dd37c1d7b52d54; # via requests; ,MatchSource.DOCS,interpreter/llvm-project/llvm/docs/requirements-hashed.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/requirements-hashed.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CMakeLists.txt:1349,Modifiability,config,config,1349,"aries are added on the top-level; # CMakeLists.txt. add_subdirectory(IR); add_subdirectory(FuzzMutate); add_subdirectory(FileCheck); add_subdirectory(InterfaceStub); add_subdirectory(IRPrinter); add_subdirectory(IRReader); add_subdirectory(CodeGen); add_subdirectory(BinaryFormat); add_subdirectory(Bitcode); add_subdirectory(Bitstream); add_subdirectory(DWARFLinker); add_subdirectory(Extensions); add_subdirectory(Frontend); add_subdirectory(Transforms); add_subdirectory(Linker); add_subdirectory(Analysis); add_subdirectory(LTO); add_subdirectory(MC); add_subdirectory(MCA); add_subdirectory(ObjCopy); add_subdirectory(Object); add_subdirectory(ObjectYAML); add_subdirectory(Option); add_subdirectory(Remarks); add_subdirectory(Debuginfod); add_subdirectory(DebugInfo); add_subdirectory(DWP); add_subdirectory(ExecutionEngine); add_subdirectory(Target); add_subdirectory(AsmParser); add_subdirectory(LineEditor); add_subdirectory(ProfileData); add_subdirectory(Passes); add_subdirectory(TargetParser); add_subdirectory(TextAPI); add_subdirectory(ToolDrivers); add_subdirectory(XRay); if (LLVM_INCLUDE_TESTS); add_subdirectory(Testing); endif(); add_subdirectory(WindowsDriver); add_subdirectory(WindowsManifest). set(LLVMCONFIGLIBRARYDEPENDENCIESINC ""${LLVM_BINARY_DIR}/tools/llvm-config/LibraryDependencies.inc""). # Special components which don't have any source attached but aggregate other; # components; add_llvm_component_group(all-targets LINK_COMPONENTS ${LLVM_TARGETS_TO_BUILD}); add_llvm_component_group(Engine). # The native target may not be enabled when cross compiling; if(TARGET ${LLVM_NATIVE_ARCH}); add_llvm_component_group(Native LINK_COMPONENTS ${LLVM_NATIVE_ARCH}); add_llvm_component_group(NativeCodeGen LINK_COMPONENTS ${LLVM_NATIVE_ARCH}CodeGen); else(); add_llvm_component_group(Native); add_llvm_component_group(NativeCodeGen); endif(). # Component post-processing; LLVMBuildResolveComponentsLink(); LLVMBuildGenerateCFragment(OUTPUT ${LLVMCONFIGLIBRARYDEPENDENCIESINC}); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt:1397,Integrability,depend,dependencies,1397,"# Discover the projects that use CMake in the subdirectories.; # Note that explicit cmake invocation is required every time a new project is; # added or removed.; file(GLOB entries *); foreach(entry ${entries}); if(IS_DIRECTORY ${entry} AND EXISTS ${entry}/CMakeLists.txt); if((NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/compiler-rt) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/dragonegg) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxx) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxxabi) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libunwind) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/test-suite) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/openmp) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/cross-project-tests)); get_filename_component(entry_name ""${entry}"" NAME); add_llvm_external_project(${entry_name}); endif(); endif(); endforeach(entry). # Also add in libc++ and compiler-rt trees if present (and we have; # a sufficiently recent version of CMake where required).; if(${LLVM_BUILD_RUNTIME}); # MSVC isn't quite working with libc++ yet, disable it until issues are; # fixed.; # FIXME: LLVM_FORCE_BUILD_RUNTIME is currently used by libc++ to force; # enable the in-tree build when targeting clang-cl.; if(NOT MSVC OR LLVM_FORCE_BUILD_RUNTIME); # Add the projects in reverse order of their dependencies so that the; # dependent projects can see the target names of their dependencies.; add_llvm_external_project(libunwind); add_llvm_external_project(pstl); add_llvm_external_project(libc); add_llvm_external_project(libcxxabi); add_llvm_external_project(libcxx); endif(); if(NOT LLVM_BUILD_EXTERNAL_COMPILER_RT); add_llvm_external_project(compiler-rt); endif(); endif(). add_llvm_external_project(dragonegg); add_llvm_external_project(openmp). if(LLVM_INCLUDE_TESTS); add_llvm_external_project(cross-project-tests); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/projects/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt:1425,Integrability,depend,dependent,1425,"# Discover the projects that use CMake in the subdirectories.; # Note that explicit cmake invocation is required every time a new project is; # added or removed.; file(GLOB entries *); foreach(entry ${entries}); if(IS_DIRECTORY ${entry} AND EXISTS ${entry}/CMakeLists.txt); if((NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/compiler-rt) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/dragonegg) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxx) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxxabi) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libunwind) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/test-suite) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/openmp) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/cross-project-tests)); get_filename_component(entry_name ""${entry}"" NAME); add_llvm_external_project(${entry_name}); endif(); endif(); endforeach(entry). # Also add in libc++ and compiler-rt trees if present (and we have; # a sufficiently recent version of CMake where required).; if(${LLVM_BUILD_RUNTIME}); # MSVC isn't quite working with libc++ yet, disable it until issues are; # fixed.; # FIXME: LLVM_FORCE_BUILD_RUNTIME is currently used by libc++ to force; # enable the in-tree build when targeting clang-cl.; if(NOT MSVC OR LLVM_FORCE_BUILD_RUNTIME); # Add the projects in reverse order of their dependencies so that the; # dependent projects can see the target names of their dependencies.; add_llvm_external_project(libunwind); add_llvm_external_project(pstl); add_llvm_external_project(libc); add_llvm_external_project(libcxxabi); add_llvm_external_project(libcxx); endif(); if(NOT LLVM_BUILD_EXTERNAL_COMPILER_RT); add_llvm_external_project(compiler-rt); endif(); endif(). add_llvm_external_project(dragonegg); add_llvm_external_project(openmp). if(LLVM_INCLUDE_TESTS); add_llvm_external_project(cross-project-tests); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/projects/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt:1478,Integrability,depend,dependencies,1478,"# Discover the projects that use CMake in the subdirectories.; # Note that explicit cmake invocation is required every time a new project is; # added or removed.; file(GLOB entries *); foreach(entry ${entries}); if(IS_DIRECTORY ${entry} AND EXISTS ${entry}/CMakeLists.txt); if((NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/compiler-rt) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/dragonegg) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxx) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxxabi) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libunwind) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/test-suite) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/openmp) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/cross-project-tests)); get_filename_component(entry_name ""${entry}"" NAME); add_llvm_external_project(${entry_name}); endif(); endif(); endforeach(entry). # Also add in libc++ and compiler-rt trees if present (and we have; # a sufficiently recent version of CMake where required).; if(${LLVM_BUILD_RUNTIME}); # MSVC isn't quite working with libc++ yet, disable it until issues are; # fixed.; # FIXME: LLVM_FORCE_BUILD_RUNTIME is currently used by libc++ to force; # enable the in-tree build when targeting clang-cl.; if(NOT MSVC OR LLVM_FORCE_BUILD_RUNTIME); # Add the projects in reverse order of their dependencies so that the; # dependent projects can see the target names of their dependencies.; add_llvm_external_project(libunwind); add_llvm_external_project(pstl); add_llvm_external_project(libc); add_llvm_external_project(libcxxabi); add_llvm_external_project(libcxx); endif(); if(NOT LLVM_BUILD_EXTERNAL_COMPILER_RT); add_llvm_external_project(compiler-rt); endif(); endif(). add_llvm_external_project(dragonegg); add_llvm_external_project(openmp). if(LLVM_INCLUDE_TESTS); add_llvm_external_project(cross-project-tests); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/projects/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt:662,Testability,test,test-suite,662,"# Discover the projects that use CMake in the subdirectories.; # Note that explicit cmake invocation is required every time a new project is; # added or removed.; file(GLOB entries *); foreach(entry ${entries}); if(IS_DIRECTORY ${entry} AND EXISTS ${entry}/CMakeLists.txt); if((NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/compiler-rt) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/dragonegg) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxx) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxxabi) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libunwind) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/test-suite) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/openmp) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/cross-project-tests)); get_filename_component(entry_name ""${entry}"" NAME); add_llvm_external_project(${entry_name}); endif(); endif(); endforeach(entry). # Also add in libc++ and compiler-rt trees if present (and we have; # a sufficiently recent version of CMake where required).; if(${LLVM_BUILD_RUNTIME}); # MSVC isn't quite working with libc++ yet, disable it until issues are; # fixed.; # FIXME: LLVM_FORCE_BUILD_RUNTIME is currently used by libc++ to force; # enable the in-tree build when targeting clang-cl.; if(NOT MSVC OR LLVM_FORCE_BUILD_RUNTIME); # Add the projects in reverse order of their dependencies so that the; # dependent projects can see the target names of their dependencies.; add_llvm_external_project(libunwind); add_llvm_external_project(pstl); add_llvm_external_project(libc); add_llvm_external_project(libcxxabi); add_llvm_external_project(libcxx); endif(); if(NOT LLVM_BUILD_EXTERNAL_COMPILER_RT); add_llvm_external_project(compiler-rt); endif(); endif(). add_llvm_external_project(dragonegg); add_llvm_external_project(openmp). if(LLVM_INCLUDE_TESTS); add_llvm_external_project(cross-project-tests); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/projects/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt:808,Testability,test,tests,808,"# Discover the projects that use CMake in the subdirectories.; # Note that explicit cmake invocation is required every time a new project is; # added or removed.; file(GLOB entries *); foreach(entry ${entries}); if(IS_DIRECTORY ${entry} AND EXISTS ${entry}/CMakeLists.txt); if((NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/compiler-rt) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/dragonegg) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxx) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxxabi) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libunwind) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/test-suite) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/openmp) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/cross-project-tests)); get_filename_component(entry_name ""${entry}"" NAME); add_llvm_external_project(${entry_name}); endif(); endif(); endforeach(entry). # Also add in libc++ and compiler-rt trees if present (and we have; # a sufficiently recent version of CMake where required).; if(${LLVM_BUILD_RUNTIME}); # MSVC isn't quite working with libc++ yet, disable it until issues are; # fixed.; # FIXME: LLVM_FORCE_BUILD_RUNTIME is currently used by libc++ to force; # enable the in-tree build when targeting clang-cl.; if(NOT MSVC OR LLVM_FORCE_BUILD_RUNTIME); # Add the projects in reverse order of their dependencies so that the; # dependent projects can see the target names of their dependencies.; add_llvm_external_project(libunwind); add_llvm_external_project(pstl); add_llvm_external_project(libc); add_llvm_external_project(libcxxabi); add_llvm_external_project(libcxx); endif(); if(NOT LLVM_BUILD_EXTERNAL_COMPILER_RT); add_llvm_external_project(compiler-rt); endif(); endif(). add_llvm_external_project(dragonegg); add_llvm_external_project(openmp). if(LLVM_INCLUDE_TESTS); add_llvm_external_project(cross-project-tests); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/projects/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt:1915,Testability,test,tests,1915,"# Discover the projects that use CMake in the subdirectories.; # Note that explicit cmake invocation is required every time a new project is; # added or removed.; file(GLOB entries *); foreach(entry ${entries}); if(IS_DIRECTORY ${entry} AND EXISTS ${entry}/CMakeLists.txt); if((NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/compiler-rt) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/dragonegg) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxx) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libcxxabi) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/libunwind) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/test-suite) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/openmp) AND; (NOT ${entry} STREQUAL ${CMAKE_CURRENT_SOURCE_DIR}/cross-project-tests)); get_filename_component(entry_name ""${entry}"" NAME); add_llvm_external_project(${entry_name}); endif(); endif(); endforeach(entry). # Also add in libc++ and compiler-rt trees if present (and we have; # a sufficiently recent version of CMake where required).; if(${LLVM_BUILD_RUNTIME}); # MSVC isn't quite working with libc++ yet, disable it until issues are; # fixed.; # FIXME: LLVM_FORCE_BUILD_RUNTIME is currently used by libc++ to force; # enable the in-tree build when targeting clang-cl.; if(NOT MSVC OR LLVM_FORCE_BUILD_RUNTIME); # Add the projects in reverse order of their dependencies so that the; # dependent projects can see the target names of their dependencies.; add_llvm_external_project(libunwind); add_llvm_external_project(pstl); add_llvm_external_project(libc); add_llvm_external_project(libcxxabi); add_llvm_external_project(libcxx); endif(); if(NOT LLVM_BUILD_EXTERNAL_COMPILER_RT); add_llvm_external_project(compiler-rt); endif(); endif(). add_llvm_external_project(dragonegg); add_llvm_external_project(openmp). if(LLVM_INCLUDE_TESTS); add_llvm_external_project(cross-project-tests); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/projects/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/projects/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:1254,Availability,error,error,1254,"LLVM_LIT=ON;-DCLANG_RESOURCE_DIR=${CLANG_RESOURCE_DIR}""); foreach(proj ${LLVM_ENABLE_RUNTIMES}); set(proj_dir ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); if(IS_DIRECTORY ${proj_dir} AND EXISTS ${proj_dir}/CMakeLists.txt); list(APPEND runtimes ${proj_dir}); else(); message(FATAL_ERROR ""LLVM_ENABLE_RUNTIMES requests ${proj} but directory not found: ${proj_dir}""); endif(); string(TOUPPER ""${proj}"" canon_name); STRING(REGEX REPLACE ""-"" ""_"" canon_name ${canon_name}); set(LLVM_EXTERNAL_${canon_name}_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); endforeach(). function(get_compiler_rt_path path); foreach(entry ${runtimes}); get_filename_component(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:2009,Availability,error,error,2009,"ent(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^watchos""); message(FATAL_ERROR ""${error}""); endif(); endforeach(); endfunction(). macro(set_enable_per_target_runtime_dir); # May have been set by llvm/CMakeLists.txt.; if (NOT DEFINED LLVM_ENABLE_PER_TARGET_RUNTIME_DIR); # AIX should fold 32-bit & 64-bit arch libraries into a single archive.; if (LLVM_TARGET_TRIPLE MATCHES ""aix""); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR OFF); else(); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR ON); endif(); endif(); endmacro(). function(builtin_default_target compiler_rt_path); cmake_parse_arguments(ARG """" """" ""DEPENDS"" ${ARGN}). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(builtins; ${compiler_rt_path}",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:2143,Availability,error,error,2143,"ent(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^watchos""); message(FATAL_ERROR ""${error}""); endif(); endforeach(); endfunction(). macro(set_enable_per_target_runtime_dir); # May have been set by llvm/CMakeLists.txt.; if (NOT DEFINED LLVM_ENABLE_PER_TARGET_RUNTIME_DIR); # AIX should fold 32-bit & 64-bit arch libraries into a single archive.; if (LLVM_TARGET_TRIPLE MATCHES ""aix""); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR OFF); else(); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR ON); endif(); endif(); endmacro(). function(builtin_default_target compiler_rt_path); cmake_parse_arguments(ARG """" """" ""DEPENDS"" ${ARGN}). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(builtins; ${compiler_rt_path}",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:2312,Availability,error,error,2312," builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^watchos""); message(FATAL_ERROR ""${error}""); endif(); endforeach(); endfunction(). macro(set_enable_per_target_runtime_dir); # May have been set by llvm/CMakeLists.txt.; if (NOT DEFINED LLVM_ENABLE_PER_TARGET_RUNTIME_DIR); # AIX should fold 32-bit & 64-bit arch libraries into a single archive.; if (LLVM_TARGET_TRIPLE MATCHES ""aix""); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR OFF); else(); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR ON); endif(); endif(); endmacro(). function(builtin_default_target compiler_rt_path); cmake_parse_arguments(ARG """" """" ""DEPENDS"" ${ARGN}). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(builtins; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; ${COMMON_CM",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:1348,Deployability,configurat,configuration,1348,"LLVM_LIT=ON;-DCLANG_RESOURCE_DIR=${CLANG_RESOURCE_DIR}""); foreach(proj ${LLVM_ENABLE_RUNTIMES}); set(proj_dir ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); if(IS_DIRECTORY ${proj_dir} AND EXISTS ${proj_dir}/CMakeLists.txt); list(APPEND runtimes ${proj_dir}); else(); message(FATAL_ERROR ""LLVM_ENABLE_RUNTIMES requests ${proj} but directory not found: ${proj_dir}""); endif(); string(TOUPPER ""${proj}"" canon_name); STRING(REGEX REPLACE ""-"" ""_"" canon_name ${canon_name}); set(LLVM_EXTERNAL_${canon_name}_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); endforeach(). function(get_compiler_rt_path path); foreach(entry ${runtimes}); get_filename_component(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:4827,Deployability,configurat,configuration,4827,"t); if(""${out}"" EQUAL 0); string(REPLACE ""BUILTINS_${name}_"" """" new_name ${variable_name}); if(new_name STREQUAL CACHE_FILES); foreach(cache IN LISTS ${variable_name}); list(APPEND ${name}_extra_args -C ${cache}); endforeach(); else(); string(REPLACE "";"" ""|"" new_value ""${${variable_name}}""); list(APPEND ${name}_extra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(). llvm_ExternalProject_Add(builtins-${name}; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=ON; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}); endfunction(). # If compiler-rt is present we need to build the builtin libraries first. This; # is required because the other runtimes need the builtin libraries present; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:5252,Deployability,install,install-builtins,5252,"xtra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(). llvm_ExternalProject_Add(builtins-${name}; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=ON; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}); endfunction(). # If compiler-rt is present we need to build the builtin libraries first. This; # is required because the other runtimes need the builtin libraries present; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); endforeach(); endif(); set(builtins_dep builtins); # We don't need to depend on the builtins if we're building instrumented; # because the next stage will use the same compiler used to build this stage.; if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); add_dependencies(clang-bootstrap-deps builtins); e",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:5289,Deployability,install,install-builtins-stripped,5289,"xtra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(). llvm_ExternalProject_Add(builtins-${name}; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=ON; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}); endfunction(). # If compiler-rt is present we need to build the builtin libraries first. This; # is required because the other runtimes need the builtin libraries present; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); endforeach(); endif(); set(builtins_dep builtins); # We don't need to depend on the builtins if we're building instrumented; # because the next stage will use the same compiler used to build this stage.; if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); add_dependencies(clang-bootstrap-deps builtins); e",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:5645,Deployability,install,install-builtins,5645,"nt; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); endforeach(); endif(); set(builtins_dep builtins); # We don't need to depend on the builtins if we're building instrumented; # because the next stage will use the same compiler used to build this stage.; if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); add_dependencies(clang-bootstrap-deps builtins); endif(); endif(). function(_get_runtime_name name out_var); string(FIND ${name} ""lib"" idx); if(idx EQUAL 0 AND NOT ${name} STREQUAL ""libc""); string(SUBSTRING ${name} 3 -1 name); endif(); set(${out_var} ${name} PARENT_SCOPE); endfunction(). # Create a list with the names of all the runtime projects in all uppercase and; # with dashes turned to underscores. This gives us the CMake variable `prefixes`; # for all variables that will apply to runtimes.; foreach(entry ${runtimes}); get_filename_component(name ${entry} NAME); string(REPLACE ""-"" ""_"" canon_name ${name}); string(TOUPPER ${canon_name} canon_name); list(APPEND prefixes ${canon_name}); if (${canon_name} STREQUAL ""OPENMP""); list(A",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:5662,Deployability,install,install-builtins,5662,"nt; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); endforeach(); endif(); set(builtins_dep builtins); # We don't need to depend on the builtins if we're building instrumented; # because the next stage will use the same compiler used to build this stage.; if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); add_dependencies(clang-bootstrap-deps builtins); endif(); endif(). function(_get_runtime_name name out_var); string(FIND ${name} ""lib"" idx); if(idx EQUAL 0 AND NOT ${name} STREQUAL ""libc""); string(SUBSTRING ${name} 3 -1 name); endif(); set(${out_var} ${name} PARENT_SCOPE); endfunction(). # Create a list with the names of all the runtime projects in all uppercase and; # with dashes turned to underscores. This gives us the CMake variable `prefixes`; # for all variables that will apply to runtimes.; foreach(entry ${runtimes}); get_filename_component(name ${entry} NAME); string(REPLACE ""-"" ""_"" canon_name ${name}); string(TOUPPER ${canon_name} canon_name); list(APPEND prefixes ${canon_name}); if (${canon_name} STREQUAL ""OPENMP""); list(A",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:5708,Deployability,install,install-builtins-stripped,5708,"nt; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); endforeach(); endif(); set(builtins_dep builtins); # We don't need to depend on the builtins if we're building instrumented; # because the next stage will use the same compiler used to build this stage.; if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); add_dependencies(clang-bootstrap-deps builtins); endif(); endif(). function(_get_runtime_name name out_var); string(FIND ${name} ""lib"" idx); if(idx EQUAL 0 AND NOT ${name} STREQUAL ""libc""); string(SUBSTRING ${name} 3 -1 name); endif(); set(${out_var} ${name} PARENT_SCOPE); endfunction(). # Create a list with the names of all the runtime projects in all uppercase and; # with dashes turned to underscores. This gives us the CMake variable `prefixes`; # for all variables that will apply to runtimes.; foreach(entry ${runtimes}); get_filename_component(name ${entry} NAME); string(REPLACE ""-"" ""_"" canon_name ${name}); string(TOUPPER ${canon_name} canon_name); list(APPEND prefixes ${canon_name}); if (${canon_name} STREQUAL ""OPENMP""); list(A",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:5734,Deployability,install,install-builtins,5734,"nt; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); endforeach(); endif(); set(builtins_dep builtins); # We don't need to depend on the builtins if we're building instrumented; # because the next stage will use the same compiler used to build this stage.; if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); add_dependencies(clang-bootstrap-deps builtins); endif(); endif(). function(_get_runtime_name name out_var); string(FIND ${name} ""lib"" idx); if(idx EQUAL 0 AND NOT ${name} STREQUAL ""libc""); string(SUBSTRING ${name} 3 -1 name); endif(); set(${out_var} ${name} PARENT_SCOPE); endfunction(). # Create a list with the names of all the runtime projects in all uppercase and; # with dashes turned to underscores. This gives us the CMake variable `prefixes`; # for all variables that will apply to runtimes.; foreach(entry ${runtimes}); get_filename_component(name ${entry} NAME); string(REPLACE ""-"" ""_"" canon_name ${name}); string(TOUPPER ${canon_name} canon_name); list(APPEND prefixes ${canon_name}); if (${canon_name} STREQUAL ""OPENMP""); list(A",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:7818,Deployability,install,install,7818,"r both.; if(canon_name STREQUAL ""COMPILER_RT""); list(APPEND prefixes SANITIZER DARWIN); endif(); if(canon_name STREQUAL ""LIBC""); list(APPEND prefixes ""LLVM_LIBC""); list(APPEND prefixes ""LIBC_""); # The `libc` project may require '-DCUDAToolkit_ROOT' in GPU mode.; if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); list(APPEND prefixes ""CUDA""); endif(); endif(). _get_runtime_name(${name} name); list(APPEND RUNTIME_NAMES ${name}); endforeach(). function(runtime_default_target); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;PREFIXES"" ${ARGN}). include(${LLVM_BINARY_DIR}/runtimes/Components.cmake OPTIONAL); set(SUB_CHECK_TARGETS ${SUB_CHECK_TARGETS} PARENT_SCOPE); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/Components.cmake). foreach(runtime_name ${RUNTIME_NAMES}); list(APPEND extra_targets; ${runtime_name}; install-${runtime_name}; install-${runtime_name}-stripped); if(LLVM_INCLUDE_TESTS); list(APPEND test_targets check-${runtime_name}); endif(); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT ${component} IN_LIST SUB_COMPONENTS); list(APPEND extra_targets install-${component} install-${component}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-bins/lit.tests""); list(APPEND test_targets runtimes-test-depends check-runtimes); endif(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=Off; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DLLVM_BUILD_TOOLS=${LLVM_BUILD_TOOLS}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:7843,Deployability,install,install,7843,"r both.; if(canon_name STREQUAL ""COMPILER_RT""); list(APPEND prefixes SANITIZER DARWIN); endif(); if(canon_name STREQUAL ""LIBC""); list(APPEND prefixes ""LLVM_LIBC""); list(APPEND prefixes ""LIBC_""); # The `libc` project may require '-DCUDAToolkit_ROOT' in GPU mode.; if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); list(APPEND prefixes ""CUDA""); endif(); endif(). _get_runtime_name(${name} name); list(APPEND RUNTIME_NAMES ${name}); endforeach(). function(runtime_default_target); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;PREFIXES"" ${ARGN}). include(${LLVM_BINARY_DIR}/runtimes/Components.cmake OPTIONAL); set(SUB_CHECK_TARGETS ${SUB_CHECK_TARGETS} PARENT_SCOPE); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/Components.cmake). foreach(runtime_name ${RUNTIME_NAMES}); list(APPEND extra_targets; ${runtime_name}; install-${runtime_name}; install-${runtime_name}-stripped); if(LLVM_INCLUDE_TESTS); list(APPEND test_targets check-${runtime_name}); endif(); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT ${component} IN_LIST SUB_COMPONENTS); list(APPEND extra_targets install-${component} install-${component}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-bins/lit.tests""); list(APPEND test_targets runtimes-test-depends check-runtimes); endif(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=Off; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DLLVM_BUILD_TOOLS=${LLVM_BUILD_TOOLS}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:8105,Deployability,install,install,8105,"r both.; if(canon_name STREQUAL ""COMPILER_RT""); list(APPEND prefixes SANITIZER DARWIN); endif(); if(canon_name STREQUAL ""LIBC""); list(APPEND prefixes ""LLVM_LIBC""); list(APPEND prefixes ""LIBC_""); # The `libc` project may require '-DCUDAToolkit_ROOT' in GPU mode.; if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); list(APPEND prefixes ""CUDA""); endif(); endif(). _get_runtime_name(${name} name); list(APPEND RUNTIME_NAMES ${name}); endforeach(). function(runtime_default_target); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;PREFIXES"" ${ARGN}). include(${LLVM_BINARY_DIR}/runtimes/Components.cmake OPTIONAL); set(SUB_CHECK_TARGETS ${SUB_CHECK_TARGETS} PARENT_SCOPE); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/Components.cmake). foreach(runtime_name ${RUNTIME_NAMES}); list(APPEND extra_targets; ${runtime_name}; install-${runtime_name}; install-${runtime_name}-stripped); if(LLVM_INCLUDE_TESTS); list(APPEND test_targets check-${runtime_name}); endif(); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT ${component} IN_LIST SUB_COMPONENTS); list(APPEND extra_targets install-${component} install-${component}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-bins/lit.tests""); list(APPEND test_targets runtimes-test-depends check-runtimes); endif(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=Off; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DLLVM_BUILD_TOOLS=${LLVM_BUILD_TOOLS}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:8126,Deployability,install,install,8126,"r both.; if(canon_name STREQUAL ""COMPILER_RT""); list(APPEND prefixes SANITIZER DARWIN); endif(); if(canon_name STREQUAL ""LIBC""); list(APPEND prefixes ""LLVM_LIBC""); list(APPEND prefixes ""LIBC_""); # The `libc` project may require '-DCUDAToolkit_ROOT' in GPU mode.; if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); list(APPEND prefixes ""CUDA""); endif(); endif(). _get_runtime_name(${name} name); list(APPEND RUNTIME_NAMES ${name}); endforeach(). function(runtime_default_target); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;PREFIXES"" ${ARGN}). include(${LLVM_BINARY_DIR}/runtimes/Components.cmake OPTIONAL); set(SUB_CHECK_TARGETS ${SUB_CHECK_TARGETS} PARENT_SCOPE); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/Components.cmake). foreach(runtime_name ${RUNTIME_NAMES}); list(APPEND extra_targets; ${runtime_name}; install-${runtime_name}; install-${runtime_name}-stripped); if(LLVM_INCLUDE_TESTS); list(APPEND test_targets check-${runtime_name}); endif(); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT ${component} IN_LIST SUB_COMPONENTS); list(APPEND extra_targets install-${component} install-${component}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-bins/lit.tests""); list(APPEND test_targets runtimes-test-depends check-runtimes); endif(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=Off; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DLLVM_BUILD_TOOLS=${LLVM_BUILD_TOOLS}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:10145,Deployability,install,install,10145,"# runtime_register_target(name); # Utility function to register external runtime target.; function(runtime_register_target name); cmake_parse_arguments(ARG """" ""BASE_NAME"" ""DEPENDS;CMAKE_ARGS;EXTRA_ARGS"" ${ARGN}); include(${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake OPTIONAL); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake). set(runtime_names ${RUNTIME_NAMES}); foreach(_name IN ITEMS ${ARG_BASE_NAME} ${name}); if(RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES); set(runtime_names); foreach(entry ${RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES}); _get_runtime_name(${entry} runtime_name); list(APPEND runtime_names ${runtime_name}); endforeach(); endif(); endforeach(). foreach(runtime_name ${runtime_names}); set(${runtime_name}-${name} ${runtime_name}); set(install-${runtime_name}-${name} install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${n",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:10177,Deployability,install,install,10177,"# runtime_register_target(name); # Utility function to register external runtime target.; function(runtime_register_target name); cmake_parse_arguments(ARG """" ""BASE_NAME"" ""DEPENDS;CMAKE_ARGS;EXTRA_ARGS"" ${ARGN}); include(${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake OPTIONAL); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake). set(runtime_names ${RUNTIME_NAMES}); foreach(_name IN ITEMS ${ARG_BASE_NAME} ${name}); if(RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES); set(runtime_names); foreach(entry ${RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES}); _get_runtime_name(${entry} runtime_name); list(APPEND runtime_names ${runtime_name}); endforeach(); endif(); endforeach(). foreach(runtime_name ${runtime_names}); set(${runtime_name}-${name} ${runtime_name}); set(install-${runtime_name}-${name} install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${n",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:10207,Deployability,install,install,10207,"# runtime_register_target(name); # Utility function to register external runtime target.; function(runtime_register_target name); cmake_parse_arguments(ARG """" ""BASE_NAME"" ""DEPENDS;CMAKE_ARGS;EXTRA_ARGS"" ${ARGN}); include(${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake OPTIONAL); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake). set(runtime_names ${RUNTIME_NAMES}); foreach(_name IN ITEMS ${ARG_BASE_NAME} ${name}); if(RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES); set(runtime_names); foreach(entry ${RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES}); _get_runtime_name(${entry} runtime_name); list(APPEND runtime_names ${runtime_name}); endforeach(); endif(); endforeach(). foreach(runtime_name ${runtime_names}); set(${runtime_name}-${name} ${runtime_name}); set(install-${runtime_name}-${name} install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${n",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:10248,Deployability,install,install,10248,"# runtime_register_target(name); # Utility function to register external runtime target.; function(runtime_register_target name); cmake_parse_arguments(ARG """" ""BASE_NAME"" ""DEPENDS;CMAKE_ARGS;EXTRA_ARGS"" ${ARGN}); include(${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake OPTIONAL); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake). set(runtime_names ${RUNTIME_NAMES}); foreach(_name IN ITEMS ${ARG_BASE_NAME} ${name}); if(RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES); set(runtime_names); foreach(entry ${RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES}); _get_runtime_name(${entry} runtime_name); list(APPEND runtime_names ${runtime_name}); endforeach(); endif(); endforeach(). foreach(runtime_name ${runtime_names}); set(${runtime_name}-${name} ${runtime_name}); set(install-${runtime_name}-${name} install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${n",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:10341,Deployability,install,install,10341,"# runtime_register_target(name); # Utility function to register external runtime target.; function(runtime_register_target name); cmake_parse_arguments(ARG """" ""BASE_NAME"" ""DEPENDS;CMAKE_ARGS;EXTRA_ARGS"" ${ARGN}); include(${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake OPTIONAL); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake). set(runtime_names ${RUNTIME_NAMES}); foreach(_name IN ITEMS ${ARG_BASE_NAME} ${name}); if(RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES); set(runtime_names); foreach(entry ${RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES}); _get_runtime_name(${entry} runtime_name); list(APPEND runtime_names ${runtime_name}); endforeach(); endif(); endforeach(). foreach(runtime_name ${runtime_names}); set(${runtime_name}-${name} ${runtime_name}); set(install-${runtime_name}-${name} install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${n",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:10373,Deployability,install,install,10373,"# runtime_register_target(name); # Utility function to register external runtime target.; function(runtime_register_target name); cmake_parse_arguments(ARG """" ""BASE_NAME"" ""DEPENDS;CMAKE_ARGS;EXTRA_ARGS"" ${ARGN}); include(${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake OPTIONAL); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/${name}/Components.cmake). set(runtime_names ${RUNTIME_NAMES}); foreach(_name IN ITEMS ${ARG_BASE_NAME} ${name}); if(RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES); set(runtime_names); foreach(entry ${RUNTIMES_${_name}_LLVM_ENABLE_RUNTIMES}); _get_runtime_name(${entry} runtime_name); list(APPEND runtime_names ${runtime_name}); endforeach(); endif(); endforeach(). foreach(runtime_name ${runtime_names}); set(${runtime_name}-${name} ${runtime_name}); set(install-${runtime_name}-${name} install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${n",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11115,Deployability,install,install,11115,"install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11144,Deployability,install,install,11144,"install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11171,Deployability,install,install,11171,"install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11209,Deployability,install,install,11209,"install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11296,Deployability,install,install,11296,"install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11325,Deployability,install,install,11325,"install-${runtime_name}); set(install-${runtime_name}-${name}-stripped install-${runtime_name}-stripped); list(APPEND ${name}_extra_targets ${runtime_name}-${name} install-${runtime_name}-${name} install-${runtime_name}-${name}-stripped); if(LLVM_INCLUDE_TESTS); set(check-${runtime_name}-${name} check-${runtime_name} ); list(APPEND ${name}_test_targets check-${runtime_name}-${name}); endif(); endforeach(). foreach(component IN LISTS SUB_COMPONENTS); set(${component}-${name} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14041,Deployability,install,install-runtimes,14041,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14058,Deployability,install,install-runtimes,14058,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14102,Deployability,install,install-runtimes-stripped,14102,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14128,Deployability,install,install-runtimes,14128,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14515,Deployability,install,install,14515,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14559,Deployability,install,install,14559,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14611,Deployability,install,install,14611,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14635,Deployability,install,install,14635,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14683,Deployability,install,install,14683,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14736,Deployability,install,install,14736,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14797,Deployability,install,install,14797,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14830,Deployability,install,install,14830,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:15017,Deployability,install,install,15017,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:15038,Deployability,install,install,15038,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:15086,Deployability,install,install,15086,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:15116,Deployability,install,install,15116,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:15303,Deployability,configurat,configuration,15303,"ch(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:16909,Deployability,install,install-runtimes,16909," invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ${dep}); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); endif(); if(NOT LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); set(test_targets check-runtimes); else(); if(""default"" IN_LIST LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); list(REMOVE_ITEM LLVM_RUNTIME_TARGETS ""default""); else(); add_custom_target(runtimes); add_custom_target(runtimes-configure); add_custom_target(install-runtimes); add_custom_target(install-runtimes-stripped); if(LLVM_INCLUDE_TESTS); add_custom_target(check-runtimes); add_custom_target(runtimes-test-depends); set(test_targets """"); endif(); if(LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_custom_target(${component}); add_custom_target(install-${component}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(nam",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:16946,Deployability,install,install-runtimes-stripped,16946," invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ${dep}); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); endif(); if(NOT LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); set(test_targets check-runtimes); else(); if(""default"" IN_LIST LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); list(REMOVE_ITEM LLVM_RUNTIME_TARGETS ""default""); else(); add_custom_target(runtimes); add_custom_target(runtimes-configure); add_custom_target(install-runtimes); add_custom_target(install-runtimes-stripped); if(LLVM_INCLUDE_TESTS); add_custom_target(check-runtimes); add_custom_target(runtimes-test-depends); set(test_targets """"); endif(); if(LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_custom_target(${component}); add_custom_target(install-${component}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(nam",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:17259,Deployability,install,install,17259," invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ${dep}); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); endif(); if(NOT LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); set(test_targets check-runtimes); else(); if(""default"" IN_LIST LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); list(REMOVE_ITEM LLVM_RUNTIME_TARGETS ""default""); else(); add_custom_target(runtimes); add_custom_target(runtimes-configure); add_custom_target(install-runtimes); add_custom_target(install-runtimes-stripped); if(LLVM_INCLUDE_TESTS); add_custom_target(check-runtimes); add_custom_target(runtimes-test-depends); set(test_targets """"); endif(); if(LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_custom_target(${component}); add_custom_target(install-${component}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(nam",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:17300,Deployability,install,install,17300," invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ${dep}); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); endif(); if(NOT LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); set(test_targets check-runtimes); else(); if(""default"" IN_LIST LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); list(REMOVE_ITEM LLVM_RUNTIME_TARGETS ""default""); else(); add_custom_target(runtimes); add_custom_target(runtimes-configure); add_custom_target(install-runtimes); add_custom_target(install-runtimes-stripped); if(LLVM_INCLUDE_TESTS); add_custom_target(check-runtimes); add_custom_target(runtimes-test-depends); set(test_targets """"); endif(); if(LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_custom_target(${component}); add_custom_target(install-${component}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(nam",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18351,Deployability,configurat,configuration,18351,"nt}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(name ${LLVM_RUNTIME_TARGETS}); if(builtins_dep); if (LLVM_BUILTIN_TARGETS); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${targ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18459,Deployability,configurat,configurations,18459,"; if (LLVM_BUILTIN_TARGETS); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${target} ${RUNTIMES_TEST_DEPENDS}); endforeach(). set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_TARGETS runtimes ${RUNTIMES_TEST_DEPEN",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18574,Deployability,configurat,configuration,18574,"S); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${target} ${RUNTIMES_TEST_DEPENDS}); endforeach(). set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_TARGETS runtimes ${RUNTIMES_TEST_DEPENDS}); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:544,Integrability,message,message,544,"# TODO: This file assumes the Clang toolchain so it'd be better if it lived in; # Clang, except there already is clang/runtime directory which contains; # similar although simpler functionality. We should figure out how to merge; # the two files. set(COMMON_CMAKE_ARGS ""-DHAVE_LLVM_LIT=ON;-DCLANG_RESOURCE_DIR=${CLANG_RESOURCE_DIR}""); foreach(proj ${LLVM_ENABLE_RUNTIMES}); set(proj_dir ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); if(IS_DIRECTORY ${proj_dir} AND EXISTS ${proj_dir}/CMakeLists.txt); list(APPEND runtimes ${proj_dir}); else(); message(FATAL_ERROR ""LLVM_ENABLE_RUNTIMES requests ${proj} but directory not found: ${proj_dir}""); endif(); string(TOUPPER ""${proj}"" canon_name); STRING(REGEX REPLACE ""-"" ""_"" canon_name ${canon_name}); set(LLVM_EXTERNAL_${canon_name}_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); endforeach(). function(get_compiler_rt_path path); foreach(entry ${runtimes}); get_filename_component(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_E",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:1986,Integrability,message,message,1986,"ent(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^watchos""); message(FATAL_ERROR ""${error}""); endif(); endforeach(); endfunction(). macro(set_enable_per_target_runtime_dir); # May have been set by llvm/CMakeLists.txt.; if (NOT DEFINED LLVM_ENABLE_PER_TARGET_RUNTIME_DIR); # AIX should fold 32-bit & 64-bit arch libraries into a single archive.; if (LLVM_TARGET_TRIPLE MATCHES ""aix""); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR OFF); else(); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR ON); endif(); endif(); endmacro(). function(builtin_default_target compiler_rt_path); cmake_parse_arguments(ARG """" """" ""DEPENDS"" ${ARGN}). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(builtins; ${compiler_rt_path}",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:2117,Integrability,message,message,2117,"ent(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^watchos""); message(FATAL_ERROR ""${error}""); endif(); endforeach(); endfunction(). macro(set_enable_per_target_runtime_dir); # May have been set by llvm/CMakeLists.txt.; if (NOT DEFINED LLVM_ENABLE_PER_TARGET_RUNTIME_DIR); # AIX should fold 32-bit & 64-bit arch libraries into a single archive.; if (LLVM_TARGET_TRIPLE MATCHES ""aix""); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR OFF); else(); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR ON); endif(); endif(); endmacro(). function(builtin_default_target compiler_rt_path); cmake_parse_arguments(ARG """" """" ""DEPENDS"" ${ARGN}). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(builtins; ${compiler_rt_path}",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:2289,Integrability,message,message,2289," builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^watchos""); message(FATAL_ERROR ""${error}""); endif(); endforeach(); endfunction(). macro(set_enable_per_target_runtime_dir); # May have been set by llvm/CMakeLists.txt.; if (NOT DEFINED LLVM_ENABLE_PER_TARGET_RUNTIME_DIR); # AIX should fold 32-bit & 64-bit arch libraries into a single archive.; if (LLVM_TARGET_TRIPLE MATCHES ""aix""); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR OFF); else(); set(LLVM_ENABLE_PER_TARGET_RUNTIME_DIR ON); endif(); endif(); endmacro(). function(builtin_default_target compiler_rt_path); cmake_parse_arguments(ARG """" """" ""DEPENDS"" ${ARGN}). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(builtins; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; ${COMMON_CM",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:5842,Integrability,depend,depend,5842,"nt; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); endforeach(); endif(); set(builtins_dep builtins); # We don't need to depend on the builtins if we're building instrumented; # because the next stage will use the same compiler used to build this stage.; if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); add_dependencies(clang-bootstrap-deps builtins); endif(); endif(). function(_get_runtime_name name out_var); string(FIND ${name} ""lib"" idx); if(idx EQUAL 0 AND NOT ${name} STREQUAL ""libc""); string(SUBSTRING ${name} 3 -1 name); endif(); set(${out_var} ${name} PARENT_SCOPE); endfunction(). # Create a list with the names of all the runtime projects in all uppercase and; # with dashes turned to underscores. This gives us the CMake variable `prefixes`; # for all variables that will apply to runtimes.; foreach(entry ${runtimes}); get_filename_component(name ${entry} NAME); string(REPLACE ""-"" ""_"" canon_name ${name}); string(TOUPPER ${canon_name} canon_name); list(APPEND prefixes ${canon_name}); if (${canon_name} STREQUAL ""OPENMP""); list(A",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:8361,Integrability,depend,depends,8361,"PEND RUNTIME_NAMES ${name}); endforeach(). function(runtime_default_target); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;PREFIXES"" ${ARGN}). include(${LLVM_BINARY_DIR}/runtimes/Components.cmake OPTIONAL); set(SUB_CHECK_TARGETS ${SUB_CHECK_TARGETS} PARENT_SCOPE); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/Components.cmake). foreach(runtime_name ${RUNTIME_NAMES}); list(APPEND extra_targets; ${runtime_name}; install-${runtime_name}; install-${runtime_name}-stripped); if(LLVM_INCLUDE_TESTS); list(APPEND test_targets check-${runtime_name}); endif(); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT ${component} IN_LIST SUB_COMPONENTS); list(APPEND extra_targets install-${component} install-${component}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-bins/lit.tests""); list(APPEND test_targets runtimes-test-depends check-runtimes); endif(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=Off; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DLLVM_BUILD_TOOLS=${LLVM_BUILD_TOOLS}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; ${COMMON_CMAKE_ARGS}; ${RUNTIMES_CMAKE_ARGS}; ${ARG_CMAKE_ARGS}; PASSTHROUGH_PREFIXES LLVM_ENABLE_RUNTIMES; LLVM_USE_LINKER; ${ARG_PREFIXES}; EXTRA_TARGETS ${extra_targets}; ${test_targets}; ${SUB_COMPONENTS}; ${SUB_CHECK_TARGETS}; ${SUB_INSTALL_TARGETS}; USE_TOOLCHAIN; TARGET_TRIPLE ${LLVM_TARGET_TRIPLE}; ${EXTRA_ARGS}); endfunction(). # runtime_register_target(name); #",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11555,Integrability,depend,depends,11555,"me} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11585,Integrability,depend,depends,11585,"me} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11686,Integrability,depend,depends,11686,"me} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14276,Integrability,depend,depends,14276,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14298,Integrability,depend,depends,14298,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:16026,Integrability,message,message,16026," invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ${dep}); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); endif(); if(NOT LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); set(test_targets check-runtimes); else(); if(""default"" IN_LIST LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); list(REMOVE_ITEM LLVM_RUNTIME_TARGETS ""default""); else(); add_custom_target(runtimes); add_custom_target(runtimes-configure); add_custom_target(install-runtimes); add_custom_target(install-runtimes-stripped); if(LLVM_INCLUDE_TESTS); add_custom_target(check-runtimes); add_custom_target(runtimes-test-depends); set(test_targets """"); endif(); if(LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_custom_target(${component}); add_custom_target(install-${component}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(nam",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:17065,Integrability,depend,depends,17065," invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ${dep}); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); endif(); if(NOT LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); set(test_targets check-runtimes); else(); if(""default"" IN_LIST LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); list(REMOVE_ITEM LLVM_RUNTIME_TARGETS ""default""); else(); add_custom_target(runtimes); add_custom_target(runtimes-configure); add_custom_target(install-runtimes); add_custom_target(install-runtimes-stripped); if(LLVM_INCLUDE_TESTS); add_custom_target(check-runtimes); add_custom_target(runtimes-test-depends); set(test_targets """"); endif(); if(LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_custom_target(${component}); add_custom_target(install-${component}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(nam",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18709,Integrability,depend,dependency,18709,"S); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${target} ${RUNTIMES_TEST_DEPENDS}); endforeach(). set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_TARGETS runtimes ${RUNTIMES_TEST_DEPENDS}); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18982,Integrability,depend,depends,18982,"S); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${target} ${RUNTIMES_TEST_DEPENDS}); endforeach(). set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_TARGETS runtimes ${RUNTIMES_TEST_DEPENDS}); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:1348,Modifiability,config,configuration,1348,"LLVM_LIT=ON;-DCLANG_RESOURCE_DIR=${CLANG_RESOURCE_DIR}""); foreach(proj ${LLVM_ENABLE_RUNTIMES}); set(proj_dir ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); if(IS_DIRECTORY ${proj_dir} AND EXISTS ${proj_dir}/CMakeLists.txt); list(APPEND runtimes ${proj_dir}); else(); message(FATAL_ERROR ""LLVM_ENABLE_RUNTIMES requests ${proj} but directory not found: ${proj_dir}""); endif(); string(TOUPPER ""${proj}"" canon_name); STRING(REGEX REPLACE ""-"" ""_"" canon_name ${canon_name}); set(LLVM_EXTERNAL_${canon_name}_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); endforeach(). function(get_compiler_rt_path path); foreach(entry ${runtimes}); get_filename_component(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:1523,Modifiability,variab,variables,1523,"UNTIMES requests ${proj} but directory not found: ${proj_dir}""); endif(); string(TOUPPER ""${proj}"" canon_name); STRING(REGEX REPLACE ""-"" ""_"" canon_name ${canon_name}); set(LLVM_EXTERNAL_${canon_name}_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); endforeach(). function(get_compiler_rt_path path); foreach(entry ${runtimes}); get_filename_component(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_ERROR ""${error}""); endif(); set_property(GLOBAL PROPERTY ${seen_property} YES); if(NOT RUNTIMES_BUILD_ALLOW_DARWIN); message(FATAL_ERROR ""\; ${error} Set RUNTIMES_BUILD_ALLOW_DARWIN to allow a single darwin triple.""); endif(); elseif(component_lower MATCHES ""^ios|^macos|^tvos|^watchos""); message(FATAL_ERROR ""${error}""); endif(); endforeach(); endfunction(). macro(set_enable_per_target_runtime_dir); # May have been set by llvm/CMakeLists.txt.; if (NOT DEFINED LLVM_ENABLE_PER_TARGET_RUNTIME_DIR); # AIX should fold 32-bit & 64-bit arch libraries into a single archive.; if (L",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:4827,Modifiability,config,configuration,4827,"t); if(""${out}"" EQUAL 0); string(REPLACE ""BUILTINS_${name}_"" """" new_name ${variable_name}); if(new_name STREQUAL CACHE_FILES); foreach(cache IN LISTS ${variable_name}); list(APPEND ${name}_extra_args -C ${cache}); endforeach(); else(); string(REPLACE "";"" ""|"" new_value ""${${variable_name}}""); list(APPEND ${name}_extra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(). llvm_ExternalProject_Add(builtins-${name}; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=ON; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}); endfunction(). # If compiler-rt is present we need to build the builtin libraries first. This; # is required because the other runtimes need the builtin libraries present; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:6467,Modifiability,variab,variable,6467,"rs; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); endforeach(); endif(); set(builtins_dep builtins); # We don't need to depend on the builtins if we're building instrumented; # because the next stage will use the same compiler used to build this stage.; if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); add_dependencies(clang-bootstrap-deps builtins); endif(); endif(). function(_get_runtime_name name out_var); string(FIND ${name} ""lib"" idx); if(idx EQUAL 0 AND NOT ${name} STREQUAL ""libc""); string(SUBSTRING ${name} 3 -1 name); endif(); set(${out_var} ${name} PARENT_SCOPE); endfunction(). # Create a list with the names of all the runtime projects in all uppercase and; # with dashes turned to underscores. This gives us the CMake variable `prefixes`; # for all variables that will apply to runtimes.; foreach(entry ${runtimes}); get_filename_component(name ${entry} NAME); string(REPLACE ""-"" ""_"" canon_name ${name}); string(TOUPPER ${canon_name} canon_name); list(APPEND prefixes ${canon_name}); if (${canon_name} STREQUAL ""OPENMP""); list(APPEND prefixes ""LIBOMP"" ""LIBOMPTARGET""); endif(); # Many compiler-rt options start with SANITIZER_ and DARWIN_ rather than; # COMPILER_RT_, so when compiler-rt is enabled, consider both.; if(canon_name STREQUAL ""COMPILER_RT""); list(APPEND prefixes SANITIZER DARWIN); endif(); if(canon_name STREQUAL ""LIBC""); list(APPEND prefixes ""LLVM_LIBC""); list(APPEND prefixes ""LIBC_""); # The `libc` project may require '-DCUDAToolkit_ROOT' in GPU mode.; if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); list(APPEND prefixes ""CUDA""); endif(); endif(). _get_runtime_name(${name} name); list(APPEND RUNTIME_NAMES ${name}); endforeach(). function(runtime_default_target); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;PREFIXE",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:6498,Modifiability,variab,variables,6498,"rs; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); endforeach(); endif(); set(builtins_dep builtins); # We don't need to depend on the builtins if we're building instrumented; # because the next stage will use the same compiler used to build this stage.; if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); add_dependencies(clang-bootstrap-deps builtins); endif(); endif(). function(_get_runtime_name name out_var); string(FIND ${name} ""lib"" idx); if(idx EQUAL 0 AND NOT ${name} STREQUAL ""libc""); string(SUBSTRING ${name} 3 -1 name); endif(); set(${out_var} ${name} PARENT_SCOPE); endfunction(). # Create a list with the names of all the runtime projects in all uppercase and; # with dashes turned to underscores. This gives us the CMake variable `prefixes`; # for all variables that will apply to runtimes.; foreach(entry ${runtimes}); get_filename_component(name ${entry} NAME); string(REPLACE ""-"" ""_"" canon_name ${name}); string(TOUPPER ${canon_name} canon_name); list(APPEND prefixes ${canon_name}); if (${canon_name} STREQUAL ""OPENMP""); list(APPEND prefixes ""LIBOMP"" ""LIBOMPTARGET""); endif(); # Many compiler-rt options start with SANITIZER_ and DARWIN_ rather than; # COMPILER_RT_, so when compiler-rt is enabled, consider both.; if(canon_name STREQUAL ""COMPILER_RT""); list(APPEND prefixes SANITIZER DARWIN); endif(); if(canon_name STREQUAL ""LIBC""); list(APPEND prefixes ""LLVM_LIBC""); list(APPEND prefixes ""LIBC_""); # The `libc` project may require '-DCUDAToolkit_ROOT' in GPU mode.; if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); list(APPEND prefixes ""CUDA""); endif(); endif(). _get_runtime_name(${name} name); list(APPEND RUNTIME_NAMES ${name}); endforeach(). function(runtime_default_target); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;PREFIXE",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:13985,Modifiability,config,configure,13985,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14012,Modifiability,config,configure,14012,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:15303,Modifiability,config,configuration,15303,"ch(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:16879,Modifiability,config,configure,16879," invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ${dep}); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); endif(); if(NOT LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); set(test_targets check-runtimes); else(); if(""default"" IN_LIST LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); list(REMOVE_ITEM LLVM_RUNTIME_TARGETS ""default""); else(); add_custom_target(runtimes); add_custom_target(runtimes-configure); add_custom_target(install-runtimes); add_custom_target(install-runtimes-stripped); if(LLVM_INCLUDE_TESTS); add_custom_target(check-runtimes); add_custom_target(runtimes-test-depends); set(test_targets """"); endif(); if(LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_custom_target(${component}); add_custom_target(install-${component}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(nam",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18351,Modifiability,config,configuration,18351,"nt}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(name ${LLVM_RUNTIME_TARGETS}); if(builtins_dep); if (LLVM_BUILTIN_TARGETS); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${targ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18459,Modifiability,config,configurations,18459,"; if (LLVM_BUILTIN_TARGETS); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${target} ${RUNTIMES_TEST_DEPENDS}); endforeach(). set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_TARGETS runtimes ${RUNTIMES_TEST_DEPEN",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18574,Modifiability,config,configuration,18574,"S); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${target} ${RUNTIMES_TEST_DEPENDS}); endforeach(). set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_TARGETS runtimes ${RUNTIMES_TEST_DEPENDS}); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18662,Modifiability,config,configure,18662,"S); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${target} ${RUNTIMES_TEST_DEPENDS}); endforeach(). set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_TARGETS runtimes ${RUNTIMES_TEST_DEPENDS}); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:3907,Performance,cache,cache,3907,"ir(). llvm_ExternalProject_Add(builtins; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; ${COMMON_CMAKE_ARGS}; ${BUILTINS_CMAKE_ARGS}; PASSTHROUGH_PREFIXES COMPILER_RT; DARWIN; SANITIZER; USE_TOOLCHAIN; TARGET_TRIPLE ${LLVM_TARGET_TRIPLE}; ${EXTRA_ARGS}); endfunction(). function(builtin_register_target compiler_rt_path name); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;EXTRA_ARGS"" ${ARGN}). set(${name}_extra_args ${ARG_CMAKE_ARGS}); get_cmake_property(variable_names VARIABLES); foreach(variable_name ${variable_names}); string(FIND ""${variable_name}"" ""BUILTINS_${name}"" out); if(""${out}"" EQUAL 0); string(REPLACE ""BUILTINS_${name}_"" """" new_name ${variable_name}); if(new_name STREQUAL CACHE_FILES); foreach(cache IN LISTS ${variable_name}); list(APPEND ${name}_extra_args -C ${cache}); endforeach(); else(); string(REPLACE "";"" ""|"" new_value ""${${variable_name}}""); list(APPEND ${name}_extra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(). llvm_ExternalProject_Add(builtins-${name}; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=ON; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}); endfunction(). # If compiler-rt is present we need to build the builtin libraries first. This; # is required because the other runtimes need the builtin libraries present; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(com",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:3977,Performance,cache,cache,3977,"ir(). llvm_ExternalProject_Add(builtins; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; ${COMMON_CMAKE_ARGS}; ${BUILTINS_CMAKE_ARGS}; PASSTHROUGH_PREFIXES COMPILER_RT; DARWIN; SANITIZER; USE_TOOLCHAIN; TARGET_TRIPLE ${LLVM_TARGET_TRIPLE}; ${EXTRA_ARGS}); endfunction(). function(builtin_register_target compiler_rt_path name); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;EXTRA_ARGS"" ${ARGN}). set(${name}_extra_args ${ARG_CMAKE_ARGS}); get_cmake_property(variable_names VARIABLES); foreach(variable_name ${variable_names}); string(FIND ""${variable_name}"" ""BUILTINS_${name}"" out); if(""${out}"" EQUAL 0); string(REPLACE ""BUILTINS_${name}_"" """" new_name ${variable_name}); if(new_name STREQUAL CACHE_FILES); foreach(cache IN LISTS ${variable_name}); list(APPEND ${name}_extra_args -C ${cache}); endforeach(); else(); string(REPLACE "";"" ""|"" new_value ""${${variable_name}}""); list(APPEND ${name}_extra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(). llvm_ExternalProject_Add(builtins-${name}; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=ON; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}); endfunction(). # If compiler-rt is present we need to build the builtin libraries first. This; # is required because the other runtimes need the builtin libraries present; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(com",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:12917,Performance,cache,cache,12917,"NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG_BASE_NAME} ${name}); foreach(variable_name ${variable_names}); string(FIND ""${variable_name}"" ""RUNTIMES_${extra_name}_"" out); if(""${out}"" EQUAL 0); string(REPLACE ""RUNTIMES_${extra_name}_"" """" new_name ${variable_name}); if(new_name STREQUAL CACHE_FILES); foreach(cache IN LISTS ${variable_name}); list(APPEND ${name}_extra_args -C ${cache}); endforeach(); else(); string(REPLACE "";"" ""|"" new_value ""${${variable_name}}""); list(APPEND ${name}_extra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(); endforeach(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes-${name}; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=OFF; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:12987,Performance,cache,cache,12987,"NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG_BASE_NAME} ${name}); foreach(variable_name ${variable_names}); string(FIND ""${variable_name}"" ""RUNTIMES_${extra_name}_"" out); if(""${out}"" EQUAL 0); string(REPLACE ""RUNTIMES_${extra_name}_"" """" new_name ${variable_name}); if(new_name STREQUAL CACHE_FILES); foreach(cache IN LISTS ${variable_name}); list(APPEND ${name}_extra_args -C ${cache}); endforeach(); else(); string(REPLACE "";"" ""|"" new_value ""${${variable_name}}""); list(APPEND ${name}_extra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(); endforeach(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes-${name}; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=OFF; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:4841,Testability,test,tests,4841,"t); if(""${out}"" EQUAL 0); string(REPLACE ""BUILTINS_${name}_"" """" new_name ${variable_name}); if(new_name STREQUAL CACHE_FILES); foreach(cache IN LISTS ${variable_name}); list(APPEND ${name}_extra_args -C ${cache}); endforeach(); else(); string(REPLACE "";"" ""|"" new_value ""${${variable_name}}""); list(APPEND ${name}_extra_args ""-D${new_name}=${new_value}""); endif(); endif(); endforeach(). llvm_ExternalProject_Add(builtins-${name}; ${compiler_rt_path}/lib/builtins; DEPENDS ${ARG_DEPENDS}; CMAKE_ARGS -DLLVM_LIBRARY_OUTPUT_INTDIR=${LLVM_LIBRARY_DIR}; -DLLVM_RUNTIME_OUTPUT_INTDIR=${LLVM_TOOLS_BINARY_DIR}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=ON; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}); endfunction(). # If compiler-rt is present we need to build the builtin libraries first. This; # is required because the other runtimes need the builtin libraries present; # before the just-built compiler can pass the configuration tests.; get_compiler_rt_path(compiler_rt_path); if(compiler_rt_path); if(NOT LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); else(); if(""default"" IN_LIST LLVM_BUILTIN_TARGETS); builtin_default_target(${compiler_rt_path}; DEPENDS clang-resource-headers); list(REMOVE_ITEM LLVM_BUILTIN_TARGETS ""default""); else(); add_custom_target(builtins); add_custom_target(install-builtins); add_custom_target(install-builtins-stripped); endif(). foreach(target ${LLVM_BUILTIN_TARGETS}); check_apple_target(${target} builtin). builtin_register_target(${compiler_rt_path} ${target}; DEPENDS clang-resource-headers; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${target}; EXTRA_ARGS TARGET_TRIPLE ${target}). add_dependencies(builtins builtins-${target}); add_dependencies(install-builtins install-builtins-${target}); add_dependencies(install-builtins-stripped install-builtins-${target}-stripped); ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:8313,Testability,test,tests,8313,"PEND RUNTIME_NAMES ${name}); endforeach(). function(runtime_default_target); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;PREFIXES"" ${ARGN}). include(${LLVM_BINARY_DIR}/runtimes/Components.cmake OPTIONAL); set(SUB_CHECK_TARGETS ${SUB_CHECK_TARGETS} PARENT_SCOPE); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/Components.cmake). foreach(runtime_name ${RUNTIME_NAMES}); list(APPEND extra_targets; ${runtime_name}; install-${runtime_name}; install-${runtime_name}-stripped); if(LLVM_INCLUDE_TESTS); list(APPEND test_targets check-${runtime_name}); endif(); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT ${component} IN_LIST SUB_COMPONENTS); list(APPEND extra_targets install-${component} install-${component}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-bins/lit.tests""); list(APPEND test_targets runtimes-test-depends check-runtimes); endif(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=Off; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DLLVM_BUILD_TOOLS=${LLVM_BUILD_TOOLS}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; ${COMMON_CMAKE_ARGS}; ${RUNTIMES_CMAKE_ARGS}; ${ARG_CMAKE_ARGS}; PASSTHROUGH_PREFIXES LLVM_ENABLE_RUNTIMES; LLVM_USE_LINKER; ${ARG_PREFIXES}; EXTRA_TARGETS ${extra_targets}; ${test_targets}; ${SUB_COMPONENTS}; ${SUB_CHECK_TARGETS}; ${SUB_INSTALL_TARGETS}; USE_TOOLCHAIN; TARGET_TRIPLE ${LLVM_TARGET_TRIPLE}; ${EXTRA_ARGS}); endfunction(). # runtime_register_target(name); #",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:8356,Testability,test,test-depends,8356,"PEND RUNTIME_NAMES ${name}); endforeach(). function(runtime_default_target); cmake_parse_arguments(ARG """" """" ""DEPENDS;CMAKE_ARGS;PREFIXES"" ${ARGN}). include(${LLVM_BINARY_DIR}/runtimes/Components.cmake OPTIONAL); set(SUB_CHECK_TARGETS ${SUB_CHECK_TARGETS} PARENT_SCOPE); set_property(DIRECTORY APPEND PROPERTY CMAKE_CONFIGURE_DEPENDS ${LLVM_BINARY_DIR}/runtimes/Components.cmake). foreach(runtime_name ${RUNTIME_NAMES}); list(APPEND extra_targets; ${runtime_name}; install-${runtime_name}; install-${runtime_name}-stripped); if(LLVM_INCLUDE_TESTS); list(APPEND test_targets check-${runtime_name}); endif(); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT ${component} IN_LIST SUB_COMPONENTS); list(APPEND extra_targets install-${component} install-${component}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-bins/lit.tests""); list(APPEND test_targets runtimes-test-depends check-runtimes); endif(). set_enable_per_target_runtime_dir(). llvm_ExternalProject_Add(runtimes; ${CMAKE_CURRENT_SOURCE_DIR}/../../runtimes; DEPENDS ${ARG_DEPENDS}; # Builtins were built separately above; CMAKE_ARGS -DCOMPILER_RT_BUILD_BUILTINS=Off; -DLLVM_INCLUDE_TESTS=${LLVM_INCLUDE_TESTS}; -DLLVM_DEFAULT_TARGET_TRIPLE=${LLVM_TARGET_TRIPLE}; -DLLVM_ENABLE_PROJECTS_USED=${LLVM_ENABLE_PROJECTS_USED}; -DLLVM_ENABLE_PER_TARGET_RUNTIME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DLLVM_BUILD_TOOLS=${LLVM_BUILD_TOOLS}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; ${COMMON_CMAKE_ARGS}; ${RUNTIMES_CMAKE_ARGS}; ${ARG_CMAKE_ARGS}; PASSTHROUGH_PREFIXES LLVM_ENABLE_RUNTIMES; LLVM_USE_LINKER; ${ARG_PREFIXES}; EXTRA_TARGETS ${extra_targets}; ${test_targets}; ${SUB_COMPONENTS}; ${SUB_CHECK_TARGETS}; ${SUB_INSTALL_TARGETS}; USE_TOOLCHAIN; TARGET_TRIPLE ${LLVM_TARGET_TRIPLE}; ${EXTRA_ARGS}); endfunction(). # runtime_register_target(name); #",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11528,Testability,test,tests,11528,"me} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11550,Testability,test,test-depends,11550,"me} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11580,Testability,test,test-depends,11580,"me} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:11681,Testability,test,test-depends,11681,"me} ${component}); list(APPEND ${name}_extra_targets ${component}-${name}); endforeach(). foreach(target IN LISTS SUB_INSTALL_TARGETS); set(${target}-${name} ${target}); set(${target}-${name}-stripped ${target}-stripped); list(APPEND ${name}_extra_targets ${target}-${name} ${target}-${name}-stripped); endforeach(). foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); if(NOT component IN_LIST SUB_COMPONENTS); set(${component}-${name} ${component}); set(install-${component}-${name} install-${component}); set(install-${component}-${name}-stripped install-${component}-stripped); list(APPEND ${name}_extra_targets ${component}-${name} install-${component}-${name} install-${component}-${name}-stripped); endif(); endforeach(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_LIT_TESTSUITES ""@${LLVM_BINARY_DIR}/runtimes/runtimes-${name}-bins/lit.tests""); set(runtimes-test-depends-${name} runtimes-test-depends); set(check-runtimes-${name} check-runtimes); list(APPEND ${name}_test_targets runtimes-test-depends-${name} check-runtimes-${name}); list(APPEND test_targets ${${name}_test_targets}). set(component_check_targets); foreach(component IN LISTS LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); if(NOT ""check-${component}"" IN_LIST SUB_CHECK_TARGETS); list(APPEND component_check_targets ""check-${component}""); endif(); endforeach(). foreach(target IN LISTS SUB_CHECK_TARGETS component_check_targets); set(${target}-${name} ${target}); list(APPEND ${name}_test_targets ${target}-${name}); list(APPEND test_targets ${target}-${name}); endforeach(); set(test_targets ""${test_targets}"" PARENT_SCOPE); endif(). set(${name}_extra_args ${ARG_CMAKE_ARGS}); string(REPLACE "";"" ""|"" LLVM_ENABLE_RUNTIMES_PASSTHROUGH ""${LLVM_ENABLE_RUNTIMES}""); list(APPEND ${name}_extra_args -DLLVM_ENABLE_RUNTIMES=${LLVM_ENABLE_RUNTIMES_PASSTHROUGH}); list(APPEND ${name}_extra_args -DLLVM_USE_LINKER=${LLVM_USE_LINKER}). get_cmake_property(variable_names VARIABLES); foreach(extra_name IN ITEMS ${ARG",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14271,Testability,test,test-depends,14271,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:14293,Testability,test,test-depends,14293,"ME_DIR=${LLVM_ENABLE_PER_TARGET_RUNTIME_DIR}; -DCMAKE_C_COMPILER_WORKS=ON; -DCMAKE_CXX_COMPILER_WORKS=ON; -DCMAKE_ASM_COMPILER_WORKS=ON; -DCOMPILER_RT_DEFAULT_TARGET_ONLY=ON; -DLLVM_RUNTIMES_TARGET=${name}; ${COMMON_CMAKE_ARGS}; ${${name}_extra_args}; EXTRA_TARGETS ${${name}_extra_targets}; ${${name}_test_targets}; USE_TOOLCHAIN; ${EXTRA_ARGS} ${ARG_EXTRA_ARGS}). add_dependencies(runtimes runtimes-${name}); add_dependencies(runtimes-configure runtimes-${name}-configure); add_dependencies(install-runtimes install-runtimes-${name}); add_dependencies(install-runtimes-stripped install-runtimes-${name}-stripped); if(LLVM_INCLUDE_TESTS); add_dependencies(check-runtimes check-runtimes-${name}); add_dependencies(runtimes-test-depends runtimes-test-depends-${name}); endif(); foreach(runtime_name ${runtime_names}); if(NOT TARGET ${runtime_name}); add_custom_target(${runtime_name}); endif(); add_dependencies(${runtime_name} ${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}); add_custom_target(install-${runtime_name}); endif(); add_dependencies(install-${runtime_name} install-${runtime_name}-${name}); if(NOT TARGET install-${runtime_name}-stripped); add_custom_target(install-${runtime_name}-stripped); endif(); add_dependencies(install-${runtime_name}-stripped install-${runtime_name}-${name}-stripped); endforeach(); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_dependencies(${component} ${component}-${name}); add_dependencies(install-${component} install-${component}-${name}); add_dependencies(install-${component}-stripped install-${component}-${name}-stripped); endforeach(); endfunction(). if(runtimes); # Create a runtimes target that uses this file as its top-level CMake file.; # The runtimes target is a configuration of all the runtime libraries; # together in a single CMake invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:17060,Testability,test,test-depends,17060," invocation.; set(extra_deps """"); if(""openmp"" IN_LIST LLVM_ENABLE_RUNTIMES); foreach(dep opt llvm-link llvm-extract clang clang-offload-packager); if(TARGET ${dep} AND OPENMP_ENABLE_LIBOMPTARGET); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); if(""libc"" IN_LIST LLVM_ENABLE_PROJECTS AND; (LLVM_LIBC_FULL_BUILD OR LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES)); if(LIBC_HDRGEN_EXE); set(hdrgen_exe ${LIBC_HDRGEN_EXE}); else(); if(TARGET ${LIBC_TABLEGEN_EXE}); set(hdrgen_exe $<TARGET_FILE:${LIBC_TABLEGEN_EXE}>); else(); set(hdrgen_exe ${LIBC_TABLEGEN_EXE}); endif(); set(hdrgen_deps ${LIBC_TABLEGEN_TARGET}); endif(); if(NOT hdrgen_exe); message(FATAL_ERROR ""libc-hdrgen executable missing""); endif(); set(libc_cmake_args ""-DLIBC_HDRGEN_EXE=${hdrgen_exe}""; ""-DLLVM_LIBC_FULL_BUILD=ON""); list(APPEND extra_deps ${hdrgen_deps}); if(LIBC_GPU_BUILD OR LIBC_GPU_ARCHITECTURES); foreach(dep clang-offload-packager nvptx-arch amdgpu-arch); if(TARGET ${dep}); list(APPEND extra_deps ${dep}); endif(); endforeach(); endif(); endif(); if(NOT LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); set(test_targets check-runtimes); else(); if(""default"" IN_LIST LLVM_RUNTIME_TARGETS); runtime_default_target(; DEPENDS ${builtins_dep} ${extra_deps}; CMAKE_ARGS ${libc_cmake_args}; PREFIXES ${prefixes}); list(REMOVE_ITEM LLVM_RUNTIME_TARGETS ""default""); else(); add_custom_target(runtimes); add_custom_target(runtimes-configure); add_custom_target(install-runtimes); add_custom_target(install-runtimes-stripped); if(LLVM_INCLUDE_TESTS); add_custom_target(check-runtimes); add_custom_target(runtimes-test-depends); set(test_targets """"); endif(); if(LLVM_RUNTIME_DISTRIBUTION_COMPONENTS); foreach(component ${LLVM_RUNTIME_DISTRIBUTION_COMPONENTS}); add_custom_target(${component}); add_custom_target(install-${component}); add_custom_target(install-${component}-stripped); endforeach(); endif(); endif(). foreach(nam",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:18977,Testability,test,test-depends,18977,"S); set(builtins_dep_name ""${builtins_dep}-${name}""); else(); set(builtins_dep_name ${builtins_dep}); endif(); endif(). check_apple_target(${name} runtime). runtime_register_target(${name}; DEPENDS ${builtins_dep_name} ${hdrgen_deps}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name} ${libc_cmake_args}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(). foreach(multilib ${LLVM_RUNTIME_MULTILIBS}); foreach(name ${LLVM_RUNTIME_MULTILIB_${multilib}_TARGETS}); runtime_register_target(${name}+${multilib}; DEPENDS runtimes-${name}; CMAKE_ARGS -DLLVM_DEFAULT_TARGET_TRIPLE=${name}; -DLLVM_RUNTIMES_PREFIX=${name}/; -DLLVM_RUNTIMES_LIBDIR_SUBDIR=${multilib}; BASE_NAME ${name}; EXTRA_ARGS TARGET_TRIPLE ${name}); endforeach(); endforeach(); endif(). if(NOT LLVM_BUILD_INSTRUMENTED AND CLANG_ENABLE_BOOTSTRAP); # TODO: This is a hack needed because the libcxx headers are copied into the; # build directory during configuration. Without that step the clang in the; # build directory cannot find the C++ headers in certain configurations.; # I need to build a mechanism for runtime projects to provide CMake code; # that executes at LLVM configuration time to handle this case.; add_dependencies(clang-bootstrap-deps runtimes-configure); # We need to add the runtimes as a dependency because compiler-rt can be; # built as part of runtimes and we need the profile runtime for PGO; add_dependencies(clang-bootstrap-deps runtimes); endif(). if(LLVM_INCLUDE_TESTS); set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_DEPENDS runtimes-test-depends). set(RUNTIMES_TEST_DEPENDS; FileCheck; count; llvm-cov; llvm-lto; llvm-nm; llvm-objdump; llvm-profdata; llvm-size; llvm-xray; not; obj2yaml; opt; sancov; sanstats; llvm_gtest_main; llvm_gtest; split-file; ); foreach(target ${test_targets} ${SUB_CHECK_TARGETS}); add_dependencies(${target} ${RUNTIMES_TEST_DEPENDS}); endforeach(). set_property(GLOBAL APPEND PROPERTY LLVM_ALL_ADDITIONAL_TEST_TARGETS runtimes ${RUNTIMES_TEST_DEPENDS}); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt:172,Usability,simpl,simpler,172,"# TODO: This file assumes the Clang toolchain so it'd be better if it lived in; # Clang, except there already is clang/runtime directory which contains; # similar although simpler functionality. We should figure out how to merge; # the two files. set(COMMON_CMAKE_ARGS ""-DHAVE_LLVM_LIT=ON;-DCLANG_RESOURCE_DIR=${CLANG_RESOURCE_DIR}""); foreach(proj ${LLVM_ENABLE_RUNTIMES}); set(proj_dir ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); if(IS_DIRECTORY ${proj_dir} AND EXISTS ${proj_dir}/CMakeLists.txt); list(APPEND runtimes ${proj_dir}); else(); message(FATAL_ERROR ""LLVM_ENABLE_RUNTIMES requests ${proj} but directory not found: ${proj_dir}""); endif(); string(TOUPPER ""${proj}"" canon_name); STRING(REGEX REPLACE ""-"" ""_"" canon_name ${canon_name}); set(LLVM_EXTERNAL_${canon_name}_SOURCE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}/../../${proj}""); endforeach(). function(get_compiler_rt_path path); foreach(entry ${runtimes}); get_filename_component(projName ${entry} NAME); if(""${projName}"" MATCHES ""compiler-rt""); set(${path} ${entry} PARENT_SCOPE); return(); endif(); endforeach(); endfunction(). include(LLVMExternalProjectUtils). if(NOT LLVM_BUILD_RUNTIMES); set(EXTRA_ARGS EXCLUDE_FROM_ALL); endif(). function(check_apple_target triple builtin_or_runtime); set(error ""\; compiler-rt for Darwin builds for all platforms and architectures using a \; single configuration. Specify only a single darwin triple (e.g. x86_64-apple-darwin) \; in your targets list (and not a triple for a specific platform such as macos). \; You can use variables such as COMPILER_RT_ENABLE_IOS and DARWIN_ios_ARCHS to \; control the specific platforms and architectures to build.""). set(seen_property ${builtin_or_runtime}_darwin_triple_seen); string(REPLACE ""-"" "";"" triple_components ${triple}); foreach(component ${triple_components}); string(TOLOWER ""${component}"" component_lower); if(component_lower MATCHES ""^darwin""); get_property(darwin_triple_seen GLOBAL PROPERTY ${seen_property}); if(darwin_triple_seen); message(FATAL_E",MatchSource.DOCS,interpreter/llvm-project/llvm/runtimes/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/runtimes/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/CMakeLists.txt:1410,Integrability,depend,depends,1410,"to create CMake options to enable/disable; # traversing each directory.; create_llvm_tool_options(). if(NOT LLVM_BUILD_LLVM_DYLIB AND NOT LLVM_BUILD_LLVM_C_DYLIB); set(LLVM_TOOL_LLVM_SHLIB_BUILD Off); endif(). if(NOT LLVM_USE_INTEL_JITEVENTS ); set(LLVM_TOOL_LLVM_JITLISTENER_BUILD Off); endif(). if(CYGWIN OR NOT LLVM_ENABLE_PIC); set(LLVM_TOOL_LTO_BUILD Off); endif(). if (LLVM_TOOL_LLVM_DRIVER_BUILD); add_llvm_tool(llvm-driver); endif(). # Add LTO, llvm-ar, llvm-config, and llvm-profdata before clang, ExternalProject; # requires targets specified in DEPENDS to exist before the call to; # ExternalProject_Add.; add_llvm_tool_subdirectory(lto); add_llvm_tool_subdirectory(gold); add_llvm_tool_subdirectory(llvm-ar); add_llvm_tool_subdirectory(llvm-config); add_llvm_tool_subdirectory(llvm-lto); add_llvm_tool_subdirectory(llvm-profdata). # Projects supported via LLVM_EXTERNAL_*_SOURCE_DIR need to be explicitly; # specified.; add_llvm_external_project(clang); add_llvm_external_project(lld); add_llvm_external_project(lldb); add_llvm_external_project(mlir); # Flang depends on mlir, so place it afterward; add_llvm_external_project(flang); add_llvm_external_project(bolt). # Automatically add remaining sub-directories containing a 'CMakeLists.txt'; # file as external projects.; add_llvm_implicit_projects(). add_llvm_external_project(polly). # Add subprojects specified using LLVM_EXTERNAL_PROJECTS; foreach(p ${LLVM_EXTERNAL_PROJECTS}); add_llvm_external_project(${p}); endforeach(p). set(LLVM_COMMON_DEPENDS ${LLVM_COMMON_DEPENDS} PARENT_SCOPE). if (LLVM_TOOL_LLVM_DRIVER_BUILD); # This is explicitly added at the end _after_ all tool projects so that it can; # scrape up tools from other projects into itself.; add_subdirectory(llvm-driver); # This must be here otherwise CMake complains in add_llvm_tool_symlink that; # it can't add_custom_command that happens after llvm-driver is built because; # llvm-driver was not created in that directory.; generate_driver_tool_targets(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/CMakeLists.txt:88,Modifiability,variab,variables,88,"# This file will recurse into all subdirectories that contain CMakeLists.txt; # Setting variables that match the pattern LLVM_TOOL_{NAME}_BUILD to Off will; # prevent traversing into a directory.; #; # The only tools that need to be explicitly added are ones that have explicit; # ordering requirements. # Iterates all the subdirectories to create CMake options to enable/disable; # traversing each directory.; create_llvm_tool_options(). if(NOT LLVM_BUILD_LLVM_DYLIB AND NOT LLVM_BUILD_LLVM_C_DYLIB); set(LLVM_TOOL_LLVM_SHLIB_BUILD Off); endif(). if(NOT LLVM_USE_INTEL_JITEVENTS ); set(LLVM_TOOL_LLVM_JITLISTENER_BUILD Off); endif(). if(CYGWIN OR NOT LLVM_ENABLE_PIC); set(LLVM_TOOL_LTO_BUILD Off); endif(). if (LLVM_TOOL_LLVM_DRIVER_BUILD); add_llvm_tool(llvm-driver); endif(). # Add LTO, llvm-ar, llvm-config, and llvm-profdata before clang, ExternalProject; # requires targets specified in DEPENDS to exist before the call to; # ExternalProject_Add.; add_llvm_tool_subdirectory(lto); add_llvm_tool_subdirectory(gold); add_llvm_tool_subdirectory(llvm-ar); add_llvm_tool_subdirectory(llvm-config); add_llvm_tool_subdirectory(llvm-lto); add_llvm_tool_subdirectory(llvm-profdata). # Projects supported via LLVM_EXTERNAL_*_SOURCE_DIR need to be explicitly; # specified.; add_llvm_external_project(clang); add_llvm_external_project(lld); add_llvm_external_project(lldb); add_llvm_external_project(mlir); # Flang depends on mlir, so place it afterward; add_llvm_external_project(flang); add_llvm_external_project(bolt). # Automatically add remaining sub-directories containing a 'CMakeLists.txt'; # file as external projects.; add_llvm_implicit_projects(). add_llvm_external_project(polly). # Add subprojects specified using LLVM_EXTERNAL_PROJECTS; foreach(p ${LLVM_EXTERNAL_PROJECTS}); add_llvm_external_project(${p}); endforeach(p). set(LLVM_COMMON_DEPENDS ${LLVM_COMMON_DEPENDS} PARENT_SCOPE). if (LLVM_TOOL_LLVM_DRIVER_BUILD); # This is explicitly added at the end _after_ all tool projects so that i",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/CMakeLists.txt:805,Modifiability,config,config,805,"# This file will recurse into all subdirectories that contain CMakeLists.txt; # Setting variables that match the pattern LLVM_TOOL_{NAME}_BUILD to Off will; # prevent traversing into a directory.; #; # The only tools that need to be explicitly added are ones that have explicit; # ordering requirements. # Iterates all the subdirectories to create CMake options to enable/disable; # traversing each directory.; create_llvm_tool_options(). if(NOT LLVM_BUILD_LLVM_DYLIB AND NOT LLVM_BUILD_LLVM_C_DYLIB); set(LLVM_TOOL_LLVM_SHLIB_BUILD Off); endif(). if(NOT LLVM_USE_INTEL_JITEVENTS ); set(LLVM_TOOL_LLVM_JITLISTENER_BUILD Off); endif(). if(CYGWIN OR NOT LLVM_ENABLE_PIC); set(LLVM_TOOL_LTO_BUILD Off); endif(). if (LLVM_TOOL_LLVM_DRIVER_BUILD); add_llvm_tool(llvm-driver); endif(). # Add LTO, llvm-ar, llvm-config, and llvm-profdata before clang, ExternalProject; # requires targets specified in DEPENDS to exist before the call to; # ExternalProject_Add.; add_llvm_tool_subdirectory(lto); add_llvm_tool_subdirectory(gold); add_llvm_tool_subdirectory(llvm-ar); add_llvm_tool_subdirectory(llvm-config); add_llvm_tool_subdirectory(llvm-lto); add_llvm_tool_subdirectory(llvm-profdata). # Projects supported via LLVM_EXTERNAL_*_SOURCE_DIR need to be explicitly; # specified.; add_llvm_external_project(clang); add_llvm_external_project(lld); add_llvm_external_project(lldb); add_llvm_external_project(mlir); # Flang depends on mlir, so place it afterward; add_llvm_external_project(flang); add_llvm_external_project(bolt). # Automatically add remaining sub-directories containing a 'CMakeLists.txt'; # file as external projects.; add_llvm_implicit_projects(). add_llvm_external_project(polly). # Add subprojects specified using LLVM_EXTERNAL_PROJECTS; foreach(p ${LLVM_EXTERNAL_PROJECTS}); add_llvm_external_project(${p}); endforeach(p). set(LLVM_COMMON_DEPENDS ${LLVM_COMMON_DEPENDS} PARENT_SCOPE). if (LLVM_TOOL_LLVM_DRIVER_BUILD); # This is explicitly added at the end _after_ all tool projects so that i",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/CMakeLists.txt:1091,Modifiability,config,config,1091,"eLists.txt; # Setting variables that match the pattern LLVM_TOOL_{NAME}_BUILD to Off will; # prevent traversing into a directory.; #; # The only tools that need to be explicitly added are ones that have explicit; # ordering requirements. # Iterates all the subdirectories to create CMake options to enable/disable; # traversing each directory.; create_llvm_tool_options(). if(NOT LLVM_BUILD_LLVM_DYLIB AND NOT LLVM_BUILD_LLVM_C_DYLIB); set(LLVM_TOOL_LLVM_SHLIB_BUILD Off); endif(). if(NOT LLVM_USE_INTEL_JITEVENTS ); set(LLVM_TOOL_LLVM_JITLISTENER_BUILD Off); endif(). if(CYGWIN OR NOT LLVM_ENABLE_PIC); set(LLVM_TOOL_LTO_BUILD Off); endif(). if (LLVM_TOOL_LLVM_DRIVER_BUILD); add_llvm_tool(llvm-driver); endif(). # Add LTO, llvm-ar, llvm-config, and llvm-profdata before clang, ExternalProject; # requires targets specified in DEPENDS to exist before the call to; # ExternalProject_Add.; add_llvm_tool_subdirectory(lto); add_llvm_tool_subdirectory(gold); add_llvm_tool_subdirectory(llvm-ar); add_llvm_tool_subdirectory(llvm-config); add_llvm_tool_subdirectory(llvm-lto); add_llvm_tool_subdirectory(llvm-profdata). # Projects supported via LLVM_EXTERNAL_*_SOURCE_DIR need to be explicitly; # specified.; add_llvm_external_project(clang); add_llvm_external_project(lld); add_llvm_external_project(lldb); add_llvm_external_project(mlir); # Flang depends on mlir, so place it afterward; add_llvm_external_project(flang); add_llvm_external_project(bolt). # Automatically add remaining sub-directories containing a 'CMakeLists.txt'; # file as external projects.; add_llvm_implicit_projects(). add_llvm_external_project(polly). # Add subprojects specified using LLVM_EXTERNAL_PROJECTS; foreach(p ${LLVM_EXTERNAL_PROJECTS}); add_llvm_external_project(${p}); endforeach(p). set(LLVM_COMMON_DEPENDS ${LLVM_COMMON_DEPENDS} PARENT_SCOPE). if (LLVM_TOOL_LLVM_DRIVER_BUILD); # This is explicitly added at the end _after_ all tool projects so that it can; # scrape up tools from other projects into itself.; add_sub",MatchSource.DOCS,interpreter/llvm-project/llvm/tools/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:1474,Availability,error,errors,1474,"ke file.; set(LLVM_EXPORTS_FILE ${llvm_cmake_builddir}/LLVMExports.cmake); get_property(LLVM_EXPORTS GLOBAL PROPERTY LLVM_EXPORTS); export(TARGETS ${LLVM_EXPORTS} FILE ${LLVM_EXPORTS_FILE}). # Then for users who want to link against the LLVM build tree, provide the; # normal targets and the build tree only targets.; set(LLVM_BUILDTREEONLY_EXPORTS_FILE ${llvm_cmake_builddir}/LLVMBuildTreeOnlyTargets.cmake); get_property(LLVM_EXPORTS_BUILDTREE_ONLY GLOBAL PROPERTY LLVM_EXPORTS_BUILDTREE_ONLY); export(TARGETS ${LLVM_EXPORTS_BUILDTREE_ONLY} FILE ${LLVM_BUILDTREEONLY_EXPORTS_FILE}). get_property(LLVM_AVAILABLE_LIBS GLOBAL PROPERTY LLVM_LIBS). foreach(lib ${LLVM_AVAILABLE_LIBS}); get_property(llvm_lib_deps GLOBAL PROPERTY LLVMBUILD_LIB_DEPS_${lib}); set(all_llvm_lib_deps; ""${all_llvm_lib_deps}\nset_property(GLOBAL PROPERTY LLVMBUILD_LIB_DEPS_${lib} ${llvm_lib_deps})""); endforeach(lib). # CMake requires that all targets expressed as dependencies exist, so we can't; # have intrinsics_gen in LLVM_COMMON_DEPENDS when it is written out, otherwise; # projects building out of tree will have CMake errors. This only gets hit when; # LLVM_ENABLE_MODULES=On. Eventually we should come up with a better solution to; # this, but there is no easy solution.; if(intrinsics_gen IN_LIST LLVM_COMMON_DEPENDS); list(REMOVE_ITEM LLVM_COMMON_DEPENDS intrinsics_gen); endif(); if(omp_gen IN_LIST LLVM_COMMON_DEPENDS); list(REMOVE_ITEM LLVM_COMMON_DEPENDS omp_gen); endif(). #; # Generate LLVMConfig.cmake for the build tree.; #. set(LLVM_CONFIG_CODE ""; # LLVM_BUILD_* values available only from LLVM build tree.; set(LLVM_BUILD_BINARY_DIR \""${LLVM_BINARY_DIR}\""); set(LLVM_BUILD_LIBRARY_DIR \""${LLVM_LIBRARY_DIR}\""); set(LLVM_BUILD_MAIN_INCLUDE_DIR \""${LLVM_MAIN_INCLUDE_DIR}\""); set(LLVM_BUILD_MAIN_SRC_DIR \""${LLVM_MAIN_SRC_DIR}\""); ""). set(LLVM_CONFIG_MAIN_INCLUDE_DIR ""${LLVM_MAIN_INCLUDE_DIR}""); set(LLVM_CONFIG_INCLUDE_DIR ""${LLVM_INCLUDE_DIR}""); set(LLVM_CONFIG_INCLUDE_DIRS; ""${LLVM_CONFIG_MAIN_INCLUDE",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:1938,Availability,avail,available,1938,"DTREEONLY_EXPORTS_FILE}). get_property(LLVM_AVAILABLE_LIBS GLOBAL PROPERTY LLVM_LIBS). foreach(lib ${LLVM_AVAILABLE_LIBS}); get_property(llvm_lib_deps GLOBAL PROPERTY LLVMBUILD_LIB_DEPS_${lib}); set(all_llvm_lib_deps; ""${all_llvm_lib_deps}\nset_property(GLOBAL PROPERTY LLVMBUILD_LIB_DEPS_${lib} ${llvm_lib_deps})""); endforeach(lib). # CMake requires that all targets expressed as dependencies exist, so we can't; # have intrinsics_gen in LLVM_COMMON_DEPENDS when it is written out, otherwise; # projects building out of tree will have CMake errors. This only gets hit when; # LLVM_ENABLE_MODULES=On. Eventually we should come up with a better solution to; # this, but there is no easy solution.; if(intrinsics_gen IN_LIST LLVM_COMMON_DEPENDS); list(REMOVE_ITEM LLVM_COMMON_DEPENDS intrinsics_gen); endif(); if(omp_gen IN_LIST LLVM_COMMON_DEPENDS); list(REMOVE_ITEM LLVM_COMMON_DEPENDS omp_gen); endif(). #; # Generate LLVMConfig.cmake for the build tree.; #. set(LLVM_CONFIG_CODE ""; # LLVM_BUILD_* values available only from LLVM build tree.; set(LLVM_BUILD_BINARY_DIR \""${LLVM_BINARY_DIR}\""); set(LLVM_BUILD_LIBRARY_DIR \""${LLVM_LIBRARY_DIR}\""); set(LLVM_BUILD_MAIN_INCLUDE_DIR \""${LLVM_MAIN_INCLUDE_DIR}\""); set(LLVM_BUILD_MAIN_SRC_DIR \""${LLVM_MAIN_SRC_DIR}\""); ""). set(LLVM_CONFIG_MAIN_INCLUDE_DIR ""${LLVM_MAIN_INCLUDE_DIR}""); set(LLVM_CONFIG_INCLUDE_DIR ""${LLVM_INCLUDE_DIR}""); set(LLVM_CONFIG_INCLUDE_DIRS; ""${LLVM_CONFIG_MAIN_INCLUDE_DIR}""; ""${LLVM_CONFIG_INCLUDE_DIR}""; ); list(REMOVE_DUPLICATES LLVM_CONFIG_INCLUDE_DIRS). set(LLVM_CONFIG_LIBRARY_DIR ""${LLVM_LIBRARY_DIR}""); set(LLVM_CONFIG_LIBRARY_DIRS; ""${LLVM_CONFIG_LIBRARY_DIR}""; # FIXME: Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""${LLVM_BINARY_DIR}""); set(LLVM_CONFIG_CMAKE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}""); set(LLVM_CONFIG_TOOLS_BINARY_DIR ""${LLVM_TOOLS_BINARY_DIR}""). # Generate a default location for lit; if (LLVM_BUILD_UTILS); if (CMAKE_HOST_WIN32 AND N",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:331,Deployability,install,installed,331,"include(ExtendPath); include(LLVMDistributionSupport); include(FindPrefixFromConfig). # CMAKE_INSTALL_PACKAGEDIR might be absolute, so don't reuse below.; string(REPLACE ""${CMAKE_CFG_INTDIR}"" ""."" llvm_cmake_builddir ""${LLVM_LIBRARY_DIR}""); set(llvm_cmake_builddir ""${llvm_cmake_builddir}/cmake/llvm""). # First for users who use an installed LLVM, create the LLVMExports.cmake file.; set(LLVM_EXPORTS_FILE ${llvm_cmake_builddir}/LLVMExports.cmake); get_property(LLVM_EXPORTS GLOBAL PROPERTY LLVM_EXPORTS); export(TARGETS ${LLVM_EXPORTS} FILE ${LLVM_EXPORTS_FILE}). # Then for users who want to link against the LLVM build tree, provide the; # normal targets and the build tree only targets.; set(LLVM_BUILDTREEONLY_EXPORTS_FILE ${llvm_cmake_builddir}/LLVMBuildTreeOnlyTargets.cmake); get_property(LLVM_EXPORTS_BUILDTREE_ONLY GLOBAL PROPERTY LLVM_EXPORTS_BUILDTREE_ONLY); export(TARGETS ${LLVM_EXPORTS_BUILDTREE_ONLY} FILE ${LLVM_BUILDTREEONLY_EXPORTS_FILE}). get_property(LLVM_AVAILABLE_LIBS GLOBAL PROPERTY LLVM_LIBS). foreach(lib ${LLVM_AVAILABLE_LIBS}); get_property(llvm_lib_deps GLOBAL PROPERTY LLVMBUILD_LIB_DEPS_${lib}); set(all_llvm_lib_deps; ""${all_llvm_lib_deps}\nset_property(GLOBAL PROPERTY LLVMBUILD_LIB_DEPS_${lib} ${llvm_lib_deps})""); endforeach(lib). # CMake requires that all targets expressed as dependencies exist, so we can't; # have intrinsics_gen in LLVM_COMMON_DEPENDS when it is written out, otherwise; # projects building out of tree will have CMake errors. This only gets hit when; # LLVM_ENABLE_MODULES=On. Eventually we should come up with a better solution to; # this, but there is no easy solution.; if(intrinsics_gen IN_LIST LLVM_COMMON_DEPENDS); list(REMOVE_ITEM LLVM_COMMON_DEPENDS intrinsics_gen); endif(); if(omp_gen IN_LIST LLVM_COMMON_DEPENDS); list(REMOVE_ITEM LLVM_COMMON_DEPENDS omp_gen); endif(). #; # Generate LLVMConfig.cmake for the build tree.; #. set(LLVM_CONFIG_CODE ""; # LLVM_BUILD_* values available only from LLVM build tree.; set(LLVM_BUILD_BINARY_DIR",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:3476,Deployability,configurat,configuration,3476,"RS). set(LLVM_CONFIG_LIBRARY_DIR ""${LLVM_LIBRARY_DIR}""); set(LLVM_CONFIG_LIBRARY_DIRS; ""${LLVM_CONFIG_LIBRARY_DIR}""; # FIXME: Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""${LLVM_BINARY_DIR}""); set(LLVM_CONFIG_CMAKE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}""); set(LLVM_CONFIG_TOOLS_BINARY_DIR ""${LLVM_TOOLS_BINARY_DIR}""). # Generate a default location for lit; if (LLVM_BUILD_UTILS); if (CMAKE_HOST_WIN32 AND NOT CYGWIN); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit.py""); else(); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit""); endif(); endif(). if (LLVM_LINK_LLVM_DYLIB); set(LLVM_CONFIG_LINK_LLVM_DYLIB; ""set(LLVM_LINK_LLVM_DYLIB ${LLVM_LINK_LLVM_DYLIB})""); endif(). # We need to use the full path to the LLVM Exports file to make sure we get the; # one from the build tree. This is due to our cmake files being split between; # this source dir and the binary dir in the build tree configuration and the; # LLVM_CONFIG_CMAKE_DIR being the source directory. In contrast in the install; # tree, both the generated LLVMExports.cmake file and the rest of the cmake; # source files are put in the same cmake directory.; set(LLVM_CONFIG_EXPORTS ""${LLVM_EXPORTS};${LLVM_EXPORTS_BUILDTREE_ONLY}""); set(LLVM_CONFIG_INCLUDE_EXPORTS ""include(\""${LLVM_EXPORTS_FILE}\"")""); set(llvm_config_include_buildtree_only_exports; ""include(\""${LLVM_BUILDTREEONLY_EXPORTS_FILE}\"")""); configure_file(; LLVMConfig.cmake.in; ${llvm_cmake_builddir}/LLVMConfig.cmake; @ONLY); set(llvm_config_include_buildtree_only_exports). # For compatibility with projects that include(LLVMConfig); # via CMAKE_MODULE_PATH, place API modules next to it.; # Copy without source permissions because the source could be read-only,; # but we need to write into the copied folder.; # This should be removed in the future.; file(COPY .; DESTINATION ${llvm_cmake_builddir}; NO_SOURCE_PERMISSIONS; FILES_MATCHING PAT",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:3570,Deployability,install,install,3570," Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""${LLVM_BINARY_DIR}""); set(LLVM_CONFIG_CMAKE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}""); set(LLVM_CONFIG_TOOLS_BINARY_DIR ""${LLVM_TOOLS_BINARY_DIR}""). # Generate a default location for lit; if (LLVM_BUILD_UTILS); if (CMAKE_HOST_WIN32 AND NOT CYGWIN); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit.py""); else(); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit""); endif(); endif(). if (LLVM_LINK_LLVM_DYLIB); set(LLVM_CONFIG_LINK_LLVM_DYLIB; ""set(LLVM_LINK_LLVM_DYLIB ${LLVM_LINK_LLVM_DYLIB})""); endif(). # We need to use the full path to the LLVM Exports file to make sure we get the; # one from the build tree. This is due to our cmake files being split between; # this source dir and the binary dir in the build tree configuration and the; # LLVM_CONFIG_CMAKE_DIR being the source directory. In contrast in the install; # tree, both the generated LLVMExports.cmake file and the rest of the cmake; # source files are put in the same cmake directory.; set(LLVM_CONFIG_EXPORTS ""${LLVM_EXPORTS};${LLVM_EXPORTS_BUILDTREE_ONLY}""); set(LLVM_CONFIG_INCLUDE_EXPORTS ""include(\""${LLVM_EXPORTS_FILE}\"")""); set(llvm_config_include_buildtree_only_exports; ""include(\""${LLVM_BUILDTREEONLY_EXPORTS_FILE}\"")""); configure_file(; LLVMConfig.cmake.in; ${llvm_cmake_builddir}/LLVMConfig.cmake; @ONLY); set(llvm_config_include_buildtree_only_exports). # For compatibility with projects that include(LLVMConfig); # via CMAKE_MODULE_PATH, place API modules next to it.; # Copy without source permissions because the source could be read-only,; # but we need to write into the copied folder.; # This should be removed in the future.; file(COPY .; DESTINATION ${llvm_cmake_builddir}; NO_SOURCE_PERMISSIONS; FILES_MATCHING PATTERN *.cmake; PATTERN CMakeFiles EXCLUDE; PATTERN llvm-driver-template.cpp.in; ). #; # Generate LLVMConfig.cmake for the inst",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:4580,Deployability,install,install,4580,"# tree, both the generated LLVMExports.cmake file and the rest of the cmake; # source files are put in the same cmake directory.; set(LLVM_CONFIG_EXPORTS ""${LLVM_EXPORTS};${LLVM_EXPORTS_BUILDTREE_ONLY}""); set(LLVM_CONFIG_INCLUDE_EXPORTS ""include(\""${LLVM_EXPORTS_FILE}\"")""); set(llvm_config_include_buildtree_only_exports; ""include(\""${LLVM_BUILDTREEONLY_EXPORTS_FILE}\"")""); configure_file(; LLVMConfig.cmake.in; ${llvm_cmake_builddir}/LLVMConfig.cmake; @ONLY); set(llvm_config_include_buildtree_only_exports). # For compatibility with projects that include(LLVMConfig); # via CMAKE_MODULE_PATH, place API modules next to it.; # Copy without source permissions because the source could be read-only,; # but we need to write into the copied folder.; # This should be removed in the future.; file(COPY .; DESTINATION ${llvm_cmake_builddir}; NO_SOURCE_PERMISSIONS; FILES_MATCHING PATTERN *.cmake; PATTERN CMakeFiles EXCLUDE; PATTERN llvm-driver-template.cpp.in; ). #; # Generate LLVMConfig.cmake for the install tree.; #. find_prefix_from_config(LLVM_CONFIG_CODE LLVM_INSTALL_PREFIX ""${LLVM_INSTALL_PACKAGE_DIR}""). extend_path(LLVM_CONFIG_MAIN_INCLUDE_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${CMAKE_INSTALL_INCLUDEDIR}""); # This is the same as the above because the handwritten and generated headers; # are combined in one directory at install time.; set(LLVM_CONFIG_INCLUDE_DIR ""${LLVM_CONFIG_MAIN_INCLUDE_DIR}""); set(LLVM_CONFIG_INCLUDE_DIRS; ""${LLVM_CONFIG_MAIN_INCLUDE_DIR}""; ""${LLVM_CONFIG_INCLUDE_DIR}""; ); list(REMOVE_DUPLICATES LLVM_CONFIG_INCLUDE_DIRS). extend_path(LLVM_CONFIG_LIBRARY_DIR ""\${LLVM_INSTALL_PREFIX}"" ""lib\${LLVM_LIBDIR_SUFFIX}""); set(LLVM_CONFIG_LIBRARY_DIRS; ""${LLVM_CONFIG_LIBRARY_DIR}""; # FIXME: Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}""); extend_path(LLVM_CONFIG_CMAKE_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_INSTALL_PACKAGE_DIR}""); extend_path(LLVM_CONFIG_TOOLS_BINARY_DIR ""\${LLVM_I",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:4904,Deployability,install,install,4904,"LUDE_EXPORTS ""include(\""${LLVM_EXPORTS_FILE}\"")""); set(llvm_config_include_buildtree_only_exports; ""include(\""${LLVM_BUILDTREEONLY_EXPORTS_FILE}\"")""); configure_file(; LLVMConfig.cmake.in; ${llvm_cmake_builddir}/LLVMConfig.cmake; @ONLY); set(llvm_config_include_buildtree_only_exports). # For compatibility with projects that include(LLVMConfig); # via CMAKE_MODULE_PATH, place API modules next to it.; # Copy without source permissions because the source could be read-only,; # but we need to write into the copied folder.; # This should be removed in the future.; file(COPY .; DESTINATION ${llvm_cmake_builddir}; NO_SOURCE_PERMISSIONS; FILES_MATCHING PATTERN *.cmake; PATTERN CMakeFiles EXCLUDE; PATTERN llvm-driver-template.cpp.in; ). #; # Generate LLVMConfig.cmake for the install tree.; #. find_prefix_from_config(LLVM_CONFIG_CODE LLVM_INSTALL_PREFIX ""${LLVM_INSTALL_PACKAGE_DIR}""). extend_path(LLVM_CONFIG_MAIN_INCLUDE_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${CMAKE_INSTALL_INCLUDEDIR}""); # This is the same as the above because the handwritten and generated headers; # are combined in one directory at install time.; set(LLVM_CONFIG_INCLUDE_DIR ""${LLVM_CONFIG_MAIN_INCLUDE_DIR}""); set(LLVM_CONFIG_INCLUDE_DIRS; ""${LLVM_CONFIG_MAIN_INCLUDE_DIR}""; ""${LLVM_CONFIG_INCLUDE_DIR}""; ); list(REMOVE_DUPLICATES LLVM_CONFIG_INCLUDE_DIRS). extend_path(LLVM_CONFIG_LIBRARY_DIR ""\${LLVM_INSTALL_PREFIX}"" ""lib\${LLVM_LIBDIR_SUFFIX}""); set(LLVM_CONFIG_LIBRARY_DIRS; ""${LLVM_CONFIG_LIBRARY_DIR}""; # FIXME: Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}""); extend_path(LLVM_CONFIG_CMAKE_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_INSTALL_PACKAGE_DIR}""); extend_path(LLVM_CONFIG_TOOLS_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_TOOLS_INSTALL_DIR}""). # Generate a default location for lit; if (LLVM_INSTALL_UTILS AND LLVM_BUILD_UTILS); if (CMAKE_HOST_WIN32 AND NOT CYGWIN); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOO",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:6201,Deployability,install,install,6201,"VM_INSTALL_PREFIX}"" ""lib\${LLVM_LIBDIR_SUFFIX}""); set(LLVM_CONFIG_LIBRARY_DIRS; ""${LLVM_CONFIG_LIBRARY_DIR}""; # FIXME: Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}""); extend_path(LLVM_CONFIG_CMAKE_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_INSTALL_PACKAGE_DIR}""); extend_path(LLVM_CONFIG_TOOLS_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_TOOLS_INSTALL_DIR}""). # Generate a default location for lit; if (LLVM_INSTALL_UTILS AND LLVM_BUILD_UTILS); if (CMAKE_HOST_WIN32 AND NOT CYGWIN); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit.py""); else(); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit""); endif(); endif(). get_config_exports_includes(LLVM LLVM_CONFIG_INCLUDE_EXPORTS); set(LLVM_CONFIG_EXPORTS ""${LLVM_EXPORTS}""); configure_file(; LLVMConfig.cmake.in; ${CMAKE_CURRENT_BINARY_DIR}/CMakeFiles/LLVMConfig.cmake; @ONLY). # Generate LLVMConfigVersion.cmake for build and install tree.; configure_file(; LLVMConfigVersion.cmake.in; ${llvm_cmake_builddir}/LLVMConfigVersion.cmake; @ONLY). if (NOT LLVM_INSTALL_TOOLCHAIN_ONLY); install_distribution_exports(LLVM). install(FILES; ${CMAKE_CURRENT_BINARY_DIR}/CMakeFiles/LLVMConfig.cmake; ${llvm_cmake_builddir}/LLVMConfigVersion.cmake; LLVM-Config.cmake; DESTINATION ${LLVM_INSTALL_PACKAGE_DIR}; COMPONENT cmake-exports). install(DIRECTORY .; DESTINATION ${LLVM_INSTALL_PACKAGE_DIR}; COMPONENT cmake-exports; FILES_MATCHING PATTERN *.cmake; PATTERN LLVMConfig.cmake EXCLUDE; PATTERN LLVMConfigExtensions.cmake EXCLUDE; PATTERN LLVMConfigVersion.cmake EXCLUDE; PATTERN LLVM-Config.cmake EXCLUDE; PATTERN GetHostTriple.cmake EXCLUDE; PATTERN llvm-driver-template.cpp.in). if (NOT LLVM_ENABLE_IDE); # Add a dummy target so this can be used with LLVM_DISTRIBUTION_COMPONENTS; add_custom_target(cmake-exports); add_llvm_install_targets(install-cmake-exports; COMPONENT cmake-exports); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:6391,Deployability,install,install,6391,"VM_INSTALL_PREFIX}"" ""lib\${LLVM_LIBDIR_SUFFIX}""); set(LLVM_CONFIG_LIBRARY_DIRS; ""${LLVM_CONFIG_LIBRARY_DIR}""; # FIXME: Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}""); extend_path(LLVM_CONFIG_CMAKE_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_INSTALL_PACKAGE_DIR}""); extend_path(LLVM_CONFIG_TOOLS_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_TOOLS_INSTALL_DIR}""). # Generate a default location for lit; if (LLVM_INSTALL_UTILS AND LLVM_BUILD_UTILS); if (CMAKE_HOST_WIN32 AND NOT CYGWIN); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit.py""); else(); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit""); endif(); endif(). get_config_exports_includes(LLVM LLVM_CONFIG_INCLUDE_EXPORTS); set(LLVM_CONFIG_EXPORTS ""${LLVM_EXPORTS}""); configure_file(; LLVMConfig.cmake.in; ${CMAKE_CURRENT_BINARY_DIR}/CMakeFiles/LLVMConfig.cmake; @ONLY). # Generate LLVMConfigVersion.cmake for build and install tree.; configure_file(; LLVMConfigVersion.cmake.in; ${llvm_cmake_builddir}/LLVMConfigVersion.cmake; @ONLY). if (NOT LLVM_INSTALL_TOOLCHAIN_ONLY); install_distribution_exports(LLVM). install(FILES; ${CMAKE_CURRENT_BINARY_DIR}/CMakeFiles/LLVMConfig.cmake; ${llvm_cmake_builddir}/LLVMConfigVersion.cmake; LLVM-Config.cmake; DESTINATION ${LLVM_INSTALL_PACKAGE_DIR}; COMPONENT cmake-exports). install(DIRECTORY .; DESTINATION ${LLVM_INSTALL_PACKAGE_DIR}; COMPONENT cmake-exports; FILES_MATCHING PATTERN *.cmake; PATTERN LLVMConfig.cmake EXCLUDE; PATTERN LLVMConfigExtensions.cmake EXCLUDE; PATTERN LLVMConfigVersion.cmake EXCLUDE; PATTERN LLVM-Config.cmake EXCLUDE; PATTERN GetHostTriple.cmake EXCLUDE; PATTERN llvm-driver-template.cpp.in). if (NOT LLVM_ENABLE_IDE); # Add a dummy target so this can be used with LLVM_DISTRIBUTION_COMPONENTS; add_custom_target(cmake-exports); add_llvm_install_targets(install-cmake-exports; COMPONENT cmake-exports); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:6597,Deployability,install,install,6597,"VM_INSTALL_PREFIX}"" ""lib\${LLVM_LIBDIR_SUFFIX}""); set(LLVM_CONFIG_LIBRARY_DIRS; ""${LLVM_CONFIG_LIBRARY_DIR}""; # FIXME: Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}""); extend_path(LLVM_CONFIG_CMAKE_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_INSTALL_PACKAGE_DIR}""); extend_path(LLVM_CONFIG_TOOLS_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_TOOLS_INSTALL_DIR}""). # Generate a default location for lit; if (LLVM_INSTALL_UTILS AND LLVM_BUILD_UTILS); if (CMAKE_HOST_WIN32 AND NOT CYGWIN); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit.py""); else(); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit""); endif(); endif(). get_config_exports_includes(LLVM LLVM_CONFIG_INCLUDE_EXPORTS); set(LLVM_CONFIG_EXPORTS ""${LLVM_EXPORTS}""); configure_file(; LLVMConfig.cmake.in; ${CMAKE_CURRENT_BINARY_DIR}/CMakeFiles/LLVMConfig.cmake; @ONLY). # Generate LLVMConfigVersion.cmake for build and install tree.; configure_file(; LLVMConfigVersion.cmake.in; ${llvm_cmake_builddir}/LLVMConfigVersion.cmake; @ONLY). if (NOT LLVM_INSTALL_TOOLCHAIN_ONLY); install_distribution_exports(LLVM). install(FILES; ${CMAKE_CURRENT_BINARY_DIR}/CMakeFiles/LLVMConfig.cmake; ${llvm_cmake_builddir}/LLVMConfigVersion.cmake; LLVM-Config.cmake; DESTINATION ${LLVM_INSTALL_PACKAGE_DIR}; COMPONENT cmake-exports). install(DIRECTORY .; DESTINATION ${LLVM_INSTALL_PACKAGE_DIR}; COMPONENT cmake-exports; FILES_MATCHING PATTERN *.cmake; PATTERN LLVMConfig.cmake EXCLUDE; PATTERN LLVMConfigExtensions.cmake EXCLUDE; PATTERN LLVMConfigVersion.cmake EXCLUDE; PATTERN LLVM-Config.cmake EXCLUDE; PATTERN GetHostTriple.cmake EXCLUDE; PATTERN llvm-driver-template.cpp.in). if (NOT LLVM_ENABLE_IDE); # Add a dummy target so this can be used with LLVM_DISTRIBUTION_COMPONENTS; add_custom_target(cmake-exports); add_llvm_install_targets(install-cmake-exports; COMPONENT cmake-exports); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:7106,Deployability,install,install-cmake-exports,7106,"VM_INSTALL_PREFIX}"" ""lib\${LLVM_LIBDIR_SUFFIX}""); set(LLVM_CONFIG_LIBRARY_DIRS; ""${LLVM_CONFIG_LIBRARY_DIR}""; # FIXME: Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}""); extend_path(LLVM_CONFIG_CMAKE_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_INSTALL_PACKAGE_DIR}""); extend_path(LLVM_CONFIG_TOOLS_BINARY_DIR ""\${LLVM_INSTALL_PREFIX}"" ""${LLVM_TOOLS_INSTALL_DIR}""). # Generate a default location for lit; if (LLVM_INSTALL_UTILS AND LLVM_BUILD_UTILS); if (CMAKE_HOST_WIN32 AND NOT CYGWIN); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit.py""); else(); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit""); endif(); endif(). get_config_exports_includes(LLVM LLVM_CONFIG_INCLUDE_EXPORTS); set(LLVM_CONFIG_EXPORTS ""${LLVM_EXPORTS}""); configure_file(; LLVMConfig.cmake.in; ${CMAKE_CURRENT_BINARY_DIR}/CMakeFiles/LLVMConfig.cmake; @ONLY). # Generate LLVMConfigVersion.cmake for build and install tree.; configure_file(; LLVMConfigVersion.cmake.in; ${llvm_cmake_builddir}/LLVMConfigVersion.cmake; @ONLY). if (NOT LLVM_INSTALL_TOOLCHAIN_ONLY); install_distribution_exports(LLVM). install(FILES; ${CMAKE_CURRENT_BINARY_DIR}/CMakeFiles/LLVMConfig.cmake; ${llvm_cmake_builddir}/LLVMConfigVersion.cmake; LLVM-Config.cmake; DESTINATION ${LLVM_INSTALL_PACKAGE_DIR}; COMPONENT cmake-exports). install(DIRECTORY .; DESTINATION ${LLVM_INSTALL_PACKAGE_DIR}; COMPONENT cmake-exports; FILES_MATCHING PATTERN *.cmake; PATTERN LLVMConfig.cmake EXCLUDE; PATTERN LLVMConfigExtensions.cmake EXCLUDE; PATTERN LLVMConfigVersion.cmake EXCLUDE; PATTERN LLVM-Config.cmake EXCLUDE; PATTERN GetHostTriple.cmake EXCLUDE; PATTERN llvm-driver-template.cpp.in). if (NOT LLVM_ENABLE_IDE); # Add a dummy target so this can be used with LLVM_DISTRIBUTION_COMPONENTS; add_custom_target(cmake-exports); add_llvm_install_targets(install-cmake-exports; COMPONENT cmake-exports); endif(); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:1313,Integrability,depend,dependencies,1313,"ke file.; set(LLVM_EXPORTS_FILE ${llvm_cmake_builddir}/LLVMExports.cmake); get_property(LLVM_EXPORTS GLOBAL PROPERTY LLVM_EXPORTS); export(TARGETS ${LLVM_EXPORTS} FILE ${LLVM_EXPORTS_FILE}). # Then for users who want to link against the LLVM build tree, provide the; # normal targets and the build tree only targets.; set(LLVM_BUILDTREEONLY_EXPORTS_FILE ${llvm_cmake_builddir}/LLVMBuildTreeOnlyTargets.cmake); get_property(LLVM_EXPORTS_BUILDTREE_ONLY GLOBAL PROPERTY LLVM_EXPORTS_BUILDTREE_ONLY); export(TARGETS ${LLVM_EXPORTS_BUILDTREE_ONLY} FILE ${LLVM_BUILDTREEONLY_EXPORTS_FILE}). get_property(LLVM_AVAILABLE_LIBS GLOBAL PROPERTY LLVM_LIBS). foreach(lib ${LLVM_AVAILABLE_LIBS}); get_property(llvm_lib_deps GLOBAL PROPERTY LLVMBUILD_LIB_DEPS_${lib}); set(all_llvm_lib_deps; ""${all_llvm_lib_deps}\nset_property(GLOBAL PROPERTY LLVMBUILD_LIB_DEPS_${lib} ${llvm_lib_deps})""); endforeach(lib). # CMake requires that all targets expressed as dependencies exist, so we can't; # have intrinsics_gen in LLVM_COMMON_DEPENDS when it is written out, otherwise; # projects building out of tree will have CMake errors. This only gets hit when; # LLVM_ENABLE_MODULES=On. Eventually we should come up with a better solution to; # this, but there is no easy solution.; if(intrinsics_gen IN_LIST LLVM_COMMON_DEPENDS); list(REMOVE_ITEM LLVM_COMMON_DEPENDS intrinsics_gen); endif(); if(omp_gen IN_LIST LLVM_COMMON_DEPENDS); list(REMOVE_ITEM LLVM_COMMON_DEPENDS omp_gen); endif(). #; # Generate LLVMConfig.cmake for the build tree.; #. set(LLVM_CONFIG_CODE ""; # LLVM_BUILD_* values available only from LLVM build tree.; set(LLVM_BUILD_BINARY_DIR \""${LLVM_BINARY_DIR}\""); set(LLVM_BUILD_LIBRARY_DIR \""${LLVM_LIBRARY_DIR}\""); set(LLVM_BUILD_MAIN_INCLUDE_DIR \""${LLVM_MAIN_INCLUDE_DIR}\""); set(LLVM_BUILD_MAIN_SRC_DIR \""${LLVM_MAIN_SRC_DIR}\""); ""). set(LLVM_CONFIG_MAIN_INCLUDE_DIR ""${LLVM_MAIN_INCLUDE_DIR}""); set(LLVM_CONFIG_INCLUDE_DIR ""${LLVM_INCLUDE_DIR}""); set(LLVM_CONFIG_INCLUDE_DIRS; ""${LLVM_CONFIG_MAIN_INCLUDE",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt:3476,Modifiability,config,configuration,3476,"RS). set(LLVM_CONFIG_LIBRARY_DIR ""${LLVM_LIBRARY_DIR}""); set(LLVM_CONFIG_LIBRARY_DIRS; ""${LLVM_CONFIG_LIBRARY_DIR}""; # FIXME: Should there be other entries here?; ); list(REMOVE_DUPLICATES LLVM_CONFIG_LIBRARY_DIRS). set(LLVM_CONFIG_BINARY_DIR ""${LLVM_BINARY_DIR}""); set(LLVM_CONFIG_CMAKE_DIR ""${CMAKE_CURRENT_SOURCE_DIR}""); set(LLVM_CONFIG_TOOLS_BINARY_DIR ""${LLVM_TOOLS_BINARY_DIR}""). # Generate a default location for lit; if (LLVM_BUILD_UTILS); if (CMAKE_HOST_WIN32 AND NOT CYGWIN); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit.py""); else(); set(LLVM_CONFIG_DEFAULT_EXTERNAL_LIT ""${LLVM_CONFIG_TOOLS_BINARY_DIR}/llvm-lit""); endif(); endif(). if (LLVM_LINK_LLVM_DYLIB); set(LLVM_CONFIG_LINK_LLVM_DYLIB; ""set(LLVM_LINK_LLVM_DYLIB ${LLVM_LINK_LLVM_DYLIB})""); endif(). # We need to use the full path to the LLVM Exports file to make sure we get the; # one from the build tree. This is due to our cmake files being split between; # this source dir and the binary dir in the build tree configuration and the; # LLVM_CONFIG_CMAKE_DIR being the source directory. In contrast in the install; # tree, both the generated LLVMExports.cmake file and the rest of the cmake; # source files are put in the same cmake directory.; set(LLVM_CONFIG_EXPORTS ""${LLVM_EXPORTS};${LLVM_EXPORTS_BUILDTREE_ONLY}""); set(LLVM_CONFIG_INCLUDE_EXPORTS ""include(\""${LLVM_EXPORTS_FILE}\"")""); set(llvm_config_include_buildtree_only_exports; ""include(\""${LLVM_BUILDTREEONLY_EXPORTS_FILE}\"")""); configure_file(; LLVMConfig.cmake.in; ${llvm_cmake_builddir}/LLVMConfig.cmake; @ONLY); set(llvm_config_include_buildtree_only_exports). # For compatibility with projects that include(LLVMConfig); # via CMAKE_MODULE_PATH, place API modules next to it.; # Copy without source permissions because the source could be read-only,; # but we need to write into the copied folder.; # This should be removed in the future.; file(COPY .; DESTINATION ${llvm_cmake_builddir}; NO_SOURCE_PERMISSIONS; FILES_MATCHING PAT",MatchSource.DOCS,interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/cmake/modules/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:1983,Energy Efficiency,power,powerful,1983,"sider (an initial list that we should continue; to modify). Note that I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A ""thread-id"" field on every instruction that allows the static; compiler to generate a set of parallel threads, and then have; the runtime compiler and hardware do what they please with it.; This has very powerful uses, but thread-id on every instruction; is expensive in terms of instruction size and code size.; We would need to compactly encode it somehow. Also, this will require some reading on at least two other; projects:; -- Multiscalar architecture from Wisconsin; -- Simultaneous multithreading architecture from Washington. o Or forget all this and stick to a traditional instruction set?. BTW, on an unrelated note, after the meeting yesterday, I did remember; that you had suggested doing instruction scheduling on SSA form instead; of a dependence DAG earlier in the semester. When we talked about; it yesterday, I didn't remember where the idea had come from but I; remembered later. Just giving credit where its due... Perhaps you can save the above as a f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:2206,Energy Efficiency,power,powerful,2206,"I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A ""thread-id"" field on every instruction that allows the static; compiler to generate a set of parallel threads, and then have; the runtime compiler and hardware do what they please with it.; This has very powerful uses, but thread-id on every instruction; is expensive in terms of instruction size and code size.; We would need to compactly encode it somehow. Also, this will require some reading on at least two other; projects:; -- Multiscalar architecture from Wisconsin; -- Simultaneous multithreading architecture from Washington. o Or forget all this and stick to a traditional instruction set?. BTW, on an unrelated note, after the meeting yesterday, I did remember; that you had suggested doing instruction scheduling on SSA form instead; of a dependence DAG earlier in the semester. When we talked about; it yesterday, I didn't remember where the idea had come from but I; remembered later. Just giving credit where its due... Perhaps you can save the above as a file under RCS so you and I can; continue to expand on this. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:2716,Energy Efficiency,schedul,scheduling,2716,"I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A ""thread-id"" field on every instruction that allows the static; compiler to generate a set of parallel threads, and then have; the runtime compiler and hardware do what they please with it.; This has very powerful uses, but thread-id on every instruction; is expensive in terms of instruction size and code size.; We would need to compactly encode it somehow. Also, this will require some reading on at least two other; projects:; -- Multiscalar architecture from Wisconsin; -- Simultaneous multithreading architecture from Washington. o Or forget all this and stick to a traditional instruction set?. BTW, on an unrelated note, after the meeting yesterday, I did remember; that you had suggested doing instruction scheduling on SSA form instead; of a dependence DAG earlier in the semester. When we talked about; it yesterday, I didn't remember where the idea had come from but I; remembered later. Just giving credit where its due... Perhaps you can save the above as a file under RCS so you and I can; continue to expand on this. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:429,Integrability,interface,interface,429,"Date: Sat, 18 Nov 2000 09:19:35 -0600 (CST); From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: a few thoughts. I've been mulling over the virtual machine problem and I had some; thoughts about some things for us to think about discuss:. 1. We need to be clear on our goals for the VM. Do we want to emphasize; portability and safety like the Java VM? Or shall we focus on the; architecture interface first (i.e., consider the code generation and; processor issues), since the architecture interface question is also; important for portable Java-type VMs?. This is important because the audiences for these two goals are very; different. Architects and many compiler people care much more about; the second question. The Java compiler and OS community care much more; about the first one. Also, while the architecture interface question is important for; Java-type VMs, the design constraints are very different. 2. Design issues to consider (an initial list that we should continue; to modify). Note that I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:528,Integrability,interface,interface,528,"Date: Sat, 18 Nov 2000 09:19:35 -0600 (CST); From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: a few thoughts. I've been mulling over the virtual machine problem and I had some; thoughts about some things for us to think about discuss:. 1. We need to be clear on our goals for the VM. Do we want to emphasize; portability and safety like the Java VM? Or shall we focus on the; architecture interface first (i.e., consider the code generation and; processor issues), since the architecture interface question is also; important for portable Java-type VMs?. This is important because the audiences for these two goals are very; different. Architects and many compiler people care much more about; the second question. The Java compiler and OS community care much more; about the first one. Also, while the architecture interface question is important for; Java-type VMs, the design constraints are very different. 2. Design issues to consider (an initial list that we should continue; to modify). Note that I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:856,Integrability,interface,interface,856,"Date: Sat, 18 Nov 2000 09:19:35 -0600 (CST); From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: a few thoughts. I've been mulling over the virtual machine problem and I had some; thoughts about some things for us to think about discuss:. 1. We need to be clear on our goals for the VM. Do we want to emphasize; portability and safety like the Java VM? Or shall we focus on the; architecture interface first (i.e., consider the code generation and; processor issues), since the architecture interface question is also; important for portable Java-type VMs?. This is important because the audiences for these two goals are very; different. Architects and many compiler people care much more about; the second question. The Java compiler and OS community care much more; about the first one. Also, while the architecture interface question is important for; Java-type VMs, the design constraints are very different. 2. Design issues to consider (an initial list that we should continue; to modify). Note that I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:1887,Integrability,depend,depending,1887,"e; about the first one. Also, while the architecture interface question is important for; Java-type VMs, the design constraints are very different. 2. Design issues to consider (an initial list that we should continue; to modify). Note that I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A ""thread-id"" field on every instruction that allows the static; compiler to generate a set of parallel threads, and then have; the runtime compiler and hardware do what they please with it.; This has very powerful uses, but thread-id on every instruction; is expensive in terms of instruction size and code size.; We would need to compactly encode it somehow. Also, this will require some reading on at least two other; projects:; -- Multiscalar architecture from Wisconsin; -- Simultaneous multithreading architecture from Washington. o Or forget all this and stick to a traditional instruction set?. BTW, on an unrelated note, after the meeting yesterday, I did remember; that you had suggested doing instruction scheduling on SSA form instead; of a dependence DAG earlier in the semester. When we ta",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:2753,Integrability,depend,dependence,2753,"I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A ""thread-id"" field on every instruction that allows the static; compiler to generate a set of parallel threads, and then have; the runtime compiler and hardware do what they please with it.; This has very powerful uses, but thread-id on every instruction; is expensive in terms of instruction size and code size.; We would need to compactly encode it somehow. Also, this will require some reading on at least two other; projects:; -- Multiscalar architecture from Wisconsin; -- Simultaneous multithreading architecture from Washington. o Or forget all this and stick to a traditional instruction set?. BTW, on an unrelated note, after the meeting yesterday, I did remember; that you had suggested doing instruction scheduling on SSA form instead; of a dependence DAG earlier in the semester. When we talked about; it yesterday, I didn't remember where the idea had come from but I; remembered later. Just giving credit where its due... Perhaps you can save the above as a file under RCS so you and I can; continue to expand on this. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:349,Modifiability,portab,portability,349,"Date: Sat, 18 Nov 2000 09:19:35 -0600 (CST); From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: a few thoughts. I've been mulling over the virtual machine problem and I had some; thoughts about some things for us to think about discuss:. 1. We need to be clear on our goals for the VM. Do we want to emphasize; portability and safety like the Java VM? Or shall we focus on the; architecture interface first (i.e., consider the code generation and; processor issues), since the architecture interface question is also; important for portable Java-type VMs?. This is important because the audiences for these two goals are very; different. Architects and many compiler people care much more about; the second question. The Java compiler and OS community care much more; about the first one. Also, while the architecture interface question is important for; Java-type VMs, the design constraints are very different. 2. Design issues to consider (an initial list that we should continue; to modify). Note that I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:570,Modifiability,portab,portable,570,"Date: Sat, 18 Nov 2000 09:19:35 -0600 (CST); From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: a few thoughts. I've been mulling over the virtual machine problem and I had some; thoughts about some things for us to think about discuss:. 1. We need to be clear on our goals for the VM. Do we want to emphasize; portability and safety like the Java VM? Or shall we focus on the; architecture interface first (i.e., consider the code generation and; processor issues), since the architecture interface question is also; important for portable Java-type VMs?. This is important because the audiences for these two goals are very; different. Architects and many compiler people care much more about; the second question. The Java compiler and OS community care much more; about the first one. Also, while the architecture interface question is important for; Java-type VMs, the design constraints are very different. 2. Design issues to consider (an initial list that we should continue; to modify). Note that I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:365,Safety,safe,safety,365,"Date: Sat, 18 Nov 2000 09:19:35 -0600 (CST); From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: a few thoughts. I've been mulling over the virtual machine problem and I had some; thoughts about some things for us to think about discuss:. 1. We need to be clear on our goals for the VM. Do we want to emphasize; portability and safety like the Java VM? Or shall we focus on the; architecture interface first (i.e., consider the code generation and; processor issues), since the architecture interface question is also; important for portable Java-type VMs?. This is important because the audiences for these two goals are very; different. Architects and many compiler people care much more about; the second question. The Java compiler and OS community care much more; about the first one. Also, while the architecture interface question is important for; Java-type VMs, the design constraints are very different. 2. Design issues to consider (an initial list that we should continue; to modify). Note that I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt:293,Usability,clear,clear,293,"Date: Sat, 18 Nov 2000 09:19:35 -0600 (CST); From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: a few thoughts. I've been mulling over the virtual machine problem and I had some; thoughts about some things for us to think about discuss:. 1. We need to be clear on our goals for the VM. Do we want to emphasize; portability and safety like the Java VM? Or shall we focus on the; architecture interface first (i.e., consider the code generation and; processor issues), since the architecture interface question is also; important for portable Java-type VMs?. This is important because the audiences for these two goals are very; different. Architects and many compiler people care much more about; the second question. The Java compiler and OS community care much more; about the first one. Also, while the architecture interface question is important for; Java-type VMs, the design constraints are very different. 2. Design issues to consider (an initial list that we should continue; to modify). Note that I'm not trying to suggest actual solutions here,; but just various directions we can pursue:. a. A single-assignment VM, which we've both already been thinking about. b. A strongly-typed VM. One question is do we need the types to be; explicitly declared or should they be inferred by the dynamic compiler?. c. How do we get more high-level information into the VM while keeping; to a low-level VM design?. o Explicit array references as operands? An alternative is; to have just an array type, and let the index computations be; separate 3-operand instructions. o Explicit instructions to handle aliasing, e.g.s:; -- an instruction to say ""I speculate that these two values are not; aliased, but check at runtime"", like speculative execution in; EPIC?; -- or an instruction to check whether two values are aliased and; execute different code depending on the answer, somewhat like; predicated code in EPIC. o (This one is a difficult but powerful idea.); A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeas.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:4107,Deployability,integrat,integration,4107,"ld focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals of the; system somewhat reduce the importance of this point (f.e. portability; reduces performance, but hopefully not much); 2. Portability to different processors. Since we are most familiar with; x86 and solaris, I think that these two are excellent candidates when; we get that far...; 3. Support for all languages & styles of programming (general purpose; VM). This is the point that disallows java style bytecodes, where all; array refs are checked for bounds, etc...; 4. Support linking between different language families. For example, call; C functions directly from Java without using the nasty/slow/gross JNI; layer. This involves several subpoints:; A. Support for languages that require garbage collectors and integration; with languages that don't. As a base point, we could insist on; always using a conservative GC, but implement free as a noop, f.e. > b. A strongly-typed VM. One question is do we need the types to be; > explicitly declared or should they be inferred by the dynamic; > compiler?. B. This is kind of similar to another idea that I have: make OOP; constructs (virtual function tables, class heirarchies, etc) explicit; in the VM representation. I believe that the number of additional; constructs would be fairly low, but would give us lots of important; information... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array typ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1469,Energy Efficiency,schedul,scheduled,1469,"he architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related proj",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:2890,Energy Efficiency,allocate,allocated,2890," generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals of the; system somewhat reduce the importance of this point (f.e. portability; reduces performance, but hopefully not much); 2. Portability to different processors. Since we are most familiar with; x86 and solaris, I think that these two are excellent candidates when; we get that far...; 3. Support for all languages & styles of programming (general purpose; VM). This is the point that disallows java style bytecodes, where all; array refs are checked for bounds, etc...; 4. Support li",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:3409,Energy Efficiency,reduce,reduce,3409,"er; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals of the; system somewhat reduce the importance of this point (f.e. portability; reduces performance, but hopefully not much); 2. Portability to different processors. Since we are most familiar with; x86 and solaris, I think that these two are excellent candidates when; we get that far...; 3. Support for all languages & styles of programming (general purpose; VM). This is the point that disallows java style bytecodes, where all; array refs are checked for bounds, etc...; 4. Support linking between different language families. For example, call; C functions directly from Java without using the nasty/slow/gross JNI; layer. This involves several subpoints:; A. Support for languages that require garbage collectors and integration; with languages that don't. As a base point, we could insist on; always using a conservative GC, but implement free as a noop, f.e. > b. A strongly-typed VM. One question is do we need the types to be; > explicitly declared or should they be inferred by the dynamic; > compiler?. B. T",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:3464,Energy Efficiency,reduce,reduces,3464,"ted projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals of the; system somewhat reduce the importance of this point (f.e. portability; reduces performance, but hopefully not much); 2. Portability to different processors. Since we are most familiar with; x86 and solaris, I think that these two are excellent candidates when; we get that far...; 3. Support for all languages & styles of programming (general purpose; VM). This is the point that disallows java style bytecodes, where all; array refs are checked for bounds, etc...; 4. Support linking between different language families. For example, call; C functions directly from Java without using the nasty/slow/gross JNI; layer. This involves several subpoints:; A. Support for languages that require garbage collectors and integration; with languages that don't. As a base point, we could insist on; always using a conservative GC, but implement free as a noop, f.e. > b. A strongly-typed VM. One question is do we need the types to be; > explicitly declared or should they be inferred by the dynamic; > compiler?. B. This is kind of similar to another idea that I have: make OOP; constructs (virt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:7001,Energy Efficiency,power,powerful,7001,"es, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to check whether two values are aliased and; > execute different code depending on the answer, somewhat like; > predicated code in EPIC. These are also very good points... if this can be determined at compile; time. I think that an epic style of representation (not the instruction; packing, just the information presented) could be a very interesting model; to use... more later... > o (This one is a difficult but powerful idea.); > A ""thread-id"" field on every instruction that allows the static; > compiler to generate a set of parallel threads, and then have; > the runtime compiler and hardware do what they please with it.; > This has very powerful uses, but thread-id on every instruction; > is expensive in terms of instruction size and code size.; > We would need to compactly encode it somehow. Yes yes yes! :) I think it would be *VERY* useful to include this kind; of information (which EPIC architectures *implicitly* encode. The trend; that we are seeing supports this greatly:. 1. Commodity processors are getting massive SIMD support:; * Intel/Amd MMX/MMX2; * AMD's 3Dnow!; * Intel's SSE/SSE2; * Sun's VIS; 2. SMP is becoming much more common, especially in the server space.; 3. Multiple processors on a die are right around the corner. If nothing else, not designing this in would severely limit our future; expansion of the project... > Also, this will require some reading on at least ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:7232,Energy Efficiency,power,powerful,7232,"would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to check whether two values are aliased and; > execute different code depending on the answer, somewhat like; > predicated code in EPIC. These are also very good points... if this can be determined at compile; time. I think that an epic style of representation (not the instruction; packing, just the information presented) could be a very interesting model; to use... more later... > o (This one is a difficult but powerful idea.); > A ""thread-id"" field on every instruction that allows the static; > compiler to generate a set of parallel threads, and then have; > the runtime compiler and hardware do what they please with it.; > This has very powerful uses, but thread-id on every instruction; > is expensive in terms of instruction size and code size.; > We would need to compactly encode it somehow. Yes yes yes! :) I think it would be *VERY* useful to include this kind; of information (which EPIC architectures *implicitly* encode. The trend; that we are seeing supports this greatly:. 1. Commodity processors are getting massive SIMD support:; * Intel/Amd MMX/MMX2; * AMD's 3Dnow!; * Intel's SSE/SSE2; * Sun's VIS; 2. SMP is becoming much more common, especially in the server space.; 3. Multiple processors on a die are right around the corner. If nothing else, not designing this in would severely limit our future; expansion of the project... > Also, this will require some reading on at least two other; > projects:; > -- Multiscalar architecture from Wisconsin; > -- Simultaneous multithreading architecture from Washington; >; > o Or forget all this and stick to a traditional instruction set?. Heh... :) Well, from a pure research point of view, it is almost more; attactive to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:8674,Energy Efficiency,schedul,scheduling,8674,"itly* encode. The trend; that we are seeing supports this greatly:. 1. Commodity processors are getting massive SIMD support:; * Intel/Amd MMX/MMX2; * AMD's 3Dnow!; * Intel's SSE/SSE2; * Sun's VIS; 2. SMP is becoming much more common, especially in the server space.; 3. Multiple processors on a die are right around the corner. If nothing else, not designing this in would severely limit our future; expansion of the project... > Also, this will require some reading on at least two other; > projects:; > -- Multiscalar architecture from Wisconsin; > -- Simultaneous multithreading architecture from Washington; >; > o Or forget all this and stick to a traditional instruction set?. Heh... :) Well, from a pure research point of view, it is almost more; attactive to go with the most extreme/different ISA possible. On one axis; you get safety and conservatism, and on the other you get degree of; influence that the results have. Of course the problem with pure research; is that often times there is no concrete product of the research... :). > BTW, on an unrelated note, after the meeting yesterday, I did remember; > that you had suggested doing instruction scheduling on SSA form instead; > of a dependence DAG earlier in the semester. When we talked about; > it yesterday, I didn't remember where the idea had come from but I; > remembered later. Just giving credit where its due... :) Thanks. . > Perhaps you can save the above as a file under RCS so you and I can; > continue to expand on this. I think it makes sense to do so when we get our ideas more formalized and; bounce it back and forth a couple of times... then I'll do a more formal; writeup of our goals and ideas. Obviously our first implementation will; not want to do all of the stuff that I pointed out above... be we will; want to design the project so that we do not artificially limit ourselves; at sometime in the future... Anyways, let me know what you think about these ideas... and if they sound; reasonable... -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:403,Integrability,interface,interface,403,"Date: Sun, 19 Nov 2000 16:23:57 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram Adve <vadve@cs.uiuc.edu>; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been remo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:504,Integrability,interface,interface,504,"Date: Sun, 19 Nov 2000 16:23:57 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram Adve <vadve@cs.uiuc.edu>; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been remo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:4107,Integrability,integrat,integration,4107,"ld focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals of the; system somewhat reduce the importance of this point (f.e. portability; reduces performance, but hopefully not much); 2. Portability to different processors. Since we are most familiar with; x86 and solaris, I think that these two are excellent candidates when; we get that far...; 3. Support for all languages & styles of programming (general purpose; VM). This is the point that disallows java style bytecodes, where all; array refs are checked for bounds, etc...; 4. Support linking between different language families. For example, call; C functions directly from Java without using the nasty/slow/gross JNI; layer. This involves several subpoints:; A. Support for languages that require garbage collectors and integration; with languages that don't. As a base point, we could insist on; always using a conservative GC, but implement free as a noop, f.e. > b. A strongly-typed VM. One question is do we need the types to be; > explicitly declared or should they be inferred by the dynamic; > compiler?. B. This is kind of similar to another idea that I have: make OOP; constructs (virtual function tables, class heirarchies, etc) explicit; in the VM representation. I believe that the number of additional; constructs would be fairly low, but would give us lots of important; information... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array typ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:6655,Integrability,depend,depending,6655,"upport dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to check whether two values are aliased and; > execute different code depending on the answer, somewhat like; > predicated code in EPIC. These are also very good points... if this can be determined at compile; time. I think that an epic style of representation (not the instruction; packing, just the information presented) could be a very interesting model; to use... more later... > o (This one is a difficult but powerful idea.); > A ""thread-id"" field on every instruction that allows the static; > compiler to generate a set of parallel threads, and then have; > the runtime compiler and hardware do what they please with it.; > This has very powerful uses, but thread-id on every instruction; > is expensive in terms of instruction size and code size.; > We would need to compactly encode it somehow. Yes yes yes! :) I think it would be *VERY* useful to include this kind; of information (which EPIC architectures *implicitly* encode. The trend; that we are seeing supports thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:8713,Integrability,depend,dependence,8713,"itly* encode. The trend; that we are seeing supports this greatly:. 1. Commodity processors are getting massive SIMD support:; * Intel/Amd MMX/MMX2; * AMD's 3Dnow!; * Intel's SSE/SSE2; * Sun's VIS; 2. SMP is becoming much more common, especially in the server space.; 3. Multiple processors on a die are right around the corner. If nothing else, not designing this in would severely limit our future; expansion of the project... > Also, this will require some reading on at least two other; > projects:; > -- Multiscalar architecture from Wisconsin; > -- Simultaneous multithreading architecture from Washington; >; > o Or forget all this and stick to a traditional instruction set?. Heh... :) Well, from a pure research point of view, it is almost more; attactive to go with the most extreme/different ISA possible. On one axis; you get safety and conservatism, and on the other you get degree of; influence that the results have. Of course the problem with pure research; is that often times there is no concrete product of the research... :). > BTW, on an unrelated note, after the meeting yesterday, I did remember; > that you had suggested doing instruction scheduling on SSA form instead; > of a dependence DAG earlier in the semester. When we talked about; > it yesterday, I didn't remember where the idea had come from but I; > remembered later. Just giving credit where its due... :) Thanks. . > Perhaps you can save the above as a file under RCS so you and I can; > continue to expand on this. I think it makes sense to do so when we get our ideas more formalized and; bounce it back and forth a couple of times... then I'll do a more formal; writeup of our goals and ideas. Obviously our first implementation will; not want to do all of the stuff that I pointed out above... be we will; want to design the project so that we do not artificially limit ourselves; at sometime in the future... Anyways, let me know what you think about these ideas... and if they sound; reasonable... -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:321,Modifiability,portab,portability,321,"Date: Sun, 19 Nov 2000 16:23:57 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram Adve <vadve@cs.uiuc.edu>; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been remo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:548,Modifiability,portab,portable,548,"Date: Sun, 19 Nov 2000 16:23:57 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram Adve <vadve@cs.uiuc.edu>; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been remo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1685,Modifiability,sandbox,sandbox,1685,"sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A sing",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:2366,Modifiability,sandbox,sandbox,2366," input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:3451,Modifiability,portab,portability,3451,"ted projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals of the; system somewhat reduce the importance of this point (f.e. portability; reduces performance, but hopefully not much); 2. Portability to different processors. Since we are most familiar with; x86 and solaris, I think that these two are excellent candidates when; we get that far...; 3. Support for all languages & styles of programming (general purpose; VM). This is the point that disallows java style bytecodes, where all; array refs are checked for bounds, etc...; 4. Support linking between different language families. For example, call; C functions directly from Java without using the nasty/slow/gross JNI; layer. This involves several subpoints:; A. Support for languages that require garbage collectors and integration; with languages that don't. As a base point, we could insist on; always using a conservative GC, but implement free as a noop, f.e. > b. A strongly-typed VM. One question is do we need the types to be; > explicitly declared or should they be inferred by the dynamic; > compiler?. B. This is kind of similar to another idea that I have: make OOP; constructs (virt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1820,Performance,load,loaded,1820,"ge as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:3221,Performance,perform,performance,3221,"much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals of the; system somewhat reduce the importance of this point (f.e. portability; reduces performance, but hopefully not much); 2. Portability to different processors. Since we are most familiar with; x86 and solaris, I think that these two are excellent candidates when; we get that far...; 3. Support for all languages & styles of programming (general purpose; VM). This is the point that disallows java style bytecodes, where all; array refs are checked for bounds, etc...; 4. Support linking between different language families. For example, call; C functions directly from Java without using the nasty/slow/gross JNI; layer. This involves several subpoints:; A. Support for languages that require garbage collectors and integration; with languages that don't. As a base point, we could insist on; always using a conservative GC, but implem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:3472,Performance,perform,performance,3472,"ted projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals of the; system somewhat reduce the importance of this point (f.e. portability; reduces performance, but hopefully not much); 2. Portability to different processors. Since we are most familiar with; x86 and solaris, I think that these two are excellent candidates when; we get that far...; 3. Support for all languages & styles of programming (general purpose; VM). This is the point that disallows java style bytecodes, where all; array refs are checked for bounds, etc...; 4. Support linking between different language families. For example, call; C functions directly from Java without using the nasty/slow/gross JNI; layer. This involves several subpoints:; A. Support for languages that require garbage collectors and integration; with languages that don't. As a base point, we could insist on; always using a conservative GC, but implement free as a noop, f.e. > b. A strongly-typed VM. One question is do we need the types to be; > explicitly declared or should they be inferred by the dynamic; > compiler?. B. This is kind of similar to another idea that I have: make OOP; constructs (virt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:5401,Performance,load,load,5401,"y the dynamic; > compiler?. B. This is kind of similar to another idea that I have: make OOP; constructs (virtual function tables, class heirarchies, etc) explicit; in the VM representation. I believe that the number of additional; constructs would be fairly low, but would give us lots of important; information... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array type, and let the index computations be; > separate 3-operand instructions. C. In the model I was thinking of (subject to change of course), we; would just have an array type (distinct from the pointer; types). This would allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Expli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:5419,Performance,load,load,5419,"y the dynamic; > compiler?. B. This is kind of similar to another idea that I have: make OOP; constructs (virtual function tables, class heirarchies, etc) explicit; in the VM representation. I believe that the number of additional; constructs would be fairly low, but would give us lots of important; information... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array type, and let the index computations be; > separate 3-operand instructions. C. In the model I was thinking of (subject to change of course), we; would just have an array type (distinct from the pointer; types). This would allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Expli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:5582,Performance,load,loading,5582,"dditional; constructs would be fairly low, but would give us lots of important; information... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array type, and let the index computations be; > separate 3-operand instructions. C. In the model I was thinking of (subject to change of course), we; would just have an array type (distinct from the pointer; types). This would allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to chec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:5664,Performance,load,loading,5664,"... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array type, and let the index computations be; > separate 3-operand instructions. C. In the model I was thinking of (subject to change of course), we; would just have an array type (distinct from the pointer; types). This would allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to check whether two values are aliased and; > execute different code depending on the answer, som",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:5720,Performance,load,loading,5720,"... something else that would/could be important is to; have exceptions as first class types so that they would be handled in; a uniform way for the entire VM... so that C functions can call Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array type, and let the index computations be; > separate 3-operand instructions. C. In the model I was thinking of (subject to change of course), we; would just have an array type (distinct from the pointer; types). This would allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to check whether two values are aliased and; > execute different code depending on the answer, som",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:5842,Performance,optimiz,optimizations,5842," Java; functions for example... > c. How do we get more high-level information into the VM while keeping; > to a low-level VM design?; > o Explicit array references as operands? An alternative is; > to have just an array type, and let the index computations be; > separate 3-operand instructions. C. In the model I was thinking of (subject to change of course), we; would just have an array type (distinct from the pointer; types). This would allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to check whether two values are aliased and; > execute different code depending on the answer, somewhat like; > predicated code in EPIC. These are also very good points... if this can be determined at compile; time. I think that an epic style of representation (not the instruction; packi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:6312,Performance,optimiz,optimizations,6312," allow us to have arbitrarily complex index; expressions, while still distinguishing ""load"" from ""Array load"",; for example. Perhaps also, switch jump tables would be first class; types as well? This would allow better reasoning about the program. 5. Support dynamic loading of code from various sources. Already; mentioned above was the example of loading java bytecodes, but we want; to support dynamic loading of VM code as well. This makes the job of; the runtime compiler much more interesting: it can do interprocedural; optimizations that the static compiler can't do, because it doesn't; have all of the required information (for example, inlining from; shared libraries, etc...). 6. Define a set of generally useful annotations to add to the VM; representation. For example, a function can be analysed to see if it; has any sideeffects when run... also, the MOD/REF sets could be; calculated, etc... we would have to determine what is reasonable. This; would generally be used to make IP optimizations cheaper for the; runtime compiler... > o Explicit instructions to handle aliasing, e.g.s:; > -- an instruction to say ""I speculate that these two values are not; > aliased, but check at runtime"", like speculative execution in; > EPIC?; > -- or an instruction to check whether two values are aliased and; > execute different code depending on the answer, somewhat like; > predicated code in EPIC. These are also very good points... if this can be determined at compile; time. I think that an epic style of representation (not the instruction; packing, just the information presented) could be a very interesting model; to use... more later... > o (This one is a difficult but powerful idea.); > A ""thread-id"" field on every instruction that allows the static; > compiler to generate a set of parallel threads, and then have; > the runtime compiler and hardware do what they please with it.; > This has very powerful uses, but thread-id on every instruction; > is expensive in terms of instr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:337,Safety,safe,safety,337,"Date: Sun, 19 Nov 2000 16:23:57 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram Adve <vadve@cs.uiuc.edu>; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been remo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:699,Safety,safe,safe,699,"Date: Sun, 19 Nov 2000 16:23:57 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram Adve <vadve@cs.uiuc.edu>; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been remo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:845,Safety,safe,safe,845,"Date: Sun, 19 Nov 2000 16:23:57 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram Adve <vadve@cs.uiuc.edu>; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been remo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1117,Safety,safe,safe,1117,">; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1142,Safety,avoid,avoid,1142,">; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1663,Safety,safe,safe,1663,"sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A sing",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:2360,Safety,safe,safe,2360," input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:8349,Safety,safe,safety,8349,"mehow. Yes yes yes! :) I think it would be *VERY* useful to include this kind; of information (which EPIC architectures *implicitly* encode. The trend; that we are seeing supports this greatly:. 1. Commodity processors are getting massive SIMD support:; * Intel/Amd MMX/MMX2; * AMD's 3Dnow!; * Intel's SSE/SSE2; * Sun's VIS; 2. SMP is becoming much more common, especially in the server space.; 3. Multiple processors on a die are right around the corner. If nothing else, not designing this in would severely limit our future; expansion of the project... > Also, this will require some reading on at least two other; > projects:; > -- Multiscalar architecture from Wisconsin; > -- Simultaneous multithreading architecture from Washington; >; > o Or forget all this and stick to a traditional instruction set?. Heh... :) Well, from a pure research point of view, it is almost more; attactive to go with the most extreme/different ISA possible. On one axis; you get safety and conservatism, and on the other you get degree of; influence that the results have. Of course the problem with pure research; is that often times there is no concrete product of the research... :). > BTW, on an unrelated note, after the meeting yesterday, I did remember; > that you had suggested doing instruction scheduling on SSA form instead; > of a dependence DAG earlier in the semester. When we talked about; > it yesterday, I didn't remember where the idea had come from but I; > remembered later. Just giving credit where its due... :) Thanks. . > Perhaps you can save the above as a file under RCS so you and I can; > continue to expand on this. I think it makes sense to do so when we get our ideas more formalized and; bounce it back and forth a couple of times... then I'll do a more formal; writeup of our goals and ideas. Obviously our first implementation will; not want to do all of the stuff that I pointed out above... be we will; want to design the project so that we do not artificially limit ourselves; a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1936,Security,validat,validate,1936," to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1685,Testability,sandbox,sandbox,1685,"sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A sing",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:2366,Testability,sandbox,sandbox,2366," input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual machine, we have much more room; for value add. The nice safe ""sandbox"" VM can be provided as a layer; on top of it. It also lets us focus on the more interesting compilers; related projects. > 2. Design issues to consider (an initial list that we should continue; > to modify). Note that I'm not trying to suggest actual solutions here,; > but just various directions we can pursue:. Understood. :). > a. A single-assignment VM, which we've both already been thinking; > about. Yup, I think that this makes a lot of sense. I am still intrigued,; however, by the prospect of a minimally allocated VM representation... I; think that it could have definite advantages for certain applications; (think very small machines, like PDAs). I don't, however, think that our; initial implementations should focus on this. :). Here are some other auxiliary goals that I think we should consider:. 1. Primary goal: Support a high performance dynamic compilation; system. This means that we have an ""ideal"" division of labor between; the runtime and static compilers. Of course, the other goals ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:263,Usability,clear,clear,263,"Date: Sun, 19 Nov 2000 16:23:57 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram Adve <vadve@cs.uiuc.edu>; Subject: Re: a few thoughts. Okay... here are a few of my thoughts on this (it's good to know that we; think so alike!):. > 1. We need to be clear on our goals for the VM. Do we want to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been remo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt:1324,Usability,simpl,simple,1324,"t to emphasize; > portability and safety like the Java VM? Or shall we focus on the; > architecture interface first (i.e., consider the code generation and; > processor issues), since the architecture interface question is also; > important for portable Java-type VMs?. I forsee the architecture looking kinda like this: (which is completely; subject to change). 1. The VM code is NOT guaranteed safe in a java sense. Doing so makes it; basically impossible to support C like languages. Besides that,; certifying a register based language as safe at run time would be a; pretty expensive operation to have to do. Additionally, we would like; to be able to statically eliminate many bounds checks in Java; programs... for example. 2. Instead, we can do the following (eventually): ; * Java bytecode is used as our ""safe"" representation (to avoid; reinventing something that we don't add much value to). When the; user chooses to execute Java bytecodes directly (ie, not; precompiled) the runtime compiler can do some very simple; transformations (JIT style) to convert it into valid input for our; VM. Performance is not wonderful, but it works right.; * The file is scheduled to be compiled (rigorously) at a later; time. This could be done by some background process or by a second; processor in the system during idle time or something...; * To keep things ""safe"" ie to enforce a sandbox on Java/foreign code,; we could sign the generated VM code with a host specific private; key. Then before the code is executed/loaded, we can check to see if; the trusted compiler generated the code. This would be much quicker; than having to validate consistency (especially if bounds checks have; been removed, for example). > This is important because the audiences for these two goals are very; > different. Architects and many compiler people care much more about; > the second question. The Java compiler and OS community care much more; > about the first one. 3. By focusing on a more low level virtual m",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-11-18-EarlyDesignIdeasResp.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-EncodingIdea.txt:240,Availability,down,down,240,"From: Chris Lattner [mailto:sabre@nondot.org]; Sent: Wednesday, December 06, 2000 6:41 PM; To: Vikram S. Adve; Subject: Additional idea with respect to encoding. Here's another idea with respect to keeping the common case instruction; size down (less than 32 bits ideally):. Instead of encoding an instruction to operate on two register numbers,; have it operate on two negative offsets based on the current register; number. Therefore, instead of using:. r57 = add r55, r56 (r57 is the implicit dest register, of course). We could use:. r57 = add -2, -1. My guess is that most SSA references are to recent values (especially if; they correspond to expressions like (x+y*z+p*q/ ...), so the negative; numbers would tend to stay small, even at the end of the procedure (where; the implicit register destination number could be quite large). Of course; the negative sign is reduntant, so you would be storing small integers; almost all of the time, and 5-6 bits worth of register number would be; plenty for most cases... What do you think?. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-EncodingIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-EncodingIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:636,Availability,down,down,636,"SUMMARY; -------. We met to discuss the LLVM instruction format and bytecode representation:. ISSUES RESOLVED; ---------------. 1. We decided that we shall use a flat namespace to represent our ; variables in SSA form, as opposed to having a two dimensional namespace; of the original variable and the SSA instance subscript. ARGUMENT AGAINST:; * A two dimensional namespace would be valuable when doing alias ; analysis because the extra information can help limit the scope of; analysis. ARGUMENT FOR:; * Including this information would require that all users of the LLVM; bytecode would have to parse and handle it. This would slow down the; common case and inflate the instruction representation with another; infinite variable space. REASONING:; * It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:196,Modifiability,variab,variables,196,"SUMMARY; -------. We met to discuss the LLVM instruction format and bytecode representation:. ISSUES RESOLVED; ---------------. 1. We decided that we shall use a flat namespace to represent our ; variables in SSA form, as opposed to having a two dimensional namespace; of the original variable and the SSA instance subscript. ARGUMENT AGAINST:; * A two dimensional namespace would be valuable when doing alias ; analysis because the extra information can help limit the scope of; analysis. ARGUMENT FOR:; * Including this information would require that all users of the LLVM; bytecode would have to parse and handle it. This would slow down the; common case and inflate the instruction representation with another; infinite variable space. REASONING:; * It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:285,Modifiability,variab,variable,285,"SUMMARY; -------. We met to discuss the LLVM instruction format and bytecode representation:. ISSUES RESOLVED; ---------------. 1. We decided that we shall use a flat namespace to represent our ; variables in SSA form, as opposed to having a two dimensional namespace; of the original variable and the SSA instance subscript. ARGUMENT AGAINST:; * A two dimensional namespace would be valuable when doing alias ; analysis because the extra information can help limit the scope of; analysis. ARGUMENT FOR:; * Including this information would require that all users of the LLVM; bytecode would have to parse and handle it. This would slow down the; common case and inflate the instruction representation with another; infinite variable space. REASONING:; * It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:724,Modifiability,variab,variable,724,"SUMMARY; -------. We met to discuss the LLVM instruction format and bytecode representation:. ISSUES RESOLVED; ---------------. 1. We decided that we shall use a flat namespace to represent our ; variables in SSA form, as opposed to having a two dimensional namespace; of the original variable and the SSA instance subscript. ARGUMENT AGAINST:; * A two dimensional namespace would be valuable when doing alias ; analysis because the extra information can help limit the scope of; analysis. ARGUMENT FOR:; * Including this information would require that all users of the LLVM; bytecode would have to parse and handle it. This would slow down the; common case and inflate the instruction representation with another; infinite variable space. REASONING:; * It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:791,Modifiability,variab,variable,791,"SUMMARY; -------. We met to discuss the LLVM instruction format and bytecode representation:. ISSUES RESOLVED; ---------------. 1. We decided that we shall use a flat namespace to represent our ; variables in SSA form, as opposed to having a two dimensional namespace; of the original variable and the SSA instance subscript. ARGUMENT AGAINST:; * A two dimensional namespace would be valuable when doing alias ; analysis because the extra information can help limit the scope of; analysis. ARGUMENT FOR:; * Including this information would require that all users of the LLVM; bytecode would have to parse and handle it. This would slow down the; common case and inflate the instruction representation with another; infinite variable space. REASONING:; * It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:2915,Modifiability,variab,variable,2915,"r for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will be of little importance for our; static compiler. This could affect us if we decided to do some IP; research. Also we do not yet understand the level of exception support; currently implemented. 2. Should we consider the requirements of a direct hardware implementation; of the LLVM when we design it? If so, several design issues should; have their priorities shifted. The other option is to focus on a; software layer interpreting the LLVM in all cases. 3. Should we use some form of packetized format to improve forward; compatibility? For example, we could design the system to encode a; packet type and length field before analysis information, to allow a; runtime to skip information that it didn't understand in a bytecode; stream. The obvious benefit would be for compatibility, the drawback; is that it would tend to splinter that 'standard' LLVM definition. 4. Should we use fixed length instructions or variable length; instructions? Fetching variable length instructions is expensive (for; either hardware or software based LLVM runtimes), but we have several; 'infinite' spaces that instructions operate in (SSA register numbers,; type spaces, or packet length [if packets were implemented]). Several; options were mentioned including: ; A. Using 16 or 32 bit numbers, which would be 'big enough'; B. A scheme similar to how UTF-8 works, to encode infinite numbers; while keeping small number small.; C. Use something similar to Huffman encoding, so that the most common; numbers are the smallest. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:2955,Modifiability,variab,variable,2955,"r for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will be of little importance for our; static compiler. This could affect us if we decided to do some IP; research. Also we do not yet understand the level of exception support; currently implemented. 2. Should we consider the requirements of a direct hardware implementation; of the LLVM when we design it? If so, several design issues should; have their priorities shifted. The other option is to focus on a; software layer interpreting the LLVM in all cases. 3. Should we use some form of packetized format to improve forward; compatibility? For example, we could design the system to encode a; packet type and length field before analysis information, to allow a; runtime to skip information that it didn't understand in a bytecode; stream. The obvious benefit would be for compatibility, the drawback; is that it would tend to splinter that 'standard' LLVM definition. 4. Should we use fixed length instructions or variable length; instructions? Fetching variable length instructions is expensive (for; either hardware or software based LLVM runtimes), but we have several; 'infinite' spaces that instructions operate in (SSA register numbers,; type spaces, or packet length [if packets were implemented]). Several; options were mentioned including: ; A. Using 16 or 32 bit numbers, which would be 'big enough'; B. A scheme similar to how UTF-8 works, to encode infinite numbers; while keeping small number small.; C. Use something similar to Huffman encoding, so that the most common; numbers are the smallest. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:965,Performance,optimiz,optimization,965,"SUMMARY; -------. We met to discuss the LLVM instruction format and bytecode representation:. ISSUES RESOLVED; ---------------. 1. We decided that we shall use a flat namespace to represent our ; variables in SSA form, as opposed to having a two dimensional namespace; of the original variable and the SSA instance subscript. ARGUMENT AGAINST:; * A two dimensional namespace would be valuable when doing alias ; analysis because the extra information can help limit the scope of; analysis. ARGUMENT FOR:; * Including this information would require that all users of the LLVM; bytecode would have to parse and handle it. This would slow down the; common case and inflate the instruction representation with another; infinite variable space. REASONING:; * It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:1122,Performance,optimiz,optimization,1122," bytecode representation:. ISSUES RESOLVED; ---------------. 1. We decided that we shall use a flat namespace to represent our ; variables in SSA form, as opposed to having a two dimensional namespace; of the original variable and the SSA instance subscript. ARGUMENT AGAINST:; * A two dimensional namespace would be valuable when doing alias ; analysis because the extra information can help limit the scope of; analysis. ARGUMENT FOR:; * Including this information would require that all users of the LLVM; bytecode would have to parse and handle it. This would slow down the; common case and inflate the instruction representation with another; infinite variable space. REASONING:; * It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will be of little importance for our; static compiler. This could affect",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:1198,Performance,optimiz,optimization,1198," ; variables in SSA form, as opposed to having a two dimensional namespace; of the original variable and the SSA instance subscript. ARGUMENT AGAINST:; * A two dimensional namespace would be valuable when doing alias ; analysis because the extra information can help limit the scope of; analysis. ARGUMENT FOR:; * Including this information would require that all users of the LLVM; bytecode would have to parse and handle it. This would slow down the; common case and inflate the instruction representation with another; infinite variable space. REASONING:; * It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will be of little importance for our; static compiler. This could affect us if we decided to do some IP; research. Also we do not yet understand the level of exception support; currently implemente",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:1964,Performance,perform,performance,1964,"xtra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will be of little importance for our; static compiler. This could affect us if we decided to do some IP; research. Also we do not yet understand the level of exception support; currently implemented. 2. Should we consider the requirements of a direct hardware implementation; of the LLVM when we design it? If so, several design issues should; have their priorities shifted. The other option is to focus on a; software layer interpreting the LLVM in all cases. 3. Should we use some form of packetized format to improve forward; compatibility? For example, we could design the system to encode a; packet type and length field before analysis information, to allow a; runtime to skip information that it didn't understand in a bytecode; stream. The obvious benefit would be for compatibility, the drawback; is that it would tend to splinter that 'standard' LLVM definition. 4. Should we use fixed length instructions or variable length; instruct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt:1745,Safety,detect,detection,1745," It was decided that because original variable sources could be; reconstructed from SSA form in linear time, that it would be an; unjustified expense for the common case to include the extra; information for one optimization. Alias analysis itself is typically; greater than linear in asymptotic complexity, so this extra analaysis; would not affect the runtime of the optimization in a significant; way. Additionally, this would be an unlikely optimization to do at; runtime. IDEAS TO CONSIDER; -----------------. 1. Including dominator information in the LLVM bytecode; representation. This is one example of an analysis result that may be; packaged with the bytecodes themselves. As a conceptual implementation ; idea, we could include an immediate dominator number for each basic block; in the LLVM bytecode program. Basic blocks could be numbered according; to the order of occurrence in the bytecode representation. 2. Including loop header and body information. This would facilitate; detection of intervals and natural loops. UNRESOLVED ISSUES ; ----------------- . 1. Will oSUIF provide enough of an infrastructure to support the research; that we will be doing? We know that it has less than stellar; performance, but hope that this will be of little importance for our; static compiler. This could affect us if we decided to do some IP; research. Also we do not yet understand the level of exception support; currently implemented. 2. Should we consider the requirements of a direct hardware implementation; of the LLVM when we design it? If so, several design issues should; have their priorities shifted. The other option is to focus on a; software layer interpreting the LLVM in all cases. 3. Should we use some form of packetized format to improve forward; compatibility? For example, we could design the system to encode a; packet type and length field before analysis information, to allow a; runtime to skip information that it didn't understand in a bytecode; stream. The obvious be",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2000-12-06-MeetingSummary.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:798,Integrability,depend,dependent,798,"Date: Wed, 31 Jan 2001 12:04:33 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:1045,Integrability,interoperab,interoperability,1045," S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLVM a bit more. That might give us clues on how to; s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:736,Modifiability,extend,extending,736,"Date: Wed, 31 Jan 2001 12:04:33 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:565,Performance,optimiz,optimization,565,"Date: Wed, 31 Jan 2001 12:04:33 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:828,Performance,optimiz,optimization,828,"Date: Wed, 31 Jan 2001 12:04:33 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:892,Performance,optimiz,optimizations,892,"Date: Wed, 31 Jan 2001 12:04:33 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:1328,Performance,optimiz,optimizations,1328,"uc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLVM a bit more. That might give us clues on how to; structure LLVM to support one or more language VMs. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:607,Safety,safe,safety,607,"Date: Wed, 31 Jan 2001 12:04:33 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:1162,Safety,safe,safety,1162,"uc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLVM a bit more. That might give us clues on how to; structure LLVM to support one or more language VMs. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:1180,Safety,safe,safety,1180,"uc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLVM a bit more. That might give us clues on how to; structure LLVM to support one or more language VMs. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:1347,Safety,risk,risk,1347,"uc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLVM a bit more. That might give us clues on how to; structure LLVM to support one or more language VMs. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:1862,Safety,safe,safety,1862,"uc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLVM a bit more. That might give us clues on how to; structure LLVM to support one or more language VMs. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:1173,Security,access,access,1173,"uc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLVM a bit more. That might give us clues on how to; structure LLVM to support one or more language VMs. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt:988,Usability,simpl,simplifying,988,"Date: Wed, 31 Jan 2001 12:04:33 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: another thought. I have a budding idea about making LLVM a little more ambitious: a; customizable runtime system that can be used to implement language-specific; virtual machines for many different languages. E.g., a C vm, a C++ vm, a; Java vm, a Lisp vm, .. The idea would be that LLVM would provide a standard set of runtime features; (some low-level like standard assembly instructions with code generation and; static and runtime optimization; some higher-level like type-safety and; perhaps a garbage collection library). Each language vm would select the; runtime features needed for that language, extending or customizing them as; needed. Most of the machine-dependent code-generation and optimization; features as well as low-level machine-independent optimizations (like PRE); could be provided by LLVM and should be sufficient for any language,; simplifying the language compiler. (This would also help interoperability; between languages.) Also, some or most of the higher-level; machine-independent features like type-safety and access safety should be; reusable by different languages, with minor extensions. The language; compiler could then focus on language-specific analyses and optimizations. The risk is that this sounds like a universal IR -- something that the; compiler community has tried and failed to develop for decades, and is; universally skeptical about. No matter what we say, we won't be able to; convince anyone that we have a universal IR that will work. We need to; think about whether LLVM is different or if has something novel that might; convince people. E.g., the idea of providing a package of separable; features that different languages select from. Also, using SSA with or; without type-safety as the intermediate representation. One interesting starting point would be to discuss how a JVM would be; implemented on top of LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-01-31-UniversalIRIdea.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt:1561,Testability,assert,assert,1561,"Date: Tue, 6 Feb 2001 20:27:37 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Type notation debate... This is the way that I am currently planning on implementing types:. Primitive Types: ; type ::= void|bool|sbyte|ubyte|short|ushort|int|uint|long|ulong. Method:; typelist ::= typelisth | /*empty*/; typelisth ::= type | typelisth ',' type; type ::= type (typelist). Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. Pointer:; type ::= type '*'. Structure:; type ::= '{' typelist '}'. Packed:; type ::= '<' INT ',' type '>'. Simple examples:. [[ %4, int ]] - array of (array of 4 (int)); [ { int, int } ] - Array of structure; [ < %4, int > ] - Array of 128 bit SIMD packets; int (int, [[int, %4]]) - Method taking a 2d array and int, returning int. Okay before you comment, please look at:. http://www.research.att.com/~bs/devXinterview.html. Search for ""In another interview, you defined the C declarator syntax as; an experiment that failed. However, this syntactic construct has been; around for 27 years and perhaps more; why do you consider it problematic; (except for its cumbersome syntax)?"" and read that response for me. :). Now with this syntax, his example would be represented as:. [ %10, bool (int, int) * ] *. vs . bool (*(*)[10])(int, int). in C. Basically, my argument for this type construction system is that it is; VERY simple to use and understand (although it IS different than C, it is; very simple and straightforward, which C is NOT). In fact, I would assert; that most programmers TODAY do not understand pointers to member; functions, and have to look up an example when they have to write them. In my opinion, it is critically important to have clear and concise type; specifications, because types are going to be all over the programs. Let me know your thoughts on this. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt:1424,Usability,simpl,simple,1424,"Date: Tue, 6 Feb 2001 20:27:37 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Type notation debate... This is the way that I am currently planning on implementing types:. Primitive Types: ; type ::= void|bool|sbyte|ubyte|short|ushort|int|uint|long|ulong. Method:; typelist ::= typelisth | /*empty*/; typelisth ::= type | typelisth ',' type; type ::= type (typelist). Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. Pointer:; type ::= type '*'. Structure:; type ::= '{' typelist '}'. Packed:; type ::= '<' INT ',' type '>'. Simple examples:. [[ %4, int ]] - array of (array of 4 (int)); [ { int, int } ] - Array of structure; [ < %4, int > ] - Array of 128 bit SIMD packets; int (int, [[int, %4]]) - Method taking a 2d array and int, returning int. Okay before you comment, please look at:. http://www.research.att.com/~bs/devXinterview.html. Search for ""In another interview, you defined the C declarator syntax as; an experiment that failed. However, this syntactic construct has been; around for 27 years and perhaps more; why do you consider it problematic; (except for its cumbersome syntax)?"" and read that response for me. :). Now with this syntax, his example would be represented as:. [ %10, bool (int, int) * ] *. vs . bool (*(*)[10])(int, int). in C. Basically, my argument for this type construction system is that it is; VERY simple to use and understand (although it IS different than C, it is; very simple and straightforward, which C is NOT). In fact, I would assert; that most programmers TODAY do not understand pointers to member; functions, and have to look up an example when they have to write them. In my opinion, it is critically important to have clear and concise type; specifications, because types are going to be all over the programs. Let me know your thoughts on this. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt:1499,Usability,simpl,simple,1499,"Date: Tue, 6 Feb 2001 20:27:37 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Type notation debate... This is the way that I am currently planning on implementing types:. Primitive Types: ; type ::= void|bool|sbyte|ubyte|short|ushort|int|uint|long|ulong. Method:; typelist ::= typelisth | /*empty*/; typelisth ::= type | typelisth ',' type; type ::= type (typelist). Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. Pointer:; type ::= type '*'. Structure:; type ::= '{' typelist '}'. Packed:; type ::= '<' INT ',' type '>'. Simple examples:. [[ %4, int ]] - array of (array of 4 (int)); [ { int, int } ] - Array of structure; [ < %4, int > ] - Array of 128 bit SIMD packets; int (int, [[int, %4]]) - Method taking a 2d array and int, returning int. Okay before you comment, please look at:. http://www.research.att.com/~bs/devXinterview.html. Search for ""In another interview, you defined the C declarator syntax as; an experiment that failed. However, this syntactic construct has been; around for 27 years and perhaps more; why do you consider it problematic; (except for its cumbersome syntax)?"" and read that response for me. :). Now with this syntax, his example would be represented as:. [ %10, bool (int, int) * ] *. vs . bool (*(*)[10])(int, int). in C. Basically, my argument for this type construction system is that it is; VERY simple to use and understand (although it IS different than C, it is; very simple and straightforward, which C is NOT). In fact, I would assert; that most programmers TODAY do not understand pointers to member; functions, and have to look up an example when they have to write them. In my opinion, it is critically important to have clear and concise type; specifications, because types are going to be all over the programs. Let me know your thoughts on this. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt:1757,Usability,clear,clear,1757,"Date: Tue, 6 Feb 2001 20:27:37 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Type notation debate... This is the way that I am currently planning on implementing types:. Primitive Types: ; type ::= void|bool|sbyte|ubyte|short|ushort|int|uint|long|ulong. Method:; typelist ::= typelisth | /*empty*/; typelisth ::= type | typelisth ',' type; type ::= type (typelist). Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. Pointer:; type ::= type '*'. Structure:; type ::= '{' typelist '}'. Packed:; type ::= '<' INT ',' type '>'. Simple examples:. [[ %4, int ]] - array of (array of 4 (int)); [ { int, int } ] - Array of structure; [ < %4, int > ] - Array of 128 bit SIMD packets; int (int, [[int, %4]]) - Method taking a 2d array and int, returning int. Okay before you comment, please look at:. http://www.research.att.com/~bs/devXinterview.html. Search for ""In another interview, you defined the C declarator syntax as; an experiment that failed. However, this syntactic construct has been; around for 27 years and perhaps more; why do you consider it problematic; (except for its cumbersome syntax)?"" and read that response for me. :). Now with this syntax, his example would be represented as:. [ %10, bool (int, int) * ] *. vs . bool (*(*)[10])(int, int). in C. Basically, my argument for this type construction system is that it is; VERY simple to use and understand (although it IS different than C, it is; very simple and straightforward, which C is NOT). In fact, I would assert; that most programmers TODAY do not understand pointers to member; functions, and have to look up an example when they have to write them. In my opinion, it is critically important to have clear and concise type; specifications, because types are going to be all over the programs. Let me know your thoughts on this. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebate.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:1994,Safety,risk,risky,1994,"tion system is that it is; > VERY simple to use and understand (although it IS different than C, it is; > very simple and straightforward, which C is NOT). In fact, I would assert; > that most programmers TODAY do not understand pointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky to me. > In my opinion, it is critically important to have clear and concise type; > specifications, because types are going to be all over the programs. I absolutely agree. But the question is, what is more clear and concise?; The syntax programmers are used to out of years of experience or a new; syntax that they have never seen that has a more logical structure. I think; the answer is the former. Sometimes, you have to give up a better idea; because you can't overcome sociological barriers to it. Qwerty keyboards; and Windows are two classic examples of bad technology that are difficult to; root out. P.S. Also, while I agree that most your syntax is more logical, there is; one part that isn't:. Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. The arrays with size lists the dimensions and the type in a single list.; That is just too confusing:; [10, 40, int]; This seems to be a 3-D array where the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:544,Testability,log,logical,544,"Date: Thu, 8 Feb 2001 08:42:04 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <sabre@nondot.org>; Subject: RE: Type notation debate... Chris,. > Okay before you comment, please look at:; >; > http://www.research.att.com/~bs/devXinterview.html. I read this argument. Even before that, I was already in agreement with you; and him that the C declarator syntax is difficult and confusing. But in fact, if you read the entire answer carefully, he came to the same; conclusion I do: that you have to go with familiar syntax over logical; syntax because familiarity is such a strong force:. ""However, familiarity is a strong force. To compare, in English, we; live; more or less happily with the absurd rules for ""to be"" (am, are, is, been,; was, were, ...) and all attempts to simplify are treated with contempt or; (preferably) humor. It be a curious world and it always beed."". > Basically, my argument for this type construction system is that it is; > VERY simple to use and understand (although it IS different than C, it is; > very simple and straightforward, which C is NOT). In fact, I would assert; > that most programmers TODAY do not understand pointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:1115,Testability,assert,assert,1115," comment, please look at:; >; > http://www.research.att.com/~bs/devXinterview.html. I read this argument. Even before that, I was already in agreement with you; and him that the C declarator syntax is difficult and confusing. But in fact, if you read the entire answer carefully, he came to the same; conclusion I do: that you have to go with familiar syntax over logical; syntax because familiarity is such a strong force:. ""However, familiarity is a strong force. To compare, in English, we; live; more or less happily with the absurd rules for ""to be"" (am, are, is, been,; was, were, ...) and all attempts to simplify are treated with contempt or; (preferably) humor. It be a curious world and it always beed."". > Basically, my argument for this type construction system is that it is; > VERY simple to use and understand (although it IS different than C, it is; > very simple and straightforward, which C is NOT). In fact, I would assert; > that most programmers TODAY do not understand pointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky to me. > In my opinion, it is critically important to have clear and concise type; > specifications, because types are going to be all over the programs. I absolutely agree. But the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:1865,Testability,log,logical,1865,"mpt or; (preferably) humor. It be a curious world and it always beed."". > Basically, my argument for this type construction system is that it is; > VERY simple to use and understand (although it IS different than C, it is; > very simple and straightforward, which C is NOT). In fact, I would assert; > that most programmers TODAY do not understand pointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky to me. > In my opinion, it is critically important to have clear and concise type; > specifications, because types are going to be all over the programs. I absolutely agree. But the question is, what is more clear and concise?; The syntax programmers are used to out of years of experience or a new; syntax that they have never seen that has a more logical structure. I think; the answer is the former. Sometimes, you have to give up a better idea; because you can't overcome sociological barriers to it. Qwerty keyboards; and Windows are two classic examples of bad technology that are difficult to; root out. P.S. Also, while I agree that most your syntax is more logical, there is; one part that isn't:. Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. The arrays with size lists the dimens",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:2349,Testability,log,logical,2349,"ointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky to me. > In my opinion, it is critically important to have clear and concise type; > specifications, because types are going to be all over the programs. I absolutely agree. But the question is, what is more clear and concise?; The syntax programmers are used to out of years of experience or a new; syntax that they have never seen that has a more logical structure. I think; the answer is the former. Sometimes, you have to give up a better idea; because you can't overcome sociological barriers to it. Qwerty keyboards; and Windows are two classic examples of bad technology that are difficult to; root out. P.S. Also, while I agree that most your syntax is more logical, there is; one part that isn't:. Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. The arrays with size lists the dimensions and the type in a single list.; That is just too confusing:; [10, 40, int]; This seems to be a 3-D array where the third dimension is something strange.; It is too confusing to have a list of 3 things, some of which are dimensions; and one is a type. Either of the following would be better:. array [10, 40] of int; or; int [10, 40]. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:2666,Testability,log,logical,2666,"ointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky to me. > In my opinion, it is critically important to have clear and concise type; > specifications, because types are going to be all over the programs. I absolutely agree. But the question is, what is more clear and concise?; The syntax programmers are used to out of years of experience or a new; syntax that they have never seen that has a more logical structure. I think; the answer is the former. Sometimes, you have to give up a better idea; because you can't overcome sociological barriers to it. Qwerty keyboards; and Windows are two classic examples of bad technology that are difficult to; root out. P.S. Also, while I agree that most your syntax is more logical, there is; one part that isn't:. Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. The arrays with size lists the dimensions and the type in a single list.; That is just too confusing:; [10, 40, int]; This seems to be a 3-D array where the third dimension is something strange.; It is too confusing to have a list of 3 things, some of which are dimensions; and one is a type. Either of the following would be better:. array [10, 40] of int; or; int [10, 40]. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:792,Usability,simpl,simplify,792,"Date: Thu, 8 Feb 2001 08:42:04 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <sabre@nondot.org>; Subject: RE: Type notation debate... Chris,. > Okay before you comment, please look at:; >; > http://www.research.att.com/~bs/devXinterview.html. I read this argument. Even before that, I was already in agreement with you; and him that the C declarator syntax is difficult and confusing. But in fact, if you read the entire answer carefully, he came to the same; conclusion I do: that you have to go with familiar syntax over logical; syntax because familiarity is such a strong force:. ""However, familiarity is a strong force. To compare, in English, we; live; more or less happily with the absurd rules for ""to be"" (am, are, is, been,; was, were, ...) and all attempts to simplify are treated with contempt or; (preferably) humor. It be a curious world and it always beed."". > Basically, my argument for this type construction system is that it is; > VERY simple to use and understand (although it IS different than C, it is; > very simple and straightforward, which C is NOT). In fact, I would assert; > that most programmers TODAY do not understand pointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:976,Usability,simpl,simple,976,"Date: Thu, 8 Feb 2001 08:42:04 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <sabre@nondot.org>; Subject: RE: Type notation debate... Chris,. > Okay before you comment, please look at:; >; > http://www.research.att.com/~bs/devXinterview.html. I read this argument. Even before that, I was already in agreement with you; and him that the C declarator syntax is difficult and confusing. But in fact, if you read the entire answer carefully, he came to the same; conclusion I do: that you have to go with familiar syntax over logical; syntax because familiarity is such a strong force:. ""However, familiarity is a strong force. To compare, in English, we; live; more or less happily with the absurd rules for ""to be"" (am, are, is, been,; was, were, ...) and all attempts to simplify are treated with contempt or; (preferably) humor. It be a curious world and it always beed."". > Basically, my argument for this type construction system is that it is; > VERY simple to use and understand (although it IS different than C, it is; > very simple and straightforward, which C is NOT). In fact, I would assert; > that most programmers TODAY do not understand pointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:1053,Usability,simpl,simple,1053,"Date: Thu, 8 Feb 2001 08:42:04 -0600; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <sabre@nondot.org>; Subject: RE: Type notation debate... Chris,. > Okay before you comment, please look at:; >; > http://www.research.att.com/~bs/devXinterview.html. I read this argument. Even before that, I was already in agreement with you; and him that the C declarator syntax is difficult and confusing. But in fact, if you read the entire answer carefully, he came to the same; conclusion I do: that you have to go with familiar syntax over logical; syntax because familiarity is such a strong force:. ""However, familiarity is a strong force. To compare, in English, we; live; more or less happily with the absurd rules for ""to be"" (am, are, is, been,; was, were, ...) and all attempts to simplify are treated with contempt or; (preferably) humor. It be a curious world and it always beed."". > Basically, my argument for this type construction system is that it is; > VERY simple to use and understand (although it IS different than C, it is; > very simple and straightforward, which C is NOT). In fact, I would assert; > that most programmers TODAY do not understand pointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:1801,Usability,clear,clear,1801,"mpt or; (preferably) humor. It be a curious world and it always beed."". > Basically, my argument for this type construction system is that it is; > VERY simple to use and understand (although it IS different than C, it is; > very simple and straightforward, which C is NOT). In fact, I would assert; > that most programmers TODAY do not understand pointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky to me. > In my opinion, it is critically important to have clear and concise type; > specifications, because types are going to be all over the programs. I absolutely agree. But the question is, what is more clear and concise?; The syntax programmers are used to out of years of experience or a new; syntax that they have never seen that has a more logical structure. I think; the answer is the former. Sometimes, you have to give up a better idea; because you can't overcome sociological barriers to it. Qwerty keyboards; and Windows are two classic examples of bad technology that are difficult to; root out. P.S. Also, while I agree that most your syntax is more logical, there is; one part that isn't:. Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. The arrays with size lists the dimens",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:2059,Usability,clear,clear,2059,", which C is NOT). In fact, I would assert; > that most programmers TODAY do not understand pointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky to me. > In my opinion, it is critically important to have clear and concise type; > specifications, because types are going to be all over the programs. I absolutely agree. But the question is, what is more clear and concise?; The syntax programmers are used to out of years of experience or a new; syntax that they have never seen that has a more logical structure. I think; the answer is the former. Sometimes, you have to give up a better idea; because you can't overcome sociological barriers to it. Qwerty keyboards; and Windows are two classic examples of bad technology that are difficult to; root out. P.S. Also, while I agree that most your syntax is more logical, there is; one part that isn't:. Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. The arrays with size lists the dimensions and the type in a single list.; That is just too confusing:; [10, 40, int]; This seems to be a 3-D array where the third dimension is something strange.; It is too confusing to have a list of 3 things, some of which are dimensions; and one is a type. E",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt:2208,Usability,clear,clear,2208,"ointers to member; > functions, and have to look up an example when they have to write them. Again, I don't disagree with this at all. But to some extent this; particular problem is inherently difficult. Your syntax for the above; example may be easier for you to read because this is the way you have been; thinking about it. Honestly, I don't find it much easier than the C syntax.; In either case, I would have to look up an example to write pointers to; member functions. But pointers to member functions are nowhere near as common as arrays. And; the old array syntax:; type [ int, int, ...]; is just much more familiar and clear to people than anything new you; introduce, no matter how logical it is. Introducing a new syntax that may; make function pointers easier but makes arrays much more difficult seems; very risky to me. > In my opinion, it is critically important to have clear and concise type; > specifications, because types are going to be all over the programs. I absolutely agree. But the question is, what is more clear and concise?; The syntax programmers are used to out of years of experience or a new; syntax that they have never seen that has a more logical structure. I think; the answer is the former. Sometimes, you have to give up a better idea; because you can't overcome sociological barriers to it. Qwerty keyboards; and Windows are two classic examples of bad technology that are difficult to; root out. P.S. Also, while I agree that most your syntax is more logical, there is; one part that isn't:. Arrays (without and with size):; type ::= '[' type ']' | '[' INT ',' type ']'. The arrays with size lists the dimensions and the type in a single list.; That is just too confusing:; [10, 40, int]; This seems to be a 3-D array where the third dimension is something strange.; It is too confusing to have a list of 3 things, some of which are dimensions; and one is a type. Either of the following would be better:. array [10, 40] of int; or; int [10, 40]. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp2.txt:748,Security,expose,expose,748,"Date: Thu, 8 Feb 2001 14:31:05 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Type notation debate... > Arrays (without and with size):; > type ::= '[' type ']' | '[' INT ',' type ']'.; > ; > The arrays with size lists the dimensions and the type in a single list.; > That is just too confusing:. > [10, 40, int]; > This seems to be a 3-D array where the third dimension is something strange.; > It is too confusing to have a list of 3 things, some of which are dimensions; > and one is a type. . The above grammar indicates that there is only one integer parameter, ie; the upper bound. The lower bound is always implied to be zero, for; several reasons:. * As a low level VM, we want to expose addressing computations; explicitly. Since the lower bound must always be known in a high level; language statically, the language front end can do the translation; automatically.; * This fits more closely with what Java needs, ie what we need in the; short term. Java arrays are always zero based. If a two element list is too confusing, I would recommend an alternate; syntax of:. type ::= '[' type ']' | '[' INT 'x' type ']'. For example:; [12 x int]; [12x int]; [ 12 x [ 4x int ]]. Which is syntactically nicer, and more explicit. > Either of the following would be better:; > array [10, 40] of int. I considered this approach for arrays in general (ie array of int/ array; of 12 int), but found that it made declarations WAY too long. Remember; that because of the nature of llvm, you get a lot of types strewn all over; the program, and using the 'typedef' like facility is not a wonderful; option, because then types aren't explicit anymore. I find this email interesting, because you contradict the previous email; you sent, where you recommend that we stick to C syntax.... -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:3188,Safety,risk,risky,3188," But pointers to member functions are nowhere near as common as arrays. Very true. If you're implementing an object oriented language, however,; remember that you have to do all the pointer to member function stuff; yourself.... so every time you invoke a virtual method one is involved; (instead of having C++ hide it for you behind ""syntactic sugar""). > And the old array syntax:; > type [ int, int, ...]; > is just much more familiar and clear to people than anything new you; > introduce, no matter how logical it is. . Erm... excuse me but how is this the ""old array syntax""? If you are; arguing for consistency with C, you should be asking for 'type int []',; which is significantly different than the above (beside the above; introduces a new operator and duplicates information; needlessly). Basically what I am suggesting is exactly the above without; the fluff. So instead of:. type [ int, int, ...]. you use:. type [ int ]. > Introducing a new syntax that may; > make function pointers easier but makes arrays much more difficult seems; > very risky to me. This is not about function pointers. This is about consistency in the; type system, and consistency with the rest of the language. The point; above does not make arrays any more difficult to use, and makes the; structure of types much more obvious than the ""c way"". > > In my opinion, it is critically important to have clear and concise type; > > specifications, because types are going to be all over the programs.; > ; > I absolutely agree. But the question is, what is more clear and concise?; > The syntax programmers are used to out of years of experience or a new; > syntax that they have never seen that has a more logical structure. I think; > the answer is the former. Sometimes, you have to give up a better idea; > because you can't overcome sociological barriers to it. Qwerty keyboards; > and Windows are two classic examples of bad technology that are difficult to; > root out. Very true, but you seem to be advocating",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:142,Testability,log,logical,142,"> But in fact, if you read the entire answer carefully, he came to the same; > conclusion I do: that you have to go with familiar syntax over logical; > syntax because familiarity is such a strong force:; > ""However, familiarity is a strong force. To compare, in English, we; live; > more or less happily with the absurd rules for ""to be"" (am, are, is, been,; > was, were, ...) and all attempts to simplify are treated with contempt or; > (preferably) humor. It be a curious world and it always beed."". Although you have to remember that his situation was considerably; different than ours. He was in a position where he was designing a high; level language that had to be COMPATIBLE with C. Our language is such; that a new person would have to learn the new, different, syntax; anyways. Making them learn about the type system does not seem like much; of a stretch from learning the opcodes and how SSA form works, and how; everything ties together... > > Basically, my argument for this type construction system is that it is; > > VERY simple to use and understand (although it IS different than C, it is; > > very simple and straightforward, which C is NOT). In fact, I would assert; > > that most programmers TODAY do not understand pointers to member; > > functions, and have to look up an example when they have to write them. > Again, I don't disagree with this at all. But to some extent this; > particular problem is inherently difficult. Your syntax for the above; > example may be easier for you to read because this is the way you have been; > thinking about it. Honestly, I don't find it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:1180,Testability,assert,assert,1180," To compare, in English, we; live; > more or less happily with the absurd rules for ""to be"" (am, are, is, been,; > was, were, ...) and all attempts to simplify are treated with contempt or; > (preferably) humor. It be a curious world and it always beed."". Although you have to remember that his situation was considerably; different than ours. He was in a position where he was designing a high; level language that had to be COMPATIBLE with C. Our language is such; that a new person would have to learn the new, different, syntax; anyways. Making them learn about the type system does not seem like much; of a stretch from learning the opcodes and how SSA form works, and how; everything ties together... > > Basically, my argument for this type construction system is that it is; > > VERY simple to use and understand (although it IS different than C, it is; > > very simple and straightforward, which C is NOT). In fact, I would assert; > > that most programmers TODAY do not understand pointers to member; > > functions, and have to look up an example when they have to write them. > Again, I don't disagree with this at all. But to some extent this; > particular problem is inherently difficult. Your syntax for the above; > example may be easier for you to read because this is the way you have been; > thinking about it. Honestly, I don't find it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is because you rarely have to; declare these pointers, and the syntax is inconsistent with the method; declaration and calling syntax. > But pointers to member functions are nowhere near as common as arrays. Very true. If you're implementing an object",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:2640,Testability,log,logical,2640,"nd it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is because you rarely have to; declare these pointers, and the syntax is inconsistent with the method; declaration and calling syntax. > But pointers to member functions are nowhere near as common as arrays. Very true. If you're implementing an object oriented language, however,; remember that you have to do all the pointer to member function stuff; yourself.... so every time you invoke a virtual method one is involved; (instead of having C++ hide it for you behind ""syntactic sugar""). > And the old array syntax:; > type [ int, int, ...]; > is just much more familiar and clear to people than anything new you; > introduce, no matter how logical it is. . Erm... excuse me but how is this the ""old array syntax""? If you are; arguing for consistency with C, you should be asking for 'type int []',; which is significantly different than the above (beside the above; introduces a new operator and duplicates information; needlessly). Basically what I am suggesting is exactly the above without; the fluff. So instead of:. type [ int, int, ...]. you use:. type [ int ]. > Introducing a new syntax that may; > make function pointers easier but makes arrays much more difficult seems; > very risky to me. This is not about function pointers. This is about consistency in the; type system, and consistency with the rest of the language. The point; above does not make arrays any more difficult to use, and makes the; structure of types much more obvious than the ""c way"". > > In my opinion, it is critically important to have clear and concise type; > > specifications, because types are going to be a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:3824,Testability,log,logical,3824,"e a virtual method one is involved; (instead of having C++ hide it for you behind ""syntactic sugar""). > And the old array syntax:; > type [ int, int, ...]; > is just much more familiar and clear to people than anything new you; > introduce, no matter how logical it is. . Erm... excuse me but how is this the ""old array syntax""? If you are; arguing for consistency with C, you should be asking for 'type int []',; which is significantly different than the above (beside the above; introduces a new operator and duplicates information; needlessly). Basically what I am suggesting is exactly the above without; the fluff. So instead of:. type [ int, int, ...]. you use:. type [ int ]. > Introducing a new syntax that may; > make function pointers easier but makes arrays much more difficult seems; > very risky to me. This is not about function pointers. This is about consistency in the; type system, and consistency with the rest of the language. The point; above does not make arrays any more difficult to use, and makes the; structure of types much more obvious than the ""c way"". > > In my opinion, it is critically important to have clear and concise type; > > specifications, because types are going to be all over the programs.; > ; > I absolutely agree. But the question is, what is more clear and concise?; > The syntax programmers are used to out of years of experience or a new; > syntax that they have never seen that has a more logical structure. I think; > the answer is the former. Sometimes, you have to give up a better idea; > because you can't overcome sociological barriers to it. Qwerty keyboards; > and Windows are two classic examples of bad technology that are difficult to; > root out. Very true, but you seem to be advocating a completely different Type; system than C has, in addition to it not offering the advantages of clear; structure that the system I recommended does... so you seem to not have a; problem with changing this, just with what I change it to. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:398,Usability,simpl,simplify,398,"> But in fact, if you read the entire answer carefully, he came to the same; > conclusion I do: that you have to go with familiar syntax over logical; > syntax because familiarity is such a strong force:; > ""However, familiarity is a strong force. To compare, in English, we; live; > more or less happily with the absurd rules for ""to be"" (am, are, is, been,; > was, were, ...) and all attempts to simplify are treated with contempt or; > (preferably) humor. It be a curious world and it always beed."". Although you have to remember that his situation was considerably; different than ours. He was in a position where he was designing a high; level language that had to be COMPATIBLE with C. Our language is such; that a new person would have to learn the new, different, syntax; anyways. Making them learn about the type system does not seem like much; of a stretch from learning the opcodes and how SSA form works, and how; everything ties together... > > Basically, my argument for this type construction system is that it is; > > VERY simple to use and understand (although it IS different than C, it is; > > very simple and straightforward, which C is NOT). In fact, I would assert; > > that most programmers TODAY do not understand pointers to member; > > functions, and have to look up an example when they have to write them. > Again, I don't disagree with this at all. But to some extent this; > particular problem is inherently difficult. Your syntax for the above; > example may be easier for you to read because this is the way you have been; > thinking about it. Honestly, I don't find it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:746,Usability,learn,learn,746,"> But in fact, if you read the entire answer carefully, he came to the same; > conclusion I do: that you have to go with familiar syntax over logical; > syntax because familiarity is such a strong force:; > ""However, familiarity is a strong force. To compare, in English, we; live; > more or less happily with the absurd rules for ""to be"" (am, are, is, been,; > was, were, ...) and all attempts to simplify are treated with contempt or; > (preferably) humor. It be a curious world and it always beed."". Although you have to remember that his situation was considerably; different than ours. He was in a position where he was designing a high; level language that had to be COMPATIBLE with C. Our language is such; that a new person would have to learn the new, different, syntax; anyways. Making them learn about the type system does not seem like much; of a stretch from learning the opcodes and how SSA form works, and how; everything ties together... > > Basically, my argument for this type construction system is that it is; > > VERY simple to use and understand (although it IS different than C, it is; > > very simple and straightforward, which C is NOT). In fact, I would assert; > > that most programmers TODAY do not understand pointers to member; > > functions, and have to look up an example when they have to write them. > Again, I don't disagree with this at all. But to some extent this; > particular problem is inherently difficult. Your syntax for the above; > example may be easier for you to read because this is the way you have been; > thinking about it. Honestly, I don't find it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:801,Usability,learn,learn,801,"> But in fact, if you read the entire answer carefully, he came to the same; > conclusion I do: that you have to go with familiar syntax over logical; > syntax because familiarity is such a strong force:; > ""However, familiarity is a strong force. To compare, in English, we; live; > more or less happily with the absurd rules for ""to be"" (am, are, is, been,; > was, were, ...) and all attempts to simplify are treated with contempt or; > (preferably) humor. It be a curious world and it always beed."". Although you have to remember that his situation was considerably; different than ours. He was in a position where he was designing a high; level language that had to be COMPATIBLE with C. Our language is such; that a new person would have to learn the new, different, syntax; anyways. Making them learn about the type system does not seem like much; of a stretch from learning the opcodes and how SSA form works, and how; everything ties together... > > Basically, my argument for this type construction system is that it is; > > VERY simple to use and understand (although it IS different than C, it is; > > very simple and straightforward, which C is NOT). In fact, I would assert; > > that most programmers TODAY do not understand pointers to member; > > functions, and have to look up an example when they have to write them. > Again, I don't disagree with this at all. But to some extent this; > particular problem is inherently difficult. Your syntax for the above; > example may be easier for you to read because this is the way you have been; > thinking about it. Honestly, I don't find it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:872,Usability,learn,learning,872,"> But in fact, if you read the entire answer carefully, he came to the same; > conclusion I do: that you have to go with familiar syntax over logical; > syntax because familiarity is such a strong force:; > ""However, familiarity is a strong force. To compare, in English, we; live; > more or less happily with the absurd rules for ""to be"" (am, are, is, been,; > was, were, ...) and all attempts to simplify are treated with contempt or; > (preferably) humor. It be a curious world and it always beed."". Although you have to remember that his situation was considerably; different than ours. He was in a position where he was designing a high; level language that had to be COMPATIBLE with C. Our language is such; that a new person would have to learn the new, different, syntax; anyways. Making them learn about the type system does not seem like much; of a stretch from learning the opcodes and how SSA form works, and how; everything ties together... > > Basically, my argument for this type construction system is that it is; > > VERY simple to use and understand (although it IS different than C, it is; > > very simple and straightforward, which C is NOT). In fact, I would assert; > > that most programmers TODAY do not understand pointers to member; > > functions, and have to look up an example when they have to write them. > Again, I don't disagree with this at all. But to some extent this; > particular problem is inherently difficult. Your syntax for the above; > example may be easier for you to read because this is the way you have been; > thinking about it. Honestly, I don't find it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:1039,Usability,simpl,simple,1039,"e came to the same; > conclusion I do: that you have to go with familiar syntax over logical; > syntax because familiarity is such a strong force:; > ""However, familiarity is a strong force. To compare, in English, we; live; > more or less happily with the absurd rules for ""to be"" (am, are, is, been,; > was, were, ...) and all attempts to simplify are treated with contempt or; > (preferably) humor. It be a curious world and it always beed."". Although you have to remember that his situation was considerably; different than ours. He was in a position where he was designing a high; level language that had to be COMPATIBLE with C. Our language is such; that a new person would have to learn the new, different, syntax; anyways. Making them learn about the type system does not seem like much; of a stretch from learning the opcodes and how SSA form works, and how; everything ties together... > > Basically, my argument for this type construction system is that it is; > > VERY simple to use and understand (although it IS different than C, it is; > > very simple and straightforward, which C is NOT). In fact, I would assert; > > that most programmers TODAY do not understand pointers to member; > > functions, and have to look up an example when they have to write them. > Again, I don't disagree with this at all. But to some extent this; > particular problem is inherently difficult. Your syntax for the above; > example may be easier for you to read because this is the way you have been; > thinking about it. Honestly, I don't find it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is because you rarely have to; declare these pointers, and th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:1118,Usability,simpl,simple,1118,"e came to the same; > conclusion I do: that you have to go with familiar syntax over logical; > syntax because familiarity is such a strong force:; > ""However, familiarity is a strong force. To compare, in English, we; live; > more or less happily with the absurd rules for ""to be"" (am, are, is, been,; > was, were, ...) and all attempts to simplify are treated with contempt or; > (preferably) humor. It be a curious world and it always beed."". Although you have to remember that his situation was considerably; different than ours. He was in a position where he was designing a high; level language that had to be COMPATIBLE with C. Our language is such; that a new person would have to learn the new, different, syntax; anyways. Making them learn about the type system does not seem like much; of a stretch from learning the opcodes and how SSA form works, and how; everything ties together... > > Basically, my argument for this type construction system is that it is; > > VERY simple to use and understand (although it IS different than C, it is; > > very simple and straightforward, which C is NOT). In fact, I would assert; > > that most programmers TODAY do not understand pointers to member; > > functions, and have to look up an example when they have to write them. > Again, I don't disagree with this at all. But to some extent this; > particular problem is inherently difficult. Your syntax for the above; > example may be easier for you to read because this is the way you have been; > thinking about it. Honestly, I don't find it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is because you rarely have to; declare these pointers, and th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:2574,Usability,clear,clear,2574,"nd it much easier than the C syntax.; > In either case, I would have to look up an example to write pointers to; > member functions. I would argue that because the lexical structure of the language is self; consistent, any person who spent a significant amount of time programming; in LLVM directly would understand how to do it without looking it up in a; manual. The reason this does not work for C is because you rarely have to; declare these pointers, and the syntax is inconsistent with the method; declaration and calling syntax. > But pointers to member functions are nowhere near as common as arrays. Very true. If you're implementing an object oriented language, however,; remember that you have to do all the pointer to member function stuff; yourself.... so every time you invoke a virtual method one is involved; (instead of having C++ hide it for you behind ""syntactic sugar""). > And the old array syntax:; > type [ int, int, ...]; > is just much more familiar and clear to people than anything new you; > introduce, no matter how logical it is. . Erm... excuse me but how is this the ""old array syntax""? If you are; arguing for consistency with C, you should be asking for 'type int []',; which is significantly different than the above (beside the above; introduces a new operator and duplicates information; needlessly). Basically what I am suggesting is exactly the above without; the fluff. So instead of:. type [ int, int, ...]. you use:. type [ int ]. > Introducing a new syntax that may; > make function pointers easier but makes arrays much more difficult seems; > very risky to me. This is not about function pointers. This is about consistency in the; type system, and consistency with the rest of the language. The point; above does not make arrays any more difficult to use, and makes the; structure of types much more obvious than the ""c way"". > > In my opinion, it is critically important to have clear and concise type; > > specifications, because types are going to be a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:3521,Usability,clear,clear,3521,"e a virtual method one is involved; (instead of having C++ hide it for you behind ""syntactic sugar""). > And the old array syntax:; > type [ int, int, ...]; > is just much more familiar and clear to people than anything new you; > introduce, no matter how logical it is. . Erm... excuse me but how is this the ""old array syntax""? If you are; arguing for consistency with C, you should be asking for 'type int []',; which is significantly different than the above (beside the above; introduces a new operator and duplicates information; needlessly). Basically what I am suggesting is exactly the above without; the fluff. So instead of:. type [ int, int, ...]. you use:. type [ int ]. > Introducing a new syntax that may; > make function pointers easier but makes arrays much more difficult seems; > very risky to me. This is not about function pointers. This is about consistency in the; type system, and consistency with the rest of the language. The point; above does not make arrays any more difficult to use, and makes the; structure of types much more obvious than the ""c way"". > > In my opinion, it is critically important to have clear and concise type; > > specifications, because types are going to be all over the programs.; > ; > I absolutely agree. But the question is, what is more clear and concise?; > The syntax programmers are used to out of years of experience or a new; > syntax that they have never seen that has a more logical structure. I think; > the answer is the former. Sometimes, you have to give up a better idea; > because you can't overcome sociological barriers to it. Qwerty keyboards; > and Windows are two classic examples of bad technology that are difficult to; > root out. Very true, but you seem to be advocating a completely different Type; system than C has, in addition to it not offering the advantages of clear; structure that the system I recommended does... so you seem to not have a; problem with changing this, just with what I change it to. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:3679,Usability,clear,clear,3679,"e a virtual method one is involved; (instead of having C++ hide it for you behind ""syntactic sugar""). > And the old array syntax:; > type [ int, int, ...]; > is just much more familiar and clear to people than anything new you; > introduce, no matter how logical it is. . Erm... excuse me but how is this the ""old array syntax""? If you are; arguing for consistency with C, you should be asking for 'type int []',; which is significantly different than the above (beside the above; introduces a new operator and duplicates information; needlessly). Basically what I am suggesting is exactly the above without; the fluff. So instead of:. type [ int, int, ...]. you use:. type [ int ]. > Introducing a new syntax that may; > make function pointers easier but makes arrays much more difficult seems; > very risky to me. This is not about function pointers. This is about consistency in the; type system, and consistency with the rest of the language. The point; above does not make arrays any more difficult to use, and makes the; structure of types much more obvious than the ""c way"". > > In my opinion, it is critically important to have clear and concise type; > > specifications, because types are going to be all over the programs.; > ; > I absolutely agree. But the question is, what is more clear and concise?; > The syntax programmers are used to out of years of experience or a new; > syntax that they have never seen that has a more logical structure. I think; > the answer is the former. Sometimes, you have to give up a better idea; > because you can't overcome sociological barriers to it. Qwerty keyboards; > and Windows are two classic examples of bad technology that are difficult to; > root out. Very true, but you seem to be advocating a completely different Type; system than C has, in addition to it not offering the advantages of clear; structure that the system I recommended does... so you seem to not have a; problem with changing this, just with what I change it to. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt:4232,Usability,clear,clear,4232,"e a virtual method one is involved; (instead of having C++ hide it for you behind ""syntactic sugar""). > And the old array syntax:; > type [ int, int, ...]; > is just much more familiar and clear to people than anything new you; > introduce, no matter how logical it is. . Erm... excuse me but how is this the ""old array syntax""? If you are; arguing for consistency with C, you should be asking for 'type int []',; which is significantly different than the above (beside the above; introduces a new operator and duplicates information; needlessly). Basically what I am suggesting is exactly the above without; the fluff. So instead of:. type [ int, int, ...]. you use:. type [ int ]. > Introducing a new syntax that may; > make function pointers easier but makes arrays much more difficult seems; > very risky to me. This is not about function pointers. This is about consistency in the; type system, and consistency with the rest of the language. The point; above does not make arrays any more difficult to use, and makes the; structure of types much more obvious than the ""c way"". > > In my opinion, it is critically important to have clear and concise type; > > specifications, because types are going to be all over the programs.; > ; > I absolutely agree. But the question is, what is more clear and concise?; > The syntax programmers are used to out of years of experience or a new; > syntax that they have never seen that has a more logical structure. I think; > the answer is the former. Sometimes, you have to give up a better idea; > because you can't overcome sociological barriers to it. Qwerty keyboards; > and Windows are two classic examples of bad technology that are difficult to; > root out. Very true, but you seem to be advocating a completely different Type; system than C has, in addition to it not offering the advantages of clear; structure that the system I recommended does... so you seem to not have a; problem with changing this, just with what I change it to. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-06-TypeNotationDebateResp4.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:1629,Availability,down,downcasts,1629,"ity. And when the types are obvious from people's experience; (e.g., in the br instruction), it doesn't seem to help as much. o On reflection, I really like your idea of having the two different switch; types (even though they encode implementation techniques rather than; semantics). It should simplify building the CFG and my guess is it could; enable some significant optimizations, though we should think about which. o In the lookup-indirect form of the switch, is there a reason not to make; the val-type uint? Most HLL switch statements (including Java and C++); require that anyway. And it would also make the val-type uniform ; in the two forms of the switch. I did see the switch-on-bool examples and, while cute, we can just use; the branch instructions in that particular case. o I agree with your comment that we don't need 'neg'. o There's a trade-off with the cast instruction:; + it avoids having to define all the upcasts and downcasts that are; valid for the operands of each instruction (you probably have thought; of other benefits also); - it could make the bytecode significantly larger because there could; be a lot of cast operations. o Making the second arg. to 'shl' a ubyte seems good enough to me.; 255 positions seems adequate for several generations of machines; and is more compact than uint. o I still have some major concerns about including malloc and free in the; language (either as builtin functions or instructions). LLVM must be; able to represent code from many different languages. Languages such as; C, C++ Java and Fortran 90 would not be able to use our malloc anyway; because each of them will want to provide a library implementation of it. This gets even worse when code from different languages is linked; into a single executable (which is fairly common in large apps).; Having a single malloc would just not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provid",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:2856,Availability,avail,available,2856,"arger because there could; be a lot of cast operations. o Making the second arg. to 'shl' a ubyte seems good enough to me.; 255 positions seems adequate for several generations of machines; and is more compact than uint. o I still have some major concerns about including malloc and free in the; language (either as builtin functions or instructions). LLVM must be; able to represent code from many different languages. Languages such as; C, C++ Java and Fortran 90 would not be able to use our malloc anyway; because each of them will want to provide a library implementation of it. This gets even worse when code from different languages is linked; into a single executable (which is fairly common in large apps).; Having a single malloc would just not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall instruction?. o I agree that we need a static data space. Otherwise, emulating global; data gets unnecessarily complex. o About explicit parallelism:. We once talked about adding a symbolic threa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:3433,Deployability,pipeline,pipeline,3433," into a single executable (which is fairly common in large apps).; Having a single malloc would just not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall instruction?. o I agree that we need a static data space. Otherwise, emulating global; data gets unnecessarily complex. o About explicit parallelism:. We once talked about adding a symbolic thread-id field to each; instruction. (It could be optional so single-threaded codes are; not penalized.) This could map well to multi-threaded architectures; while providing easy ILP for single-threaded onces. But it is probably; too radical an idea to include in a base version of LLVM. Instead, it; could a great topic for a separate study. What is the semantics of the IA64 stop bit?. o And finally, another thought about the syntax for arrays :-). Although this syntax:; 	 array <dimension-list> of <type>; is verbose, it will be used only in the human-readable assembly code so; size should not matter. I think we should consider it because I find i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:155,Integrability,synchroniz,synchronization,155,"Ok, here are my comments and suggestions about the LLVM instruction set.; We should discuss some now, but can discuss many of them later, when we; revisit synchronization, type inference, and other issues.; (We have discussed some of the comments already.). o We should consider eliminating the type annotation in cases where it is; essentially obvious from the instruction type, e.g., in br, it is obvious; that the first arg. should be a bool and the other args should be labels:. 	br bool <cond>, label <iftrue>, label <iffalse>. I think your point was that making all types explicit improves clarity; and readability. I agree to some extent, but it also comes at the cost; of verbosity. And when the types are obvious from people's experience; (e.g., in the br instruction), it doesn't seem to help as much. o On reflection, I really like your idea of having the two different switch; types (even though they encode implementation techniques rather than; semantics). It should simplify building the CFG and my guess is it could; enable some significant optimizations, though we should think about which. o In the lookup-indirect form of the switch, is there a reason not to make; the val-type uint? Most HLL switch statements (including Java and C++); require that anyway. And it would also make the val-type uniform ; in the two forms of the switch. I did see the switch-on-bool examples and, while cute, we can just use; the branch instructions in that particular case. o I agree with your comment that we don't need 'neg'. o There's a trade-off with the cast instruction:; + it avoids having to define all the upcasts and downcasts that are; valid for the operands of each instruction (you probably have thought; of other benefits also); - it could make the bytecode significantly larger because there could; be a lot of cast operations. o Making the second arg. to 'shl' a ubyte seems good enough to me.; 255 positions seems adequate for several generations of machines; and is more compact th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:3305,Integrability,synchroniz,synchronization,3305,"to provide a library implementation of it. This gets even worse when code from different languages is linked; into a single executable (which is fairly common in large apps).; Having a single malloc would just not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall instruction?. o I agree that we need a static data space. Otherwise, emulating global; data gets unnecessarily complex. o About explicit parallelism:. We once talked about adding a symbolic thread-id field to each; instruction. (It could be optional so single-threaded codes are; not penalized.) This could map well to multi-threaded architectures; while providing easy ILP for single-threaded onces. But it is probably; too radical an idea to include in a base version of LLVM. Instead, it; could a great topic for a separate study. What is the semantics of the IA64 stop bit?. o And finally, another thought about the syntax for arrays :-). Although this syntax:; 	 array <dimension-list> of <type>; is verbose, it will be used only ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:1057,Performance,optimiz,optimizations,1057,"ns about the LLVM instruction set.; We should discuss some now, but can discuss many of them later, when we; revisit synchronization, type inference, and other issues.; (We have discussed some of the comments already.). o We should consider eliminating the type annotation in cases where it is; essentially obvious from the instruction type, e.g., in br, it is obvious; that the first arg. should be a bool and the other args should be labels:. 	br bool <cond>, label <iftrue>, label <iffalse>. I think your point was that making all types explicit improves clarity; and readability. I agree to some extent, but it also comes at the cost; of verbosity. And when the types are obvious from people's experience; (e.g., in the br instruction), it doesn't seem to help as much. o On reflection, I really like your idea of having the two different switch; types (even though they encode implementation techniques rather than; semantics). It should simplify building the CFG and my guess is it could; enable some significant optimizations, though we should think about which. o In the lookup-indirect form of the switch, is there a reason not to make; the val-type uint? Most HLL switch statements (including Java and C++); require that anyway. And it would also make the val-type uniform ; in the two forms of the switch. I did see the switch-on-bool examples and, while cute, we can just use; the branch instructions in that particular case. o I agree with your comment that we don't need 'neg'. o There's a trade-off with the cast instruction:; + it avoids having to define all the upcasts and downcasts that are; valid for the operands of each instruction (you probably have thought; of other benefits also); - it could make the bytecode significantly larger because there could; be a lot of cast operations. o Making the second arg. to 'shl' a ubyte seems good enough to me.; 255 positions seems adequate for several generations of machines; and is more compact than uint. o I still have some major con",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:3347,Performance,load,load-linked,3347,"to provide a library implementation of it. This gets even worse when code from different languages is linked; into a single executable (which is fairly common in large apps).; Having a single malloc would just not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall instruction?. o I agree that we need a static data space. Otherwise, emulating global; data gets unnecessarily complex. o About explicit parallelism:. We once talked about adding a symbolic thread-id field to each; instruction. (It could be optional so single-threaded codes are; not penalized.) This could map well to multi-threaded architectures; while providing easy ILP for single-threaded onces. But it is probably; too radical an idea to include in a base version of LLVM. Instead, it; could a great topic for a separate study. What is the semantics of the IA64 stop bit?. o And finally, another thought about the syntax for arrays :-). Although this syntax:; 	 array <dimension-list> of <type>; is verbose, it will be used only ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:3442,Performance,perform,performance,3442," into a single executable (which is fairly common in large apps).; Having a single malloc would just not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall instruction?. o I agree that we need a static data space. Otherwise, emulating global; data gets unnecessarily complex. o About explicit parallelism:. We once talked about adding a symbolic thread-id field to each; instruction. (It could be optional so single-threaded codes are; not penalized.) This could map well to multi-threaded architectures; while providing easy ILP for single-threaded onces. But it is probably; too radical an idea to include in a base version of LLVM. Instead, it; could a great topic for a separate study. What is the semantics of the IA64 stop bit?. o And finally, another thought about the syntax for arrays :-). Although this syntax:; 	 array <dimension-list> of <type>; is verbose, it will be used only in the human-readable assembly code so; size should not matter. I think we should consider it because I find i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:3914,Performance,multi-thread,multi-threaded,3914,"st not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall instruction?. o I agree that we need a static data space. Otherwise, emulating global; data gets unnecessarily complex. o About explicit parallelism:. We once talked about adding a symbolic thread-id field to each; instruction. (It could be optional so single-threaded codes are; not penalized.) This could map well to multi-threaded architectures; while providing easy ILP for single-threaded onces. But it is probably; too radical an idea to include in a base version of LLVM. Instead, it; could a great topic for a separate study. What is the semantics of the IA64 stop bit?. o And finally, another thought about the syntax for arrays :-). Although this syntax:; 	 array <dimension-list> of <type>; is verbose, it will be used only in the human-readable assembly code so; size should not matter. I think we should consider it because I find it; to be the clearest syntax. It could even make arrays of function; pointers somewhat readable. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:1585,Safety,avoid,avoids,1585,"ity. And when the types are obvious from people's experience; (e.g., in the br instruction), it doesn't seem to help as much. o On reflection, I really like your idea of having the two different switch; types (even though they encode implementation techniques rather than; semantics). It should simplify building the CFG and my guess is it could; enable some significant optimizations, though we should think about which. o In the lookup-indirect form of the switch, is there a reason not to make; the val-type uint? Most HLL switch statements (including Java and C++); require that anyway. And it would also make the val-type uniform ; in the two forms of the switch. I did see the switch-on-bool examples and, while cute, we can just use; the branch instructions in that particular case. o I agree with your comment that we don't need 'neg'. o There's a trade-off with the cast instruction:; + it avoids having to define all the upcasts and downcasts that are; valid for the operands of each instruction (you probably have thought; of other benefits also); - it could make the bytecode significantly larger because there could; be a lot of cast operations. o Making the second arg. to 'shl' a ubyte seems good enough to me.; 255 positions seems adequate for several generations of machines; and is more compact than uint. o I still have some major concerns about including malloc and free in the; language (either as builtin functions or instructions). LLVM must be; able to represent code from many different languages. Languages such as; C, C++ Java and Fortran 90 would not be able to use our malloc anyway; because each of them will want to provide a library implementation of it. This gets even worse when code from different languages is linked; into a single executable (which is fairly common in large apps).; Having a single malloc would just not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provid",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:981,Usability,simpl,simplify,981,"ns about the LLVM instruction set.; We should discuss some now, but can discuss many of them later, when we; revisit synchronization, type inference, and other issues.; (We have discussed some of the comments already.). o We should consider eliminating the type annotation in cases where it is; essentially obvious from the instruction type, e.g., in br, it is obvious; that the first arg. should be a bool and the other args should be labels:. 	br bool <cond>, label <iftrue>, label <iffalse>. I think your point was that making all types explicit improves clarity; and readability. I agree to some extent, but it also comes at the cost; of verbosity. And when the types are obvious from people's experience; (e.g., in the br instruction), it doesn't seem to help as much. o On reflection, I really like your idea of having the two different switch; types (even though they encode implementation techniques rather than; semantics). It should simplify building the CFG and my guess is it could; enable some significant optimizations, though we should think about which. o In the lookup-indirect form of the switch, is there a reason not to make; the val-type uint? Most HLL switch statements (including Java and C++); require that anyway. And it would also make the val-type uniform ; in the two forms of the switch. I did see the switch-on-bool examples and, while cute, we can just use; the branch instructions in that particular case. o I agree with your comment that we don't need 'neg'. o There's a trade-off with the cast instruction:; + it avoids having to define all the upcasts and downcasts that are; valid for the operands of each instruction (you probably have thought; of other benefits also); - it could make the bytecode significantly larger because there could; be a lot of cast operations. o Making the second arg. to 'shl' a ubyte seems good enough to me.; 255 positions seems adequate for several generations of machines; and is more compact than uint. o I still have some major con",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:2571,Usability,simpl,simply,2571,"g to define all the upcasts and downcasts that are; valid for the operands of each instruction (you probably have thought; of other benefits also); - it could make the bytecode significantly larger because there could; be a lot of cast operations. o Making the second arg. to 'shl' a ubyte seems good enough to me.; 255 positions seems adequate for several generations of machines; and is more compact than uint. o I still have some major concerns about including malloc and free in the; language (either as builtin functions or instructions). LLVM must be; able to represent code from many different languages. Languages such as; C, C++ Java and Fortran 90 would not be able to use our malloc anyway; because each of them will want to provide a library implementation of it. This gets even worse when code from different languages is linked; into a single executable (which is fairly common in large apps).; Having a single malloc would just not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt:4453,Usability,clear,clearest,4453,"st not suffice, and instead would simply; complicate the picture further because it adds an extra variant in; addition to the one each language provides. Instead, providing a default library version of malloc and free; (and perhaps a malloc_gc with garbage collection instead of free); would make a good implementation available to anyone who wants it. I don't recall all your arguments in favor so let's discuss this again,; and soon. o 'alloca' on the other hand sounds like a good idea, and the; implementation seems fairly language-independent so it doesn't have the; problems with malloc listed above. o About indirect call:; Your option #2 sounded good to me. I'm not sure I understand your; concern about an explicit 'icall' instruction?. o A pair of important synchronization instr'ns to think about:; load-linked; store-conditional. o Other classes of instructions that are valuable for pipeline performance:; conditional-move		 ; predicated instructions. o I believe tail calls are relatively easy to identify; do you know why; .NET has a tailcall instruction?. o I agree that we need a static data space. Otherwise, emulating global; data gets unnecessarily complex. o About explicit parallelism:. We once talked about adding a symbolic thread-id field to each; instruction. (It could be optional so single-threaded codes are; not penalized.) This could map well to multi-threaded architectures; while providing easy ILP for single-threaded onces. But it is probably; too radical an idea to include in a base version of LLVM. Instead, it; could a great topic for a separate study. What is the semantics of the IA64 stop bit?. o And finally, another thought about the syntax for arrays :-). Although this syntax:; 	 array <dimension-list> of <type>; is verbose, it will be used only in the human-readable assembly code so; size should not matter. I think we should consider it because I find it; to be the clearest syntax. It could even make arrays of function; pointers somewhat readable. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveComments.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:1007,Availability,avail,available,1007,"From: Chris Lattner <sabre@nondot.org>; To: ""Vikram S. Adve"" <vadve@cs.uiuc.edu>; Subject: Re: LLVM Feedback. I've included your feedback in the /home/vadve/lattner/llvm/docs directory; so that it will live in CVS eventually with the rest of LLVM. I've; significantly updated the documentation to reflect the changes you; suggested, as specified below:. > We should consider eliminating the type annotation in cases where it is; > essentially obvious from the instruction type:; > br bool <cond>, label <iftrue>, label <iffalse>; > I think your point was that making all types explicit improves clarity; > and readability. I agree to some extent, but it also comes at the; > cost of verbosity. And when the types are obvious from people's; > experience (e.g., in the br instruction), it doesn't seem to help as; > much. Very true. We should discuss this more, but my reasoning is more of a; consistency argument. There are VERY few instructions that can have all; of the types eliminated, and doing so when available unnecessarily makes; the language more difficult to handle. Especially when you see 'int; %this' and 'bool %that' all over the place, I think it would be; disorienting to see:. br %predicate, %iftrue, %iffalse. for branches. Even just typing that once gives me the creeps. ;) Like I; said, we should probably discuss this further in person... > On reflection, I really like your idea of having the two different; > switch types (even though they encode implementation techniques rather; > than semantics). It should simplify building the CFG and my guess is it; > could enable some significant optimizations, though we should think; > about which. Great. I added a note to the switch section commenting on how the VM; should just use the instruction type as a hint, and that the; implementation may choose altermate representations (such as predicated; branches). > In the lookup-indirect form of the switch, is there a reason not to; > make the val-type uint?. No. This was something",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:2455,Availability,down,downcasts,2455,"ntics). It should simplify building the CFG and my guess is it; > could enable some significant optimizations, though we should think; > about which. Great. I added a note to the switch section commenting on how the VM; should just use the instruction type as a hint, and that the; implementation may choose altermate representations (such as predicated; branches). > In the lookup-indirect form of the switch, is there a reason not to; > make the val-type uint?. No. This was something I was debating for a while, and didn't really feel; strongly about either way. It is common to switch on other types in HLL's; (for example signed int's are particularly common), but in this case, all; that will be added is an additional 'cast' instruction. I removed that; from the spec. > I agree with your comment that we don't need 'neg'. Removed. > There's a trade-off with the cast instruction:; > + it avoids having to define all the upcasts and downcasts that are; > valid for the operands of each instruction (you probably have; > thought of other benefits also); > - it could make the bytecode significantly larger because there could; > be a lot of cast operations. + You NEED casts to represent things like:; void foo(float);; ...; int x;; ...; foo(x);; in a language like C. Even in a Java like language, you need upcasts; and some way to implement dynamic downcasts.; + Not all forms of instructions take every type (for example you can't; shift by a floating point number of bits), thus SOME programs will need; implicit casts. To be efficient and to avoid your '-' point above, we just have to be; careful to specify that the instructions shall operate on all common; types, therefore casting should be relatively uncommon. For example all; of the arithmetic operations work on almost all data types. > Making the second arg. to 'shl' a ubyte seems good enough to me.; > 255 positions seems adequate for several generations of machines. Okay, that comment is removed. > and is more compact than uin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:2872,Availability,down,downcasts,2872,"presentations (such as predicated; branches). > In the lookup-indirect form of the switch, is there a reason not to; > make the val-type uint?. No. This was something I was debating for a while, and didn't really feel; strongly about either way. It is common to switch on other types in HLL's; (for example signed int's are particularly common), but in this case, all; that will be added is an additional 'cast' instruction. I removed that; from the spec. > I agree with your comment that we don't need 'neg'. Removed. > There's a trade-off with the cast instruction:; > + it avoids having to define all the upcasts and downcasts that are; > valid for the operands of each instruction (you probably have; > thought of other benefits also); > - it could make the bytecode significantly larger because there could; > be a lot of cast operations. + You NEED casts to represent things like:; void foo(float);; ...; int x;; ...; foo(x);; in a language like C. Even in a Java like language, you need upcasts; and some way to implement dynamic downcasts.; + Not all forms of instructions take every type (for example you can't; shift by a floating point number of bits), thus SOME programs will need; implicit casts. To be efficient and to avoid your '-' point above, we just have to be; careful to specify that the instructions shall operate on all common; types, therefore casting should be relatively uncommon. For example all; of the arithmetic operations work on almost all data types. > Making the second arg. to 'shl' a ubyte seems good enough to me.; > 255 positions seems adequate for several generations of machines. Okay, that comment is removed. > and is more compact than uint. No, it isn't. Remember that the bytecode encoding saves value slots into; the bytecode instructions themselves, not constant values. This is; another case where we may introduce more cast instructions (but we will; also reduce the number of opcode variants that must be supported by a; virtual machine). Because most ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:4640,Availability,avail,available,4640,"ue slots into; the bytecode instructions themselves, not constant values. This is; another case where we may introduce more cast instructions (but we will; also reduce the number of opcode variants that must be supported by a; virtual machine). Because most shifts are by constant values, I don't; think that we'll have to cast many shifts. :). > I still have some major concerns about including malloc and free in the; > language (either as builtin functions or instructions). Agreed. How about this proposal:. malloc/free are either built in functions or actual opcodes. They provide; all of the type safety that the document would indicate, blah blah; blah. :). Now, because of all of the excellent points that you raised, an; implementation may want to override the default malloc/free behavior of; the program. To do this, they simply implement a ""malloc"" and; ""free"" function. The virtual machine will then be defined to use the user; defined malloc/free function (which return/take void*'s, not type'd; pointers like the builtin function would) if one is available, otherwise; fall back on a system malloc/free. Does this sound like a good compromise? It would give us all of the; typesafety/elegance in the language while still allowing the user to do; all the cool stuff they want to... > 'alloca' on the other hand sounds like a good idea, and the; > implementation seems fairly language-independent so it doesn't have the; > problems with malloc listed above. Okay, once we get the above stuff figured out, I'll put it all in the; spec. > About indirect call:; > Your option #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively easy to identify; do you know why; > .NET has a tailcall instruction?. Although I am just guessing, I believe it probably has to do with the fact; that",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:7476,Availability,avail,available,7476,"ange things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space. Otherwise, emulating global; > data gets unnecessarily complex. Definitely. Also a later item though. :). > We once talked about adding a symbolic thread-id field to each; > ..; > Instead, it could a great topic for a separate study. Agreed. :). > What is the semantics of the IA64 stop bit?. Basically, the IA64 writes instructions like this:; mov ...; add ...; sub ...; op xxx; op xxx; ;;; mov ...; add ...; sub ...; op xxx; op xxx; ;;. Where the ;; delimits a group of instruction with no dependencies between; them, which can all be executed concurrently (to the limits of the; available functional units). The ;; gets translated into a bit set in one; of the opcodes. The advantages of this representation is that you don't have to do some; kind of 'thread id scheduling' pass by having to specify ahead of time how; many threads to use, and the representation doesn't have a per instruction; overhead... > And finally, another thought about the syntax for arrays :-); > Although this syntax:; > array <dimension-list> of <type>; > is verbose, it will be used only in the human-readable assembly code so; > size should not matter. I think we should consider it because I find it; > to be the clearest syntax. It could even make arrays of function; > pointers somewhat readable. My only comment will be to give you an example of why this is a bad; idea. :). Here is an example of using the switch statement (with my recommended; syntax):. switch uint %val, label %otherwise, ; [%3 x {uint, label}] [ { uint %57, label %l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:268,Deployability,update,updated,268,"From: Chris Lattner <sabre@nondot.org>; To: ""Vikram S. Adve"" <vadve@cs.uiuc.edu>; Subject: Re: LLVM Feedback. I've included your feedback in the /home/vadve/lattner/llvm/docs directory; so that it will live in CVS eventually with the rest of LLVM. I've; significantly updated the documentation to reflect the changes you; suggested, as specified below:. > We should consider eliminating the type annotation in cases where it is; > essentially obvious from the instruction type:; > br bool <cond>, label <iftrue>, label <iffalse>; > I think your point was that making all types explicit improves clarity; > and readability. I agree to some extent, but it also comes at the; > cost of verbosity. And when the types are obvious from people's; > experience (e.g., in the br instruction), it doesn't seem to help as; > much. Very true. We should discuss this more, but my reasoning is more of a; consistency argument. There are VERY few instructions that can have all; of the types eliminated, and doing so when available unnecessarily makes; the language more difficult to handle. Especially when you see 'int; %this' and 'bool %that' all over the place, I think it would be; disorienting to see:. br %predicate, %iftrue, %iffalse. for branches. Even just typing that once gives me the creeps. ;) Like I; said, we should probably discuss this further in person... > On reflection, I really like your idea of having the two different; > switch types (even though they encode implementation techniques rather; > than semantics). It should simplify building the CFG and my guess is it; > could enable some significant optimizations, though we should think; > about which. Great. I added a note to the switch section commenting on how the VM; should just use the instruction type as a hint, and that the; implementation may choose altermate representations (such as predicated; branches). > In the lookup-indirect form of the switch, is there a reason not to; > make the val-type uint?. No. This was something",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:6154,Deployability,pipeline,pipeline,6154," #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively easy to identify; do you know why; > .NET has a tailcall instruction?. Although I am just guessing, I believe it probably has to do with the fact; that they want languages like Haskell and lisp to be efficiently runnable; on their VM. Of course this means that the VM MUST implement tail calls; 'correctly', or else life will suck. :) I would put this into a future; feature bin, because it could be pretty handy... > A pair of important synchronization instr'ns to think about:; > load-linked; > store-conditional. What is 'load-linked'? I think that (at least for now) I should add these; to the 'possible extensions' section, because they are not immediately; needed... > Other classes of instructions that are valuable for pipeline; > performance:; > conditional-move ; > predicated instructions. Conditional move is effectly a special case of a predicated; instruction... and I think that all predicated instructions can possibly; be implemented later in LLVM. It would significantly change things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space. Otherwise, emulating global; > data gets unnecessarily complex. Definitely. Also a later item though. :). > We once talked about adding a symbolic thread-id field to each; > ..; > Instead, it could a great topic for a separate study. Agreed. :). > What is the semantics",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:3051,Energy Efficiency,efficient,efficient,3051,"signed int's are particularly common), but in this case, all; that will be added is an additional 'cast' instruction. I removed that; from the spec. > I agree with your comment that we don't need 'neg'. Removed. > There's a trade-off with the cast instruction:; > + it avoids having to define all the upcasts and downcasts that are; > valid for the operands of each instruction (you probably have; > thought of other benefits also); > - it could make the bytecode significantly larger because there could; > be a lot of cast operations. + You NEED casts to represent things like:; void foo(float);; ...; int x;; ...; foo(x);; in a language like C. Even in a Java like language, you need upcasts; and some way to implement dynamic downcasts.; + Not all forms of instructions take every type (for example you can't; shift by a floating point number of bits), thus SOME programs will need; implicit casts. To be efficient and to avoid your '-' point above, we just have to be; careful to specify that the instructions shall operate on all common; types, therefore casting should be relatively uncommon. For example all; of the arithmetic operations work on almost all data types. > Making the second arg. to 'shl' a ubyte seems good enough to me.; > 255 positions seems adequate for several generations of machines. Okay, that comment is removed. > and is more compact than uint. No, it isn't. Remember that the bytecode encoding saves value slots into; the bytecode instructions themselves, not constant values. This is; another case where we may introduce more cast instructions (but we will; also reduce the number of opcode variants that must be supported by a; virtual machine). Because most shifts are by constant values, I don't; think that we'll have to cast many shifts. :). > I still have some major concerns about including malloc and free in the; > language (either as builtin functions or instructions). Agreed. How about this proposal:. malloc/free are either built in functions or actual o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:3739,Energy Efficiency,reduce,reduce,3739,"t);; ...; int x;; ...; foo(x);; in a language like C. Even in a Java like language, you need upcasts; and some way to implement dynamic downcasts.; + Not all forms of instructions take every type (for example you can't; shift by a floating point number of bits), thus SOME programs will need; implicit casts. To be efficient and to avoid your '-' point above, we just have to be; careful to specify that the instructions shall operate on all common; types, therefore casting should be relatively uncommon. For example all; of the arithmetic operations work on almost all data types. > Making the second arg. to 'shl' a ubyte seems good enough to me.; > 255 positions seems adequate for several generations of machines. Okay, that comment is removed. > and is more compact than uint. No, it isn't. Remember that the bytecode encoding saves value slots into; the bytecode instructions themselves, not constant values. This is; another case where we may introduce more cast instructions (but we will; also reduce the number of opcode variants that must be supported by a; virtual machine). Because most shifts are by constant values, I don't; think that we'll have to cast many shifts. :). > I still have some major concerns about including malloc and free in the; > language (either as builtin functions or instructions). Agreed. How about this proposal:. malloc/free are either built in functions or actual opcodes. They provide; all of the type safety that the document would indicate, blah blah; blah. :). Now, because of all of the excellent points that you raised, an; implementation may want to override the default malloc/free behavior of; the program. To do this, they simply implement a ""malloc"" and; ""free"" function. The virtual machine will then be defined to use the user; defined malloc/free function (which return/take void*'s, not type'd; pointers like the builtin function would) if one is available, otherwise; fall back on a system malloc/free. Does this sound like a good compromise? ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:5627,Energy Efficiency,efficient,efficiently,5627," type'd; pointers like the builtin function would) if one is available, otherwise; fall back on a system malloc/free. Does this sound like a good compromise? It would give us all of the; typesafety/elegance in the language while still allowing the user to do; all the cool stuff they want to... > 'alloca' on the other hand sounds like a good idea, and the; > implementation seems fairly language-independent so it doesn't have the; > problems with malloc listed above. Okay, once we get the above stuff figured out, I'll put it all in the; spec. > About indirect call:; > Your option #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively easy to identify; do you know why; > .NET has a tailcall instruction?. Although I am just guessing, I believe it probably has to do with the fact; that they want languages like Haskell and lisp to be efficiently runnable; on their VM. Of course this means that the VM MUST implement tail calls; 'correctly', or else life will suck. :) I would put this into a future; feature bin, because it could be pretty handy... > A pair of important synchronization instr'ns to think about:; > load-linked; > store-conditional. What is 'load-linked'? I think that (at least for now) I should add these; to the 'possible extensions' section, because they are not immediately; needed... > Other classes of instructions that are valuable for pipeline; > performance:; > conditional-move ; > predicated instructions. Conditional move is effectly a special case of a predicated; instruction... and I think that all predicated instructions can possibly; be implemented later in LLVM. It would significantly change things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:7659,Energy Efficiency,schedul,scheduling,7659,"dicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space. Otherwise, emulating global; > data gets unnecessarily complex. Definitely. Also a later item though. :). > We once talked about adding a symbolic thread-id field to each; > ..; > Instead, it could a great topic for a separate study. Agreed. :). > What is the semantics of the IA64 stop bit?. Basically, the IA64 writes instructions like this:; mov ...; add ...; sub ...; op xxx; op xxx; ;;; mov ...; add ...; sub ...; op xxx; op xxx; ;;. Where the ;; delimits a group of instruction with no dependencies between; them, which can all be executed concurrently (to the limits of the; available functional units). The ;; gets translated into a bit set in one; of the opcodes. The advantages of this representation is that you don't have to do some; kind of 'thread id scheduling' pass by having to specify ahead of time how; many threads to use, and the representation doesn't have a per instruction; overhead... > And finally, another thought about the syntax for arrays :-); > Although this syntax:; > array <dimension-list> of <type>; > is verbose, it will be used only in the human-readable assembly code so; > size should not matter. I think we should consider it because I find it; > to be the clearest syntax. It could even make arrays of function; > pointers somewhat readable. My only comment will be to give you an example of why this is a bad; idea. :). Here is an example of using the switch statement (with my recommended; syntax):. switch uint %val, label %otherwise, ; [%3 x {uint, label}] [ { uint %57, label %l1 }, ; { uint %20, label %l2 }, ; { uint %14, label %l3 } ]. Here it is with the syntax you are proposing:. switch uint %val, label %otherwise, ; array %3 of {uint, label} ; array of {uint, label}; { uint %57, label %l1 },; { uint %20, label %l2 },; { uint %14, labe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:5865,Integrability,synchroniz,synchronization,5865,"other hand sounds like a good idea, and the; > implementation seems fairly language-independent so it doesn't have the; > problems with malloc listed above. Okay, once we get the above stuff figured out, I'll put it all in the; spec. > About indirect call:; > Your option #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively easy to identify; do you know why; > .NET has a tailcall instruction?. Although I am just guessing, I believe it probably has to do with the fact; that they want languages like Haskell and lisp to be efficiently runnable; on their VM. Of course this means that the VM MUST implement tail calls; 'correctly', or else life will suck. :) I would put this into a future; feature bin, because it could be pretty handy... > A pair of important synchronization instr'ns to think about:; > load-linked; > store-conditional. What is 'load-linked'? I think that (at least for now) I should add these; to the 'possible extensions' section, because they are not immediately; needed... > Other classes of instructions that are valuable for pipeline; > performance:; > conditional-move ; > predicated instructions. Conditional move is effectly a special case of a predicated; instruction... and I think that all predicated instructions can possibly; be implemented later in LLVM. It would significantly change things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:7386,Integrability,depend,dependencies,7386,"ange things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space. Otherwise, emulating global; > data gets unnecessarily complex. Definitely. Also a later item though. :). > We once talked about adding a symbolic thread-id field to each; > ..; > Instead, it could a great topic for a separate study. Agreed. :). > What is the semantics of the IA64 stop bit?. Basically, the IA64 writes instructions like this:; mov ...; add ...; sub ...; op xxx; op xxx; ;;; mov ...; add ...; sub ...; op xxx; op xxx; ;;. Where the ;; delimits a group of instruction with no dependencies between; them, which can all be executed concurrently (to the limits of the; available functional units). The ;; gets translated into a bit set in one; of the opcodes. The advantages of this representation is that you don't have to do some; kind of 'thread id scheduling' pass by having to specify ahead of time how; many threads to use, and the representation doesn't have a per instruction; overhead... > And finally, another thought about the syntax for arrays :-); > Although this syntax:; > array <dimension-list> of <type>; > is verbose, it will be used only in the human-readable assembly code so; > size should not matter. I think we should consider it because I find it; > to be the clearest syntax. It could even make arrays of function; > pointers somewhat readable. My only comment will be to give you an example of why this is a bad; idea. :). Here is an example of using the switch statement (with my recommended; syntax):. switch uint %val, label %otherwise, ; [%3 x {uint, label}] [ { uint %57, label %l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:1611,Performance,optimiz,optimizations,1611,"s clarity; > and readability. I agree to some extent, but it also comes at the; > cost of verbosity. And when the types are obvious from people's; > experience (e.g., in the br instruction), it doesn't seem to help as; > much. Very true. We should discuss this more, but my reasoning is more of a; consistency argument. There are VERY few instructions that can have all; of the types eliminated, and doing so when available unnecessarily makes; the language more difficult to handle. Especially when you see 'int; %this' and 'bool %that' all over the place, I think it would be; disorienting to see:. br %predicate, %iftrue, %iffalse. for branches. Even just typing that once gives me the creeps. ;) Like I; said, we should probably discuss this further in person... > On reflection, I really like your idea of having the two different; > switch types (even though they encode implementation techniques rather; > than semantics). It should simplify building the CFG and my guess is it; > could enable some significant optimizations, though we should think; > about which. Great. I added a note to the switch section commenting on how the VM; should just use the instruction type as a hint, and that the; implementation may choose altermate representations (such as predicated; branches). > In the lookup-indirect form of the switch, is there a reason not to; > make the val-type uint?. No. This was something I was debating for a while, and didn't really feel; strongly about either way. It is common to switch on other types in HLL's; (for example signed int's are particularly common), but in this case, all; that will be added is an additional 'cast' instruction. I removed that; from the spec. > I agree with your comment that we don't need 'neg'. Removed. > There's a trade-off with the cast instruction:; > + it avoids having to define all the upcasts and downcasts that are; > valid for the operands of each instruction (you probably have; > thought of other benefits also); > - it could make ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:5909,Performance,load,load-linked,5909,"other hand sounds like a good idea, and the; > implementation seems fairly language-independent so it doesn't have the; > problems with malloc listed above. Okay, once we get the above stuff figured out, I'll put it all in the; spec. > About indirect call:; > Your option #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively easy to identify; do you know why; > .NET has a tailcall instruction?. Although I am just guessing, I believe it probably has to do with the fact; that they want languages like Haskell and lisp to be efficiently runnable; on their VM. Of course this means that the VM MUST implement tail calls; 'correctly', or else life will suck. :) I would put this into a future; feature bin, because it could be pretty handy... > A pair of important synchronization instr'ns to think about:; > load-linked; > store-conditional. What is 'load-linked'? I think that (at least for now) I should add these; to the 'possible extensions' section, because they are not immediately; needed... > Other classes of instructions that are valuable for pipeline; > performance:; > conditional-move ; > predicated instructions. Conditional move is effectly a special case of a predicated; instruction... and I think that all predicated instructions can possibly; be implemented later in LLVM. It would significantly change things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:5952,Performance,load,load-linked,5952,"ems with malloc listed above. Okay, once we get the above stuff figured out, I'll put it all in the; spec. > About indirect call:; > Your option #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively easy to identify; do you know why; > .NET has a tailcall instruction?. Although I am just guessing, I believe it probably has to do with the fact; that they want languages like Haskell and lisp to be efficiently runnable; on their VM. Of course this means that the VM MUST implement tail calls; 'correctly', or else life will suck. :) I would put this into a future; feature bin, because it could be pretty handy... > A pair of important synchronization instr'ns to think about:; > load-linked; > store-conditional. What is 'load-linked'? I think that (at least for now) I should add these; to the 'possible extensions' section, because they are not immediately; needed... > Other classes of instructions that are valuable for pipeline; > performance:; > conditional-move ; > predicated instructions. Conditional move is effectly a special case of a predicated; instruction... and I think that all predicated instructions can possibly; be implemented later in LLVM. It would significantly change things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space. Otherwise, emulating global; > data gets unnecessarily complex. Definitely. Also a later item though. :). > We once talked abo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:6166,Performance,perform,performance,6166," #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively easy to identify; do you know why; > .NET has a tailcall instruction?. Although I am just guessing, I believe it probably has to do with the fact; that they want languages like Haskell and lisp to be efficiently runnable; on their VM. Of course this means that the VM MUST implement tail calls; 'correctly', or else life will suck. :) I would put this into a future; feature bin, because it could be pretty handy... > A pair of important synchronization instr'ns to think about:; > load-linked; > store-conditional. What is 'load-linked'? I think that (at least for now) I should add these; to the 'possible extensions' section, because they are not immediately; needed... > Other classes of instructions that are valuable for pipeline; > performance:; > conditional-move ; > predicated instructions. Conditional move is effectly a special case of a predicated; instruction... and I think that all predicated instructions can possibly; be implemented later in LLVM. It would significantly change things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space. Otherwise, emulating global; > data gets unnecessarily complex. Definitely. Also a later item though. :). > We once talked about adding a symbolic thread-id field to each; > ..; > Instead, it could a great topic for a separate study. Agreed. :). > What is the semantics",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:7440,Performance,concurren,concurrently,7440,"ange things, and; it doesn't seem to be very necessary right now. It would seem to; complicate flow control analysis a LOT in the virtual machine. I would; tend to prefer that a predicated architecture like IA64 convert from a; ""basic block"" representation to a predicated rep as part of it's dynamic; complication phase. Also, if a basic block contains ONLY a move, then; that can be trivally translated into a conditional move... > I agree that we need a static data space. Otherwise, emulating global; > data gets unnecessarily complex. Definitely. Also a later item though. :). > We once talked about adding a symbolic thread-id field to each; > ..; > Instead, it could a great topic for a separate study. Agreed. :). > What is the semantics of the IA64 stop bit?. Basically, the IA64 writes instructions like this:; mov ...; add ...; sub ...; op xxx; op xxx; ;;; mov ...; add ...; sub ...; op xxx; op xxx; ;;. Where the ;; delimits a group of instruction with no dependencies between; them, which can all be executed concurrently (to the limits of the; available functional units). The ;; gets translated into a bit set in one; of the opcodes. The advantages of this representation is that you don't have to do some; kind of 'thread id scheduling' pass by having to specify ahead of time how; many threads to use, and the representation doesn't have a per instruction; overhead... > And finally, another thought about the syntax for arrays :-); > Although this syntax:; > array <dimension-list> of <type>; > is verbose, it will be used only in the human-readable assembly code so; > size should not matter. I think we should consider it because I find it; > to be the clearest syntax. It could even make arrays of function; > pointers somewhat readable. My only comment will be to give you an example of why this is a bad; idea. :). Here is an example of using the switch statement (with my recommended; syntax):. switch uint %val, label %otherwise, ; [%3 x {uint, label}] [ { uint %57, label %l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:2411,Safety,avoid,avoids,2411,"ntics). It should simplify building the CFG and my guess is it; > could enable some significant optimizations, though we should think; > about which. Great. I added a note to the switch section commenting on how the VM; should just use the instruction type as a hint, and that the; implementation may choose altermate representations (such as predicated; branches). > In the lookup-indirect form of the switch, is there a reason not to; > make the val-type uint?. No. This was something I was debating for a while, and didn't really feel; strongly about either way. It is common to switch on other types in HLL's; (for example signed int's are particularly common), but in this case, all; that will be added is an additional 'cast' instruction. I removed that; from the spec. > I agree with your comment that we don't need 'neg'. Removed. > There's a trade-off with the cast instruction:; > + it avoids having to define all the upcasts and downcasts that are; > valid for the operands of each instruction (you probably have; > thought of other benefits also); > - it could make the bytecode significantly larger because there could; > be a lot of cast operations. + You NEED casts to represent things like:; void foo(float);; ...; int x;; ...; foo(x);; in a language like C. Even in a Java like language, you need upcasts; and some way to implement dynamic downcasts.; + Not all forms of instructions take every type (for example you can't; shift by a floating point number of bits), thus SOME programs will need; implicit casts. To be efficient and to avoid your '-' point above, we just have to be; careful to specify that the instructions shall operate on all common; types, therefore casting should be relatively uncommon. For example all; of the arithmetic operations work on almost all data types. > Making the second arg. to 'shl' a ubyte seems good enough to me.; > 255 positions seems adequate for several generations of machines. Okay, that comment is removed. > and is more compact than uin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:3068,Safety,avoid,avoid,3068,"signed int's are particularly common), but in this case, all; that will be added is an additional 'cast' instruction. I removed that; from the spec. > I agree with your comment that we don't need 'neg'. Removed. > There's a trade-off with the cast instruction:; > + it avoids having to define all the upcasts and downcasts that are; > valid for the operands of each instruction (you probably have; > thought of other benefits also); > - it could make the bytecode significantly larger because there could; > be a lot of cast operations. + You NEED casts to represent things like:; void foo(float);; ...; int x;; ...; foo(x);; in a language like C. Even in a Java like language, you need upcasts; and some way to implement dynamic downcasts.; + Not all forms of instructions take every type (for example you can't; shift by a floating point number of bits), thus SOME programs will need; implicit casts. To be efficient and to avoid your '-' point above, we just have to be; careful to specify that the instructions shall operate on all common; types, therefore casting should be relatively uncommon. For example all; of the arithmetic operations work on almost all data types. > Making the second arg. to 'shl' a ubyte seems good enough to me.; > 255 positions seems adequate for several generations of machines. Okay, that comment is removed. > and is more compact than uint. No, it isn't. Remember that the bytecode encoding saves value slots into; the bytecode instructions themselves, not constant values. This is; another case where we may introduce more cast instructions (but we will; also reduce the number of opcode variants that must be supported by a; virtual machine). Because most shifts are by constant values, I don't; think that we'll have to cast many shifts. :). > I still have some major concerns about including malloc and free in the; > language (either as builtin functions or instructions). Agreed. How about this proposal:. malloc/free are either built in functions or actual o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:4181,Safety,safe,safety,4181,"herefore casting should be relatively uncommon. For example all; of the arithmetic operations work on almost all data types. > Making the second arg. to 'shl' a ubyte seems good enough to me.; > 255 positions seems adequate for several generations of machines. Okay, that comment is removed. > and is more compact than uint. No, it isn't. Remember that the bytecode encoding saves value slots into; the bytecode instructions themselves, not constant values. This is; another case where we may introduce more cast instructions (but we will; also reduce the number of opcode variants that must be supported by a; virtual machine). Because most shifts are by constant values, I don't; think that we'll have to cast many shifts. :). > I still have some major concerns about including malloc and free in the; > language (either as builtin functions or instructions). Agreed. How about this proposal:. malloc/free are either built in functions or actual opcodes. They provide; all of the type safety that the document would indicate, blah blah; blah. :). Now, because of all of the excellent points that you raised, an; implementation may want to override the default malloc/free behavior of; the program. To do this, they simply implement a ""malloc"" and; ""free"" function. The virtual machine will then be defined to use the user; defined malloc/free function (which return/take void*'s, not type'd; pointers like the builtin function would) if one is available, otherwise; fall back on a system malloc/free. Does this sound like a good compromise? It would give us all of the; typesafety/elegance in the language while still allowing the user to do; all the cool stuff they want to... > 'alloca' on the other hand sounds like a good idea, and the; > implementation seems fairly language-independent so it doesn't have the; > problems with malloc listed above. Okay, once we get the above stuff figured out, I'll put it all in the; spec. > About indirect call:; > Your option #2 sounded good to me. I'm not",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:129,Usability,feedback,feedback,129,"From: Chris Lattner <sabre@nondot.org>; To: ""Vikram S. Adve"" <vadve@cs.uiuc.edu>; Subject: Re: LLVM Feedback. I've included your feedback in the /home/vadve/lattner/llvm/docs directory; so that it will live in CVS eventually with the rest of LLVM. I've; significantly updated the documentation to reflect the changes you; suggested, as specified below:. > We should consider eliminating the type annotation in cases where it is; > essentially obvious from the instruction type:; > br bool <cond>, label <iftrue>, label <iffalse>; > I think your point was that making all types explicit improves clarity; > and readability. I agree to some extent, but it also comes at the; > cost of verbosity. And when the types are obvious from people's; > experience (e.g., in the br instruction), it doesn't seem to help as; > much. Very true. We should discuss this more, but my reasoning is more of a; consistency argument. There are VERY few instructions that can have all; of the types eliminated, and doing so when available unnecessarily makes; the language more difficult to handle. Especially when you see 'int; %this' and 'bool %that' all over the place, I think it would be; disorienting to see:. br %predicate, %iftrue, %iffalse. for branches. Even just typing that once gives me the creeps. ;) Like I; said, we should probably discuss this further in person... > On reflection, I really like your idea of having the two different; > switch types (even though they encode implementation techniques rather; > than semantics). It should simplify building the CFG and my guess is it; > could enable some significant optimizations, though we should think; > about which. Great. I added a note to the switch section commenting on how the VM; should just use the instruction type as a hint, and that the; implementation may choose altermate representations (such as predicated; branches). > In the lookup-indirect form of the switch, is there a reason not to; > make the val-type uint?. No. This was something",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:1533,Usability,simpl,simplify,1533,"s clarity; > and readability. I agree to some extent, but it also comes at the; > cost of verbosity. And when the types are obvious from people's; > experience (e.g., in the br instruction), it doesn't seem to help as; > much. Very true. We should discuss this more, but my reasoning is more of a; consistency argument. There are VERY few instructions that can have all; of the types eliminated, and doing so when available unnecessarily makes; the language more difficult to handle. Especially when you see 'int; %this' and 'bool %that' all over the place, I think it would be; disorienting to see:. br %predicate, %iftrue, %iffalse. for branches. Even just typing that once gives me the creeps. ;) Like I; said, we should probably discuss this further in person... > On reflection, I really like your idea of having the two different; > switch types (even though they encode implementation techniques rather; > than semantics). It should simplify building the CFG and my guess is it; > could enable some significant optimizations, though we should think; > about which. Great. I added a note to the switch section commenting on how the VM; should just use the instruction type as a hint, and that the; implementation may choose altermate representations (such as predicated; branches). > In the lookup-indirect form of the switch, is there a reason not to; > make the val-type uint?. No. This was something I was debating for a while, and didn't really feel; strongly about either way. It is common to switch on other types in HLL's; (for example signed int's are particularly common), but in this case, all; that will be added is an additional 'cast' instruction. I removed that; from the spec. > I agree with your comment that we don't need 'neg'. Removed. > There's a trade-off with the cast instruction:; > + it avoids having to define all the upcasts and downcasts that are; > valid for the operands of each instruction (you probably have; > thought of other benefits also); > - it could make ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:4411,Usability,simpl,simply,4411,"ral generations of machines. Okay, that comment is removed. > and is more compact than uint. No, it isn't. Remember that the bytecode encoding saves value slots into; the bytecode instructions themselves, not constant values. This is; another case where we may introduce more cast instructions (but we will; also reduce the number of opcode variants that must be supported by a; virtual machine). Because most shifts are by constant values, I don't; think that we'll have to cast many shifts. :). > I still have some major concerns about including malloc and free in the; > language (either as builtin functions or instructions). Agreed. How about this proposal:. malloc/free are either built in functions or actual opcodes. They provide; all of the type safety that the document would indicate, blah blah; blah. :). Now, because of all of the excellent points that you raised, an; implementation may want to override the default malloc/free behavior of; the program. To do this, they simply implement a ""malloc"" and; ""free"" function. The virtual machine will then be defined to use the user; defined malloc/free function (which return/take void*'s, not type'd; pointers like the builtin function would) if one is available, otherwise; fall back on a system malloc/free. Does this sound like a good compromise? It would give us all of the; typesafety/elegance in the language while still allowing the user to do; all the cool stuff they want to... > 'alloca' on the other hand sounds like a good idea, and the; > implementation seems fairly language-independent so it doesn't have the; > problems with malloc listed above. Okay, once we get the above stuff figured out, I'll put it all in the; spec. > About indirect call:; > Your option #2 sounded good to me. I'm not sure I understand your; > concern about an explicit 'icall' instruction?. I worry too much. :) The other alternative has been removed. 'icall' is; now up in the instruction list next to 'call'. > I believe tail calls are relatively ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt:8091,Usability,clear,clearest,8091,"..; > Instead, it could a great topic for a separate study. Agreed. :). > What is the semantics of the IA64 stop bit?. Basically, the IA64 writes instructions like this:; mov ...; add ...; sub ...; op xxx; op xxx; ;;; mov ...; add ...; sub ...; op xxx; op xxx; ;;. Where the ;; delimits a group of instruction with no dependencies between; them, which can all be executed concurrently (to the limits of the; available functional units). The ;; gets translated into a bit set in one; of the opcodes. The advantages of this representation is that you don't have to do some; kind of 'thread id scheduling' pass by having to specify ahead of time how; many threads to use, and the representation doesn't have a per instruction; overhead... > And finally, another thought about the syntax for arrays :-); > Although this syntax:; > array <dimension-list> of <type>; > is verbose, it will be used only in the human-readable assembly code so; > size should not matter. I think we should consider it because I find it; > to be the clearest syntax. It could even make arrays of function; > pointers somewhat readable. My only comment will be to give you an example of why this is a bad; idea. :). Here is an example of using the switch statement (with my recommended; syntax):. switch uint %val, label %otherwise, ; [%3 x {uint, label}] [ { uint %57, label %l1 }, ; { uint %20, label %l2 }, ; { uint %14, label %l3 } ]. Here it is with the syntax you are proposing:. switch uint %val, label %otherwise, ; array %3 of {uint, label} ; array of {uint, label}; { uint %57, label %l1 },; { uint %20, label %l2 },; { uint %14, label %l3 }. Which is ambiguous and very verbose. It would be possible to specify; constants with [] brackets as in my syntax, which would look like this:. switch uint %val, label %otherwise,; array %3 of {uint, label} [ { uint %57, label %l1 },; { uint %20, label %l2 },; { uint %14, label %l3 } ]. But then the syntax is inconsistent between type definition and constant; definition (wh",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-09-AdveCommentsResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt:156,Deployability,update,updated,156,"Date: Tue, 13 Feb 2001 13:29:52 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: LLVM Concerns... I've updated the documentation to include load store and allocation; instructions (please take a look and let me know if I'm on the right; track):. file:/home/vadve/lattner/llvm/docs/LangRef.html#memoryops. I have a couple of concerns I would like to bring up:. 1. Reference types; Right now, I've spec'd out the language to have a pointer type, which; works fine for lots of stuff... except that Java really has; references: constrained pointers that cannot be manipulated: added and; subtracted, moved, etc... Do we want to have a type like this? It; could be very nice for analysis (pointer always points to the start of; an object, etc...) and more closely matches Java semantics. The; pointer type would be kept for C++ like semantics. Through analysis,; C++ pointers could be promoted to references in the LLVM; representation. 2. Our ""implicit"" memory references in assembly language:; After thinking about it, this model has two problems:; A. If you do pointer analysis and realize that two stores are; independent and can share the same memory source object, there is; no way to represent this in either the bytecode or assembly.; B. When parsing assembly/bytecode, we effectively have to do a full; SSA generation/PHI node insertion pass to build the dependencies; when we don't want the ""pinned"" representation. This is not; cool.; I'm tempted to make memory references explicit in both the assembly and; bytecode to get around this... what do you think?. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt:1412,Integrability,depend,dependencies,1412,"Date: Tue, 13 Feb 2001 13:29:52 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: LLVM Concerns... I've updated the documentation to include load store and allocation; instructions (please take a look and let me know if I'm on the right; track):. file:/home/vadve/lattner/llvm/docs/LangRef.html#memoryops. I have a couple of concerns I would like to bring up:. 1. Reference types; Right now, I've spec'd out the language to have a pointer type, which; works fine for lots of stuff... except that Java really has; references: constrained pointers that cannot be manipulated: added and; subtracted, moved, etc... Do we want to have a type like this? It; could be very nice for analysis (pointer always points to the start of; an object, etc...) and more closely matches Java semantics. The; pointer type would be kept for C++ like semantics. Through analysis,; C++ pointers could be promoted to references in the LLVM; representation. 2. Our ""implicit"" memory references in assembly language:; After thinking about it, this model has two problems:; A. If you do pointer analysis and realize that two stores are; independent and can share the same memory source object, there is; no way to represent this in either the bytecode or assembly.; B. When parsing assembly/bytecode, we effectively have to do a full; SSA generation/PHI node insertion pass to build the dependencies; when we don't want the ""pinned"" representation. This is not; cool.; I'm tempted to make memory references explicit in both the assembly and; bytecode to get around this... what do you think?. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt:193,Performance,load,load,193,"Date: Tue, 13 Feb 2001 13:29:52 -0600 (CST); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: LLVM Concerns... I've updated the documentation to include load store and allocation; instructions (please take a look and let me know if I'm on the right; track):. file:/home/vadve/lattner/llvm/docs/LangRef.html#memoryops. I have a couple of concerns I would like to bring up:. 1. Reference types; Right now, I've spec'd out the language to have a pointer type, which; works fine for lots of stuff... except that Java really has; references: constrained pointers that cannot be manipulated: added and; subtracted, moved, etc... Do we want to have a type like this? It; could be very nice for analysis (pointer always points to the start of; an object, etc...) and more closely matches Java semantics. The; pointer type would be kept for C++ like semantics. Through analysis,; C++ pointers could be promoted to references in the LLVM; representation. 2. Our ""implicit"" memory references in assembly language:; After thinking about it, this model has two problems:; A. If you do pointer analysis and realize that two stores are; independent and can share the same memory source object, there is; no way to represent this in either the bytecode or assembly.; B. When parsing assembly/bytecode, we effectively have to do a full; SSA generation/PHI node insertion pass to build the dependencies; when we don't want the ""pinned"" representation. This is not; cool.; I'm tempted to make memory references explicit in both the assembly and; bytecode to get around this... what do you think?. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-Memory.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt:2104,Energy Efficiency,power,power,2104,"ypes; > Right now, I've spec'd out the language to have a pointer type, which; > works fine for lots of stuff... except that Java really has; > references: constrained pointers that cannot be manipulated: added and; > subtracted, moved, etc... Do we want to have a type like this? It; > could be very nice for analysis (pointer always points to the start of; > an object, etc...) and more closely matches Java semantics. The; > pointer type would be kept for C++ like semantics. Through analysis,; > C++ pointers could be promoted to references in the LLVM; > representation. You're right, having references would be useful. Even for C++ the *static*; compiler could generate references instead of pointers with fairly; straightforward analysis. Let's include a reference type for now. But I'm; also really concerned that LLVM is becoming big and complex and (perhaps); too high-level. After we get some initial performance results, we may have; a clearer idea of what our goals should be and we should revisit this; question then. > 2. Our ""implicit"" memory references in assembly language:; > After thinking about it, this model has two problems:; > A. If you do pointer analysis and realize that two stores are; > independent and can share the same memory source object,. not sure what you meant by ""share the same memory source object"". > there is; > no way to represent this in either the bytecode or assembly.; > B. When parsing assembly/bytecode, we effectively have to do a full; > SSA generation/PHI node insertion pass to build the dependencies; > when we don't want the ""pinned"" representation. This is not; > cool. I understand the concern. But again, let's focus on the performance first; and then look at the language design issues. E.g., it would be good to know; how big the bytecode files are before expanding them further. I am pretty; keen to explore the implications of LLVM for mobile devices. Both bytecode; size and power consumption are important to consider there. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt:2110,Energy Efficiency,consumption,consumption,2110,"ypes; > Right now, I've spec'd out the language to have a pointer type, which; > works fine for lots of stuff... except that Java really has; > references: constrained pointers that cannot be manipulated: added and; > subtracted, moved, etc... Do we want to have a type like this? It; > could be very nice for analysis (pointer always points to the start of; > an object, etc...) and more closely matches Java semantics. The; > pointer type would be kept for C++ like semantics. Through analysis,; > C++ pointers could be promoted to references in the LLVM; > representation. You're right, having references would be useful. Even for C++ the *static*; compiler could generate references instead of pointers with fairly; straightforward analysis. Let's include a reference type for now. But I'm; also really concerned that LLVM is becoming big and complex and (perhaps); too high-level. After we get some initial performance results, we may have; a clearer idea of what our goals should be and we should revisit this; question then. > 2. Our ""implicit"" memory references in assembly language:; > After thinking about it, this model has two problems:; > A. If you do pointer analysis and realize that two stores are; > independent and can share the same memory source object,. not sure what you meant by ""share the same memory source object"". > there is; > no way to represent this in either the bytecode or assembly.; > B. When parsing assembly/bytecode, we effectively have to do a full; > SSA generation/PHI node insertion pass to build the dependencies; > when we don't want the ""pinned"" representation. This is not; > cool. I understand the concern. But again, let's focus on the performance first; and then look at the language design issues. E.g., it would be good to know; how big the bytecode files are before expanding them further. I am pretty; keen to explore the implications of LLVM for mobile devices. Both bytecode; size and power consumption are important to consider there. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt:1707,Integrability,depend,dependencies,1707,"ypes; > Right now, I've spec'd out the language to have a pointer type, which; > works fine for lots of stuff... except that Java really has; > references: constrained pointers that cannot be manipulated: added and; > subtracted, moved, etc... Do we want to have a type like this? It; > could be very nice for analysis (pointer always points to the start of; > an object, etc...) and more closely matches Java semantics. The; > pointer type would be kept for C++ like semantics. Through analysis,; > C++ pointers could be promoted to references in the LLVM; > representation. You're right, having references would be useful. Even for C++ the *static*; compiler could generate references instead of pointers with fairly; straightforward analysis. Let's include a reference type for now. But I'm; also really concerned that LLVM is becoming big and complex and (perhaps); too high-level. After we get some initial performance results, we may have; a clearer idea of what our goals should be and we should revisit this; question then. > 2. Our ""implicit"" memory references in assembly language:; > After thinking about it, this model has two problems:; > A. If you do pointer analysis and realize that two stores are; > independent and can share the same memory source object,. not sure what you meant by ""share the same memory source object"". > there is; > no way to represent this in either the bytecode or assembly.; > B. When parsing assembly/bytecode, we effectively have to do a full; > SSA generation/PHI node insertion pass to build the dependencies; > when we don't want the ""pinned"" representation. This is not; > cool. I understand the concern. But again, let's focus on the performance first; and then look at the language design issues. E.g., it would be good to know; how big the bytecode files are before expanding them further. I am pretty; keen to explore the implications of LLVM for mobile devices. Both bytecode; size and power consumption are important to consider there. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt:1077,Performance,perform,performance,1077,"ect: RE: LLVM Concerns... > 1. Reference types; > Right now, I've spec'd out the language to have a pointer type, which; > works fine for lots of stuff... except that Java really has; > references: constrained pointers that cannot be manipulated: added and; > subtracted, moved, etc... Do we want to have a type like this? It; > could be very nice for analysis (pointer always points to the start of; > an object, etc...) and more closely matches Java semantics. The; > pointer type would be kept for C++ like semantics. Through analysis,; > C++ pointers could be promoted to references in the LLVM; > representation. You're right, having references would be useful. Even for C++ the *static*; compiler could generate references instead of pointers with fairly; straightforward analysis. Let's include a reference type for now. But I'm; also really concerned that LLVM is becoming big and complex and (perhaps); too high-level. After we get some initial performance results, we may have; a clearer idea of what our goals should be and we should revisit this; question then. > 2. Our ""implicit"" memory references in assembly language:; > After thinking about it, this model has two problems:; > A. If you do pointer analysis and realize that two stores are; > independent and can share the same memory source object,. not sure what you meant by ""share the same memory source object"". > there is; > no way to represent this in either the bytecode or assembly.; > B. When parsing assembly/bytecode, we effectively have to do a full; > SSA generation/PHI node insertion pass to build the dependencies; > when we don't want the ""pinned"" representation. This is not; > cool. I understand the concern. But again, let's focus on the performance first; and then look at the language design issues. E.g., it would be good to know; how big the bytecode files are before expanding them further. I am pretty; keen to explore the implications of LLVM for mobile devices. Both bytecode; size and power consumption a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt:1848,Performance,perform,performance,1848,"ypes; > Right now, I've spec'd out the language to have a pointer type, which; > works fine for lots of stuff... except that Java really has; > references: constrained pointers that cannot be manipulated: added and; > subtracted, moved, etc... Do we want to have a type like this? It; > could be very nice for analysis (pointer always points to the start of; > an object, etc...) and more closely matches Java semantics. The; > pointer type would be kept for C++ like semantics. Through analysis,; > C++ pointers could be promoted to references in the LLVM; > representation. You're right, having references would be useful. Even for C++ the *static*; compiler could generate references instead of pointers with fairly; straightforward analysis. Let's include a reference type for now. But I'm; also really concerned that LLVM is becoming big and complex and (perhaps); too high-level. After we get some initial performance results, we may have; a clearer idea of what our goals should be and we should revisit this; question then. > 2. Our ""implicit"" memory references in assembly language:; > After thinking about it, this model has two problems:; > A. If you do pointer analysis and realize that two stores are; > independent and can share the same memory source object,. not sure what you meant by ""share the same memory source object"". > there is; > no way to represent this in either the bytecode or assembly.; > B. When parsing assembly/bytecode, we effectively have to do a full; > SSA generation/PHI node insertion pass to build the dependencies; > when we don't want the ""pinned"" representation. This is not; > cool. I understand the concern. But again, let's focus on the performance first; and then look at the language design issues. E.g., it would be good to know; how big the bytecode files are before expanding them further. I am pretty; keen to explore the implications of LLVM for mobile devices. Both bytecode; size and power consumption are important to consider there. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt:1113,Usability,clear,clearer,1113,"ect: RE: LLVM Concerns... > 1. Reference types; > Right now, I've spec'd out the language to have a pointer type, which; > works fine for lots of stuff... except that Java really has; > references: constrained pointers that cannot be manipulated: added and; > subtracted, moved, etc... Do we want to have a type like this? It; > could be very nice for analysis (pointer always points to the start of; > an object, etc...) and more closely matches Java semantics. The; > pointer type would be kept for C++ like semantics. Through analysis,; > C++ pointers could be promoted to references in the LLVM; > representation. You're right, having references would be useful. Even for C++ the *static*; compiler could generate references instead of pointers with fairly; straightforward analysis. Let's include a reference type for now. But I'm; also really concerned that LLVM is becoming big and complex and (perhaps); too high-level. After we get some initial performance results, we may have; a clearer idea of what our goals should be and we should revisit this; question then. > 2. Our ""implicit"" memory references in assembly language:; > After thinking about it, this model has two problems:; > A. If you do pointer analysis and realize that two stores are; > independent and can share the same memory source object,. not sure what you meant by ""share the same memory source object"". > there is; > no way to represent this in either the bytecode or assembly.; > B. When parsing assembly/bytecode, we effectively have to do a full; > SSA generation/PHI node insertion pass to build the dependencies; > when we don't want the ""pinned"" representation. This is not; > cool. I understand the concern. But again, let's focus on the performance first; and then look at the language design issues. E.g., it would be good to know; how big the bytecode files are before expanding them further. I am pretty; keen to explore the implications of LLVM for mobile devices. Both bytecode; size and power consumption a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-02-13-Reference-MemoryResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1783,Deployability,continuous,continuously,1783," approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to ena",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:2346,Deployability,upgrade,upgraded,2346,"a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to enable powerful dynamic and link time optimizations ; to be performed quickly and efficiently. This combination enables an ; increase in effective system performance for real world environments.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:205,Energy Efficiency,power,powerful,205,"By Chris:. LLVM has been designed with two primary goals in mind. First we strive to ; enable the best possible division of labor between static and dynamic ; compilers, and second, we need a flexible and powerful interface ; between these two complementary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:2851,Energy Efficiency,power,powerful,2851,"a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to enable powerful dynamic and link time optimizations ; to be performed quickly and efficiently. This combination enables an ; increase in effective system performance for real world environments.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:2926,Energy Efficiency,efficient,efficiently,2926,"a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to enable powerful dynamic and link time optimizations ; to be performed quickly and efficiently. This combination enables an ; increase in effective system performance for real world environments.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:214,Integrability,interface,interface,214,"By Chris:. LLVM has been designed with two primary goals in mind. First we strive to ; enable the best possible division of labor between static and dynamic ; compilers, and second, we need a flexible and powerful interface ; between these two complementary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:192,Modifiability,flexible,flexible,192,"By Chris:. LLVM has been designed with two primary goals in mind. First we strive to ; enable the best possible division of labor between static and dynamic ; compilers, and second, we need a flexible and powerful interface ; between these two complementary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:924,Modifiability,portab,portability,924,"By Chris:. LLVM has been designed with two primary goals in mind. First we strive to ; enable the best possible division of labor between static and dynamic ; compilers, and second, we need a flexible and powerful interface ; between these two complementary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1798,Modifiability,evolve,evolve,1798," approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to ena",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:378,Performance,perform,performance,378,"By Chris:. LLVM has been designed with two primary goals in mind. First we strive to ; enable the best possible division of labor between static and dynamic ; compilers, and second, we need a flexible and powerful interface ; between these two complementary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:908,Performance,perform,performance,908,"By Chris:. LLVM has been designed with two primary goals in mind. First we strive to ; enable the best possible division of labor between static and dynamic ; compilers, and second, we need a flexible and powerful interface ; between these two complementary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1009,Performance,perform,performance,1009,"wo primary goals in mind. First we strive to ; enable the best possible division of labor between static and dynamic ; compilers, and second, we need a flexible and powerful interface ; between these two complementary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1227,Performance,perform,performance,1227,"tary stages of compilation. We feel that ; providing a solution to these two goals will yield an excellent solution ; to the performance problem faced by modern architectures and programming ; languages. A key insight into current compiler and runtime systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1407,Performance,optimiz,optimization,1407,"e systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1479,Performance,optimiz,optimization,1479,"e systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1492,Performance,tune,tunes,1492,"e systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1725,Performance,perform,performance,1725,"quivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:2703,Performance,optimiz,optimizations,2703,"a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to enable powerful dynamic and link time optimizations ; to be performed quickly and efficiently. This combination enables an ; increase in effective system performance for real world environments.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:2882,Performance,optimiz,optimizations,2882,"a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to enable powerful dynamic and link time optimizations ; to be performed quickly and efficiently. This combination enables an ; increase in effective system performance for real world environments.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:2904,Performance,perform,performed,2904,"a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to enable powerful dynamic and link time optimizations ; to be performed quickly and efficiently. This combination enables an ; increase in effective system performance for real world environments.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:2998,Performance,perform,performance,2998,"a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execution environment. For these reasons, LLVM has been designed from the beginning as a long-term ; solution to these problems. Its design allows the large body of platform ; independent, static, program optimizations currently in compilers to be ; reused unchanged in their current form. It also provides important static ; type information to enable powerful dynamic and link time optimizations ; to be performed quickly and efficiently. This combination enables an ; increase in effective system performance for real world environments.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1589,Testability,benchmark,benchmarks,1589,"e systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt:1400,Usability,guid,guided,1400,"e systems is that a ; compiler may fall in anywhere in a ""continuum of compilation"" to do its ; job. On one side, scripting languages statically compile nothing and ; dynamically compile (or equivalently, interpret) everything. On the far ; other side, traditional static compilers process everything statically and ; nothing dynamically. These approaches have typically been seen as a ; tradeoff between performance and portability. On a deeper level, however, ; there are two reasons that optimal system performance may be obtained by a; system somewhere in between these two extremes: Dynamic application ; behavior and social constraints. From a technical perspective, pure static compilation cannot ever give ; optimal performance in all cases, because applications have varying dynamic; behavior that the static compiler cannot take into consideration. Even ; compilers that support profile guided optimization generate poor code in ; the real world, because using such optimization tunes that application ; to one particular usage pattern, whereas real programs (as opposed to ; benchmarks) often have several different usage patterns. On a social level, static compilation is a very shortsighted solution to ; the performance problem. Instruction set architectures (ISAs) continuously ; evolve, and each implementation of an ISA (a processor) must choose a set ; of tradeoffs that make sense in the market context that it is designed for. ; With every new processor introduced, the vendor faces two fundamental ; problems: First, there is a lag time between when a processor is introduced ; to when compilers generate quality code for the architecture. Secondly, ; even when compilers catch up to the new architecture there is often a large ; body of legacy code that was compiled for previous generations and will ; not or can not be upgraded. Thus a large percentage of code running on a ; processor may be compiled quite sub-optimally for the current ; characteristics of the dynamic execu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-04-16-DynamicCompilation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:4839,Deployability,install,installed,4839,"ecuted if an exception escapes the try block ; c->~C(); barCleanup: // Executed if an exception escapes from bar(); // fall through; fooCleanup: // Executed if an exception escapes from foo(); b->~B(); a->~A(); Exception *E = getThreadLocalException(); call throw(E) // Implemented by the C++ runtime, described below. Which does the work one would expect. getThreadLocalException is a function; implemented by the C++ support library. It returns the current exception ; object for the current thread. Note that we do not attempt to recycle the ; shutdown code from before, because performance of the mainline code is ; critically important. Also, obviously fooCleanup and barCleanup may be ; merged and one of them eliminated. This just shows how the code generator ; would most likely emit code. The bazCleanup label is more interesting. Because the exception may be caught; by the try block, we must dispatch to its handler... but it does not exist; on the call stack (it does not have a VM Call->Label mapping installed), so ; we must dispatch statically with a goto. The bazHandler thus appears as:. bazHandler:; d->~D(); // destruct D as it goes out of scope when entering catch clauses; goto TryHandler. In general, TryHandler is not the same as bazHandler, because multiple ; function calls could be made from the try block. In this case, trivial ; optimization could merge the two basic blocks. TryHandler is the code ; that actually determines the type of exception, based on the Exception object; itself. For this discussion, assume that the exception object contains *at; least*:. 1. A pointer to the RTTI info for the contained object; 2. A pointer to the dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalEx",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:7227,Integrability,synchroniz,synchronized,7227,"he dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers access to this...; call doNonLocalBranch(%L); }; // No handler found!; call BlowUp() // Ends up calling the terminate() method in use; }. That's a brief rundown of how C++ exception handling could be implemented in; llvm. Java would be very similar, except it only uses destructors to unlock; synchronized blocks, not to destroy data. Also, it uses two stack walks: a; nondestructive walk that builds a stack trace, then a destructive walk that; unwinds the stack as shown here. . It would be trivial to get exception interoperability between C++ and Java. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:7452,Integrability,interoperab,interoperability,7452,"he dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers access to this...; call doNonLocalBranch(%L); }; // No handler found!; call BlowUp() // Ends up calling the terminate() method in use; }. That's a brief rundown of how C++ exception handling could be implemented in; llvm. Java would be very similar, except it only uses destructors to unlock; synchronized blocks, not to destroy data. Also, it uses two stack walks: a; nondestructive walk that builds a stack trace, then a destructive walk that; unwinds the stack as shown here. . It would be trivial to get exception interoperability between C++ and Java. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:4407,Performance,perform,performance,4407," this is a very straight forward and literal translation: exactly; what we want for zero cost (when unused) exception handling. Especially on; platforms with many registers (ie, the IA64) setjmp/longjmp style exception; handling is *very* impractical. Also, the ""with"" clauses describe the ; control flow paths explicitly so that analysis is not adversly effected. The foo/barCleanup labels are implemented as:. TryCleanup: // Executed if an exception escapes the try block ; c->~C(); barCleanup: // Executed if an exception escapes from bar(); // fall through; fooCleanup: // Executed if an exception escapes from foo(); b->~B(); a->~A(); Exception *E = getThreadLocalException(); call throw(E) // Implemented by the C++ runtime, described below. Which does the work one would expect. getThreadLocalException is a function; implemented by the C++ support library. It returns the current exception ; object for the current thread. Note that we do not attempt to recycle the ; shutdown code from before, because performance of the mainline code is ; critically important. Also, obviously fooCleanup and barCleanup may be ; merged and one of them eliminated. This just shows how the code generator ; would most likely emit code. The bazCleanup label is more interesting. Because the exception may be caught; by the try block, we must dispatch to its handler... but it does not exist; on the call stack (it does not have a VM Call->Label mapping installed), so ; we must dispatch statically with a goto. The bazHandler thus appears as:. bazHandler:; d->~D(); // destruct D as it goes out of scope when entering catch clauses; goto TryHandler. In general, TryHandler is not the same as bazHandler, because multiple ; function calls could be made from the try block. In this case, trivial ; optimization could merge the two basic blocks. TryHandler is the code ; that actually determines the type of exception, based on the Exception object; itself. For this discussion, assume that the exception object c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:5182,Performance,optimiz,optimization,5182,"ocalException is a function; implemented by the C++ support library. It returns the current exception ; object for the current thread. Note that we do not attempt to recycle the ; shutdown code from before, because performance of the mainline code is ; critically important. Also, obviously fooCleanup and barCleanup may be ; merged and one of them eliminated. This just shows how the code generator ; would most likely emit code. The bazCleanup label is more interesting. Because the exception may be caught; by the try block, we must dispatch to its handler... but it does not exist; on the call stack (it does not have a VM Call->Label mapping installed), so ; we must dispatch statically with a goto. The bazHandler thus appears as:. bazHandler:; d->~D(); // destruct D as it goes out of scope when entering catch clauses; goto TryHandler. In general, TryHandler is not the same as bazHandler, because multiple ; function calls could be made from the try block. In this case, trivial ; optimization could merge the two basic blocks. TryHandler is the code ; that actually determines the type of exception, based on the Exception object; itself. For this discussion, assume that the exception object contains *at; least*:. 1. A pointer to the RTTI info for the contained object; 2. A pointer to the dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:5913,Performance,perform,perform,5913,"andler:; d->~D(); // destruct D as it goes out of scope when entering catch clauses; goto TryHandler. In general, TryHandler is not the same as bazHandler, because multiple ; function calls could be made from the try block. In this case, trivial ; optimization could merge the two basic blocks. TryHandler is the code ; that actually determines the type of exception, based on the Exception object; itself. For this discussion, assume that the exception object contains *at; least*:. 1. A pointer to the RTTI info for the contained object; 2. A pointer to the dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:6009,Performance,perform,perform,6009,"tiple ; function calls could be made from the try block. In this case, trivial ; optimization could merge the two basic blocks. TryHandler is the code ; that actually determines the type of exception, based on the Exception object; itself. For this discussion, assume that the exception object contains *at; least*:. 1. A pointer to the RTTI info for the contained object; 2. A pointer to the dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers access to this...; call doNonLocalBranch(%L); }; // No handler found!; call BlowUp() // Ends up calling the terminate() method in use; }. That's a brief rundown of how",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:6138,Performance,optimiz,optimizer,6138,"tiple ; function calls could be made from the try block. In this case, trivial ; optimization could merge the two basic blocks. TryHandler is the code ; that actually determines the type of exception, based on the Exception object; itself. For this discussion, assume that the exception object contains *at; least*:. 1. A pointer to the RTTI info for the contained object; 2. A pointer to the dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers access to this...; call doNonLocalBranch(%L); }; // No handler found!; call BlowUp() // Ends up calling the terminate() method in use; }. That's a brief rundown of how",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:6507,Performance,optimiz,optimization,6507,"contained object; 2. A pointer to the dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers access to this...; call doNonLocalBranch(%L); }; // No handler found!; call BlowUp() // Ends up calling the terminate() method in use; }. That's a brief rundown of how C++ exception handling could be implemented in; llvm. Java would be very similar, except it only uses destructors to unlock; synchronized blocks, not to destroy data. Also, it uses two stack walks: a; nondestructive walk that builds a stack trace, then a destructive walk that; unwinds the stack as shown here. . It would be trivial to get exception inter",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:6934,Security,access,access,6934,"he dtor for the contained object; 3. The contained object itself. Note that it is necessary to maintain #1 & #2 in the exception object itself; because objects without virtual function tables may be thrown (as in this ; example). Assuming this, TryHandler would look something like this:. TryHandler: ; Exception *E = getThreadLocalException();; switch (E->RTTIType) {; case IntRTTIInfo:; ...int Stuff... // The action to perform from the catch block; break;; case DoubleRTTIInfo:; ...double Stuff... // The action to perform from the catch block; goto TryCleanup // This catch block rethrows the exception; break; // Redundant, eliminated by the optimizer; default:; goto TryCleanup // Exception not caught, rethrow; }. // Exception was consumed; if (E->dtor); E->dtor(E->object) // Invoke the dtor on the object if it exists; goto EndTry // Continue mainline code... And that is all there is to it. The throw(E) function would then be implemented like this (which may be ; inlined into the caller through standard optimization):. function throw(Exception *E) {; // Get the start of the stack trace...; %frame %f = call getStackCurrentFrame(). // Get the label information that corresponds to it; label * %L = call getFrameLabel(%f); while (%L == 0 && !isFirstFrame(%f)) {; // Loop until a cleanup handler is found; %f = call getNextFrame(%f); %L = call getFrameLabel(%f); }. if (%L != 0) {; call setThreadLocalException(E) // Allow handlers access to this...; call doNonLocalBranch(%L); }; // No handler found!; call BlowUp() // Ends up calling the terminate() method in use; }. That's a brief rundown of how C++ exception handling could be implemented in; llvm. Java would be very similar, except it only uses destructors to unlock; synchronized blocks, not to destroy data. Also, it uses two stack walks: a; nondestructive walk that builds a stack trace, then a destructive walk that; unwinds the stack as shown here. . It would be trivial to get exception interoperability between C++ and Java. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:938,Usability,simpl,simple,938,"Meeting notes: Implementation idea: Exception Handling in C++/Java. The 5/18/01 meeting discussed ideas for implementing exceptions in LLVM.; We decided that the best solution requires a set of library calls provided by; the VM, as well as an extension to the LLVM function invocation syntax. The LLVM function invocation instruction previously looks like this (ignoring; types):. call func(arg1, arg2, arg3). The extension discussed today adds an optional ""with"" clause that ; associates a label with the call site. The new syntax looks like this:. call func(arg1, arg2, arg3) with funcCleanup. This funcHandler always stays tightly associated with the call site (being; encoded directly into the call opcode itself), and should be used whenever; there is cleanup work that needs to be done for the current function if ; an exception is thrown by func (or if we are in a try block). To support this, the VM/Runtime provide the following simple library ; functions (all syntax in this document is very abstract):. typedef struct { something } %frame;; The VM must export a ""frame type"", that is an opaque structure used to ; implement different types of stack walking that may be used by various; language runtime libraries. We imagine that it would be typical to ; represent a frame with a PC and frame pointer pair, although that is not ; required. %frame getStackCurrentFrame();; Get a frame object for the current function. Note that if the current; function was inlined into its caller, the ""current"" frame will belong to; the ""caller"". bool isFirstFrame(%frame f);; Returns true if the specified frame is the top level (first activated) frame; for this thread. For the main thread, this corresponds to the main() ; function, for a spawned thread, it corresponds to the thread function. %frame getNextFrame(%frame f);; Return the previous frame on the stack. This function is undefined if f; satisfies the predicate isFirstFrame(f). Label *getFrameLabel(%frame f);; If a label was associated with",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt:2145,Usability,clear,clear,2145,"types of stack walking that may be used by various; language runtime libraries. We imagine that it would be typical to ; represent a frame with a PC and frame pointer pair, although that is not ; required. %frame getStackCurrentFrame();; Get a frame object for the current function. Note that if the current; function was inlined into its caller, the ""current"" frame will belong to; the ""caller"". bool isFirstFrame(%frame f);; Returns true if the specified frame is the top level (first activated) frame; for this thread. For the main thread, this corresponds to the main() ; function, for a spawned thread, it corresponds to the thread function. %frame getNextFrame(%frame f);; Return the previous frame on the stack. This function is undefined if f; satisfies the predicate isFirstFrame(f). Label *getFrameLabel(%frame f);; If a label was associated with f (as discussed below), this function returns; it. Otherwise, it returns a null pointer. doNonLocalBranch(Label *L);; At this point, it is not clear whether this should be a function or ; intrinsic. It should probably be an intrinsic in LLVM, but we'll deal with; this issue later. Here is a motivating example that illustrates how these facilities could be; used to implement the C++ exception model:. void TestFunction(...) {; A a; B b;; foo(); // Any function call may throw; bar();; C c;. try {; D d;; baz();; } catch (int) {; ...int Stuff...; // execution continues after the try block: the exception is consumed; } catch (double) {; ...double stuff...; throw; // Exception is propogated; }; }. This function would compile to approximately the following code (heavy ; pseudo code follows):. Func:; %a = alloca A; A::A(%a) // These ctors & dtors could throw, but we ignore this ; %b = alloca B // minor detail for this example; B::B(%b). call foo() with fooCleanup // An exception in foo is propogated to fooCleanup; call bar() with barCleanup // An exception in bar is propogated to barCleanup. %c = alloca C; C::C(c); %d = alloca D; D::D",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-18-ExceptionHandling.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-19-ExceptionResponse.txt:1940,Availability,down,down,1940,"e@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Meeting writeup. > I read it through and it looks great!. Thanks!. > The finally clause in Java may need more thought. The code for this clause; > is like a subroutine because it needs to be entered from many points (end of; > try block and beginning of each catch block), and then needs to *return to; > the place from where the code was entered*. That's why JVM has the; > jsr/jsr_w instruction. Hrm... I guess that is an implementation decision. It can either be; modelled as a subroutine (as java bytecodes do), which is really; gross... or it can be modelled as code duplication (emitted once inline,; then once in the exception path). Because this could, at worst,; slightly less than double the amount of code in a function (it is; bounded) I don't think this is a big deal. One of the really nice things; about the LLVM representation is that it still allows for runtime code; generation for exception paths (exceptions paths are not compiled until; needed). Obviously a static compiler couldn't do this though. :). In this case, only one copy of the code would be compiled... until the; other one is needed on demand. Also this strategy fits with the ""zero; cost"" exception model... the standard case is not burdened with extra; branches or ""call""s. > I suppose you could save the return address in a particular register; > (specific to this finally block), jump to the finally block, and then at the; > end of the finally block, jump back indirectly through this register. It; > will complicate building the CFG but I suppose that can be handled. It is; > also unsafe in terms of checking where control returns (which is I suppose; > why the JVM doesn't use this). I think that a code duplication method would be cleaner, and would avoid; the caveats that you mention. Also, it does not slow down the normal case; with an indirect branch... Like everything, we can probably defer a final decision until later. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-19-ExceptionResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-19-ExceptionResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-19-ExceptionResponse.txt:1709,Safety,unsafe,unsafe,1709,"e@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Meeting writeup. > I read it through and it looks great!. Thanks!. > The finally clause in Java may need more thought. The code for this clause; > is like a subroutine because it needs to be entered from many points (end of; > try block and beginning of each catch block), and then needs to *return to; > the place from where the code was entered*. That's why JVM has the; > jsr/jsr_w instruction. Hrm... I guess that is an implementation decision. It can either be; modelled as a subroutine (as java bytecodes do), which is really; gross... or it can be modelled as code duplication (emitted once inline,; then once in the exception path). Because this could, at worst,; slightly less than double the amount of code in a function (it is; bounded) I don't think this is a big deal. One of the really nice things; about the LLVM representation is that it still allows for runtime code; generation for exception paths (exceptions paths are not compiled until; needed). Obviously a static compiler couldn't do this though. :). In this case, only one copy of the code would be compiled... until the; other one is needed on demand. Also this strategy fits with the ""zero; cost"" exception model... the standard case is not burdened with extra; branches or ""call""s. > I suppose you could save the return address in a particular register; > (specific to this finally block), jump to the finally block, and then at the; > end of the finally block, jump back indirectly through this register. It; > will complicate building the CFG but I suppose that can be handled. It is; > also unsafe in terms of checking where control returns (which is I suppose; > why the JVM doesn't use this). I think that a code duplication method would be cleaner, and would avoid; the caveats that you mention. Also, it does not slow down the normal case; with an indirect branch... Like everything, we can probably defer a final decision until later. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-19-ExceptionResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-19-ExceptionResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-19-ExceptionResponse.txt:1880,Safety,avoid,avoid,1880,"e@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Meeting writeup. > I read it through and it looks great!. Thanks!. > The finally clause in Java may need more thought. The code for this clause; > is like a subroutine because it needs to be entered from many points (end of; > try block and beginning of each catch block), and then needs to *return to; > the place from where the code was entered*. That's why JVM has the; > jsr/jsr_w instruction. Hrm... I guess that is an implementation decision. It can either be; modelled as a subroutine (as java bytecodes do), which is really; gross... or it can be modelled as code duplication (emitted once inline,; then once in the exception path). Because this could, at worst,; slightly less than double the amount of code in a function (it is; bounded) I don't think this is a big deal. One of the really nice things; about the LLVM representation is that it still allows for runtime code; generation for exception paths (exceptions paths are not compiled until; needed). Obviously a static compiler couldn't do this though. :). In this case, only one copy of the code would be compiled... until the; other one is needed on demand. Also this strategy fits with the ""zero; cost"" exception model... the standard case is not burdened with extra; branches or ""call""s. > I suppose you could save the return address in a particular register; > (specific to this finally block), jump to the finally block, and then at the; > end of the finally block, jump back indirectly through this register. It; > will complicate building the CFG but I suppose that can be handled. It is; > also unsafe in terms of checking where control returns (which is I suppose; > why the JVM doesn't use this). I think that a code duplication method would be cleaner, and would avoid; the caveats that you mention. Also, it does not slow down the normal case; with an indirect branch... Like everything, we can probably defer a final decision until later. :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-19-ExceptionResponse.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-05-19-ExceptionResponse.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1674,Availability,down,down,1674,"reachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phase for now. We; can talk about this more on Monday. Wouldn't it be nice if there were a obvious decision to be made? :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:796,Energy Efficiency,schedul,scheduling,796,"Date: Fri, 1 Jun 2001 16:38:17 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Interesting: GCC passes. Take a look at this document (which describes the order of optimizations; that GCC performs):. http://gcc.gnu.org/onlinedocs/gcc_17.html. The rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1483,Energy Efficiency,adapt,adapted,1483,"reachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phase for now. We; can talk about this more on Monday. Wouldn't it be nice if there were a obvious decision to be made? :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1811,Integrability,depend,dependencies,1811,"reachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phase for now. We; can talk about this more on Monday. Wouldn't it be nice if there were a obvious decision to be made? :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1483,Modifiability,adapt,adapted,1483,"reachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phase for now. We; can talk about this more on Monday. Wouldn't it be nice if there were a obvious decision to be made? :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1802,Modifiability,variab,variable,1802,"reachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phase for now. We; can talk about this more on Monday. Wouldn't it be nice if there were a obvious decision to be made? :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:217,Performance,optimiz,optimizations,217,"Date: Fri, 1 Jun 2001 16:38:17 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Interesting: GCC passes. Take a look at this document (which describes the order of optimizations; that GCC performs):. http://gcc.gnu.org/onlinedocs/gcc_17.html. The rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:241,Performance,perform,performs,241,"Date: Fri, 1 Jun 2001 16:38:17 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Interesting: GCC passes. Take a look at this document (which describes the order of optimizations; that GCC performs):. http://gcc.gnu.org/onlinedocs/gcc_17.html. The rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:375,Performance,optimiz,optimization,375,"Date: Fri, 1 Jun 2001 16:38:17 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Interesting: GCC passes. Take a look at this document (which describes the order of optimizations; that GCC performs):. http://gcc.gnu.org/onlinedocs/gcc_17.html. The rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:827,Performance,optimiz,optimizations,827,"Date: Fri, 1 Jun 2001 16:38:17 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Interesting: GCC passes. Take a look at this document (which describes the order of optimizations; that GCC performs):. http://gcc.gnu.org/onlinedocs/gcc_17.html. The rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1172,Performance,optimiz,optimizations,1172,"nteresting: GCC passes. Take a look at this document (which describes the order of optimizations; that GCC performs):. http://gcc.gnu.org/onlinedocs/gcc_17.html. The rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1251,Performance,optimiz,optimizations,1251," rundown is that after RTL generation, the following happens:. 1 . [t] jump optimization (jumps to jumps, etc); 2 . [t] Delete unreachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:1455,Performance,optimiz,optimizations,1455,"reachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phase for now. We; can talk about this more on Monday. Wouldn't it be nice if there were a obvious decision to be made? :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt:2117,Usability,clear,clear,2117,"reachable code; 3 . Compute live ranges for CSE; 4 . [t] Jump threading (jumps to jumps with identical or inverse conditions); 5 . [t] CSE; 6 . *** Conversion to SSA ; 7 . [t] SSA Based DCE; 8 . *** Conversion to LLVM; 9 . UnSSA; 10. GCSE; 11. LICM; 12. Strength Reduction; 13. Loop unrolling; 14. [t] CSE; 15. [t] DCE; 16. Instruction combination, register movement, scheduling... etc. I've marked optimizations with a [t] to indicate things that I believe to; be relatively trivial to implement in LLVM itself. The time consuming; things to reimplement would be SSA based PRE, Strength reduction & loop; unrolling... these would be the major things we would miss out on if we; did LLVM creation from tree code [inlining and other high level; optimizations are done on the tree representation]. Given the lack of ""strong"" optimizations that would take a long time to; reimplement, I am leaning a bit more towards creating LLVM from the tree; code. Especially given that SGI has GPL'd their compiler, including many; SSA based optimizations that could be adapted (besides the fact that their; code looks MUCH nicer than GCC :). Even if we choose to do LLVM code emission from RTL, we will almost; certainly want to move LLVM emission from step 8 down until at least CSE; has been rerun... which causes me to wonder if the SSA generation code; will still work (due to global variable dependencies and stuff). I assume; that it can be made to work, but might be a little more involved than we; would like. I'm continuing to look at the Tree -> RTL code. It is pretty gross; because they do some of the translation a statement at a time, and some; of it a function at a time... I'm not quite clear why and how the; distinction is drawn, but it does not appear that there is a wonderful; place to attach extra info. Anyways, I'm proceeding with the RTL -> LLVM conversion phase for now. We; can talk about this more on Monday. Wouldn't it be nice if there were a obvious decision to be made? :). -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2327,Availability,avail,available,2327,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:524,Deployability,pipeline,pipeline,524,"Date: Fri, 1 Jun 2001 17:08:44 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Interesting: GCC passes. > That is very interesting. I agree that some of these could be done on LLVM; > at link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mix",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:1333,Energy Efficiency,adapt,adapted,1333,"link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks som",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:1516,Energy Efficiency,adapt,adapt,1516," does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the fi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2570,Energy Efficiency,schedul,scheduling,2570,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2763,Energy Efficiency,schedul,scheduling,2763,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:1333,Modifiability,adapt,adapted,1333,"link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks som",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:1516,Modifiability,adapt,adapt,1516," does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the fi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:1544,Modifiability,rewrite,rewrite,1544," does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the fi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:321,Performance,optimiz,optimization,321,"Date: Fri, 1 Jun 2001 17:08:44 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Interesting: GCC passes. > That is very interesting. I agree that some of these could be done on LLVM; > at link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mix",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:403,Performance,optimiz,optimizations,403,"Date: Fri, 1 Jun 2001 17:08:44 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Interesting: GCC passes. > That is very interesting. I agree that some of these could be done on LLVM; > at link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mix",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:552,Performance,optimiz,optimizations,552,"Date: Fri, 1 Jun 2001 17:08:44 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Interesting: GCC passes. > That is very interesting. I agree that some of these could be done on LLVM; > at link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mix",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:641,Performance,optimiz,optimizations,641,"Date: Fri, 1 Jun 2001 17:08:44 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Interesting: GCC passes. > That is very interesting. I agree that some of these could be done on LLVM; > at link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mix",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:725,Performance,optimiz,optimizations,725,"Date: Fri, 1 Jun 2001 17:08:44 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Interesting: GCC passes. > That is very interesting. I agree that some of these could be done on LLVM; > at link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mix",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:954,Performance,optimiz,optimizations,954,"Date: Fri, 1 Jun 2001 17:08:44 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: RE: Interesting: GCC passes. > That is very interesting. I agree that some of these could be done on LLVM; > at link-time, but it is the extra time required that concerns me. Link-time; > optimization is severely time-constrained. If we were to reimplement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mix",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:1354,Performance,optimiz,optimizations,1354,"lement any of these optimizations, I assume that we; could do them a translation unit at a time, just as GCC does now. This; would lead to a pipeline like this:. Static optimizations, xlation unit at a time:; .c --GCC--> .llvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:1611,Performance,optimiz,optimizations,1611,"lvm --llvmopt--> .llvm . Link time optimizations:; .llvm --llvm-ld--> .llvm --llvm-link-opt--> .llvm . Of course, many optimizations could be shared between llvmopt and; llvm-link-opt, but the wouldn't need to be shared... Thus compile time; could be faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Registe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:1877,Performance,optimiz,optimization,1877," faster, because we are using a ""smarter"" IR (SSA based). > BTW, about SGI, ""borrowing"" SSA-based optimizations from one compiler and; > putting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optim",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2088,Performance,optimiz,optimization,2088,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2138,Performance,optimiz,optimizations,2138,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2215,Performance,optimiz,optimizations,2215,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2734,Performance,optimiz,optimization,2734,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2851,Performance,optimiz,optimization,2851,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2426,Safety,avoid,avoid,2426,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt:2914,Usability,usab,usable,2914,"tting it into another is not necessarily easier than re-doing it.; > Optimization code is usually heavily tied in to the specific IR they use. Understood. The only reason that I brought this up is because SGI's IR is; more similar to LLVM than it is different in many respects (SSA based,; relatively low level, etc), and could be easily adapted. Also their; optimizations are written in C++ and are actually somewhat; structured... of course it would be no walk in the park, but it would be; much less time consuming to adapt, say, SSA-PRE than to rewrite it. > But your larger point is valid that adding SSA based optimizations is; > feasible and should be fun. (Again, link time cost is the issue.). Assuming linktime cost wasn't an issue, the question is: ; Does using GCC's backend buy us anything?. > It also occurs to me that GCC is probably doing quite a bit of back-end; > optimization (step 16 in your list). Do you have a breakdown of that?. Not really. The irritating part of GCC is that it mixes it all up and; doesn't have a clean separation of concerns. A lot of the ""back end; optimization"" happens right along with other data optimizations (ie, CSE; of machine specific things). As far as REAL back end optimizations go, it looks something like this:. 1. Instruction combination: try to make CISCy instructions, if available; 2. Register movement: try to get registers in the right places for the; architecture to avoid register to register moves. For example, try to get; the first argument of a function to naturally land in %o0 for sparc.; 3. Instruction scheduling: 'nuff said :); 4. Register class preferencing: ??; 5. Local register allocation; 6. global register allocation; 7. Spilling; 8. Local regalloc; 9. Jump optimization; 10. Delay slot scheduling; 11. Branch shorting for CISC machines; 12. Instruction selection & peephole optimization; 13. Debug info output. But none of this would be usable for LLVM anyways, unless we were using; GCC as a static compiler. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-01-GCCOptimizations2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:261,Modifiability,inherit,inheritance,261,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:457,Performance,optimiz,optimization,457,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:540,Performance,load,loading,540,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:734,Performance,optimiz,optimization,734,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:1169,Performance,optimiz,optimization,1169,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:1269,Performance,optimiz,optimize,1269,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:1375,Performance,optimiz,optimization,1375,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt:405,Usability,simpl,simple,405,"Date: Wed, 20 Jun 2001 12:32:22 -0500; From: Vikram Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: .NET vs. our VM. One significant difference between .NET CLR and our VM is that the CLR; includes full information about classes and inheritance. In fact, I just; sat through the paper on adding templates to .NET CLR, and the speaker; indicated that the goal seems to be to do simple static compilation (very; little lowering or optimization). Also, the templates implementation in CLR; ""relies on dynamic class loading and JIT compilation"". This is an important difference because I think there are some significant; advantages to have a much lower level VM layer, and do significant static; analysis and optimization. I also talked to the lead guy for KAI's C++ compiler (Arch Robison) and he; said that SGI and other commercial compilers have included options to export; their *IR* next to the object code (i.e., .il files) and use them for; link-time code generation. In fact, he said that the .o file was nearly; empty and was entirely generated from the .il at link-time. But he agreed; that this limited the link-time interprocedural optimization to modules; compiled by the same compiler, whereas our approach allows us to link and; optimize modules from multiple different compilers. (Also, of course, they; don't do anything for runtime optimization). All issues to bring up in Related Work. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-06-20-.NET-Differences.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt:952,Modifiability,portab,portability,952,"Date: Fri, 6 Jul 2001 16:56:56 -0500; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: lowering the IR. BTW, I do think that we should consider lowering the IR as you said. I; didn't get time to raise it today, but it comes up with the SPARC; move-conditional instruction. I don't think we want to put that in the core; VM -- it is a little too specialized. But without a corresponding; conditional move instruction in the VM, it is pretty difficult to maintain a; close mapping between VM and machine code. Other architectures may have; other such instructions. What I was going to suggest was that for a particular processor, we define; additional VM instructions that match some of the unusual opcodes on the; processor but have VM semantics otherwise, i.e., all operands are in SSA; form and typed. This means that we can re-generate core VM code from the; more specialized code any time we want (so that portability is not lost). Typically, a static compiler like gcc would generate just the core VM, which; is relatively portable. Anyone (an offline tool, the linker, etc., or even; the static compiler itself if it chooses) can transform that into more; specialized target-specific VM code for a particular architecture. If the; linker does it, it can do it after all machine-independent optimizations.; This would be the most convenient, but not necessary. The main benefit of lowering will be that we will be able to retain a close; mapping between VM and machine code. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt:1070,Modifiability,portab,portable,1070,"Date: Fri, 6 Jul 2001 16:56:56 -0500; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: lowering the IR. BTW, I do think that we should consider lowering the IR as you said. I; didn't get time to raise it today, but it comes up with the SPARC; move-conditional instruction. I don't think we want to put that in the core; VM -- it is a little too specialized. But without a corresponding; conditional move instruction in the VM, it is pretty difficult to maintain a; close mapping between VM and machine code. Other architectures may have; other such instructions. What I was going to suggest was that for a particular processor, we define; additional VM instructions that match some of the unusual opcodes on the; processor but have VM semantics otherwise, i.e., all operands are in SSA; form and typed. This means that we can re-generate core VM code from the; more specialized code any time we want (so that portability is not lost). Typically, a static compiler like gcc would generate just the core VM, which; is relatively portable. Anyone (an offline tool, the linker, etc., or even; the static compiler itself if it chooses) can transform that into more; specialized target-specific VM code for a particular architecture. If the; linker does it, it can do it after all machine-independent optimizations.; This would be the most convenient, but not necessary. The main benefit of lowering will be that we will be able to retain a close; mapping between VM and machine code. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt:1338,Performance,optimiz,optimizations,1338,"Date: Fri, 6 Jul 2001 16:56:56 -0500; From: Vikram S. Adve <vadve@cs.uiuc.edu>; To: Chris Lattner <lattner@cs.uiuc.edu>; Subject: lowering the IR. BTW, I do think that we should consider lowering the IR as you said. I; didn't get time to raise it today, but it comes up with the SPARC; move-conditional instruction. I don't think we want to put that in the core; VM -- it is a little too specialized. But without a corresponding; conditional move instruction in the VM, it is pretty difficult to maintain a; close mapping between VM and machine code. Other architectures may have; other such instructions. What I was going to suggest was that for a particular processor, we define; additional VM instructions that match some of the unusual opcodes on the; processor but have VM semantics otherwise, i.e., all operands are in SSA; form and typed. This means that we can re-generate core VM code from the; more specialized code any time we want (so that portability is not lost). Typically, a static compiler like gcc would generate just the core VM, which; is relatively portable. Anyone (an offline tool, the linker, etc., or even; the static compiler itself if it chooses) can transform that into more; specialized target-specific VM code for a particular architecture. If the; linker does it, it can do it after all machine-independent optimizations.; This would be the most convenient, but not necessary. The main benefit of lowering will be that we will be able to retain a close; mapping between VM and machine code. --Vikram. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-07-06-LoweringIRForCodeGen.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:790,Energy Efficiency,reduce,reduce,790,"Date: Tue, 18 Sep 2001 00:38:37 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Idea for a simple, useful link time optimization. In C++ programs, exceptions suck, and here's why:. 1. In virtually all function calls, you must assume that the function; throws an exception, unless it is defined as 'nothrow'. This means; that every function call has to have code to invoke dtors on objects; locally if one is thrown by the function. Most functions don't throw; exceptions, so this code is dead [with all the bad effects of dead; code, including icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:170,Performance,optimiz,optimization,170,"Date: Tue, 18 Sep 2001 00:38:37 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Idea for a simple, useful link time optimization. In C++ programs, exceptions suck, and here's why:. 1. In virtually all function calls, you must assume that the function; throws an exception, unless it is defined as 'nothrow'. This means; that every function call has to have code to invoke dtors on objects; locally if one is thrown by the function. Most functions don't throw; exceptions, so this code is dead [with all the bad effects of dead; code, including icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:1656,Performance,optimiz,optimization,1656,"ng icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and could have decent; payoffs... without being a overly complex optimization. After I wrote all of that, I found this page that is talking about; basically the same thing I just wrote, except that it is translation unit; at a time, tree based approach:; http://www.ocston.org/~jls/ehopt.html. but is very useful from ""expected gain"" and references perspective. Note; that their compiler is apparently unable to inline functions that use; exceptions, so there numbers are pretty worthless... also our results; would (hopefully) be better because it's interprocedural... What do you think?. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:1757,Performance,optimiz,optimized,1757,"ng icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and could have decent; payoffs... without being a overly complex optimization. After I wrote all of that, I found this page that is talking about; basically the same thing I just wrote, except that it is translation unit; at a time, tree based approach:; http://www.ocston.org/~jls/ehopt.html. but is very useful from ""expected gain"" and references perspective. Note; that their compiler is apparently unable to inline functions that use; exceptions, so there numbers are pretty worthless... also our results; would (hopefully) be better because it's interprocedural... What do you think?. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:1899,Performance,optimiz,optimization,1899,"ng icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and could have decent; payoffs... without being a overly complex optimization. After I wrote all of that, I found this page that is talking about; basically the same thing I just wrote, except that it is translation unit; at a time, tree based approach:; http://www.ocston.org/~jls/ehopt.html. but is very useful from ""expected gain"" and references perspective. Note; that their compiler is apparently unable to inline functions that use; exceptions, so there numbers are pretty worthless... also our results; would (hopefully) be better because it's interprocedural... What do you think?. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:2062,Performance,optimiz,optimization,2062,"ng icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and could have decent; payoffs... without being a overly complex optimization. After I wrote all of that, I found this page that is talking about; basically the same thing I just wrote, except that it is translation unit; at a time, tree based approach:; http://www.ocston.org/~jls/ehopt.html. but is very useful from ""expected gain"" and references perspective. Note; that their compiler is apparently unable to inline functions that use; exceptions, so there numbers are pretty worthless... also our results; would (hopefully) be better because it's interprocedural... What do you think?. -Chris. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:145,Usability,simpl,simple,145,"Date: Tue, 18 Sep 2001 00:38:37 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: Vikram S. Adve <vadve@cs.uiuc.edu>; Subject: Idea for a simple, useful link time optimization. In C++ programs, exceptions suck, and here's why:. 1. In virtually all function calls, you must assume that the function; throws an exception, unless it is defined as 'nothrow'. This means; that every function call has to have code to invoke dtors on objects; locally if one is thrown by the function. Most functions don't throw; exceptions, so this code is dead [with all the bad effects of dead; code, including icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt:1469,Usability,simpl,simplistic,1469,"tors on objects; locally if one is thrown by the function. Most functions don't throw; exceptions, so this code is dead [with all the bad effects of dead; code, including icache pollution].; 2. Declaring a function nothrow causes catch blocks to be added to every; call that isnot provably nothrow. This makes them very slow.; 3. Extra extraneous exception edges reduce the opportunity for code; motion.; 4. EH is typically implemented with large lookup tables. Ours is going to; be much smaller (than the ""standard"" way of doing it) to start with,; but eliminating it entirely would be nice. :); 5. It is physically impossible to correctly put (accurate, correct); exception specifications on generic, templated code. But it is trivial; to analyze instantiations of said code.; 6. Most large C++ programs throw few exceptions. Most well designed; programs only throw exceptions in specific planned portions of the; code. Given our _planned_ model of handling exceptions, all of this would be; pretty trivial to eliminate through some pretty simplistic interprocedural; analysis. The DCE factor alone could probably be pretty significant. The; extra code motion opportunities could also be exploited though... Additionally, this optimization can be implemented in a straight forward; conservative manner, allowing libraries to be optimized or individual; files even (if there are leaf functions visible in the translation unit; that are called). I think it's a reasonable optimization that hasn't really been addressed; (because assembly is way too low level for this), and could have decent; payoffs... without being a overly complex optimization. After I wrote all of that, I found this page that is talking about; basically the same thing I just wrote, except that it is translation unit; at a time, tree based approach:; http://www.ocston.org/~jls/ehopt.html. but is very useful from ""expected gain"" and references perspective. Note; that their compiler is apparently unable to inline functions t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2001-09-18-OptimizeExceptions.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt:1633,Integrability,depend,depends,1633,"rate over ; the instlist, and access the instructions through the instlist. To add or remove an instruction from a basic block, we need to get an ; iterator to an instruction, which, given just an Instruction*, requires a ; linear search of the basic block the instruction is contained in... just ; to insert an instruction before another instruction, or to delete an ; instruction! This complicates algorithms that should be very simple (like ; simple constant propagation), because they aren't actually sparse anymore,; they have to traverse basic blocks to remove constant propogated ; instructions. Additionally, adding or removing instructions to a basic block ; _invalidates all iterators_ pointing into that block, which is really ; irritating. To fix these problems (and others), I would like to make the ordering of; the instructions be represented with a doubly linked list in the; instructions themselves, instead of an external data structure. This is ; how many other representations do it, and frankly I can't remember why I ; originally implemented it the way I did. Long term, all of the code that depends on the nasty features in the ; instruction list (which can be found by grep'ing for getInstList()) will ; be changed to do nice local transformations. In the short term, I'll ; change the representation, but preserve the interface (including ; getInstList()) so that all of the code doesn't have to change. Iteration over the instructions in a basic block remains the simple:; for (BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; ++I) ... But we will also support:; for (Instruction *I = BB->front(); I; I = I->getNext()) ... After converting instructions over, I'll convert basic blocks and ; functions to have a similar interface. The only negative aspect of this change that I see is that it increases ; the amount of memory consumed by one pointer per instruction. Given the ; benefits, I think this is a very reasonable tradeoff. . What do you think?. -Chris; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt:1862,Integrability,interface,interface,1862,"rate over ; the instlist, and access the instructions through the instlist. To add or remove an instruction from a basic block, we need to get an ; iterator to an instruction, which, given just an Instruction*, requires a ; linear search of the basic block the instruction is contained in... just ; to insert an instruction before another instruction, or to delete an ; instruction! This complicates algorithms that should be very simple (like ; simple constant propagation), because they aren't actually sparse anymore,; they have to traverse basic blocks to remove constant propogated ; instructions. Additionally, adding or removing instructions to a basic block ; _invalidates all iterators_ pointing into that block, which is really ; irritating. To fix these problems (and others), I would like to make the ordering of; the instructions be represented with a doubly linked list in the; instructions themselves, instead of an external data structure. This is ; how many other representations do it, and frankly I can't remember why I ; originally implemented it the way I did. Long term, all of the code that depends on the nasty features in the ; instruction list (which can be found by grep'ing for getInstList()) will ; be changed to do nice local transformations. In the short term, I'll ; change the representation, but preserve the interface (including ; getInstList()) so that all of the code doesn't have to change. Iteration over the instructions in a basic block remains the simple:; for (BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; ++I) ... But we will also support:; for (Instruction *I = BB->front(); I; I = I->getNext()) ... After converting instructions over, I'll convert basic blocks and ; functions to have a similar interface. The only negative aspect of this change that I see is that it increases ; the amount of memory consumed by one pointer per instruction. Given the ; benefits, I think this is a very reasonable tradeoff. . What do you think?. -Chris; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt:2276,Integrability,interface,interface,2276,"rate over ; the instlist, and access the instructions through the instlist. To add or remove an instruction from a basic block, we need to get an ; iterator to an instruction, which, given just an Instruction*, requires a ; linear search of the basic block the instruction is contained in... just ; to insert an instruction before another instruction, or to delete an ; instruction! This complicates algorithms that should be very simple (like ; simple constant propagation), because they aren't actually sparse anymore,; they have to traverse basic blocks to remove constant propogated ; instructions. Additionally, adding or removing instructions to a basic block ; _invalidates all iterators_ pointing into that block, which is really ; irritating. To fix these problems (and others), I would like to make the ordering of; the instructions be represented with a doubly linked list in the; instructions themselves, instead of an external data structure. This is ; how many other representations do it, and frankly I can't remember why I ; originally implemented it the way I did. Long term, all of the code that depends on the nasty features in the ; instruction list (which can be found by grep'ing for getInstList()) will ; be changed to do nice local transformations. In the short term, I'll ; change the representation, but preserve the interface (including ; getInstList()) so that all of the code doesn't have to change. Iteration over the instructions in a basic block remains the simple:; for (BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; ++I) ... But we will also support:; for (Instruction *I = BB->front(); I; I = I->getNext()) ... After converting instructions over, I'll convert basic blocks and ; functions to have a similar interface. The only negative aspect of this change that I see is that it increases ; the amount of memory consumed by one pointer per instruction. Given the ; benefits, I think this is a very reasonable tradeoff. . What do you think?. -Chris; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt:549,Security,access,access,549,"Date: Sun, 12 May 2002 17:12:53 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: ""Vikram S. Adve"" <vadve@cs.uiuc.edu>; Subject: LLVM change. There is a fairly fundemental change that I would like to make to the LLVM ; infrastructure, but I'd like to know if you see any drawbacks that I ; don't... Basically right now at the basic block level, each basic block contains an ; instruction list (returned by getInstList()) that is a ValueHolder of ; instructions. To iterate over instructions, we must actually iterate over ; the instlist, and access the instructions through the instlist. To add or remove an instruction from a basic block, we need to get an ; iterator to an instruction, which, given just an Instruction*, requires a ; linear search of the basic block the instruction is contained in... just ; to insert an instruction before another instruction, or to delete an ; instruction! This complicates algorithms that should be very simple (like ; simple constant propagation), because they aren't actually sparse anymore,; they have to traverse basic blocks to remove constant propogated ; instructions. Additionally, adding or removing instructions to a basic block ; _invalidates all iterators_ pointing into that block, which is really ; irritating. To fix these problems (and others), I would like to make the ordering of; the instructions be represented with a doubly linked list in the; instructions themselves, instead of an external data structure. This is ; how many other representations do it, and frankly I can't remember why I ; originally implemented it the way I did. Long term, all of the code that depends on the nasty features in the ; instruction list (which can be found by grep'ing for getInstList()) will ; be changed to do nice local transformations. In the short term, I'll ; change the representation, but preserve the interface (including ; getInstList()) so that all of the code doesn't have to change. Iteration over the instructions in a basic block rema",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt:950,Usability,simpl,simple,950,"Date: Sun, 12 May 2002 17:12:53 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: ""Vikram S. Adve"" <vadve@cs.uiuc.edu>; Subject: LLVM change. There is a fairly fundemental change that I would like to make to the LLVM ; infrastructure, but I'd like to know if you see any drawbacks that I ; don't... Basically right now at the basic block level, each basic block contains an ; instruction list (returned by getInstList()) that is a ValueHolder of ; instructions. To iterate over instructions, we must actually iterate over ; the instlist, and access the instructions through the instlist. To add or remove an instruction from a basic block, we need to get an ; iterator to an instruction, which, given just an Instruction*, requires a ; linear search of the basic block the instruction is contained in... just ; to insert an instruction before another instruction, or to delete an ; instruction! This complicates algorithms that should be very simple (like ; simple constant propagation), because they aren't actually sparse anymore,; they have to traverse basic blocks to remove constant propogated ; instructions. Additionally, adding or removing instructions to a basic block ; _invalidates all iterators_ pointing into that block, which is really ; irritating. To fix these problems (and others), I would like to make the ordering of; the instructions be represented with a doubly linked list in the; instructions themselves, instead of an external data structure. This is ; how many other representations do it, and frankly I can't remember why I ; originally implemented it the way I did. Long term, all of the code that depends on the nasty features in the ; instruction list (which can be found by grep'ing for getInstList()) will ; be changed to do nice local transformations. In the short term, I'll ; change the representation, but preserve the interface (including ; getInstList()) so that all of the code doesn't have to change. Iteration over the instructions in a basic block rema",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt:965,Usability,simpl,simple,965,"Date: Sun, 12 May 2002 17:12:53 -0500 (CDT); From: Chris Lattner <sabre@nondot.org>; To: ""Vikram S. Adve"" <vadve@cs.uiuc.edu>; Subject: LLVM change. There is a fairly fundemental change that I would like to make to the LLVM ; infrastructure, but I'd like to know if you see any drawbacks that I ; don't... Basically right now at the basic block level, each basic block contains an ; instruction list (returned by getInstList()) that is a ValueHolder of ; instructions. To iterate over instructions, we must actually iterate over ; the instlist, and access the instructions through the instlist. To add or remove an instruction from a basic block, we need to get an ; iterator to an instruction, which, given just an Instruction*, requires a ; linear search of the basic block the instruction is contained in... just ; to insert an instruction before another instruction, or to delete an ; instruction! This complicates algorithms that should be very simple (like ; simple constant propagation), because they aren't actually sparse anymore,; they have to traverse basic blocks to remove constant propogated ; instructions. Additionally, adding or removing instructions to a basic block ; _invalidates all iterators_ pointing into that block, which is really ; irritating. To fix these problems (and others), I would like to make the ordering of; the instructions be represented with a doubly linked list in the; instructions themselves, instead of an external data structure. This is ; how many other representations do it, and frankly I can't remember why I ; originally implemented it the way I did. Long term, all of the code that depends on the nasty features in the ; instruction list (which can be found by grep'ing for getInstList()) will ; be changed to do nice local transformations. In the short term, I'll ; change the representation, but preserve the interface (including ; getInstList()) so that all of the code doesn't have to change. Iteration over the instructions in a basic block rema",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt:2009,Usability,simpl,simple,2009,"rate over ; the instlist, and access the instructions through the instlist. To add or remove an instruction from a basic block, we need to get an ; iterator to an instruction, which, given just an Instruction*, requires a ; linear search of the basic block the instruction is contained in... just ; to insert an instruction before another instruction, or to delete an ; instruction! This complicates algorithms that should be very simple (like ; simple constant propagation), because they aren't actually sparse anymore,; they have to traverse basic blocks to remove constant propogated ; instructions. Additionally, adding or removing instructions to a basic block ; _invalidates all iterators_ pointing into that block, which is really ; irritating. To fix these problems (and others), I would like to make the ordering of; the instructions be represented with a doubly linked list in the; instructions themselves, instead of an external data structure. This is ; how many other representations do it, and frankly I can't remember why I ; originally implemented it the way I did. Long term, all of the code that depends on the nasty features in the ; instruction list (which can be found by grep'ing for getInstList()) will ; be changed to do nice local transformations. In the short term, I'll ; change the representation, but preserve the interface (including ; getInstList()) so that all of the code doesn't have to change. Iteration over the instructions in a basic block remains the simple:; for (BasicBlock::iterator I = BB->begin(), E = BB->end(); I != E; ++I) ... But we will also support:; for (Instruction *I = BB->front(); I; I = I->getNext()) ... After converting instructions over, I'll convert basic blocks and ; functions to have a similar interface. The only negative aspect of this change that I see is that it increases ; the amount of memory consumed by one pointer per instruction. Given the ; benefits, I think this is a very reasonable tradeoff. . What do you think?. -Chris; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-05-12-InstListChange.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt:453,Availability,failure,failure,453,"Changes:; * Change the casting code to be const correct. Now, doing this is invalid:; const Value *V = ...;; Instruction *I = dyn_cast<Instruction>(V);; instead, the second line should be:; const Instruction *I = dyn_cast<Instruction>(V);. * Change the casting code to allow casting a reference value thus:; const Value &V = ...;; Instruction &I = cast<Instruction>(V);. dyn_cast does not work with references, because it must return a null pointer; on failure. * Fundamentally change how instructions and other values are represented.; Before, every llvm container was an instance of the ValueHolder template,; instantiated for each container type. This ValueHolder was effectively a; wrapper around a vector of pointers to the sub-objects. Now, instead of having a vector to pointers of objects, the objects are; maintained in a doubly linked list of values (ie each Instruction now has; Next & Previous fields). The containers are now instances of ilist (intrusive; linked list class), which use the next and previous fields to chain them; together. The advantage of this implementation is that iterators can be; formed directly from pointers to the LLVM value, and invalidation is much; easier to handle. * As part of the above change, dereferencing an iterator (for example:; BasicBlock::iterator) now produces a reference to the underlying type (same; example: Instruction&) instead of a pointer to the underlying object. This; makes it much easier to write nested loops that iterator over things, changing; this:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = (*BI)->begin(); II != (*BI)->end(); ++II); (*II)->dump();. into:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = BI->begin(); II != BI->end(); ++II); II->dump();. which is much more natural and what users expect. * Simplification of #include's: Before, it was necessary for a .cpp file to; include every .h file that it used.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt:686,Integrability,wrap,wrapper,686,"Changes:; * Change the casting code to be const correct. Now, doing this is invalid:; const Value *V = ...;; Instruction *I = dyn_cast<Instruction>(V);; instead, the second line should be:; const Instruction *I = dyn_cast<Instruction>(V);. * Change the casting code to allow casting a reference value thus:; const Value &V = ...;; Instruction &I = cast<Instruction>(V);. dyn_cast does not work with references, because it must return a null pointer; on failure. * Fundamentally change how instructions and other values are represented.; Before, every llvm container was an instance of the ValueHolder template,; instantiated for each container type. This ValueHolder was effectively a; wrapper around a vector of pointers to the sub-objects. Now, instead of having a vector to pointers of objects, the objects are; maintained in a doubly linked list of values (ie each Instruction now has; Next & Previous fields). The containers are now instances of ilist (intrusive; linked list class), which use the next and previous fields to chain them; together. The advantage of this implementation is that iterators can be; formed directly from pointers to the LLVM value, and invalidation is much; easier to handle. * As part of the above change, dereferencing an iterator (for example:; BasicBlock::iterator) now produces a reference to the underlying type (same; example: Instruction&) instead of a pointer to the underlying object. This; makes it much easier to write nested loops that iterator over things, changing; this:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = (*BI)->begin(); II != (*BI)->end(); ++II); (*II)->dump();. into:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = BI->begin(); II != BI->end(); ++II); II->dump();. which is much more natural and what users expect. * Simplification of #include's: Before, it was necessary for a .cpp file to; include every .h file that it used.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt:3135,Security,access,accessing,3135,"a reference to the underlying type (same; example: Instruction&) instead of a pointer to the underlying object. This; makes it much easier to write nested loops that iterator over things, changing; this:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = (*BI)->begin(); II != (*BI)->end(); ++II); (*II)->dump();. into:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = BI->begin(); II != BI->end(); ++II); II->dump();. which is much more natural and what users expect. * Simplification of #include's: Before, it was necessary for a .cpp file to; include every .h file that it used. Now things are batched a little bit more; to make it easier to use. Specifically, the include graph now includes these; edges:; Module.h -> Function.h, GlobalVariable.h; Function.h -> BasicBlock.h, Argument.h; BasicBlock.h -> Instruction.h. Which means that #including Function.h is usually sufficient for getting the; lower level #includes. * Printing out a Value* has now changed: Printing a Value* will soon print out; the address of the value instead of the contents of the Value. To print out; the contents, you must convert it to a reference with (for example); 'cout << *I' instead of 'cout << I;'. This conversion is not yet complete,; but will be eventually. In the mean time, both forms print out the contents. * References are used much more throughout the code base. In general, if a; pointer is known to never be null, it is passed in as a reference instead of a; pointer. For example, the instruction visitor class uses references instead; of pointers, and that Pass subclasses now all receive references to Values; instead of pointers, because they may never be null. * The Function class now has helper functions for accessing the Arguments list.; Instead of having to go through getArgumentList for simple things like; iterator over the arguments, now the a*() methods can be used to access them. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt:3303,Security,access,access,3303,"a reference to the underlying type (same; example: Instruction&) instead of a pointer to the underlying object. This; makes it much easier to write nested loops that iterator over things, changing; this:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = (*BI)->begin(); II != (*BI)->end(); ++II); (*II)->dump();. into:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = BI->begin(); II != BI->end(); ++II); II->dump();. which is much more natural and what users expect. * Simplification of #include's: Before, it was necessary for a .cpp file to; include every .h file that it used. Now things are batched a little bit more; to make it easier to use. Specifically, the include graph now includes these; edges:; Module.h -> Function.h, GlobalVariable.h; Function.h -> BasicBlock.h, Argument.h; BasicBlock.h -> Instruction.h. Which means that #including Function.h is usually sufficient for getting the; lower level #includes. * Printing out a Value* has now changed: Printing a Value* will soon print out; the address of the value instead of the contents of the Value. To print out; the contents, you must convert it to a reference with (for example); 'cout << *I' instead of 'cout << I;'. This conversion is not yet complete,; but will be eventually. In the mean time, both forms print out the contents. * References are used much more throughout the code base. In general, if a; pointer is known to never be null, it is passed in as a reference instead of a; pointer. For example, the instruction visitor class uses references instead; of pointers, and that Pass subclasses now all receive references to Values; instead of pointers, because they may never be null. * The Function class now has helper functions for accessing the Arguments list.; Instead of having to go through getArgumentList for simple things like; iterator over the arguments, now the a*() methods can be used to access them. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt:3218,Usability,simpl,simple,3218,"a reference to the underlying type (same; example: Instruction&) instead of a pointer to the underlying object. This; makes it much easier to write nested loops that iterator over things, changing; this:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = (*BI)->begin(); II != (*BI)->end(); ++II); (*II)->dump();. into:. for (Function::iterator BI = Func->begin(); BI != Func->end(); ++BI); for (BasicBlock::iterator II = BI->begin(); II != BI->end(); ++II); II->dump();. which is much more natural and what users expect. * Simplification of #include's: Before, it was necessary for a .cpp file to; include every .h file that it used. Now things are batched a little bit more; to make it easier to use. Specifically, the include graph now includes these; edges:; Module.h -> Function.h, GlobalVariable.h; Function.h -> BasicBlock.h, Argument.h; BasicBlock.h -> Instruction.h. Which means that #including Function.h is usually sufficient for getting the; lower level #includes. * Printing out a Value* has now changed: Printing a Value* will soon print out; the address of the value instead of the contents of the Value. To print out; the contents, you must convert it to a reference with (for example); 'cout << *I' instead of 'cout << I;'. This conversion is not yet complete,; but will be eventually. In the mean time, both forms print out the contents. * References are used much more throughout the code base. In general, if a; pointer is known to never be null, it is passed in as a reference instead of a; pointer. For example, the instruction visitor class uses references instead; of pointers, and that Pass subclasses now all receive references to Values; instead of pointers, because they may never be null. * The Function class now has helper functions for accessing the Arguments list.; Instead of having to go through getArgumentList for simple things like; iterator over the arguments, now the a*() methods can be used to access them. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2002-06-25-MegaPatchInfo.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:4941,Integrability,rout,routine,4941,"n the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_tri",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:5183,Integrability,rout,routine,5183,"ce cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_trigger() or countPath() call.). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:558,Performance,perform,perform,558,"Wed Jun 25 15:13:51 CDT 2003. First-level instrumentation; ---------------------------. We use opt to do Bytecode-to-bytecode instrumentation. Look at; back-edges and insert llvm_first_trigger() function call which takes; no arguments and no return value. This instrumentation is designed to; be easy to remove, for instance by writing a NOP over the function; call instruction. Keep count of every call to llvm_first_trigger(), and maintain; counters in a map indexed by return address. If the trigger count; exceeds a threshold, we identify a hot loop and perform second-level; instrumentation on the hot loop region (the instructions between the; target of the back-edge and the branch that causes the back-edge). We; do not move code across basic-block boundaries. Second-level instrumentation; ---------------------------. We remove the first-level instrumentation by overwriting the CALL to; llvm_first_trigger() with a NOP. The reoptimizer maintains a map between machine-code basic blocks and; LLVM BasicBlock*s. We only keep track of paths that start at the; first machine-code basic block of the hot loop region. How do we keep track of which edges to instrument, and which edges are; exits from the hot region? 3 step process. 1) Do a DFS from the first machine-code basic block of the hot loop; region and mark reachable edges. 2) Do a DFS from the last machine-code basic block of the hot loop; region IGNORING back edges, and mark the edges which are reachable in; 1) and also in 2) (i.e., must be reachable from both the start BB and; the end BB of the hot region). 3) Mark BBs which end in edges that exit the hot region; we need to; instrument these differently. Assume that there is 1 free register. On SPARC we use %g1, which LLC; has agreed not to use. Shift a 1 into it at the beginning. At every; edge which corresponds to a conditional branch, we shift 0 for not; taken and 1 for taken into a register. This uniquely numbers the paths; through the hot region. Silently fail if w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3116,Performance,queue,queue,3116,"f the countPath call. We keep track of the; number of iterations and the number of paths. We only run this; version 30 or 40 times. Find the BBs that total 90% or more of execution, and aggregate them; together to form our trace. But we do not allow more than 5 paths; if; we have more than 5 we take the ones that are executed the most. We; verify our assumption that we picked a hot back-edge in first-level; instrumentation, by making sure that the number of times we took an; exit edge from the hot trace is less than 10% of the number of; iterations. LLC has been taught to recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3378,Performance,queue,queue,3378,"e than 5 we take the ones that are executed the most. We; verify our assumption that we picked a hot back-edge in first-level; instrumentation, by making sure that the number of times we took an; exit edge from the hot trace is less than 10% of the number of; iterations. LLC has been taught to recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instruction",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3504,Performance,optimiz,optimized,3504," first-level; instrumentation, by making sure that the number of times we took an; exit edge from the hot trace is less than 10% of the number of; iterations. LLC has been taught to recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3559,Performance,cache,cache,3559," first-level; instrumentation, by making sure that the number of times we took an; exit edge from the hot trace is less than 10% of the number of; iterations. LLC has been taught to recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3659,Performance,optimiz,optimized,3659,"e hot trace is less than 10% of the number of; iterations. LLC has been taught to recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3681,Performance,optimiz,optimized,3681," recognize llvm_first_trigger() calls and NOT; generate saves and restores of caller-saved registers around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3817,Performance,cache,cache,3817,"egisters around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:3834,Performance,optimiz,optimized,3834,"egisters around these; calls. Phase behavior; --------------. We turn off llvm_first_trigger() calls with NOPs, but this would hide; phase behavior from us (when some funcs/traces stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:4074,Performance,cache,cache,4074,"es stop being hot and; others become hot.). We have a SIGALRM timer that counts time for us. Every time we get a; SIGALRM we look at our priority queue of locations where we have; removed llvm_first_trigger() calls. Each location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original sta",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:4160,Performance,optimiz,optimized,4160,"ach location is inserted along; with a time when we will next turn instrumentation back on for that; call site. If the time has arrived for a particular call site, we pop; that off the prio. queue and turn instrumentation back on for that; call site. Generating traces; -----------------. When we finally generate an optimized trace we first copy the code; into the trace cache. This leaves us with 3 copies of the code: the; original code, the instrumented code, and the optimized trace. The; optimized trace does not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion rout",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:4715,Performance,cache,cache,4715,"oes not have instrumentation. The original code and; the instrumented code are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous tr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:4814,Performance,cache,cache,4814,"are modified to have a branch to the trace; cache, where the optimized traces are kept. We copy the code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:4909,Performance,cache,cache,4909," code from the original to the instrumentation version; by tracing the LLVM-to-Machine code basic block map and then copying; each machine code basic block we think is in the hot region into the; trace cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:5294,Performance,optimiz,optimized,5294,"ce cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_trigger() or countPath() call.). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:5352,Performance,cache,cache,5352,"ce cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_trigger() or countPath() call.). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:5702,Performance,cache,cache,5702,"ce cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_trigger() or countPath() call.). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:5725,Performance,cache,cache,5725,"ce cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_trigger() or countPath() call.). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:5750,Performance,cache,cache-line-aligned,5750,"ce cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_trigger() or countPath() call.). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:5837,Performance,optimiz,optimized,5837,"ce cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_trigger() or countPath() call.). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt:5875,Performance,cache,caches,5875,"ce cache. Then we instrument that code. The process is similar for; generating the final optimized trace; we copy the same basic blocks; because we might need to put in fixup code for exit BBs. LLVM basic blocks are not typically used in the Reoptimizer except; for the mapping information. We are restricted to using single instructions to branch between the; original code, trace, and instrumented code. So we have to keep the; code copies in memory near the original code (they can't be far enough; away that a single pc-relative branch would not work.) Malloc() or; data region space is too far away. this impacts the design of the ; trace cache. We use a dummy function that is full of a bunch of for loops which we; overwrite with trace-cache code. The trace manager keeps track of; whether or not we have enough space in the trace cache, etc. The trace insertion routine takes an original start address, a vector; of machine instructions representing the trace, index of branches and; their corresponding absolute targets, and index of calls and their; corresponding absolute targets. The trace insertion routine is responsible for inserting branches from; the beginning of the original code to the beginning of the optimized; trace. This is because at some point the trace cache may run out of; space and it may have to evict a trace, at which point the branch to; the trace would also have to be removed. It uses a round-robin; replacement policy; we have found that this is almost as good as LRU; and better than random (especially because of problems fitting the new; trace in.). We cannot deal with discontiguous trace cache areas. The trace cache; is supposed to be cache-line-aligned, but it is not page-aligned. We generate instrumentation traces and optimized traces into separate; trace caches. We keep the instrumented code around because you don't; want to delete a trace when you still might have to return to it; (i.e., return from an llvm_first_trigger() or countPath() call.). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-25-Reoptimizer1.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:378,Deployability,update,update,378,"Thu Jun 26 14:43:04 CDT 2003. Information about BinInterface; ------------------------------. Take in a set of instructions with some particular register; allocation. It allows you to add, modify, or delete some instructions,; in SSA form (kind of like LLVM's MachineInstrs.) Then re-allocate; registers. It assumes that the transformations you are doing are safe.; It does not update the mapping information or the LLVM representation; for the modified trace (so it would not, for instance, support; multiple optimization passes; passes have to be aware of and update; manually the mapping information.). The way you use it is you take the original code and provide it to; BinInterface; then you do optimizations to it, then you put it in the; trace cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:562,Deployability,update,update,562,"Thu Jun 26 14:43:04 CDT 2003. Information about BinInterface; ------------------------------. Take in a set of instructions with some particular register; allocation. It allows you to add, modify, or delete some instructions,; in SSA form (kind of like LLVM's MachineInstrs.) Then re-allocate; registers. It assumes that the transformations you are doing are safe.; It does not update the mapping information or the LLVM representation; for the modified trace (so it would not, for instance, support; multiple optimization passes; passes have to be aware of and update; manually the mapping information.). The way you use it is you take the original code and provide it to; BinInterface; then you do optimizations to it, then you put it in the; trace cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:284,Energy Efficiency,allocate,allocate,284,"Thu Jun 26 14:43:04 CDT 2003. Information about BinInterface; ------------------------------. Take in a set of instructions with some particular register; allocation. It allows you to add, modify, or delete some instructions,; in SSA form (kind of like LLVM's MachineInstrs.) Then re-allocate; registers. It assumes that the transformations you are doing are safe.; It does not update the mapping information or the LLVM representation; for the modified trace (so it would not, for instance, support; multiple optimization passes; passes have to be aware of and update; manually the mapping information.). The way you use it is you take the original code and provide it to; BinInterface; then you do optimizations to it, then you put it in the; trace cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:1629,Energy Efficiency,allocate,allocated,1629," way you use it is you take the original code and provide it to; BinInterface; then you do optimizations to it, then you put it in the; trace cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is a demo of the Ball & Larus version that does NOT use 2-level; profiling. 1. Compile program with llvm-gcc.; 2. Run opt -lowerswitch -paths -emitfuncs on the bytecode.; -lowerswitch change switch statements to branches; -paths Ball & Larus path-profiling algorithm; -emitfuncs emit the table of functions; 3. Run llc to generate SPARC assembly code for the result of step 2.; 4. Use g++ to link the (instrumented) assembly code. We use a script to do all this:; ------------------------------------------------------------------------------; #!/bin/sh; llvm-gcc $1.c -o $1; opt -lowerswitch -paths -emitfuncs ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:1786,Modifiability,variab,variable-sized,1786,"e cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is a demo of the Ball & Larus version that does NOT use 2-level; profiling. 1. Compile program with llvm-gcc.; 2. Run opt -lowerswitch -paths -emitfuncs on the bytecode.; -lowerswitch change switch statements to branches; -paths Ball & Larus path-profiling algorithm; -emitfuncs emit the table of functions; 3. Run llc to generate SPARC assembly code for the result of step 2.; 4. Use g++ to link the (instrumented) assembly code. We use a script to do all this:; ------------------------------------------------------------------------------; #!/bin/sh; llvm-gcc $1.c -o $1; opt -lowerswitch -paths -emitfuncs $1.bc > $1.run.bc; llc -f $1.run.bc ; LIBS=$HOME/llvm_sparc/lib/Debug; GXX=/usr/dcs/software/evaluation/bin/g++; $GXX -g -L $LIBS $1.run.s -o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:510,Performance,optimiz,optimization,510,"Thu Jun 26 14:43:04 CDT 2003. Information about BinInterface; ------------------------------. Take in a set of instructions with some particular register; allocation. It allows you to add, modify, or delete some instructions,; in SSA form (kind of like LLVM's MachineInstrs.) Then re-allocate; registers. It assumes that the transformations you are doing are safe.; It does not update the mapping information or the LLVM representation; for the modified trace (so it would not, for instance, support; multiple optimization passes; passes have to be aware of and update; manually the mapping information.). The way you use it is you take the original code and provide it to; BinInterface; then you do optimizations to it, then you put it in the; trace cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:700,Performance,optimiz,optimizations,700,"Thu Jun 26 14:43:04 CDT 2003. Information about BinInterface; ------------------------------. Take in a set of instructions with some particular register; allocation. It allows you to add, modify, or delete some instructions,; in SSA form (kind of like LLVM's MachineInstrs.) Then re-allocate; registers. It assumes that the transformations you are doing are safe.; It does not update the mapping information or the LLVM representation; for the modified trace (so it would not, for instance, support; multiple optimization passes; passes have to be aware of and update; manually the mapping information.). The way you use it is you take the original code and provide it to; BinInterface; then you do optimizations to it, then you put it in the; trace cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:751,Performance,cache,cache,751,"Thu Jun 26 14:43:04 CDT 2003. Information about BinInterface; ------------------------------. Take in a set of instructions with some particular register; allocation. It allows you to add, modify, or delete some instructions,; in SSA form (kind of like LLVM's MachineInstrs.) Then re-allocate; registers. It assumes that the transformations you are doing are safe.; It does not update the mapping information or the LLVM representation; for the modified trace (so it would not, for instance, support; multiple optimization passes; passes have to be aware of and update; manually the mapping information.). The way you use it is you take the original code and provide it to; BinInterface; then you do optimizations to it, then you put it in the; trace cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt:359,Safety,safe,safe,359,"Thu Jun 26 14:43:04 CDT 2003. Information about BinInterface; ------------------------------. Take in a set of instructions with some particular register; allocation. It allows you to add, modify, or delete some instructions,; in SSA form (kind of like LLVM's MachineInstrs.) Then re-allocate; registers. It assumes that the transformations you are doing are safe.; It does not update the mapping information or the LLVM representation; for the modified trace (so it would not, for instance, support; multiple optimization passes; passes have to be aware of and update; manually the mapping information.). The way you use it is you take the original code and provide it to; BinInterface; then you do optimizations to it, then you put it in the; trace cache. The BinInterface tries to find live-outs for traces so that it can do; register allocation on just the trace, and stitch the trace back into; the original code. It has to preserve the live-ins and live-outs when; it does its register allocation. (On exits from the trace we have; epilogues that copy live-outs back into the right registers, but; live-ins have to be in the right registers.). Limitations of BinInterface; ---------------------------. It does copy insertions for PHIs, which it infers from the machine; code. The mapping info inserted by LLC is not sufficient to determine; the PHIs. It does not handle integer or floating-point condition codes and it; does not handle floating-point register allocation. It is not aggressively able to use lots of registers. There is a problem with alloca: we cannot find our spill space for; spilling registers, normally allocated on the stack, if the trace; follows an alloca(). What might be an acceptable solution would be to; disable trace generation on functions that have variable-sized; alloca()s. Variable-sized allocas in the trace would also probably; screw things up. Because of the FP and alloca limitations, the BinInterface is; completely disabled right now. Demo; ----. This is ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2003-06-26-Reoptimizer2.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:4808,Availability,error,errors,4808," In the future I hope this toolkit will grow to include new and interesting; components, including a C++ front-end, ObjC support, and a whole lot of other; things. Finally, it should be pointed out that the goal here is to build something that; is high-quality and industrial-strength: all the obnoxious features of the C; family must be correctly supported (trigraphs, preprocessor arcana, K&R-style; prototypes, GCC/MS extensions, etc). It cannot be used if it is not 'real'. II. Usage of clang driver:. * Basic Command-Line Options:; - Help: clang --help; - Standard GCC options accepted: -E, -I*, -i*, -pedantic, -std=c90, etc.; - To make diagnostics more gcc-like: -fno-caret-diagnostics -fno-show-column; - Enable metric printing: -stats. * -fsyntax-only is currently the default mode. * -E mode works the same way as GCC. * -Eonly mode does all preprocessing, but does not print the output,; useful for timing the preprocessor.; ; * -fsyntax-only is currently partially implemented, lacking some; semantic analysis (some errors and warnings are not produced). * -parse-noop parses code without building an AST. This is useful; for timing the cost of the parser without including AST building; time.; ; * -parse-ast builds ASTs, but doesn't print them. This is most; useful for timing AST building vs -parse-noop.; ; * -parse-ast-print pretty prints most expression and statements nodes. * -parse-ast-check checks that diagnostic messages that are expected; are reported and that those which are reported are expected. * -dump-cfg builds ASTs and then CFGs. CFGs are then pretty-printed. * -view-cfg builds ASTs and then CFGs. CFGs are then visualized by; invoking Graphviz. For more information on getting Graphviz to work with clang/LLVM,; see: https://llvm.org/docs/ProgrammersManual.html#ViewGraph. III. Current advantages over GCC:. * Column numbers are fully tracked (no 256 col limit, no GCC-style pruning).; * All diagnostics have column numbers, includes 'caret diagnostics', and they;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:1298,Integrability,depend,depends,1298," or goose.; 3. C-language family front-end toolkit. The world needs better compiler tools, tools which are built as libraries. This; design point allows reuse of the tools in new and novel ways. However, building; the tools as libraries isn't enough: they must have clean APIs, be as; decoupled from each other as possible, and be easy to modify/extend. This; requires clean layering, decent design, and avoiding tying the libraries to a; specific use. Oh yeah, did I mention that we want the resultant libraries to; be as fast as possible? :). This front-end is built as a component of the LLVM toolkit that can be used; with the LLVM backend or independently of it. In this spirit, the API has been; carefully designed as the following components:; ; libsupport - Basic support library, reused from LLVM. libsystem - System abstraction library, reused from LLVM.; ; libbasic - Diagnostics, SourceLocations, SourceBuffer abstraction,; file system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntacti",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:1533,Integrability,depend,depends,1533,"s as libraries isn't enough: they must have clean APIs, be as; decoupled from each other as possible, and be easy to modify/extend. This; requires clean layering, decent design, and avoiding tying the libraries to a; specific use. Oh yeah, did I mention that we want the resultant libraries to; be as fast as possible? :). This front-end is built as a component of the LLVM toolkit that can be used; with the LLVM backend or independently of it. In this spirit, the API has been; carefully designed as the following components:; ; libsupport - Basic support library, reused from LLVM. libsystem - System abstraction library, reused from LLVM.; ; libbasic - Diagnostics, SourceLocations, SourceBuffer abstraction,; file system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; highe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:1666,Integrability,depend,depends,1666,"requires clean layering, decent design, and avoiding tying the libraries to a; specific use. Oh yeah, did I mention that we want the resultant libraries to; be as fast as possible? :). This front-end is built as a component of the LLVM toolkit that can be used; with the LLVM backend or independently of it. In this spirit, the API has been; carefully designed as the following components:; ; libsupport - Basic support library, reused from LLVM. libsystem - System abstraction library, reused from LLVM.; ; libbasic - Diagnostics, SourceLocations, SourceBuffer abstraction,; file system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:1863,Integrability,depend,depends,1863,"end is built as a component of the LLVM toolkit that can be used; with the LLVM backend or independently of it. In this spirit, the API has been; carefully designed as the following components:; ; libsupport - Basic support library, reused from LLVM. libsystem - System abstraction library, reused from LLVM.; ; libbasic - Diagnostics, SourceLocations, SourceBuffer abstraction,; file system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. Thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2152,Integrability,depend,depending,2152,"- Basic support library, reused from LLVM. libsystem - System abstraction library, reused from LLVM.; ; libbasic - Diagnostics, SourceLocations, SourceBuffer abstraction,; file system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2193,Integrability,depend,depends,2193,"ourceLocations, SourceBuffer abstraction,; file system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentiona",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:3120,Integrability,depend,depends,3120,"ram' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; easy to reuse individual parts or replace pieces if desired. For example, to; build a preprocessor, you take the Basic and Lexer libraries. If you want an; indexer, you take those plus the Parser library and provide some actions for; indexing. If you want a refactoring, static analysis, or source-to-source; compiler tool, it makes sense to take those plus the AST building and semantic; analyzer library. Finally, if you want to use this with the LLVM backend,; you'd take these components plus the AST to LLVM lowering code.; ; In the future I hope this toolkit will grow to include new and interesting; components, including a C++ front-end, ObjC support, and a whole lot of other; things. Finally, it should be pointed out that the goal here is to build something that; is high-quality and industrial-strength: all the obnoxious features of the C; family must be correctly supported (tri",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:5216,Integrability,message,messages,5216,"f it is not 'real'. II. Usage of clang driver:. * Basic Command-Line Options:; - Help: clang --help; - Standard GCC options accepted: -E, -I*, -i*, -pedantic, -std=c90, etc.; - To make diagnostics more gcc-like: -fno-caret-diagnostics -fno-show-column; - Enable metric printing: -stats. * -fsyntax-only is currently the default mode. * -E mode works the same way as GCC. * -Eonly mode does all preprocessing, but does not print the output,; useful for timing the preprocessor.; ; * -fsyntax-only is currently partially implemented, lacking some; semantic analysis (some errors and warnings are not produced). * -parse-noop parses code without building an AST. This is useful; for timing the cost of the parser without including AST building; time.; ; * -parse-ast builds ASTs, but doesn't print them. This is most; useful for timing AST building vs -parse-noop.; ; * -parse-ast-print pretty prints most expression and statements nodes. * -parse-ast-check checks that diagnostic messages that are expected; are reported and that those which are reported are expected. * -dump-cfg builds ASTs and then CFGs. CFGs are then pretty-printed. * -view-cfg builds ASTs and then CFGs. CFGs are then visualized by; invoking Graphviz. For more information on getting Graphviz to work with clang/LLVM,; see: https://llvm.org/docs/ProgrammersManual.html#ViewGraph. III. Current advantages over GCC:. * Column numbers are fully tracked (no 256 col limit, no GCC-style pruning).; * All diagnostics have column numbers, includes 'caret diagnostics', and they; highlight regions of interesting code (e.g. the LHS and RHS of a binop).; * Full diagnostic customization by client (can format diagnostics however they; like, e.g. in an IDE or refactoring tool) through DiagnosticClient interface.; * Built as a framework, can be reused by multiple tools.; * All languages supported linked into same library (no cc1,cc1obj, ...).; * mmap's code in read-only, does not dirty the pages like GCC (mem footprint).; * LLVM Licens",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:6002,Integrability,interface,interface,6002,"; time.; ; * -parse-ast builds ASTs, but doesn't print them. This is most; useful for timing AST building vs -parse-noop.; ; * -parse-ast-print pretty prints most expression and statements nodes. * -parse-ast-check checks that diagnostic messages that are expected; are reported and that those which are reported are expected. * -dump-cfg builds ASTs and then CFGs. CFGs are then pretty-printed. * -view-cfg builds ASTs and then CFGs. CFGs are then visualized by; invoking Graphviz. For more information on getting Graphviz to work with clang/LLVM,; see: https://llvm.org/docs/ProgrammersManual.html#ViewGraph. III. Current advantages over GCC:. * Column numbers are fully tracked (no 256 col limit, no GCC-style pruning).; * All diagnostics have column numbers, includes 'caret diagnostics', and they; highlight regions of interesting code (e.g. the LHS and RHS of a binop).; * Full diagnostic customization by client (can format diagnostics however they; like, e.g. in an IDE or refactoring tool) through DiagnosticClient interface.; * Built as a framework, can be reused by multiple tools.; * All languages supported linked into same library (no cc1,cc1obj, ...).; * mmap's code in read-only, does not dirty the pages like GCC (mem footprint).; * LLVM License, can be linked into non-GPL projects.; * Full diagnostic control, per diagnostic. Diagnostics are identified by ID.; * Significantly faster than GCC at semantic analysis, parsing, preprocessing; and lexing.; * Defers exposing platform-specific stuff to as late as possible, tracks use of; platform-specific features (e.g. #ifdef PPC) to allow 'portable bytecodes'.; * The lexer doesn't rely on the ""lexer hack"": it has no notion of scope and; does not categorize identifiers as types or variables -- this is up to the; parser to decide. Potential Future Features:. * Fine grained diag control within the source (#pragma enable/disable warning).; * Better token tracking within macros? (Token came from this line, which is; a macro argumen",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:7164,Integrability,depend,depends,7164,"column numbers, includes 'caret diagnostics', and they; highlight regions of interesting code (e.g. the LHS and RHS of a binop).; * Full diagnostic customization by client (can format diagnostics however they; like, e.g. in an IDE or refactoring tool) through DiagnosticClient interface.; * Built as a framework, can be reused by multiple tools.; * All languages supported linked into same library (no cc1,cc1obj, ...).; * mmap's code in read-only, does not dirty the pages like GCC (mem footprint).; * LLVM License, can be linked into non-GPL projects.; * Full diagnostic control, per diagnostic. Diagnostics are identified by ID.; * Significantly faster than GCC at semantic analysis, parsing, preprocessing; and lexing.; * Defers exposing platform-specific stuff to as late as possible, tracks use of; platform-specific features (e.g. #ifdef PPC) to allow 'portable bytecodes'.; * The lexer doesn't rely on the ""lexer hack"": it has no notion of scope and; does not categorize identifiers as types or variables -- this is up to the; parser to decide. Potential Future Features:. * Fine grained diag control within the source (#pragma enable/disable warning).; * Better token tracking within macros? (Token came from this line, which is; a macro argument instantiated here, recursively instantiated here).; * Fast #import with a module system.; * Dependency tracking: change to header file doesn't recompile every function; that texually depends on it: recompile only those functions that need it.; This is aka 'incremental parsing'. IV. Missing Functionality / Improvements. Lexer:; * Source character mapping. GCC supports ASCII and UTF-8.; See GCC options: -ftarget-charset and -ftarget-wide-charset.; * Universal character support. Experimental in GCC, enabled with; -fextended-identifiers.; * -fpreprocessed mode. Preprocessor:; * #assert/#unassert; * MSExtension: ""L#param"" stringizes to a wide string literal.; * Add support for -M*. Traditional Preprocessor:; * Currently, we have none. :). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:659,Modifiability,extend,extend,659,"//===----------------------------------------------------------------------===//; // C Language Family Front-end; //===----------------------------------------------------------------------===//; Chris Lattner. I. Introduction:; ; clang: noun; 1. A loud, resonant, metallic sound.; 2. The strident call of a crane or goose.; 3. C-language family front-end toolkit. The world needs better compiler tools, tools which are built as libraries. This; design point allows reuse of the tools in new and novel ways. However, building; the tools as libraries isn't enough: they must have clean APIs, be as; decoupled from each other as possible, and be easy to modify/extend. This; requires clean layering, decent design, and avoiding tying the libraries to a; specific use. Oh yeah, did I mention that we want the resultant libraries to; be as fast as possible? :). This front-end is built as a component of the LLVM toolkit that can be used; with the LLVM backend or independently of it. In this spirit, the API has been; carefully designed as the following components:; ; libsupport - Basic support library, reused from LLVM. libsystem - System abstraction library, reused from LLVM.; ; libbasic - Diagnostics, SourceLocations, SourceBuffer abstraction,; file system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-le",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2579,Modifiability,refactor,refactoring,2579,"ry depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; easy to reuse individual parts or replace pieces if desired. For example, to; build a preprocessor, you take the Basic and Lexer libraries. If you want an; indexer, you take those plus the Parser library and provide some actions for; indexing. If you want a refactoring, static ana",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2692,Modifiability,variab,variables,2692,"s, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; easy to reuse individual parts or replace pieces if desired. For example, to; build a preprocessor, you take the Basic and Lexer libraries. If you want an; indexer, you take those plus the Parser library and provide some actions for; indexing. If you want a refactoring, static analysis, or source-to-source; compiler tool, it makes sense to take those plus the AST building and semantic; analyzer",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:3508,Modifiability,refactor,refactoring,3508,"code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; easy to reuse individual parts or replace pieces if desired. For example, to; build a preprocessor, you take the Basic and Lexer libraries. If you want an; indexer, you take those plus the Parser library and provide some actions for; indexing. If you want a refactoring, static analysis, or source-to-source; compiler tool, it makes sense to take those plus the AST building and semantic; analyzer library. Finally, if you want to use this with the LLVM backend,; you'd take these components plus the AST to LLVM lowering code.; ; In the future I hope this toolkit will grow to include new and interesting; components, including a C++ front-end, ObjC support, and a whole lot of other; things. Finally, it should be pointed out that the goal here is to build something that; is high-quality and industrial-strength: all the obnoxious features of the C; family must be correctly supported (trigraphs, preprocessor arcana, K&R-style; prototypes, GCC/MS extensions, etc). It cannot be used if it is not 'real'. II. Usage of clang driver:. * Basic Command-Line Options:; - Help: clang --help; - Standard GCC options accepted: -E, -I*, -i*, -pedantic, -std=c90, etc.; - To make diagnostics more gcc-like: -fno-caret-diagnostics -fno-show-column; - Enable metric printing: -stats. * -fsyntax-only is currently the default mode. * -",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:5959,Modifiability,refactor,refactoring,5959,"; time.; ; * -parse-ast builds ASTs, but doesn't print them. This is most; useful for timing AST building vs -parse-noop.; ; * -parse-ast-print pretty prints most expression and statements nodes. * -parse-ast-check checks that diagnostic messages that are expected; are reported and that those which are reported are expected. * -dump-cfg builds ASTs and then CFGs. CFGs are then pretty-printed. * -view-cfg builds ASTs and then CFGs. CFGs are then visualized by; invoking Graphviz. For more information on getting Graphviz to work with clang/LLVM,; see: https://llvm.org/docs/ProgrammersManual.html#ViewGraph. III. Current advantages over GCC:. * Column numbers are fully tracked (no 256 col limit, no GCC-style pruning).; * All diagnostics have column numbers, includes 'caret diagnostics', and they; highlight regions of interesting code (e.g. the LHS and RHS of a binop).; * Full diagnostic customization by client (can format diagnostics however they; like, e.g. in an IDE or refactoring tool) through DiagnosticClient interface.; * Built as a framework, can be reused by multiple tools.; * All languages supported linked into same library (no cc1,cc1obj, ...).; * mmap's code in read-only, does not dirty the pages like GCC (mem footprint).; * LLVM License, can be linked into non-GPL projects.; * Full diagnostic control, per diagnostic. Diagnostics are identified by ID.; * Significantly faster than GCC at semantic analysis, parsing, preprocessing; and lexing.; * Defers exposing platform-specific stuff to as late as possible, tracks use of; platform-specific features (e.g. #ifdef PPC) to allow 'portable bytecodes'.; * The lexer doesn't rely on the ""lexer hack"": it has no notion of scope and; does not categorize identifiers as types or variables -- this is up to the; parser to decide. Potential Future Features:. * Fine grained diag control within the source (#pragma enable/disable warning).; * Better token tracking within macros? (Token came from this line, which is; a macro argumen",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:6585,Modifiability,portab,portable,6585,"raph. III. Current advantages over GCC:. * Column numbers are fully tracked (no 256 col limit, no GCC-style pruning).; * All diagnostics have column numbers, includes 'caret diagnostics', and they; highlight regions of interesting code (e.g. the LHS and RHS of a binop).; * Full diagnostic customization by client (can format diagnostics however they; like, e.g. in an IDE or refactoring tool) through DiagnosticClient interface.; * Built as a framework, can be reused by multiple tools.; * All languages supported linked into same library (no cc1,cc1obj, ...).; * mmap's code in read-only, does not dirty the pages like GCC (mem footprint).; * LLVM License, can be linked into non-GPL projects.; * Full diagnostic control, per diagnostic. Diagnostics are identified by ID.; * Significantly faster than GCC at semantic analysis, parsing, preprocessing; and lexing.; * Defers exposing platform-specific stuff to as late as possible, tracks use of; platform-specific features (e.g. #ifdef PPC) to allow 'portable bytecodes'.; * The lexer doesn't rely on the ""lexer hack"": it has no notion of scope and; does not categorize identifiers as types or variables -- this is up to the; parser to decide. Potential Future Features:. * Fine grained diag control within the source (#pragma enable/disable warning).; * Better token tracking within macros? (Token came from this line, which is; a macro argument instantiated here, recursively instantiated here).; * Fast #import with a module system.; * Dependency tracking: change to header file doesn't recompile every function; that texually depends on it: recompile only those functions that need it.; This is aka 'incremental parsing'. IV. Missing Functionality / Improvements. Lexer:; * Source character mapping. GCC supports ASCII and UTF-8.; See GCC options: -ftarget-charset and -ftarget-wide-charset.; * Universal character support. Experimental in GCC, enabled with; -fextended-identifiers.; * -fpreprocessed mode. Preprocessor:; * #assert/#unassert; * M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:6728,Modifiability,variab,variables,6728,"pruning).; * All diagnostics have column numbers, includes 'caret diagnostics', and they; highlight regions of interesting code (e.g. the LHS and RHS of a binop).; * Full diagnostic customization by client (can format diagnostics however they; like, e.g. in an IDE or refactoring tool) through DiagnosticClient interface.; * Built as a framework, can be reused by multiple tools.; * All languages supported linked into same library (no cc1,cc1obj, ...).; * mmap's code in read-only, does not dirty the pages like GCC (mem footprint).; * LLVM License, can be linked into non-GPL projects.; * Full diagnostic control, per diagnostic. Diagnostics are identified by ID.; * Significantly faster than GCC at semantic analysis, parsing, preprocessing; and lexing.; * Defers exposing platform-specific stuff to as late as possible, tracks use of; platform-specific features (e.g. #ifdef PPC) to allow 'portable bytecodes'.; * The lexer doesn't rely on the ""lexer hack"": it has no notion of scope and; does not categorize identifiers as types or variables -- this is up to the; parser to decide. Potential Future Features:. * Fine grained diag control within the source (#pragma enable/disable warning).; * Better token tracking within macros? (Token came from this line, which is; a macro argument instantiated here, recursively instantiated here).; * Fast #import with a module system.; * Dependency tracking: change to header file doesn't recompile every function; that texually depends on it: recompile only those functions that need it.; This is aka 'incremental parsing'. IV. Missing Functionality / Improvements. Lexer:; * Source character mapping. GCC supports ASCII and UTF-8.; See GCC options: -ftarget-charset and -ftarget-wide-charset.; * Universal character support. Experimental in GCC, enabled with; -fextended-identifiers.; * -fpreprocessed mode. Preprocessor:; * #assert/#unassert; * MSExtension: ""L#param"" stringizes to a wide string literal.; * Add support for -M*. Traditional Preprocessor",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2245,Performance,scalab,scalable,2245,"e system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; ea",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2648,Performance,perform,performing,2648,"s, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; easy to reuse individual parts or replace pieces if desired. For example, to; build a preprocessor, you take the Basic and Lexer libraries. If you want an; indexer, you take those plus the Parser library and provide some actions for; indexing. If you want a refactoring, static analysis, or source-to-source; compiler tool, it makes sense to take those plus the AST building and semantic; analyzer",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:2997,Performance,optimiz,optimization,2997,"' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - Source-level dataflow analysis useful for performing analyses; such as computing live variables. It also includes a; path-sensitive ""graph-reachability"" engine for writing; analyses that reason about different possible paths of; execution through source code. This is currently being; employed to write a set of checks for finding bugs in software. libcodegen - Lower the AST to LLVM IR for optimization & codegen. Depends; on libast.; ; clang - An example driver, client of the libraries at various levels.; This depends on all these libraries, and on LLVM VMCore. This front-end has been intentionally built as a DAG of libraries, making it; easy to reuse individual parts or replace pieces if desired. For example, to; build a preprocessor, you take the Basic and Lexer libraries. If you want an; indexer, you take those plus the Parser library and provide some actions for; indexing. If you want a refactoring, static analysis, or source-to-source; compiler tool, it makes sense to take those plus the AST building and semantic; analyzer library. Finally, if you want to use this with the LLVM backend,; you'd take these components plus the AST to LLVM lowering code.; ; In the future I hope this toolkit will grow to include new and interesting; components, including a C++ front-end, ObjC support, and a whole lot of other; things. Finally, it should be pointed out that the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:717,Safety,avoid,avoiding,717,"//===----------------------------------------------------------------------===//; // C Language Family Front-end; //===----------------------------------------------------------------------===//; Chris Lattner. I. Introduction:; ; clang: noun; 1. A loud, resonant, metallic sound.; 2. The strident call of a crane or goose.; 3. C-language family front-end toolkit. The world needs better compiler tools, tools which are built as libraries. This; design point allows reuse of the tools in new and novel ways. However, building; the tools as libraries isn't enough: they must have clean APIs, be as; decoupled from each other as possible, and be easy to modify/extend. This; requires clean layering, decent design, and avoiding tying the libraries to a; specific use. Oh yeah, did I mention that we want the resultant libraries to; be as fast as possible? :). This front-end is built as a component of the LLVM toolkit that can be used; with the LLVM backend or independently of it. In this spirit, the API has been; carefully designed as the following components:; ; libsupport - Basic support library, reused from LLVM. libsystem - System abstraction library, reused from LLVM.; ; libbasic - Diagnostics, SourceLocations, SourceBuffer abstraction,; file system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-le",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:1611,Security,hash,hash,1611,"d from each other as possible, and be easy to modify/extend. This; requires clean layering, decent design, and avoiding tying the libraries to a; specific use. Oh yeah, did I mention that we want the resultant libraries to; be as fast as possible? :). This front-end is built as a component of the LLVM toolkit that can be used; with the LLVM backend or independently of it. In this spirit, the API has been; carefully designed as the following components:; ; libsupport - Basic support library, reused from LLVM. libsystem - System abstraction library, reused from LLVM.; ; libbasic - Diagnostics, SourceLocations, SourceBuffer abstraction,; file system caching for input source files. This depends on; libsupport and libsystem. libast - Provides classes to represent the C AST, the C type system,; builtin functions, and various helpers for analyzing and; manipulating the AST (visitors, pretty printers, etc). This; library depends on libbasic. liblex - C/C++/ObjC lexing and preprocessing, identifier hash table,; pragma handling, tokens, and macros. This depends on libbasic. libparse - C (for now) parsing and local semantic analysis. This library; invokes coarse-grained 'Actions' provided by the client to do; stuff (e.g. libsema builds ASTs). This depends on liblex. libsema - Provides a set of parser actions to build a standardized AST; for programs. AST's are 'streamed' out a top-level declaration; at a time, allowing clients to use decl-at-a-time processing,; build up entire translation units, or even build 'whole; program' ASTs depending on how they use the APIs. This depends; on libast and libparse. librewrite - Fast, scalable rewriting of source code. This operates on; the raw syntactic text of source code, allowing a client; to insert and delete text in very large source files using; the same source location information embedded in ASTs. This; is intended to be a low-level API that is useful for; higher-level clients and libraries such as code refactoring. libanalysis - S",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt:7563,Testability,assert,assert,7563,"column numbers, includes 'caret diagnostics', and they; highlight regions of interesting code (e.g. the LHS and RHS of a binop).; * Full diagnostic customization by client (can format diagnostics however they; like, e.g. in an IDE or refactoring tool) through DiagnosticClient interface.; * Built as a framework, can be reused by multiple tools.; * All languages supported linked into same library (no cc1,cc1obj, ...).; * mmap's code in read-only, does not dirty the pages like GCC (mem footprint).; * LLVM License, can be linked into non-GPL projects.; * Full diagnostic control, per diagnostic. Diagnostics are identified by ID.; * Significantly faster than GCC at semantic analysis, parsing, preprocessing; and lexing.; * Defers exposing platform-specific stuff to as late as possible, tracks use of; platform-specific features (e.g. #ifdef PPC) to allow 'portable bytecodes'.; * The lexer doesn't rely on the ""lexer hack"": it has no notion of scope and; does not categorize identifiers as types or variables -- this is up to the; parser to decide. Potential Future Features:. * Fine grained diag control within the source (#pragma enable/disable warning).; * Better token tracking within macros? (Token came from this line, which is; a macro argument instantiated here, recursively instantiated here).; * Fast #import with a module system.; * Dependency tracking: change to header file doesn't recompile every function; that texually depends on it: recompile only those functions that need it.; This is aka 'incremental parsing'. IV. Missing Functionality / Improvements. Lexer:; * Source character mapping. GCC supports ASCII and UTF-8.; See GCC options: -ftarget-charset and -ftarget-wide-charset.; * Universal character support. Experimental in GCC, enabled with; -fextended-identifiers.; * -fpreprocessed mode. Preprocessor:; * #assert/#unassert; * MSExtension: ""L#param"" stringizes to a wide string literal.; * Add support for -M*. Traditional Preprocessor:; * Currently, we have none. :). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/HistoricalNotes/2007-OriginalClangReadme.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt:521,Deployability,install,install,521,"if(LLVM_BYE_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_BYE_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(Bye; Bye.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt:30,Integrability,message,message,30,"if(LLVM_BYE_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_BYE_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(Bye; Bye.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt:138,Modifiability,plugin,plugin,138,"if(LLVM_BYE_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_BYE_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(Bye; Bye.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt:261,Modifiability,plugin,plugin,261,"if(LLVM_BYE_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_BYE_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(Bye; Bye.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt:249,Performance,load,loading,249,"if(LLVM_BYE_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_BYE_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(Bye; Bye.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt:104,Testability,test,testing,104,"if(LLVM_BYE_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_BYE_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(Bye; Bye.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Bye/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ExceptionDemo/CMakeLists.txt:168,Integrability,message,message,168,"set(LLVM_LINK_COMPONENTS; Core; ExecutionEngine; MC; MCJIT; RuntimeDyld; Support; Target; nativecodegen; ). # Enable EH and RTTI for this demo; if(NOT LLVM_ENABLE_EH); message(FATAL_ERROR ""ExceptionDemo must require EH.""); endif(). add_llvm_example(ExceptionDemo; ExceptionDemo.cpp; ). export_executable_symbols(ExceptionDemo); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/ExceptionDemo/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ExceptionDemo/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt:577,Deployability,install,install,577,"if(LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(ExampleIRTransforms; SimplifyCFG.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt:46,Integrability,message,message,46,"if(LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(ExampleIRTransforms; SimplifyCFG.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt:170,Modifiability,plugin,plugin,170,"if(LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(ExampleIRTransforms; SimplifyCFG.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt:293,Modifiability,plugin,plugin,293,"if(LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(ExampleIRTransforms; SimplifyCFG.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt:281,Performance,load,loading,281,"if(LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(ExampleIRTransforms; SimplifyCFG.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt:136,Testability,test,testing,136,"if(LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS); message(WARNING ""Setting LLVM_EXAMPLEIRTRANSFORMS_LINK_INTO_TOOLS=ON only makes sense for testing purpose""); endif(). # The plugin expects to not link against the Support and Core libraries,; # but expects them to exist in the process loading the plugin. This doesn't; # work with DLLs on Windows (where a shared library can't have undefined; # references), so just skip this example on Windows.; if (NOT WIN32 AND NOT CYGWIN); add_llvm_pass_plugin(ExampleIRTransforms; SimplifyCFG.cpp; DEPENDS; intrinsics_gen; BUILDTREE_ONLY; ). install(TARGETS ${name} RUNTIME DESTINATION ""${LLVM_EXAMPLES_INSTALL_DIR}""); set_target_properties(${name} PROPERTIES FOLDER ""Examples""); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/IRTransforms/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ModuleMaker/README.txt:224,Usability,simpl,simple,224,"//===----------------------------------------------------------------------===//; // ModuleMaker Sample project; //===----------------------------------------------------------------------===//. This project is an extremely simple example of using some simple pieces of the ; LLVM API. The actual executable generated by this project simply emits an ; LLVM bitcode file to standard output. It is designed to show some basic ; usage of LLVM APIs, and how to link to LLVM libraries.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/ModuleMaker/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ModuleMaker/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ModuleMaker/README.txt:253,Usability,simpl,simple,253,"//===----------------------------------------------------------------------===//; // ModuleMaker Sample project; //===----------------------------------------------------------------------===//. This project is an extremely simple example of using some simple pieces of the ; LLVM API. The actual executable generated by this project simply emits an ; LLVM bitcode file to standard output. It is designed to show some basic ; usage of LLVM APIs, and how to link to LLVM libraries.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/ModuleMaker/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ModuleMaker/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ModuleMaker/README.txt:334,Usability,simpl,simply,334,"//===----------------------------------------------------------------------===//; // ModuleMaker Sample project; //===----------------------------------------------------------------------===//. This project is an extremely simple example of using some simple pieces of the ; LLVM API. The actual executable generated by this project simply emits an ; LLVM bitcode file to standard output. It is designed to show some basic ; usage of LLVM APIs, and how to link to LLVM libraries.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/ModuleMaker/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ModuleMaker/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt:1028,Deployability,release,release,1028,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the third blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-08-02-object-caching-with-kaleidoscope/. The source code in this directory demonstrates the third version of the; program, now modified to accept an input IR file on the command line and,; optionally, to use a basic caching mechanism to store generated object images. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to support the input IR file command line option. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt:961,Modifiability,config,config,961,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the third blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-08-02-object-caching-with-kaleidoscope/. The source code in this directory demonstrates the third version of the; program, now modified to accept an input IR file on the command line and,; optionally, to use a basic caching mechanism to store generated object images. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to support the input IR file command line option. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt:1229,Performance,perform,performance,1229,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the third blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-08-02-object-caching-with-kaleidoscope/. The source code in this directory demonstrates the third version of the; program, now modified to accept an input IR file on the command line and,; optionally, to use a basic caching mechanism to store generated object images. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to support the input IR file command line option. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt:1190,Testability,test,test,1190,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the third blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-08-02-object-caching-with-kaleidoscope/. The source code in this directory demonstrates the third version of the; program, now modified to accept an input IR file on the command line and,; optionally, to use a basic caching mechanism to store generated object images. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to support the input IR file command line option. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt:1366,Testability,test,testing,1366,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the third blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-08-02-object-caching-with-kaleidoscope/. The source code in this directory demonstrates the third version of the; program, now modified to accept an input IR file on the command line and,; optionally, to use a basic caching mechanism to store generated object images. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to support the input IR file command line option. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt:768,Deployability,release,release,768,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The source code in this directory combines all previous versions, including the; old JIT-based implementation, into a single file for easy comparison with; command line options to select between the various possibilities. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt:701,Modifiability,config,config,701,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The source code in this directory combines all previous versions, including the; old JIT-based implementation, into a single file for easy comparison with; command line options to select between the various possibilities. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt:969,Performance,perform,performance,969,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The source code in this directory combines all previous versions, including the; old JIT-based implementation, into a single file for easy comparison with; command line options to select between the various possibilities. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt:930,Testability,test,test,930,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The source code in this directory combines all previous versions, including the; old JIT-based implementation, into a single file for easy comparison with; command line options to select between the various possibilities. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt:1106,Testability,test,testing,1106,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The source code in this directory combines all previous versions, including the; old JIT-based implementation, into a single file for easy comparison with; command line options to select between the various possibilities. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons. Another Python script will split generated input files into; definitions and function calls for the purpose of testing the IR input and; caching facilities.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/complete/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt:802,Deployability,release,release,802,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-22-using-mcjit-with-kaleidoscope-tutorial/. The source code in this directory demonstrates the initial working version of; the program before subsequent performance improvements are applied. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required, as mentioned in the blog posts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt:735,Modifiability,config,config,735,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-22-using-mcjit-with-kaleidoscope-tutorial/. The source code in this directory demonstrates the initial working version of; the program before subsequent performance improvements are applied. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required, as mentioned in the blog posts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt:634,Performance,perform,performance,634,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the first blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-22-using-mcjit-with-kaleidoscope-tutorial/. The source code in this directory demonstrates the initial working version of; the program before subsequent performance improvements are applied. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required, as mentioned in the blog posts.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/initial/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt:945,Deployability,release,release,945,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the second blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-29-kaleidoscope-performance-with-mcjit/. The source code in this directory demonstrates the second version of the; program, now modified to implement a sort of 'lazy' compilation. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to disable most stderr output for timing purposes. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt:878,Modifiability,config,config,878,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the second blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-29-kaleidoscope-performance-with-mcjit/. The source code in this directory demonstrates the second version of the; program, now modified to implement a sort of 'lazy' compilation. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to disable most stderr output for timing purposes. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt:498,Performance,perform,performance-with-mcjit,498,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the second blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-29-kaleidoscope-performance-with-mcjit/. The source code in this directory demonstrates the second version of the; program, now modified to implement a sort of 'lazy' compilation. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to disable most stderr output for timing purposes. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt:1146,Performance,perform,performance,1146,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the second blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-29-kaleidoscope-performance-with-mcjit/. The source code in this directory demonstrates the second version of the; program, now modified to implement a sort of 'lazy' compilation. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to disable most stderr output for timing purposes. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt:1107,Testability,test,test,1107,"//===----------------------------------------------------------------------===/; // Kaleidoscope with MCJIT; //===----------------------------------------------------------------------===//. The files in this directory are meant to accompany the second blog in a series of; three blog posts that describe the process of porting the Kaleidoscope tutorial; to use the MCJIT execution engine instead of the older JIT engine. The link of blog post-; https://blog.llvm.org/posts/2013-07-29-kaleidoscope-performance-with-mcjit/. The source code in this directory demonstrates the second version of the; program, now modified to implement a sort of 'lazy' compilation. The toy-jit.cpp file contains a version of the original JIT-based source code; that has been modified to disable most stderr output for timing purposes. To build the program you will need to have 'clang++' and 'llvm-config' in your ; path. If you attempt to build using the LLVM 3.3 release, some minor ; modifications will be required. This directory also contains a Python script that may be used to generate random; input for the program and test scripts to capture data for rough performance; comparisons.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/lazy/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:1074,Availability,avail,available,1074,"r Library; ==============================. Introduction; ------------. This directory contains the generic itanium name demangler; library. The main purpose of the library is to demangle C++ symbols,; i.e. convert the string ""_Z1fv"" into ""f()"". You can also use the CRTP; base ManglingParser to perform some simple analysis on the mangled; name, or (in LLVM) use the opaque ItaniumPartialDemangler to query the; demangled AST. Why are there multiple copies of the this library in the source tree?; ---------------------------------------------------------------------. The canonical sources are in libcxxabi/src/demangle and some of the; files are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduc",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:833,Integrability,depend,depend,833,"Itanium Name Demangler Library; ==============================. Introduction; ------------. This directory contains the generic itanium name demangler; library. The main purpose of the library is to demangle C++ symbols,; i.e. convert the string ""_Z1fv"" into ""f()"". You can also use the CRTP; base ManglingParser to perform some simple analysis on the mangled; name, or (in LLVM) use the opaque ItaniumPartialDemangler to query the; demangled AST. Why are there multiple copies of the this library in the source tree?; ---------------------------------------------------------------------. The canonical sources are in libcxxabi/src/demangle and some of the; files are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), an",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:1314,Integrability,depend,depend,1314,"f()"". You can also use the CRTP; base ManglingParser to perform some simple analysis on the mangled; name, or (in LLVM) use the opaque ItaniumPartialDemangler to query the; demangled AST. Why are there multiple copies of the this library in the source tree?; ---------------------------------------------------------------------. The canonical sources are in libcxxabi/src/demangle and some of the; files are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduce any code that; depends on the libcxx dylib. FIXME: Now that LLVM is a monorepo, it should be possible to; de-duplicate this code, and have both LLVM and libcxxabi depend on a; shared demangler library. Testing; -------. The tests are spli",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:2038,Integrability,depend,depends,2038,"les are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduce any code that; depends on the libcxx dylib. FIXME: Now that LLVM is a monorepo, it should be possible to; de-duplicate this code, and have both LLVM and libcxxabi depend on a; shared demangler library. Testing; -------. The tests are split up between libcxxabi/test/{unit,}test_demangle.cpp, and; llvm/unittest/Demangle. The llvm directory should only get tests for stuff not; included in the core library. In the future though, we should probably move all; the tests to LLVM. It is also a really good idea to run libFuzzer after non-trivial changes, see; libcxxabi/fuzz/cxa_demangle_fuzzer.cpp and https://llvm.org/docs/LibFuzzer.html.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:2186,Integrability,depend,depend,2186,"les are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduce any code that; depends on the libcxx dylib. FIXME: Now that LLVM is a monorepo, it should be possible to; de-duplicate this code, and have both LLVM and libcxxabi depend on a; shared demangler library. Testing; -------. The tests are split up between libcxxabi/test/{unit,}test_demangle.cpp, and; llvm/unittest/Demangle. The llvm directory should only get tests for stuff not; included in the core library. In the future though, we should probably move all; the tests to LLVM. It is also a really good idea to run libFuzzer after non-trivial changes, see; libcxxabi/fuzz/cxa_demangle_fuzzer.cpp and https://llvm.org/docs/LibFuzzer.html.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:316,Performance,perform,perform,316,"Itanium Name Demangler Library; ==============================. Introduction; ------------. This directory contains the generic itanium name demangler; library. The main purpose of the library is to demangle C++ symbols,; i.e. convert the string ""_Z1fv"" into ""f()"". You can also use the CRTP; base ManglingParser to perform some simple analysis on the mangled; name, or (in LLVM) use the opaque ItaniumPartialDemangler to query the; demangled AST. Why are there multiple copies of the this library in the source tree?; ---------------------------------------------------------------------. The canonical sources are in libcxxabi/src/demangle and some of the; files are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), an",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:1496,Testability,test,testing,1496,"he; demangled AST. Why are there multiple copies of the this library in the source tree?; ---------------------------------------------------------------------. The canonical sources are in libcxxabi/src/demangle and some of the; files are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduce any code that; depends on the libcxx dylib. FIXME: Now that LLVM is a monorepo, it should be possible to; de-duplicate this code, and have both LLVM and libcxxabi depend on a; shared demangler library. Testing; -------. The tests are split up between libcxxabi/test/{unit,}test_demangle.cpp, and; llvm/unittest/Demangle. The llvm directory should only get tests for stuff not; included in the core library.",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:2247,Testability,test,tests,2247,"les are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduce any code that; depends on the libcxx dylib. FIXME: Now that LLVM is a monorepo, it should be possible to; de-duplicate this code, and have both LLVM and libcxxabi depend on a; shared demangler library. Testing; -------. The tests are split up between libcxxabi/test/{unit,}test_demangle.cpp, and; llvm/unittest/Demangle. The llvm directory should only get tests for stuff not; included in the core library. In the future though, we should probably move all; the tests to LLVM. It is also a really good idea to run libFuzzer after non-trivial changes, see; libcxxabi/fuzz/cxa_demangle_fuzzer.cpp and https://llvm.org/docs/LibFuzzer.html.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:2284,Testability,test,test,2284,"les are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduce any code that; depends on the libcxx dylib. FIXME: Now that LLVM is a monorepo, it should be possible to; de-duplicate this code, and have both LLVM and libcxxabi depend on a; shared demangler library. Testing; -------. The tests are split up between libcxxabi/test/{unit,}test_demangle.cpp, and; llvm/unittest/Demangle. The llvm directory should only get tests for stuff not; included in the core library. In the future though, we should probably move all; the tests to LLVM. It is also a really good idea to run libFuzzer after non-trivial changes, see; libcxxabi/fuzz/cxa_demangle_fuzzer.cpp and https://llvm.org/docs/LibFuzzer.html.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:2379,Testability,test,tests,2379,"les are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduce any code that; depends on the libcxx dylib. FIXME: Now that LLVM is a monorepo, it should be possible to; de-duplicate this code, and have both LLVM and libcxxabi depend on a; shared demangler library. Testing; -------. The tests are split up between libcxxabi/test/{unit,}test_demangle.cpp, and; llvm/unittest/Demangle. The llvm directory should only get tests for stuff not; included in the core library. In the future though, we should probably move all; the tests to LLVM. It is also a really good idea to run libFuzzer after non-trivial changes, see; libcxxabi/fuzz/cxa_demangle_fuzzer.cpp and https://llvm.org/docs/LibFuzzer.html.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:2485,Testability,test,tests,2485,"les are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduce any code that; depends on the libcxx dylib. FIXME: Now that LLVM is a monorepo, it should be possible to; de-duplicate this code, and have both LLVM and libcxxabi depend on a; shared demangler library. Testing; -------. The tests are split up between libcxxabi/test/{unit,}test_demangle.cpp, and; llvm/unittest/Demangle. The llvm directory should only get tests for stuff not; included in the core library. In the future though, we should probably move all; the tests to LLVM. It is also a really good idea to run libFuzzer after non-trivial changes, see; libcxxabi/fuzz/cxa_demangle_fuzzer.cpp and https://llvm.org/docs/LibFuzzer.html.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:329,Usability,simpl,simple,329,"Itanium Name Demangler Library; ==============================. Introduction; ------------. This directory contains the generic itanium name demangler; library. The main purpose of the library is to demangle C++ symbols,; i.e. convert the string ""_Z1fv"" into ""f()"". You can also use the CRTP; base ManglingParser to perform some simple analysis on the mangled; name, or (in LLVM) use the opaque ItaniumPartialDemangler to query the; demangled AST. Why are there multiple copies of the this library in the source tree?; ---------------------------------------------------------------------. The canonical sources are in libcxxabi/src/demangle and some of the; files are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), an",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:711,Usability,simpl,simple,711,"Itanium Name Demangler Library; ==============================. Introduction; ------------. This directory contains the generic itanium name demangler; library. The main purpose of the library is to demangle C++ symbols,; i.e. convert the string ""_Z1fv"" into ""f()"". You can also use the CRTP; base ManglingParser to perform some simple analysis on the mangled; name, or (in LLVM) use the opaque ItaniumPartialDemangler to query the; demangled AST. Why are there multiple copies of the this library in the source tree?; ---------------------------------------------------------------------. The canonical sources are in libcxxabi/src/demangle and some of the; files are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), an",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt:1471,Usability,simpl,simplify,1471,"he; demangled AST. Why are there multiple copies of the this library in the source tree?; ---------------------------------------------------------------------. The canonical sources are in libcxxabi/src/demangle and some of the; files are copied to llvm/include/llvm/Demangle. The simple reason for; this comes from before the monorepo, and both [sub]projects need to; demangle symbols, but neither can depend on each other. * libcxxabi needs the demangler to implement __cxa_demangle, which is; part of the itanium ABI spec. * LLVM needs a copy for a bunch of places, and cannot rely on the; system's __cxa_demangle because it a) might not be available (i.e.,; on Windows), and b) may not be up-to-date on the latest language; features. The copy of the demangler in LLVM has some extra stuff that aren't; needed in libcxxabi (ie, the MSVC demangler, ItaniumPartialDemangler),; which depend on the shared generic components. Despite these; differences, we want to keep the ""core"" generic demangling library; identical between both copies to simplify development and testing. If you're working on the generic library, then do the work first in; libcxxabi, then run libcxxabi/src/demangle/cp-to-llvm.sh. This; script takes as an optional argument the path to llvm, and copies the; changes you made to libcxxabi over. Note that this script just; blindly overwrites all changes to the generic library in llvm, so be; careful. Because the core demangler needs to work in libcxxabi, everything; needs to be declared in an anonymous namespace (see; DEMANGLE_NAMESPACE_BEGIN), and you can't introduce any code that; depends on the libcxx dylib. FIXME: Now that LLVM is a monorepo, it should be possible to; de-duplicate this code, and have both LLVM and libcxxabi depend on a; shared demangler library. Testing; -------. The tests are split up between libcxxabi/test/{unit,}test_demangle.cpp, and; llvm/unittest/Demangle. The llvm directory should only get tests for stuff not; included in the core library.",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Demangle/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Support/CMakeLists.txt:486,Availability,avail,available,486,"find_first_existing_vc_file(""${LLVM_MAIN_SRC_DIR}"" llvm_vc). # The VC revision include that we want to generate.; set(version_inc ""${CMAKE_CURRENT_BINARY_DIR}/VCSRevision.h""). set(generate_vcs_version_script ""${LLVM_CMAKE_DIR}/GenerateVersionFromVCS.cmake""). if(LLVM_APPEND_VC_REV); set(llvm_source_dir ${LLVM_MAIN_SRC_DIR}). # A fake version file and is not expected to exist. It is being used to; # force regeneration of VCSRevision.h for source directory with no write; # permission available.; if (llvm_vc STREQUAL """"); set(fake_version_inc ""${CMAKE_CURRENT_BINARY_DIR}/__FakeVCSRevision.h""); endif(); endif(). set(generated_files ""${version_inc}""); if (fake_version_inc); list(APPEND generated_files ""${fake_version_inc}""); endif(). # Create custom target to generate the VC revision include.; if (fake_version_inc); add_custom_command(OUTPUT ""${version_inc}"" ""${fake_version_inc}""; DEPENDS ""${llvm_vc}"" ""${generate_vcs_version_script}""; COMMAND ${CMAKE_COMMAND} ""-DNAMES=LLVM""; ""-DLLVM_SOURCE_DIR=${llvm_source_dir}""; ""-DHEADER_FILE=${version_inc}""; ""-DLLVM_FORCE_VC_REVISION=${LLVM_FORCE_VC_REVISION}""; ""-DLLVM_FORCE_VC_REPOSITORY=${LLVM_FORCE_VC_REPOSITORY}""; -P ""${generate_vcs_version_script}""); else(); add_custom_command(OUTPUT ""${version_inc}""; DEPENDS ""${llvm_vc}"" ""${generate_vcs_version_script}""; COMMAND ${CMAKE_COMMAND} ""-DNAMES=LLVM""; ""-DLLVM_SOURCE_DIR=${llvm_source_dir}""; ""-DHEADER_FILE=${version_inc}""; ""-DLLVM_FORCE_VC_REVISION=${LLVM_FORCE_VC_REVISION}""; ""-DLLVM_FORCE_VC_REPOSITORY=${LLVM_FORCE_VC_REPOSITORY}""; -P ""${generate_vcs_version_script}""); endif(). # Mark the generated header as being generated.; set_source_files_properties(""${version_inc}""; PROPERTIES GENERATED TRUE; HEADER_FILE_ONLY TRUE). add_custom_target(llvm_vcsrevision_h ALL DEPENDS ""${generated_files}""); set_target_properties(llvm_vcsrevision_h PROPERTIES FOLDER ""Misc""); ",MatchSource.DOCS,interpreter/llvm-project/llvm/include/llvm/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CMakeLists.txt:209,Availability,down,download,209,"if (DEFINED LLVM_HAVE_TF_AOT OR LLVM_HAVE_TFLITE); include(TensorFlowCompile); set(LLVM_INLINER_MODEL_PATH_DEFAULT ""models/inliner-Oz""). set(LLVM_INLINER_MODEL_CURRENT_URL ""<UNSPECIFIED>"" CACHE STRING ""URL to download the LLVM inliner model""). if (DEFINED LLVM_HAVE_TF_AOT); tf_find_and_compile(; ${LLVM_INLINER_MODEL_PATH}; ${LLVM_INLINER_MODEL_CURRENT_URL}; ${LLVM_INLINER_MODEL_PATH_DEFAULT}; ""models/gen-inline-oz-test-model.py""; serve; action; InlinerSizeModel; llvm::InlinerSizeModel; ); endif(). if (LLVM_HAVE_TFLITE); list(APPEND MLLinkDeps; tensorflow-lite::tensorflow-lite); endif(); endif(). add_llvm_component_library(LLVMAnalysis; AliasAnalysis.cpp; AliasAnalysisEvaluator.cpp; AliasSetTracker.cpp; Analysis.cpp; AssumeBundleQueries.cpp; AssumptionCache.cpp; BasicAliasAnalysis.cpp; BlockFrequencyInfo.cpp; BlockFrequencyInfoImpl.cpp; BranchProbabilityInfo.cpp; CFG.cpp; CFGPrinter.cpp; CFGSCCPrinter.cpp; CGSCCPassManager.cpp; CallGraph.cpp; CallGraphSCCPass.cpp; CallPrinter.cpp; CaptureTracking.cpp; CmpInstAnalysis.cpp; CostModel.cpp; CodeMetrics.cpp; ConstantFolding.cpp; CycleAnalysis.cpp; DDG.cpp; DDGPrinter.cpp; ConstraintSystem.cpp; Delinearization.cpp; DemandedBits.cpp; DependenceAnalysis.cpp; DependenceGraphBuilder.cpp; DevelopmentModeInlineAdvisor.cpp; DomConditionCache.cpp; DomPrinter.cpp; DomTreeUpdater.cpp; DominanceFrontier.cpp; FunctionPropertiesAnalysis.cpp; GlobalsModRef.cpp; GuardUtils.cpp; HeatUtils.cpp; IRSimilarityIdentifier.cpp; IVDescriptors.cpp; IVUsers.cpp; ImportedFunctionsInliningStatistics.cpp; IndirectCallPromotionAnalysis.cpp; InlineCost.cpp; InlineAdvisor.cpp; InlineOrder.cpp; InlineSizeEstimatorAnalysis.cpp; InstCount.cpp; InstructionPrecedenceTracking.cpp; InstructionSimplify.cpp; InteractiveModelRunner.cpp; Interval.cpp; IntervalPartition.cpp; LazyBranchProbabilityInfo.cpp; LazyBlockFrequencyInfo.cpp; LazyCallGraph.cpp; LazyValueInfo.cpp; Lint.cpp; Loads.cpp; Local.cpp; LoopAccessAnalysis.cpp; LoopAnalysisManager.cpp; LoopCacheAnalysis",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Analysis/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CMakeLists.txt:418,Testability,test,test-model,418,"if (DEFINED LLVM_HAVE_TF_AOT OR LLVM_HAVE_TFLITE); include(TensorFlowCompile); set(LLVM_INLINER_MODEL_PATH_DEFAULT ""models/inliner-Oz""). set(LLVM_INLINER_MODEL_CURRENT_URL ""<UNSPECIFIED>"" CACHE STRING ""URL to download the LLVM inliner model""). if (DEFINED LLVM_HAVE_TF_AOT); tf_find_and_compile(; ${LLVM_INLINER_MODEL_PATH}; ${LLVM_INLINER_MODEL_CURRENT_URL}; ${LLVM_INLINER_MODEL_PATH_DEFAULT}; ""models/gen-inline-oz-test-model.py""; serve; action; InlinerSizeModel; llvm::InlinerSizeModel; ); endif(). if (LLVM_HAVE_TFLITE); list(APPEND MLLinkDeps; tensorflow-lite::tensorflow-lite); endif(); endif(). add_llvm_component_library(LLVMAnalysis; AliasAnalysis.cpp; AliasAnalysisEvaluator.cpp; AliasSetTracker.cpp; Analysis.cpp; AssumeBundleQueries.cpp; AssumptionCache.cpp; BasicAliasAnalysis.cpp; BlockFrequencyInfo.cpp; BlockFrequencyInfoImpl.cpp; BranchProbabilityInfo.cpp; CFG.cpp; CFGPrinter.cpp; CFGSCCPrinter.cpp; CGSCCPassManager.cpp; CallGraph.cpp; CallGraphSCCPass.cpp; CallPrinter.cpp; CaptureTracking.cpp; CmpInstAnalysis.cpp; CostModel.cpp; CodeMetrics.cpp; ConstantFolding.cpp; CycleAnalysis.cpp; DDG.cpp; DDGPrinter.cpp; ConstraintSystem.cpp; Delinearization.cpp; DemandedBits.cpp; DependenceAnalysis.cpp; DependenceGraphBuilder.cpp; DevelopmentModeInlineAdvisor.cpp; DomConditionCache.cpp; DomPrinter.cpp; DomTreeUpdater.cpp; DominanceFrontier.cpp; FunctionPropertiesAnalysis.cpp; GlobalsModRef.cpp; GuardUtils.cpp; HeatUtils.cpp; IRSimilarityIdentifier.cpp; IVDescriptors.cpp; IVUsers.cpp; ImportedFunctionsInliningStatistics.cpp; IndirectCallPromotionAnalysis.cpp; InlineCost.cpp; InlineAdvisor.cpp; InlineOrder.cpp; InlineSizeEstimatorAnalysis.cpp; InstCount.cpp; InstructionPrecedenceTracking.cpp; InstructionSimplify.cpp; InteractiveModelRunner.cpp; Interval.cpp; IntervalPartition.cpp; LazyBranchProbabilityInfo.cpp; LazyBlockFrequencyInfo.cpp; LazyCallGraph.cpp; LazyValueInfo.cpp; Lint.cpp; Loads.cpp; Local.cpp; LoopAccessAnalysis.cpp; LoopAnalysisManager.cpp; LoopCacheAnalysis",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Analysis/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CMakeLists.txt:216,Availability,down,download,216,"if (DEFINED LLVM_HAVE_TF_AOT OR LLVM_HAVE_TFLITE); include(TensorFlowCompile); set(LLVM_RAEVICT_MODEL_PATH_DEFAULT ""models/regalloc-eviction""). set(LLVM_RAEVICT_MODEL_CURRENT_URL ""<UNSPECIFIED>"" CACHE STRING ""URL to download the LLVM register allocator eviction model""). if (DEFINED LLVM_HAVE_TF_AOT); tf_find_and_compile(; ${LLVM_RAEVICT_MODEL_PATH}; ${LLVM_RAEVICT_MODEL_CURRENT_URL}; ${LLVM_RAEVICT_MODEL_PATH_DEFAULT}; ""../Analysis/models/gen-regalloc-eviction-test-model.py""; serve; action; RegAllocEvictModel; llvm::RegAllocEvictModel; ); endif(). if (LLVM_HAVE_TFLITE); list(APPEND MLLinkDeps ${tensorflow_c_api} ${tensorflow_fx}); endif(); endif(). # This provides the implementation of MVT and LLT.; # Be careful to append deps on this, since Targets' tablegens depend on this.; add_llvm_component_library(LLVMCodeGenTypes; LowLevelType.cpp; PARTIAL_SOURCES_INTENDED. DEPENDS; vt_gen. LINK_COMPONENTS; Support; ). add_llvm_component_library(LLVMCodeGen; AggressiveAntiDepBreaker.cpp; AllocationOrder.cpp; Analysis.cpp; AssignmentTrackingAnalysis.cpp; AtomicExpandPass.cpp; BasicTargetTransformInfo.cpp; BranchFolding.cpp; BranchRelaxation.cpp; BreakFalseDeps.cpp; BasicBlockSections.cpp; BasicBlockPathCloning.cpp; BasicBlockSectionsProfileReader.cpp; CalcSpillWeights.cpp; CallBrPrepare.cpp; CallingConvLower.cpp; CFGuardLongjmp.cpp; CFIFixup.cpp; CFIInstrInserter.cpp; CodeGen.cpp; CodeGenCommonISel.cpp; CodeGenPassBuilder.cpp; CodeGenPrepare.cpp; CommandFlags.cpp; ComplexDeinterleavingPass.cpp; CriticalAntiDepBreaker.cpp; DeadMachineInstructionElim.cpp; DetectDeadLanes.cpp; DFAPacketizer.cpp; DwarfEHPrepare.cpp; EarlyIfConversion.cpp; EdgeBundles.cpp; EHContGuardCatchret.cpp; ExecutionDomainFix.cpp; ExpandLargeDivRem.cpp; ExpandLargeFpConvert.cpp; ExpandMemCmp.cpp; ExpandPostRAPseudos.cpp; ExpandReductions.cpp; ExpandVectorPredication.cpp; FaultMaps.cpp; FEntryInserter.cpp; FinalizeISel.cpp; FixupStatepointCallerSaved.cpp; FuncletLayout.cpp; GCMetadata.cpp; GCMetadataPrinter.cp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CMakeLists.txt:771,Integrability,depend,depend,771,"if (DEFINED LLVM_HAVE_TF_AOT OR LLVM_HAVE_TFLITE); include(TensorFlowCompile); set(LLVM_RAEVICT_MODEL_PATH_DEFAULT ""models/regalloc-eviction""). set(LLVM_RAEVICT_MODEL_CURRENT_URL ""<UNSPECIFIED>"" CACHE STRING ""URL to download the LLVM register allocator eviction model""). if (DEFINED LLVM_HAVE_TF_AOT); tf_find_and_compile(; ${LLVM_RAEVICT_MODEL_PATH}; ${LLVM_RAEVICT_MODEL_CURRENT_URL}; ${LLVM_RAEVICT_MODEL_PATH_DEFAULT}; ""../Analysis/models/gen-regalloc-eviction-test-model.py""; serve; action; RegAllocEvictModel; llvm::RegAllocEvictModel; ); endif(). if (LLVM_HAVE_TFLITE); list(APPEND MLLinkDeps ${tensorflow_c_api} ${tensorflow_fx}); endif(); endif(). # This provides the implementation of MVT and LLT.; # Be careful to append deps on this, since Targets' tablegens depend on this.; add_llvm_component_library(LLVMCodeGenTypes; LowLevelType.cpp; PARTIAL_SOURCES_INTENDED. DEPENDS; vt_gen. LINK_COMPONENTS; Support; ). add_llvm_component_library(LLVMCodeGen; AggressiveAntiDepBreaker.cpp; AllocationOrder.cpp; Analysis.cpp; AssignmentTrackingAnalysis.cpp; AtomicExpandPass.cpp; BasicTargetTransformInfo.cpp; BranchFolding.cpp; BranchRelaxation.cpp; BreakFalseDeps.cpp; BasicBlockSections.cpp; BasicBlockPathCloning.cpp; BasicBlockSectionsProfileReader.cpp; CalcSpillWeights.cpp; CallBrPrepare.cpp; CallingConvLower.cpp; CFGuardLongjmp.cpp; CFIFixup.cpp; CFIInstrInserter.cpp; CodeGen.cpp; CodeGenCommonISel.cpp; CodeGenPassBuilder.cpp; CodeGenPrepare.cpp; CommandFlags.cpp; ComplexDeinterleavingPass.cpp; CriticalAntiDepBreaker.cpp; DeadMachineInstructionElim.cpp; DetectDeadLanes.cpp; DFAPacketizer.cpp; DwarfEHPrepare.cpp; EarlyIfConversion.cpp; EdgeBundles.cpp; EHContGuardCatchret.cpp; ExecutionDomainFix.cpp; ExpandLargeDivRem.cpp; ExpandLargeFpConvert.cpp; ExpandMemCmp.cpp; ExpandPostRAPseudos.cpp; ExpandReductions.cpp; ExpandVectorPredication.cpp; FaultMaps.cpp; FEntryInserter.cpp; FinalizeISel.cpp; FixupStatepointCallerSaved.cpp; FuncletLayout.cpp; GCMetadata.cpp; GCMetadataPrinter.cp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CMakeLists.txt:465,Testability,test,test-model,465,"if (DEFINED LLVM_HAVE_TF_AOT OR LLVM_HAVE_TFLITE); include(TensorFlowCompile); set(LLVM_RAEVICT_MODEL_PATH_DEFAULT ""models/regalloc-eviction""). set(LLVM_RAEVICT_MODEL_CURRENT_URL ""<UNSPECIFIED>"" CACHE STRING ""URL to download the LLVM register allocator eviction model""). if (DEFINED LLVM_HAVE_TF_AOT); tf_find_and_compile(; ${LLVM_RAEVICT_MODEL_PATH}; ${LLVM_RAEVICT_MODEL_CURRENT_URL}; ${LLVM_RAEVICT_MODEL_PATH_DEFAULT}; ""../Analysis/models/gen-regalloc-eviction-test-model.py""; serve; action; RegAllocEvictModel; llvm::RegAllocEvictModel; ); endif(). if (LLVM_HAVE_TFLITE); list(APPEND MLLinkDeps ${tensorflow_c_api} ${tensorflow_fx}); endif(); endif(). # This provides the implementation of MVT and LLT.; # Be careful to append deps on this, since Targets' tablegens depend on this.; add_llvm_component_library(LLVMCodeGenTypes; LowLevelType.cpp; PARTIAL_SOURCES_INTENDED. DEPENDS; vt_gen. LINK_COMPONENTS; Support; ). add_llvm_component_library(LLVMCodeGen; AggressiveAntiDepBreaker.cpp; AllocationOrder.cpp; Analysis.cpp; AssignmentTrackingAnalysis.cpp; AtomicExpandPass.cpp; BasicTargetTransformInfo.cpp; BranchFolding.cpp; BranchRelaxation.cpp; BreakFalseDeps.cpp; BasicBlockSections.cpp; BasicBlockPathCloning.cpp; BasicBlockSectionsProfileReader.cpp; CalcSpillWeights.cpp; CallBrPrepare.cpp; CallingConvLower.cpp; CFGuardLongjmp.cpp; CFIFixup.cpp; CFIInstrInserter.cpp; CodeGen.cpp; CodeGenCommonISel.cpp; CodeGenPassBuilder.cpp; CodeGenPrepare.cpp; CommandFlags.cpp; ComplexDeinterleavingPass.cpp; CriticalAntiDepBreaker.cpp; DeadMachineInstructionElim.cpp; DetectDeadLanes.cpp; DFAPacketizer.cpp; DwarfEHPrepare.cpp; EarlyIfConversion.cpp; EdgeBundles.cpp; EHContGuardCatchret.cpp; ExecutionDomainFix.cpp; ExpandLargeDivRem.cpp; ExpandLargeFpConvert.cpp; ExpandMemCmp.cpp; ExpandPostRAPseudos.cpp; ExpandReductions.cpp; ExpandVectorPredication.cpp; FaultMaps.cpp; FEntryInserter.cpp; FinalizeISel.cpp; FixupStatepointCallerSaved.cpp; FuncletLayout.cpp; GCMetadata.cpp; GCMetadataPrinter.cp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:6102,Availability,recover,recover,6102,"g.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of explicit predicates on them (e.g. no side; effects). Once this is in place, it would be even better to have tblgen; synthesize the various copy insertion/inspection methods in TargetInstrInfo. //===---------------------------------------------------------------------===//. Stack coloring improvements:. 1. Do proper LiveStacks analysis on all stack objects including those which are; not spill slots.; 2. Reorder objects to fill in gaps between objects.; e.g. 4, 1, <gap>, 4, 1, 1, 1, <gap>, 4 => 4, 1, 1, 1, 1, 4, 4. //===---------------------------------------------------------------------===//. The scheduler should be able to sort nearby instructions by their address. For; example, in an expanded memset sequence it's not uncommon to see code like this:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). Each of the stores is independent, and the scheduler is currently making an; arbitrary decision about the order. //===---------------------------------------------------------------------===//. Another opportunitiy in this code is that the $0 could be moved to a register:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). This would save substantial code size, especially for longer sequences like; this. It would be easy to have a rule telling isel to avoid matching MOV32mi; if the immediate has more than some fixed number of uses. It's more involved; to teach the register allocator how to do late folding to recover from; excessive register pressure. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:1176,Energy Efficiency,schedul,scheduled,1176,"; ldr lr, [r1, #+32]; sxth r3, r3; ldr r4, [sp, #+52]; mla r4, r3, lr, r4. can be:. mul lr, r4, lr; mov r4, lr; str lr, [sp, #+52]; ldr lr, [r1, #+32]; sxth r3, r3; mla r4, r3, lr, r4. and then ""merge"" mul and mov:. mul r4, r4, lr; str r4, [sp, #+52]; ldr lr, [r1, #+32]; sxth r3, r3; mla r4, r3, lr, r4. It also increase the likelihood the store may become dead. //===---------------------------------------------------------------------===//. bb27 ...; ...; %reg1037 = ADDri %reg1039, 1; %reg1038 = ADDrs %reg1032, %reg1039, %noreg, 10; Successors according to CFG: 0x8b03bf0 (#5). bb76 (0x8b03bf0, LLVM BB @0x8b032d0, ID#5):; Predecessors according to CFG: 0x8b0c5f0 (#3) 0x8b0a7c0 (#4); %reg1039 = PHI %reg1070, mbb<bb76.outer,0x8b0c5f0>, %reg1037, mbb<bb27,0x8b0a7c0>. Note ADDri is not a two-address instruction. However, its result %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:5233,Energy Efficiency,schedul,scheduler,5233,"g.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of explicit predicates on them (e.g. no side; effects). Once this is in place, it would be even better to have tblgen; synthesize the various copy insertion/inspection methods in TargetInstrInfo. //===---------------------------------------------------------------------===//. Stack coloring improvements:. 1. Do proper LiveStacks analysis on all stack objects including those which are; not spill slots.; 2. Reorder objects to fill in gaps between objects.; e.g. 4, 1, <gap>, 4, 1, 1, 1, <gap>, 4 => 4, 1, 1, 1, 1, 4, 4. //===---------------------------------------------------------------------===//. The scheduler should be able to sort nearby instructions by their address. For; example, in an expanded memset sequence it's not uncommon to see code like this:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). Each of the stores is independent, and the scheduler is currently making an; arbitrary decision about the order. //===---------------------------------------------------------------------===//. Another opportunitiy in this code is that the $0 could be moved to a register:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). This would save substantial code size, especially for longer sequences like; this. It would be easy to have a rule telling isel to avoid matching MOV32mi; if the immediate has more than some fixed number of uses. It's more involved; to teach the register allocator how to do late folding to recover from; excessive register pressure. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:5507,Energy Efficiency,schedul,scheduler,5507,"g.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of explicit predicates on them (e.g. no side; effects). Once this is in place, it would be even better to have tblgen; synthesize the various copy insertion/inspection methods in TargetInstrInfo. //===---------------------------------------------------------------------===//. Stack coloring improvements:. 1. Do proper LiveStacks analysis on all stack objects including those which are; not spill slots.; 2. Reorder objects to fill in gaps between objects.; e.g. 4, 1, <gap>, 4, 1, 1, 1, <gap>, 4 => 4, 1, 1, 1, 1, 4, 4. //===---------------------------------------------------------------------===//. The scheduler should be able to sort nearby instructions by their address. For; example, in an expanded memset sequence it's not uncommon to see code like this:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). Each of the stores is independent, and the scheduler is currently making an; arbitrary decision about the order. //===---------------------------------------------------------------------===//. Another opportunitiy in this code is that the $0 could be moved to a register:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). This would save substantial code size, especially for longer sequences like; this. It would be easy to have a rule telling isel to avoid matching MOV32mi; if the immediate has more than some fixed number of uses. It's more involved; to teach the register allocator how to do late folding to recover from; excessive register pressure. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3421,Integrability,depend,dependent,3421,"ation over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the w",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:1748,Performance,load,load,1748,"(0x8b03bf0, LLVM BB @0x8b032d0, ID#5):; Predecessors according to CFG: 0x8b0c5f0 (#3) 0x8b0a7c0 (#4); %reg1039 = PHI %reg1070, mbb<bb76.outer,0x8b0c5f0>, %reg1037, mbb<bb27,0x8b0a7c0>. Note ADDri is not a two-address instruction. However, its result %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived poin",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:1768,Performance,load,load,1768,"8b032d0, ID#5):; Predecessors according to CFG: 0x8b0c5f0 (#3) 0x8b0a7c0 (#4); %reg1039 = PHI %reg1070, mbb<bb76.outer,0x8b0c5f0>, %reg1037, mbb<bb27,0x8b0a7c0>. Note ADDri is not a two-address instruction. However, its result %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retain",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:1788,Performance,load,load,1788,"edecessors according to CFG: 0x8b0c5f0 (#3) 0x8b0a7c0 (#4); %reg1039 = PHI %reg1070, mbb<bb76.outer,0x8b0c5f0>, %reg1037, mbb<bb27,0x8b0a7c0>. Note ADDri is not a two-address instruction. However, its result %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collecto",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2011,Performance,load,load,2011,"t %reg1037 is an; operand of the PHI node in bb76 and its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2061,Performance,load,load,2061," its operand %reg1039 is the result of the; PHI node. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2112,Performance,load,load,2112,"ode. We should treat it as a two-address code and make sure the ADDri is; scheduled after any node that reads %reg1039. //===---------------------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = lo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2240,Performance,load,load,2240,"----------------------------------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe poin",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2259,Performance,load,load,2259,"-------------------------------===//. Use local info (i.e. register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for th",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2278,Performance,load,load,2278,"register scavenger) to assign it a free register to allow; reuse:; ldr r3, [sp, #+4]; add r3, r3, #3; ldr r2, [sp, #+8]; add r2, r2, #2; ldr r1, [sp, #+4] <==; add r1, r1, #1; ldr r0, [sp, #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2457,Performance,load,load,2457," #+4]; add r0, r0, #2. //===---------------------------------------------------------------------===//. LLVM aggressively lift CSE out of loop. Sometimes this can be negative side-; effects:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; load [i + R1]; ...; load [i + R2]; ...; load [i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be cop",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2986,Performance,load,load,2986,"...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===-----------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3101,Performance,load,load,3101,"...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===-----------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3494,Performance,load,loads,3494,"n't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really ni",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3599,Performance,concurren,concurrent,3599,"---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of e",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3773,Performance,optimiz,optimizations,3773,"afe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of explicit predicates on them (e.g. no side; effects). Once this is in place, it would be even better to have tblgen; synthesize the various copy insertion/inspection meth",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2796,Safety,safe,safe,2796,"[i + R3]. Suppose there is high register pressure, R1, R2, R3, can be spilled. We need; to implement proper re-materialization to handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would p",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:2926,Safety,safe,safe,2926,"handle this:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; R1 = X + 4 @ re-materialized; load [i + R1]; ...; R2 = X + 7 @ re-materialized; load [i + R2]; ...; R3 = X + 15 @ re-materialized; load [i + R3]. Furthermore, with re-association, we can enable sharing:. R1 = X + 4; R2 = X + 7; R3 = X + 15. loop:; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The oca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3229,Safety,safe,safe,3229,"; T = i + X; load [T + 4]; ...; load [T + 7]; ...; load [T + 15]; //===---------------------------------------------------------------------===//. It's not always a good idea to choose rematerialization over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of direct",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3463,Safety,safe,safe,3463,"ation over spilling. If all; the load / store instructions would be folded then spilling is cheaper because; it won't require new live intervals / registers. See 2003-05-31-LongShifts for; an example. //===---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the w",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3658,Safety,safe,safe,3658,"---------------------------------------------------------------------===//. With a copying garbage collector, derived pointers must not be retained across; collector safe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of e",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:4354,Safety,detect,detect,4354,"to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of explicit predicates on them (e.g. no side; effects). Once this is in place, it would be even better to have tblgen; synthesize the various copy insertion/inspection methods in TargetInstrInfo. //===---------------------------------------------------------------------===//. Stack coloring improvements:. 1. Do proper LiveStacks analysis on all stack objects including those which are; not spill slots.; 2. Reorder objects to fill in gaps between objects.; e.g. 4, 1, <gap>, 4, 1, 1, 1, <gap>, 4 => 4, 1, 1, 1, 1, 4, 4. //===---------------------------------------------------------------------===//. The scheduler should be able to sort nearby instructions by their address. For; example, in an expanded memset sequence it's not uncommon to see code like ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:5942,Safety,avoid,avoid,5942,"g.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of explicit predicates on them (e.g. no side; effects). Once this is in place, it would be even better to have tblgen; synthesize the various copy insertion/inspection methods in TargetInstrInfo. //===---------------------------------------------------------------------===//. Stack coloring improvements:. 1. Do proper LiveStacks analysis on all stack objects including those which are; not spill slots.; 2. Reorder objects to fill in gaps between objects.; e.g. 4, 1, <gap>, 4, 1, 1, 1, <gap>, 4 => 4, 1, 1, 1, 1, 4, 4. //===---------------------------------------------------------------------===//. The scheduler should be able to sort nearby instructions by their address. For; example, in an expanded memset sequence it's not uncommon to see code like this:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). Each of the stores is independent, and the scheduler is currently making an; arbitrary decision about the order. //===---------------------------------------------------------------------===//. Another opportunitiy in this code is that the $0 could be moved to a register:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). This would save substantial code size, especially for longer sequences like; this. It would be easy to have a rule telling isel to avoid matching MOV32mi; if the immediate has more than some fixed number of uses. It's more involved; to teach the register allocator how to do late folding to recover from; excessive register pressure. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:6102,Safety,recover,recover,6102,"g.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of explicit predicates on them (e.g. no side; effects). Once this is in place, it would be even better to have tblgen; synthesize the various copy insertion/inspection methods in TargetInstrInfo. //===---------------------------------------------------------------------===//. Stack coloring improvements:. 1. Do proper LiveStacks analysis on all stack objects including those which are; not spill slots.; 2. Reorder objects to fill in gaps between objects.; e.g. 4, 1, <gap>, 4, 1, 1, 1, <gap>, 4 => 4, 1, 1, 1, 1, 4, 4. //===---------------------------------------------------------------------===//. The scheduler should be able to sort nearby instructions by their address. For; example, in an expanded memset sequence it's not uncommon to see code like this:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). Each of the stores is independent, and the scheduler is currently making an; arbitrary decision about the order. //===---------------------------------------------------------------------===//. Another opportunitiy in this code is that the $0 could be moved to a register:. movl $0, 4(%rdi); movl $0, 8(%rdi); movl $0, 12(%rdi); movl $0, 0(%rdi). This would save substantial code size, especially for longer sequences like; this. It would be easy to have a rule telling isel to avoid matching MOV32mi; if the immediate has more than some fixed number of uses. It's more involved; to teach the register allocator how to do late folding to recover from; excessive register pressure. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt:3802,Usability,undo,undo,3802,"afe points; the collector could move the objects and invalidate the; derived pointer. This is bad enough in the first place, but safe points can; crop up unpredictably. Consider:. %array = load { i32, [0 x %obj] }** %array_addr; %nth_el = getelementptr { i32, [0 x %obj] }* %array, i32 0, i32 %n; %old = load %obj** %nth_el; %z = div i64 %x, %y; store %obj* %new, %obj** %nth_el. If the i64 division is lowered to a libcall, then a safe point will (must); appear for the call site. If a collection occurs, %array and %nth_el no longer; point into the correct object. The fix for this is to copy address calculations so that dependent pointers; are never live across safe point boundaries. But the loads cannot be copied; like this if there was an intervening store, so may be hard to get right. Only a concurrent mutator can trigger a collection at the libcall safe point.; So single-threaded programs do not have this requirement, even with a copying; collector. Still, LLVM optimizations would probably undo a front-end's careful; work. //===---------------------------------------------------------------------===//. The ocaml frametable structure supports liveness information. It would be good; to support it. //===---------------------------------------------------------------------===//. The FIXME in ComputeCommonTailLength in BranchFolding.cpp needs to be; revisited. The check is there to work around a misuse of directives in inline; assembly. //===---------------------------------------------------------------------===//. It would be good to detect collector/target compatibility instead of silently; doing the wrong thing. //===---------------------------------------------------------------------===//. It would be really nice to be able to write patterns in .td files for copies,; which would eliminate a bunch of explicit predicates on them (e.g. no side; effects). Once this is in place, it would be even better to have tblgen; synthesize the various copy insertion/inspection meth",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/CodeGen/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Debuginfod/CMakeLists.txt:453,Integrability,depend,dependency,453,"# Link LibCURL if the user wants it; if (LLVM_ENABLE_CURL); set(imported_libs CURL::libcurl); endif(). # Link cpp-httplib if the user wants it; if (LLVM_ENABLE_HTTPLIB); set(imported_libs ${imported_libs} httplib::httplib); endif(). # Make sure pthread is linked if this is a unix host; if (CMAKE_HOST_UNIX); set(imported_libs ${imported_libs} ${LLVM_PTHREAD_LIB}); endif(). # Note: This isn't a component, since that could potentially add a libcurl; # dependency to libLLVM.; add_llvm_library(LLVMDebuginfod; BuildIDFetcher.cpp; Debuginfod.cpp; HTTPClient.cpp; HTTPServer.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Debuginfod. LINK_LIBS; ${imported_libs}. LINK_COMPONENTS; Support; Symbolize; DebugInfoDWARF; BinaryFormat; Object; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Debuginfod/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Debuginfod/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/FuzzMutate/CMakeLists.txt:57,Integrability,depend,depend,57,# Generic helper for fuzzer binaries.; # This should not depend on LLVM IR etc.; add_llvm_component_library(LLVMFuzzerCLI; FuzzerCLI.cpp; PARTIAL_SOURCES_INTENDED. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/FuzzMutate. LINK_COMPONENTS; Support; TargetParser; ). # Library for using LLVM IR together with fuzzers.; add_llvm_component_library(LLVMFuzzMutate; IRMutator.cpp; OpDescriptor.cpp; Operations.cpp; RandomIRBuilder.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/FuzzMutate. DEPENDS; intrinsics_gen. LINK_COMPONENTS; Analysis; BitReader; BitWriter; Core; Scalar; Support; TargetParser; TransformUtils; ); ,MatchSource.DOCS,interpreter/llvm-project/llvm/lib/FuzzMutate/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/FuzzMutate/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:4842,Deployability,patch,patch,4842,"dedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CRT_ALLOC ""${LLVM_INTEGRATED_CRT_ALLOC}""). if(NOT EXISTS ""${LLVM_INTEGRATED_CRT_ALLOC}""); message(FATAL_ERROR ""Cannot find the path to `git clone` for the CRT allocator! (${LLVM_INTEGRATED_CRT_ALLOC}). Currently, rpmalloc, snmalloc and mimalloc are supported.""); endif(). if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""rpmalloc$""); add_compile_definitions(ENABLE_OVERRIDE ENABLE_PRELOAD); set(ALLOCATOR_FILES ""${LLVM_INTEGRATED_CRT_ALLOC}/rpmalloc/rpmalloc.c""); elseif(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""snmalloc$""); set(ALLOCATOR_FILES ""${LLVM_INTEGRATED_CRT_ALLOC}/src/snmalloc/override/new.cc""); set(system_libs ${system_libs} ""mincore.lib"" ""-INCLUDE:malloc""); elseif(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""mimalloc$""); set(MIMALLOC_LIB ""${LLVM_INTEGRATED_CRT_ALLOC}/out/msvc-x64/Release/mimalloc-static.lib""); if(NOT EXISTS ""${MIMALLOC_LIB}""); 	 message(FATAL_ERROR ""Cannot find the mimalloc static library. To build it, first apply the patch from https://github.com/microsoft/mimalloc/issues/268 then build the Release x64 target through ${LLVM_INTEGRATED_CRT_ALLOC}\\ide\\vs2019\\mimalloc.sln""); endif(); set(system_libs ${system_libs} ""${MIMALLOC_LIB}"" ""-INCLUDE:malloc""); endif(); endif(). # FIXME: We are currently guarding AIX headers with _XOPEN_SOURCE=700.; # See llvm/CMakeLists.txt. However, we need _SC_NPROCESSORS_ONLN in; # unistd.h and it is guarded by _ALL_SOURCE, so we remove the _XOPEN_SOURCE; # guard here. We should remove the guards all together once AIX cleans up; # the system headers.; if (UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""AIX""); remove_definitions(""-D_XOPEN_SOURCE=700""); endif(). add_subdirectory(BLAKE3). add_llvm_component_library(LLVMSupport; ABIBreak.cpp; AMDGPUMetadata.cpp; APFixedPoint.cpp; APFloat.cpp; APInt.cpp; APSInt.cpp; ARMBuildAttrs.cpp; ARMAttributeParser.cpp; ARMWinEH.cpp; Allocator.cpp; AutoConvert.cpp; Base64.cpp; BalancedPartitioning.cpp; BinaryStreamError.cpp; BinaryStreamReader.",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:8589,Deployability,configurat,configuration,8589,"nicodeCaseFold.cpp; UnicodeNameToCodepoint.cpp; UnicodeNameToCodepointGenerated.cpp; VersionTuple.cpp; VirtualFileSystem.cpp; WithColor.cpp; YAMLParser.cpp; YAMLTraits.cpp; raw_os_ostream.cpp; raw_ostream.cpp; raw_socket_stream.cpp; regcomp.c; regerror.c; regexec.c; regfree.c; regstrlcpy.c; xxhash.cpp; Z3Solver.cpp. ${ALLOCATOR_FILES}; $<TARGET_OBJECTS:LLVMSupportBlake3>. # System; Atomic.cpp; DynamicLibrary.cpp; Errno.cpp; Memory.cpp; Path.cpp; Process.cpp; Program.cpp; RWMutex.cpp; Signals.cpp; Threading.cpp; Valgrind.cpp; Watchdog.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ADT; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Support; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${system_libs} ${imported_libs} ${delayload_flags}. LINK_COMPONENTS; Demangle; ). set(llvm_system_libs ${system_libs}). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_ZLIB); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION_${build_type}); endif(); if(NOT zlib_library); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION); endif(); get_library_name(${zlib_library} zlib_library); set(llvm_system_libs ${llvm_system_libs} ""${zlib_library}""); endif(). if(LLVM_ENABLE_ZSTD); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION_${build_type}); endif(); if(NOT zstd_library); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION); endif(); get_library_name(${zstd_library} zstd_library); set(llvm_system_libs ${llvm_system_libs} ""${zstd_library}""); endif(). if(LLVM_ENABLE_TERMINFO); if(NOT terminfo_library); get_property(terminfo_library TARGET Terminfo::termin",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:9056,Deployability,configurat,configuration,9056,"am.cpp; RWMutex.cpp; Signals.cpp; Threading.cpp; Valgrind.cpp; Watchdog.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ADT; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Support; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${system_libs} ${imported_libs} ${delayload_flags}. LINK_COMPONENTS; Demangle; ). set(llvm_system_libs ${system_libs}). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_ZLIB); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION_${build_type}); endif(); if(NOT zlib_library); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION); endif(); get_library_name(${zlib_library} zlib_library); set(llvm_system_libs ${llvm_system_libs} ""${zlib_library}""); endif(). if(LLVM_ENABLE_ZSTD); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION_${build_type}); endif(); if(NOT zstd_library); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION); endif(); get_library_name(${zstd_library} zstd_library); set(llvm_system_libs ${llvm_system_libs} ""${zstd_library}""); endif(). if(LLVM_ENABLE_TERMINFO); if(NOT terminfo_library); get_property(terminfo_library TARGET Terminfo::terminfo PROPERTY LOCATION); endif(); get_library_name(${terminfo_library} terminfo_library); set(llvm_system_libs ${llvm_system_libs} ""${terminfo_library}""); endif(). set_property(TARGET LLVMSupport PROPERTY LLVM_SYSTEM_LIBS ""${llvm_system_libs}""). if(LLVM_INTEGRATED_CRT_ALLOC); if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""snmalloc$""); set_property(TARGET LLVMSupport PROPERTY CXX_STANDARD 17); add_compile_definitions(_SILENCE_CXX17_ITERATOR_BASE_CLASS_DEPRECATION_WARNING); i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:3716,Integrability,message,message,3716,"_libs} kstat socket); endif(); if( FUCHSIA ); set(system_libs ${system_libs} zircon); endif(); if ( HAIKU ); add_compile_definitions(_BSD_SOURCE); set(system_libs ${system_libs} bsd network); endif(); endif( MSVC OR MINGW ). # Delay load shell32.dll if possible to speed up process startup.; set (delayload_flags); if (MSVC); # When linking with Swift, `swiftc.exe` is used as the linker drive rather; # than invoking `link.exe` directly. In such a case, the flags should be; # marked as `-Xlinker` to pass them directly to the linker. As a temporary; # workaround simply elide the delay loading.; set (delayload_flags $<$<NOT:$<LINK_LANGUAGE:Swift>>:delayimp -delayload:shell32.dll -delayload:ole32.dll>); endif(). # Link Z3 if the user wants to build it.; if(LLVM_WITH_Z3); set(system_libs ${system_libs} ${Z3_LIBRARIES}); endif(). # Override the C runtime allocator on Windows and embed it into LLVM tools & libraries; if(LLVM_INTEGRATED_CRT_ALLOC); if (NOT CMAKE_MSVC_RUNTIME_LIBRARY OR CMAKE_MSVC_RUNTIME_LIBRARY MATCHES ""DLL$""); message(FATAL_ERROR ""LLVM_INTEGRATED_CRT_ALLOC only works with CMAKE_MSVC_RUNTIME_LIBRARY set to MultiThreaded or MultiThreadedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CRT_ALLOC ""${LLVM_INTEGRATED_CRT_ALLOC}""). if(NOT EXISTS ""${LLVM_INTEGRATED_CRT_ALLOC}""); message(FATAL_ERROR ""Cannot find the path to `git clone` for the CRT allocator! (${LLVM_INTEGRATED_CRT_ALLOC}). Currently, rpmalloc, snmalloc and mimalloc are supported.""); endif(). if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""rpmalloc$""); add_compile_definitions(ENABLE_OVERRIDE ENABLE_PRELOAD); set(ALLOCATOR_FILES ""${LLVM_INTEGRATED_CRT_ALLOC}/rpmalloc/rpmalloc.c""); elseif(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""snmalloc$""); set(ALLOCATOR_FILES ""${LLVM_INTEGRATED_CRT_ALLOC}/src/snmalloc/override/new.cc""); set(system_libs ${system_libs} ""mincore.lib"" ""-INCLUDE:malloc""); elseif(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""mimalloc$""); set(MIMALLOC_LIB ""${LLVM_INTEGRATED_CRT_ALLOC}/out/msvc-x6",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:4004,Integrability,message,message,4004,"swiftc.exe` is used as the linker drive rather; # than invoking `link.exe` directly. In such a case, the flags should be; # marked as `-Xlinker` to pass them directly to the linker. As a temporary; # workaround simply elide the delay loading.; set (delayload_flags $<$<NOT:$<LINK_LANGUAGE:Swift>>:delayimp -delayload:shell32.dll -delayload:ole32.dll>); endif(). # Link Z3 if the user wants to build it.; if(LLVM_WITH_Z3); set(system_libs ${system_libs} ${Z3_LIBRARIES}); endif(). # Override the C runtime allocator on Windows and embed it into LLVM tools & libraries; if(LLVM_INTEGRATED_CRT_ALLOC); if (NOT CMAKE_MSVC_RUNTIME_LIBRARY OR CMAKE_MSVC_RUNTIME_LIBRARY MATCHES ""DLL$""); message(FATAL_ERROR ""LLVM_INTEGRATED_CRT_ALLOC only works with CMAKE_MSVC_RUNTIME_LIBRARY set to MultiThreaded or MultiThreadedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CRT_ALLOC ""${LLVM_INTEGRATED_CRT_ALLOC}""). if(NOT EXISTS ""${LLVM_INTEGRATED_CRT_ALLOC}""); message(FATAL_ERROR ""Cannot find the path to `git clone` for the CRT allocator! (${LLVM_INTEGRATED_CRT_ALLOC}). Currently, rpmalloc, snmalloc and mimalloc are supported.""); endif(). if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""rpmalloc$""); add_compile_definitions(ENABLE_OVERRIDE ENABLE_PRELOAD); set(ALLOCATOR_FILES ""${LLVM_INTEGRATED_CRT_ALLOC}/rpmalloc/rpmalloc.c""); elseif(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""snmalloc$""); set(ALLOCATOR_FILES ""${LLVM_INTEGRATED_CRT_ALLOC}/src/snmalloc/override/new.cc""); set(system_libs ${system_libs} ""mincore.lib"" ""-INCLUDE:malloc""); elseif(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""mimalloc$""); set(MIMALLOC_LIB ""${LLVM_INTEGRATED_CRT_ALLOC}/out/msvc-x64/Release/mimalloc-static.lib""); if(NOT EXISTS ""${MIMALLOC_LIB}""); 	 message(FATAL_ERROR ""Cannot find the mimalloc static library. To build it, first apply the patch from https://github.com/microsoft/mimalloc/issues/268 then build the Release x64 target through ${LLVM_INTEGRATED_CRT_ALLOC}\\ide\\vs2019\\mimalloc.sln""); endif(); set(system_libs ${system",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:4751,Integrability,message,message,4751,"OC only works with CMAKE_MSVC_RUNTIME_LIBRARY set to MultiThreaded or MultiThreadedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CRT_ALLOC ""${LLVM_INTEGRATED_CRT_ALLOC}""). if(NOT EXISTS ""${LLVM_INTEGRATED_CRT_ALLOC}""); message(FATAL_ERROR ""Cannot find the path to `git clone` for the CRT allocator! (${LLVM_INTEGRATED_CRT_ALLOC}). Currently, rpmalloc, snmalloc and mimalloc are supported.""); endif(). if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""rpmalloc$""); add_compile_definitions(ENABLE_OVERRIDE ENABLE_PRELOAD); set(ALLOCATOR_FILES ""${LLVM_INTEGRATED_CRT_ALLOC}/rpmalloc/rpmalloc.c""); elseif(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""snmalloc$""); set(ALLOCATOR_FILES ""${LLVM_INTEGRATED_CRT_ALLOC}/src/snmalloc/override/new.cc""); set(system_libs ${system_libs} ""mincore.lib"" ""-INCLUDE:malloc""); elseif(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""mimalloc$""); set(MIMALLOC_LIB ""${LLVM_INTEGRATED_CRT_ALLOC}/out/msvc-x64/Release/mimalloc-static.lib""); if(NOT EXISTS ""${MIMALLOC_LIB}""); 	 message(FATAL_ERROR ""Cannot find the mimalloc static library. To build it, first apply the patch from https://github.com/microsoft/mimalloc/issues/268 then build the Release x64 target through ${LLVM_INTEGRATED_CRT_ALLOC}\\ide\\vs2019\\mimalloc.sln""); endif(); set(system_libs ${system_libs} ""${MIMALLOC_LIB}"" ""-INCLUDE:malloc""); endif(); endif(). # FIXME: We are currently guarding AIX headers with _XOPEN_SOURCE=700.; # See llvm/CMakeLists.txt. However, we need _SC_NPROCESSORS_ONLN in; # unistd.h and it is guarded by _ALL_SOURCE, so we remove the _XOPEN_SOURCE; # guard here. We should remove the guards all together once AIX cleans up; # the system headers.; if (UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""AIX""); remove_definitions(""-D_XOPEN_SOURCE=700""); endif(). add_subdirectory(BLAKE3). add_llvm_component_library(LLVMSupport; ABIBreak.cpp; AMDGPUMetadata.cpp; APFixedPoint.cpp; APFloat.cpp; APInt.cpp; APSInt.cpp; ARMBuildAttrs.cpp; ARMAttributeParser.cpp; ARMWinEH.cpp; Allocator.cpp; AutoConvert.cpp; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:8418,Modifiability,config,config,8418,"eNode.cpp; SuffixTree.cpp; SystemUtils.cpp; TarWriter.cpp; ThreadPool.cpp; TimeProfiler.cpp; Timer.cpp; ToolOutputFile.cpp; Twine.cpp; TypeSize.cpp; Unicode.cpp; UnicodeCaseFold.cpp; UnicodeNameToCodepoint.cpp; UnicodeNameToCodepointGenerated.cpp; VersionTuple.cpp; VirtualFileSystem.cpp; WithColor.cpp; YAMLParser.cpp; YAMLTraits.cpp; raw_os_ostream.cpp; raw_ostream.cpp; raw_socket_stream.cpp; regcomp.c; regerror.c; regexec.c; regfree.c; regstrlcpy.c; xxhash.cpp; Z3Solver.cpp. ${ALLOCATOR_FILES}; $<TARGET_OBJECTS:LLVMSupportBlake3>. # System; Atomic.cpp; DynamicLibrary.cpp; Errno.cpp; Memory.cpp; Path.cpp; Process.cpp; Program.cpp; RWMutex.cpp; Signals.cpp; Threading.cpp; Valgrind.cpp; Watchdog.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ADT; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Support; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${system_libs} ${imported_libs} ${delayload_flags}. LINK_COMPONENTS; Demangle; ). set(llvm_system_libs ${system_libs}). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_ZLIB); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION_${build_type}); endif(); if(NOT zlib_library); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION); endif(); get_library_name(${zlib_library} zlib_library); set(llvm_system_libs ${llvm_system_libs} ""${zlib_library}""); endif(). if(LLVM_ENABLE_ZSTD); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION_${build_type}); endif(); if(NOT zstd_library); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION); endif(); get_library_name(${zstd_library} zstd_library); set(llvm_sys",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:8449,Modifiability,config,config,8449,".cpp; TimeProfiler.cpp; Timer.cpp; ToolOutputFile.cpp; Twine.cpp; TypeSize.cpp; Unicode.cpp; UnicodeCaseFold.cpp; UnicodeNameToCodepoint.cpp; UnicodeNameToCodepointGenerated.cpp; VersionTuple.cpp; VirtualFileSystem.cpp; WithColor.cpp; YAMLParser.cpp; YAMLTraits.cpp; raw_os_ostream.cpp; raw_ostream.cpp; raw_socket_stream.cpp; regcomp.c; regerror.c; regexec.c; regfree.c; regstrlcpy.c; xxhash.cpp; Z3Solver.cpp. ${ALLOCATOR_FILES}; $<TARGET_OBJECTS:LLVMSupportBlake3>. # System; Atomic.cpp; DynamicLibrary.cpp; Errno.cpp; Memory.cpp; Path.cpp; Process.cpp; Program.cpp; RWMutex.cpp; Signals.cpp; Threading.cpp; Valgrind.cpp; Watchdog.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ADT; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Support; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${system_libs} ${imported_libs} ${delayload_flags}. LINK_COMPONENTS; Demangle; ). set(llvm_system_libs ${system_libs}). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_ZLIB); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION_${build_type}); endif(); if(NOT zlib_library); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION); endif(); get_library_name(${zlib_library} zlib_library); set(llvm_system_libs ${llvm_system_libs} ""${zlib_library}""); endif(). if(LLVM_ENABLE_ZSTD); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION_${build_type}); endif(); if(NOT zstd_library); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION); endif(); get_library_name(${zstd_library} zstd_library); set(llvm_system_libs ${llvm_system_libs} ""${zstd_library}""); endif(). if(LLVM_EN",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:8589,Modifiability,config,configuration,8589,"nicodeCaseFold.cpp; UnicodeNameToCodepoint.cpp; UnicodeNameToCodepointGenerated.cpp; VersionTuple.cpp; VirtualFileSystem.cpp; WithColor.cpp; YAMLParser.cpp; YAMLTraits.cpp; raw_os_ostream.cpp; raw_ostream.cpp; raw_socket_stream.cpp; regcomp.c; regerror.c; regexec.c; regfree.c; regstrlcpy.c; xxhash.cpp; Z3Solver.cpp. ${ALLOCATOR_FILES}; $<TARGET_OBJECTS:LLVMSupportBlake3>. # System; Atomic.cpp; DynamicLibrary.cpp; Errno.cpp; Memory.cpp; Path.cpp; Process.cpp; Program.cpp; RWMutex.cpp; Signals.cpp; Threading.cpp; Valgrind.cpp; Watchdog.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ADT; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Support; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${system_libs} ${imported_libs} ${delayload_flags}. LINK_COMPONENTS; Demangle; ). set(llvm_system_libs ${system_libs}). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_ZLIB); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION_${build_type}); endif(); if(NOT zlib_library); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION); endif(); get_library_name(${zlib_library} zlib_library); set(llvm_system_libs ${llvm_system_libs} ""${zlib_library}""); endif(). if(LLVM_ENABLE_ZSTD); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION_${build_type}); endif(); if(NOT zstd_library); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION); endif(); get_library_name(${zstd_library} zstd_library); set(llvm_system_libs ${llvm_system_libs} ""${zstd_library}""); endif(). if(LLVM_ENABLE_TERMINFO); if(NOT terminfo_library); get_property(terminfo_library TARGET Terminfo::termin",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:9056,Modifiability,config,configuration,9056,"am.cpp; RWMutex.cpp; Signals.cpp; Threading.cpp; Valgrind.cpp; Watchdog.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows; ${LLVM_MAIN_INCLUDE_DIR}/llvm/ADT; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Support; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${system_libs} ${imported_libs} ${delayload_flags}. LINK_COMPONENTS; Demangle; ). set(llvm_system_libs ${system_libs}). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_ZLIB); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION_${build_type}); endif(); if(NOT zlib_library); get_property(zlib_library TARGET ZLIB::ZLIB PROPERTY LOCATION); endif(); get_library_name(${zlib_library} zlib_library); set(llvm_system_libs ${llvm_system_libs} ""${zlib_library}""); endif(). if(LLVM_ENABLE_ZSTD); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION_${build_type}); endif(); if(NOT zstd_library); get_property(zstd_library TARGET ${zstd_target} PROPERTY LOCATION); endif(); get_library_name(${zstd_library} zstd_library); set(llvm_system_libs ${llvm_system_libs} ""${zstd_library}""); endif(). if(LLVM_ENABLE_TERMINFO); if(NOT terminfo_library); get_property(terminfo_library TARGET Terminfo::terminfo PROPERTY LOCATION); endif(); get_library_name(${terminfo_library} terminfo_library); set(llvm_system_libs ${llvm_system_libs} ""${terminfo_library}""); endif(). set_property(TARGET LLVMSupport PROPERTY LLVM_SYSTEM_LIBS ""${llvm_system_libs}""). if(LLVM_INTEGRATED_CRT_ALLOC); if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""snmalloc$""); set_property(TARGET LLVMSupport PROPERTY CXX_STANDARD 17); add_compile_definitions(_SILENCE_CXX17_ITERATOR_BASE_CLASS_DEPRECATION_WARNING); i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:162,Performance,load,load,162,"include(GetLibraryName). # Ensure that libSupport does not carry any static global initializer.; # libSupport can be embedded in use cases where we don't want to load all; # cl::opt unless we want to parse the command line.; # ManagedStatic can be used to enable lazy-initialization of globals.; # We don't use `add_flag_if_supported` as instead of compiling an empty file we; # check if the current platform is able to compile global std::mutex with this; # flag (Linux can, Darwin can't for example).; check_cxx_compiler_flag(""-Werror=global-constructors"" HAS_WERROR_GLOBAL_CTORS); if (HAS_WERROR_GLOBAL_CTORS); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Werror=global-constructors""); CHECK_CXX_SOURCE_COMPILES(""; #include <mutex>; static std::mutex TestGlobalCtorDtor;; static std::recursive_mutex TestGlobalCtorDtor2;; int main() { (void)TestGlobalCtorDtor; (void)TestGlobalCtorDtor2; return 0;}; "" LLVM_HAS_NOGLOBAL_CTOR_MUTEX); if (NOT LLVM_HAS_NOGLOBAL_CTOR_MUTEX); string(REPLACE ""-Werror=global-constructors"" """" CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS}); endif(); endif(). if(LLVM_ENABLE_ZLIB); list(APPEND imported_libs ZLIB::ZLIB); endif(). if(LLVM_ENABLE_ZSTD); if(TARGET zstd::libzstd_shared AND NOT LLVM_USE_STATIC_ZSTD); set(zstd_target zstd::libzstd_shared); else(); set(zstd_target zstd::libzstd_static); endif(); endif(). if(LLVM_ENABLE_ZSTD); list(APPEND imported_libs ${zstd_target}); endif(). if( MSVC OR MINGW ); # libuuid required for FOLDERID_Profile usage in lib/Support/Windows/Path.inc.; # advapi32 required for CryptAcquireContextW in lib/Support/Windows/Path.inc.; set(system_libs ${system_libs} psapi shell32 ole32 uuid advapi32 ws2_32); elseif( CMAKE_HOST_UNIX ); if( HAVE_LIBRT ); set(system_libs ${system_libs} rt); endif(); if( HAVE_LIBDL ); set(system_libs ${system_libs} ${CMAKE_DL_LIBS}); endif(); if( HAVE_BACKTRACE AND NOT ""${Backtrace_LIBRARIES}"" STREQUAL """" ); # On BSDs, CMake returns a fully qualified path to the backtrace library.; # We need to remove the path and",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:2914,Performance,load,load,2914,"ns a fully qualified path to the backtrace library.; # We need to remove the path and the 'lib' prefix, to make it look like a; # regular short library name, suitable for appending to a -l link flag.; get_filename_component(Backtrace_LIBFILE ${Backtrace_LIBRARIES} NAME_WE); STRING(REGEX REPLACE ""^lib"" """" Backtrace_LIBFILE ${Backtrace_LIBFILE}); set(system_libs ${system_libs} ${Backtrace_LIBFILE}); endif(); if( LLVM_ENABLE_TERMINFO ); set(imported_libs ${imported_libs} Terminfo::terminfo); endif(); set(system_libs ${system_libs} ${LLVM_ATOMIC_LIB}); set(system_libs ${system_libs} ${LLVM_PTHREAD_LIB}); if( UNIX AND NOT (BEOS OR HAIKU) ); set(system_libs ${system_libs} m); endif(); if( UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""SunOS"" ); set(system_libs ${system_libs} kstat socket); endif(); if( FUCHSIA ); set(system_libs ${system_libs} zircon); endif(); if ( HAIKU ); add_compile_definitions(_BSD_SOURCE); set(system_libs ${system_libs} bsd network); endif(); endif( MSVC OR MINGW ). # Delay load shell32.dll if possible to speed up process startup.; set (delayload_flags); if (MSVC); # When linking with Swift, `swiftc.exe` is used as the linker drive rather; # than invoking `link.exe` directly. In such a case, the flags should be; # marked as `-Xlinker` to pass them directly to the linker. As a temporary; # workaround simply elide the delay loading.; set (delayload_flags $<$<NOT:$<LINK_LANGUAGE:Swift>>:delayimp -delayload:shell32.dll -delayload:ole32.dll>); endif(). # Link Z3 if the user wants to build it.; if(LLVM_WITH_Z3); set(system_libs ${system_libs} ${Z3_LIBRARIES}); endif(). # Override the C runtime allocator on Windows and embed it into LLVM tools & libraries; if(LLVM_INTEGRATED_CRT_ALLOC); if (NOT CMAKE_MSVC_RUNTIME_LIBRARY OR CMAKE_MSVC_RUNTIME_LIBRARY MATCHES ""DLL$""); message(FATAL_ERROR ""LLVM_INTEGRATED_CRT_ALLOC only works with CMAKE_MSVC_RUNTIME_LIBRARY set to MultiThreaded or MultiThreadedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CR",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:3269,Performance,load,loading,3269,"trace_LIBFILE}); set(system_libs ${system_libs} ${Backtrace_LIBFILE}); endif(); if( LLVM_ENABLE_TERMINFO ); set(imported_libs ${imported_libs} Terminfo::terminfo); endif(); set(system_libs ${system_libs} ${LLVM_ATOMIC_LIB}); set(system_libs ${system_libs} ${LLVM_PTHREAD_LIB}); if( UNIX AND NOT (BEOS OR HAIKU) ); set(system_libs ${system_libs} m); endif(); if( UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""SunOS"" ); set(system_libs ${system_libs} kstat socket); endif(); if( FUCHSIA ); set(system_libs ${system_libs} zircon); endif(); if ( HAIKU ); add_compile_definitions(_BSD_SOURCE); set(system_libs ${system_libs} bsd network); endif(); endif( MSVC OR MINGW ). # Delay load shell32.dll if possible to speed up process startup.; set (delayload_flags); if (MSVC); # When linking with Swift, `swiftc.exe` is used as the linker drive rather; # than invoking `link.exe` directly. In such a case, the flags should be; # marked as `-Xlinker` to pass them directly to the linker. As a temporary; # workaround simply elide the delay loading.; set (delayload_flags $<$<NOT:$<LINK_LANGUAGE:Swift>>:delayimp -delayload:shell32.dll -delayload:ole32.dll>); endif(). # Link Z3 if the user wants to build it.; if(LLVM_WITH_Z3); set(system_libs ${system_libs} ${Z3_LIBRARIES}); endif(). # Override the C runtime allocator on Windows and embed it into LLVM tools & libraries; if(LLVM_INTEGRATED_CRT_ALLOC); if (NOT CMAKE_MSVC_RUNTIME_LIBRARY OR CMAKE_MSVC_RUNTIME_LIBRARY MATCHES ""DLL$""); message(FATAL_ERROR ""LLVM_INTEGRATED_CRT_ALLOC only works with CMAKE_MSVC_RUNTIME_LIBRARY set to MultiThreaded or MultiThreadedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CRT_ALLOC ""${LLVM_INTEGRATED_CRT_ALLOC}""). if(NOT EXISTS ""${LLVM_INTEGRATED_CRT_ALLOC}""); message(FATAL_ERROR ""Cannot find the path to `git clone` for the CRT allocator! (${LLVM_INTEGRATED_CRT_ALLOC}). Currently, rpmalloc, snmalloc and mimalloc are supported.""); endif(). if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""rpmalloc$""); add_compil",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt:3246,Usability,simpl,simply,3246,"trace_LIBFILE}); set(system_libs ${system_libs} ${Backtrace_LIBFILE}); endif(); if( LLVM_ENABLE_TERMINFO ); set(imported_libs ${imported_libs} Terminfo::terminfo); endif(); set(system_libs ${system_libs} ${LLVM_ATOMIC_LIB}); set(system_libs ${system_libs} ${LLVM_PTHREAD_LIB}); if( UNIX AND NOT (BEOS OR HAIKU) ); set(system_libs ${system_libs} m); endif(); if( UNIX AND ${CMAKE_SYSTEM_NAME} MATCHES ""SunOS"" ); set(system_libs ${system_libs} kstat socket); endif(); if( FUCHSIA ); set(system_libs ${system_libs} zircon); endif(); if ( HAIKU ); add_compile_definitions(_BSD_SOURCE); set(system_libs ${system_libs} bsd network); endif(); endif( MSVC OR MINGW ). # Delay load shell32.dll if possible to speed up process startup.; set (delayload_flags); if (MSVC); # When linking with Swift, `swiftc.exe` is used as the linker drive rather; # than invoking `link.exe` directly. In such a case, the flags should be; # marked as `-Xlinker` to pass them directly to the linker. As a temporary; # workaround simply elide the delay loading.; set (delayload_flags $<$<NOT:$<LINK_LANGUAGE:Swift>>:delayimp -delayload:shell32.dll -delayload:ole32.dll>); endif(). # Link Z3 if the user wants to build it.; if(LLVM_WITH_Z3); set(system_libs ${system_libs} ${Z3_LIBRARIES}); endif(). # Override the C runtime allocator on Windows and embed it into LLVM tools & libraries; if(LLVM_INTEGRATED_CRT_ALLOC); if (NOT CMAKE_MSVC_RUNTIME_LIBRARY OR CMAKE_MSVC_RUNTIME_LIBRARY MATCHES ""DLL$""); message(FATAL_ERROR ""LLVM_INTEGRATED_CRT_ALLOC only works with CMAKE_MSVC_RUNTIME_LIBRARY set to MultiThreaded or MultiThreadedDebug.""); endif(). string(REGEX REPLACE ""(/|\\\\)$"" """" LLVM_INTEGRATED_CRT_ALLOC ""${LLVM_INTEGRATED_CRT_ALLOC}""). if(NOT EXISTS ""${LLVM_INTEGRATED_CRT_ALLOC}""); message(FATAL_ERROR ""Cannot find the path to `git clone` for the CRT allocator! (${LLVM_INTEGRATED_CRT_ALLOC}). Currently, rpmalloc, snmalloc and mimalloc are supported.""); endif(). if(LLVM_INTEGRATED_CRT_ALLOC MATCHES ""rpmalloc$""); add_compil",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt:924,Integrability,message,message,924,"list(APPEND LLVM_COMMON_DEPENDS intrinsics_gen). list(APPEND LLVM_TABLEGEN_FLAGS -I ${LLVM_MAIN_SRC_DIR}/lib/Target). add_llvm_component_library(LLVMTarget; Target.cpp; TargetIntrinsicInfo.cpp; TargetLoweringObjectFile.cpp; TargetMachine.cpp; TargetMachineC.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Target. LINK_COMPONENTS; Analysis; Core; MC; Support; TargetParser; ). # When building shared objects for each target there are some internal APIs; # that are used across shared objects which we can't hide.; if (NOT BUILD_SHARED_LIBS AND NOT APPLE AND; (NOT (WIN32 OR CYGWIN) OR (MINGW AND CMAKE_CXX_COMPILER_ID MATCHES ""Clang"")) AND; NOT (${CMAKE_SYSTEM_NAME} MATCHES ""AIX"") AND; NOT DEFINED CMAKE_CXX_VISIBILITY_PRESET); # Set default visibility to hidden, so we don't export all the Target classes; # in libLLVM.so.; set(CMAKE_CXX_VISIBILITY_PRESET hidden); endif(). foreach(t ${LLVM_TARGETS_TO_BUILD}); message(STATUS ""Targeting ${t}""); add_subdirectory(${t}); endforeach(). # Currently we do not allow libraries from lib to reference targets directly.; # This property is used to enforce that convention. It is important because the; # logic in llvm_map_components_to_libnames is order dependent on the target; # libraries being created.; set_property(GLOBAL PROPERTY LLVM_TARGETS_CONFIGURED On); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt:1208,Integrability,depend,dependent,1208,"list(APPEND LLVM_COMMON_DEPENDS intrinsics_gen). list(APPEND LLVM_TABLEGEN_FLAGS -I ${LLVM_MAIN_SRC_DIR}/lib/Target). add_llvm_component_library(LLVMTarget; Target.cpp; TargetIntrinsicInfo.cpp; TargetLoweringObjectFile.cpp; TargetMachine.cpp; TargetMachineC.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Target. LINK_COMPONENTS; Analysis; Core; MC; Support; TargetParser; ). # When building shared objects for each target there are some internal APIs; # that are used across shared objects which we can't hide.; if (NOT BUILD_SHARED_LIBS AND NOT APPLE AND; (NOT (WIN32 OR CYGWIN) OR (MINGW AND CMAKE_CXX_COMPILER_ID MATCHES ""Clang"")) AND; NOT (${CMAKE_SYSTEM_NAME} MATCHES ""AIX"") AND; NOT DEFINED CMAKE_CXX_VISIBILITY_PRESET); # Set default visibility to hidden, so we don't export all the Target classes; # in libLLVM.so.; set(CMAKE_CXX_VISIBILITY_PRESET hidden); endif(). foreach(t ${LLVM_TARGETS_TO_BUILD}); message(STATUS ""Targeting ${t}""); add_subdirectory(${t}); endforeach(). # Currently we do not allow libraries from lib to reference targets directly.; # This property is used to enforce that convention. It is important because the; # logic in llvm_map_components_to_libnames is order dependent on the target; # libraries being created.; set_property(GLOBAL PROPERTY LLVM_TARGETS_CONFIGURED On); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt:1158,Testability,log,logic,1158,"list(APPEND LLVM_COMMON_DEPENDS intrinsics_gen). list(APPEND LLVM_TABLEGEN_FLAGS -I ${LLVM_MAIN_SRC_DIR}/lib/Target). add_llvm_component_library(LLVMTarget; Target.cpp; TargetIntrinsicInfo.cpp; TargetLoweringObjectFile.cpp; TargetMachine.cpp; TargetMachineC.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/Target. LINK_COMPONENTS; Analysis; Core; MC; Support; TargetParser; ). # When building shared objects for each target there are some internal APIs; # that are used across shared objects which we can't hide.; if (NOT BUILD_SHARED_LIBS AND NOT APPLE AND; (NOT (WIN32 OR CYGWIN) OR (MINGW AND CMAKE_CXX_COMPILER_ID MATCHES ""Clang"")) AND; NOT (${CMAKE_SYSTEM_NAME} MATCHES ""AIX"") AND; NOT DEFINED CMAKE_CXX_VISIBILITY_PRESET); # Set default visibility to hidden, so we don't export all the Target classes; # in libLLVM.so.; set(CMAKE_CXX_VISIBILITY_PRESET hidden); endif(). foreach(t ${LLVM_TARGETS_TO_BUILD}); message(STATUS ""Targeting ${t}""); add_subdirectory(${t}); endforeach(). # Currently we do not allow libraries from lib to reference targets directly.; # This property is used to enforce that convention. It is important because the; # logic in llvm_map_components_to_libnames is order dependent on the target; # libraries being created.; set_property(GLOBAL PROPERTY LLVM_TARGETS_CONFIGURED On); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:457,Availability,robust,robust,457,"Target Independent Opportunities:. //===---------------------------------------------------------------------===//. We should recognized various ""overflow detection"" idioms and translate them into; llvm.uadd.with.overflow and similar intrinsics. Here is a multiply idiom:. unsigned int mul(unsigned int a,unsigned int b) {; if ((unsigned long long)a*b>0xffffffff); exit(0);; return a*b;; }. The legalization code for mul-with-overflow needs to be made more robust before; this can be implemented though. //===---------------------------------------------------------------------===//. Get the C front-end to expand hypot(x,y) -> llvm.sqrt(x*x+y*y) when errno and; precision don't matter (ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:2825,Availability,avail,available,2825,"---------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alig",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8869,Availability,down,down,8869,"ign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===-------------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12515,Availability,down,down,12515,"functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:28581,Availability,redundant,redundant,28581,"-------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31095,Availability,fault,fault,31095," phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31184,Availability,redundant,redundant,31184," = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31375,Availability,redundant,redundant,31375,"p10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33230,Availability,redundant,redundant,33230,"..; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:39356,Availability,redundant,redundant,39356,"0, i32 0; %3073 = call i8* @strcpy(i8* %3072, i8* %3071) nounwind; %strlen = call i32 @strlen(i8* %3072) ; uses = 1; %endptr = getelementptr [100 x i8]* %tempString, i32 0, i32 %strlen; call void @llvm.memcpy.i32(i8* %endptr, ; i8* getelementptr ([5 x i8]* @""\01LC42"", i32 0, i32 0), i32 5, i32 1); %3074 = call i32 @strlen(i8* %endptr) nounwind readonly ; ; This is interesting for a couple reasons. First, in this:. The memcpy+strlen strlen can be replaced with:. %3074 = call i32 @strlen([5 x i8]* @""\01LC42"") nounwind readonly . Because the destination was just copied into the specified memory buffer. This,; in turn, can be constant folded to ""4"". In other code, it contains:. %endptr6978 = bitcast i8* %endptr69 to i32* ; store i32 7107374, i32* %endptr6978, align 1; %3167 = call i32 @strlen(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:40138,Availability,redundant,redundant,40138,"(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:40891,Availability,redundant,redundant,40891," i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value of %177 is actually just %171-1. A sub is cheaper than a; strlen!. This pattern repeats several times, basically doing:. A = strlen(P);; P[A-1] = 0;; B = strlen(P);; where it is ""obvious"" that B = A-1. //===---------------------------------------------------------------------===//. 186.crafty has this interesting pattern with the ""out.4543"" variable:. call void @llvm.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forw",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:45329,Availability,mask,masked,45329,"readnone ssp {; entry:; %0 = and i32 %b, -129 ; <i32> [#uses=1]; %1 = and i32 %a, 128 ; <i32> [#uses=1]; %2 = or i32 %0, %1 ; <i32> [#uses=1]; ret i32 %2; }. This can be generalized for other forms:. b = (b & ~0x80) | (a & 0x40) << 1;. //===---------------------------------------------------------------------===//. These two functions produce different code. They shouldn't:. #include <stdint.h>; ; uint8_t p1(uint8_t b, uint8_t a) {; b = (b & ~0xc0) | (a & 0xc0);; return (b);; }; ; uint8_t p2(uint8_t b, uint8_t a) {; b = (b & ~0x40) | (a & 0x40);; b = (b & ~0x80) | (a & 0x80);; return (b);; }. define zeroext i8 @p1(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of co",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:45462,Availability,mask,masked,45462,"; <i32> [#uses=1]; %2 = or i32 %0, %1 ; <i32> [#uses=1]; ret i32 %2; }. This can be generalized for other forms:. b = (b & ~0x80) | (a & 0x40) << 1;. //===---------------------------------------------------------------------===//. These two functions produce different code. They shouldn't:. #include <stdint.h>; ; uint8_t p1(uint8_t b, uint8_t a) {; b = (b & ~0xc0) | (a & 0xc0);; return (b);; }; ; uint8_t p2(uint8_t b, uint8_t a) {; b = (b & ~0x40) | (a & 0x40);; b = (b & ~0x80) | (a & 0x80);; return (b);; }. define zeroext i8 @p1(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:55191,Availability,avail,available,55191,"lowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize which by its; current definition always folds to a constant. We also should make sure that; we remove checking in code like. char *p = malloc(strlen(s)+1);; __strcpy_chk(p, s, __builtin_object_size(p, 0));. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===---------------------------------------------------------------------===//. memcpyopt should turn this:. define i8* @test10(i32 %x) {; %alloc = call noalias i8* @malloc(i32 %x) nounwind; call void @llvm.memset.p0i8.i32(i8* %alloc, i8 0, i32 %x, i32 1, i1 false); ret i8* %alloc; }. into a call to calloc. We should make sure that we analyze calloc as; aggressively as malloc though. //===---------------------------------------------------------------------===//. clang -O3 doesn't optimize this:. void f1(int* begin, int* end) {; std::fill(begin, end, 0);; }. into a memset. This is PR8942. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(int N) {; std::vector<int> v(N);. extern void sink(void*); sink(&v);; }. into. define void @_Z1fi(i32 %N) nounwind {; entry:; %v2 = alloca [3 x i32*], align 8; %v2.sub = getelementptr inbounds [3 x i32*]* %v2, i64 0, i6",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3340,Deployability,patch,patch,3340,"mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	inc",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31247,Deployability,update,updated,31247," 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tm",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:50790,Deployability,update,updates,50790,"ssues (PR6212):. int test1(int mainType, int subType) {; if (mainType == 7); subType = 4;; else if (mainType == 9); subType = 6;; else if (mainType == 11); subType = 9;; return subType;; }. int test2(int mainType, int subType) {; if (mainType == 7); subType = 4;; if (mainType == 9); subType = 6;; if (mainType == 11); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; ca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:59372,Deployability,update,updated,59372,"-------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(char* a, int n) {; __builtin_memset(a, 0, n);; for (int i = 0; i < n; ++i); a[i] = 0;; }. into:. define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {; entry:; %conv = sext i32 %n to i64; tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false); %cmp8 = icmp sgt i32 %n, 0; br i1 %cmp8, label %for.body.lr.ph, label %for.end. for.body.lr.ph: ; preds = %entry; %tmp10 = add i32 %n, -1; %tmp11 = zext i32 %tmp10 to i64; %tmp12 = add i64 %tmp11, 1; call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false); ret void. for.end: ; preds = %entry; ret void; }. This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold; the two memset's together. The issue with the addition only occurs in 64-bit mode, and appears to be at; least partially caused by Scalar Evolution not keeping its cache updated: it; returns the ""wrong"" result immediately after indvars runs, but figures out the; expected result if it is run from scratch on IR resulting from running indvars. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. struct S {; unsigned short m1, m2;; unsigned char m3, m4;; };. void f(int N) {; std::vector<S> v(N);; extern void sink(void*); sink(&v);; }. into poor code for zero-initializing 'v' when N is >0. The problem is that; S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and; 4 stores on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:16789,Energy Efficiency,reduce,reduces,16789,"return y;; }. //===---------------------------------------------------------------------===//. The loop unroller should partially unroll loops (instead of peeling them); when code growth isn't too bad and when an unroll count allows simplification; of some code within the loop. One trivial example is:. #include <stdio.h>; int main() {; int nRet = 17;; int nLoop;; for ( nLoop = 0; nLoop < 1000; nLoop++ ) {; if ( nLoop & 1 ); nRet += 2;; else; nRet -= 1;; }; return nRet;; }. Unrolling by 2 would eliminate the '&1' in both copies, leading to a net; reduction in code size. The resultant code would then also be suitable for; exit value computation. //===---------------------------------------------------------------------===//. We miss a bunch of rotate opportunities on various targets, including ppc, x86,; etc. On X86, we miss a bunch of 'rotate by variable' cases because the rotate; matching code in dag combine doesn't look through truncates aggressively ; enough. Here are some testcases reduces from GCC PR17886:. unsigned long long f5(unsigned long long x, unsigned long long y) {; return (x << 8) | ((y >> 48) & 0xffull);; }; unsigned long long f6(unsigned long long x, unsigned long long y, int z) {; switch(z) {; case 1:; return (x << 8) | ((y >> 48) & 0xffull);; case 2:; return (x << 16) | ((y >> 40) & 0xffffull);; case 3:; return (x << 24) | ((y >> 32) & 0xffffffull);; case 4:; return (x << 32) | ((y >> 24) & 0xffffffffull);; default:; return (x << 40) | ((y >> 16) & 0xffffffffffull);; }; }. //===---------------------------------------------------------------------===//. This (and similar related idioms):. unsigned int foo(unsigned char i) {; return i | (i<<8) | (i<<16) | (i<<24);; } . compiles into:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18510,Energy Efficiency,reduce,reduce,18510,"o:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by const",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:29658,Energy Efficiency,reduce,reduce,29658,"us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; pre",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:50527,Energy Efficiency,reduce,reduced,50527,"shift should be eliminated. Testcase derived from gcc. //===---------------------------------------------------------------------===//. These compile into different code, one gets recognized as a switch and the; other doesn't due to phase ordering issues (PR6212):. int test1(int mainType, int subType) {; if (mainType == 7); subType = 4;; else if (mainType == 9); subType = 6;; else if (mainType == 11); subType = 9;; return subType;; }. int test2(int mainType, int subType) {; if (mainType == 7); subType = 4;; if (mainType == 9); subType = 6;; if (mainType == 11); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===-------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:52916,Energy Efficiency,power,power,52916,"id @a(i32 %x) nounwind {; entry:; switch i32 %x, label %if.end [; i32 0, label %if.then; i32 1, label %if.then; i32 2, label %if.then; i32 3, label %if.then; i32 5, label %if.then; ]; if.then:; tail call void @foo() nounwind; ret void; if.end:; ret void; }; declare void @foo(). Generated code on x86-64 (other platforms give similar results):; a:; 	cmpl	$5, %edi; 	ja	LBB2_2; 	cmpl	$4, %edi; 	jne	LBB2_3; .LBB0_2:; 	ret; .LBB0_3:; 	jmp	foo # TAILCALL. If we wanted to be really clever, we could simplify the whole thing to; something like the following, which eliminates a branch:; 	xorl $1, %edi; 	cmpl	$4, %edi; 	ja	.LBB0_2; 	ret; .LBB0_2:; 	jmp	foo # TAILCALL. //===---------------------------------------------------------------------===//. We compile this:. int foo(int a) { return (a & (~15)) / 16; }. Into:. define i32 @foo(i32 %a) nounwind readnone ssp {; entry:; %and = and i32 %a, -16; %div = sdiv i32 %and, 16; ret i32 %div; }. but this code (X & -A)/A is X >> log2(A) when A is a power of 2, so this case; should be instcombined into just ""a >> 4"". We do get this at the codegen level, so something knows about it, but ; instcombine should catch it earlier:. _foo: ## @foo; ## %bb.0: ## %entry; 	movl	%edi, %eax; 	sarl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. This code (from GCC PR28685):. int test(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code ca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3863,Integrability,depend,dependent,3863,"eady available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	incl _foo; LBB16_2:	#cond_next. emit:; 	movl	_foo, %eax; 	cmpl	$1, %edi; 	sbbl	$-1, %eax; 	movl	%eax, _foo. //===---------------------------------------------------------------------===//. Combine: a = sin(x), b = cos(x) into a,b = sincos(x). Expand these to calls of sin/cos and stores:; double sincos(double x, double *sin, double *cos);; float sincosf(float x, float *sin, float *cos);; long double sincosl(long double x, long double *sin, long double *cos);. Doing so could allow SROA o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18161,Integrability,depend,depending,18161,"fffull);; case 4:; return (x << 32) | ((y >> 24) & 0xffffffffull);; default:; return (x << 40) | ((y >> 16) & 0xffffffffffull);; }; }. //===---------------------------------------------------------------------===//. This (and similar related idioms):. unsigned int foo(unsigned char i) {; return i | (i<<8) | (i<<16) | (i<<24);; } . compiles into:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:32415,Integrability,wrap,wrap,32415,". In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:45627,Integrability,depend,dependent,45627,"-------------------------------------------------------------===//. These two functions produce different code. They shouldn't:. #include <stdint.h>; ; uint8_t p1(uint8_t b, uint8_t a) {; b = (b & ~0xc0) | (a & 0xc0);; return (b);; }; ; uint8_t p2(uint8_t b, uint8_t a) {; b = (b & ~0x40) | (a & 0x40);; b = (b & ~0x80) | (a & 0x80);; return (b);; }. define zeroext i8 @p1(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. Thi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:1644,Modifiability,extend,extended,1644,"(ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:1717,Modifiability,enhance,enhanced,1717,"(ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8488,Modifiability,variab,variable,8488,"eturn adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:9790,Modifiability,extend,extended,9790,"$131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===---------------------------------------------------------------------===//. Tail call elim should be more aggressive, checking to see if the call is; followed by an uncond branch to an exit block. ; This testcase is due to tail-duplication not wanting to copy the return; ; instruction into the terminating blocks because there was other code; ; optimized out of the function after the taildup happened.; ; RUN: llvm-as < %s | opt -tailcallelim | llvm-dis | not grep call. define i32 @t4(i32 %a) {; entry:; 	%tmp.1 = and i32 %a, 1		; <i32> [#uses=1]; 	%tmp.2 = icmp ne i32 %tmp.1, 0		; <i1> [#uses=1]; 	br i1 %tmp.2, label %then.0, label %else.0. then.0:		; preds = %entry; 	%tmp.5 = add i32 %a, -1		; <i32> [#uses=1]; 	%tmp.3 = call i32 @t4( i32 %tmp.5 )		; <i32> [#uses=1]; 	br label %return. else.0:		; preds = %entry; 	%tmp.7 = icmp ne i32 %a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:16646,Modifiability,variab,variable,16646,"et int undef"". Instead, LLVM; produces ""ret int 0"":. int f() {; int x = 4;; int y;; if (x == 3) y = 0;; return y;; }. //===---------------------------------------------------------------------===//. The loop unroller should partially unroll loops (instead of peeling them); when code growth isn't too bad and when an unroll count allows simplification; of some code within the loop. One trivial example is:. #include <stdio.h>; int main() {; int nRet = 17;; int nLoop;; for ( nLoop = 0; nLoop < 1000; nLoop++ ) {; if ( nLoop & 1 ); nRet += 2;; else; nRet -= 1;; }; return nRet;; }. Unrolling by 2 would eliminate the '&1' in both copies, leading to a net; reduction in code size. The resultant code would then also be suitable for; exit value computation. //===---------------------------------------------------------------------===//. We miss a bunch of rotate opportunities on various targets, including ppc, x86,; etc. On X86, we miss a bunch of 'rotate by variable' cases because the rotate; matching code in dag combine doesn't look through truncates aggressively ; enough. Here are some testcases reduces from GCC PR17886:. unsigned long long f5(unsigned long long x, unsigned long long y) {; return (x << 8) | ((y >> 48) & 0xffull);; }; unsigned long long f6(unsigned long long x, unsigned long long y, int z) {; switch(z) {; case 1:; return (x << 8) | ((y >> 48) & 0xffull);; case 2:; return (x << 16) | ((y >> 40) & 0xffffull);; case 3:; return (x << 24) | ((y >> 32) & 0xffffffull);; case 4:; return (x << 32) | ((y >> 24) & 0xffffffffull);; default:; return (x << 40) | ((y >> 16) & 0xffffffffffull);; }; }. //===---------------------------------------------------------------------===//. This (and similar related idioms):. unsigned int foo(unsigned char i) {; return i | (i<<8) | (i<<16) | (i<<24);; } . compiles into:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i3",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:41486,Modifiability,variab,variable,41486,"entptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value of %177 is actually just %171-1. A sub is cheaper than a; strlen!. This pattern repeats several times, basically doing:. A = strlen(P);; P[A-1] = 0;; B = strlen(P);; where it is ""obvious"" that B = A-1. //===---------------------------------------------------------------------===//. 186.crafty has this interesting pattern with the ""out.4543"" variable:. call void @llvm.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:42514,Modifiability,extend,extending,42514,"m.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could be dagcombine'd to a sign-extending load with a shift.; For example, on x86 this currently gets this:. 	movb	(%eax), %al; 	sarb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should make use of nocapture to make its alias analysis; stuff much more precise. //===-------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46148,Modifiability,extend,extend,46148," ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:50886,Modifiability,variab,variables,50886,"ainType == 9); subType = 6;; else if (mainType == 11); subType = 9;; return subType;; }. int test2(int mainType, int subType) {; if (mainType == 7); subType = 4;; if (mainType == 9); subType = 6;; if (mainType == 11); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; case it choses instead to keep the max operation obvious. //===-----------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:60963,Modifiability,extend,extended,60963," on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:62123,Modifiability,enhance,enhance,62123,"/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. struct x {; char a;; int b[4];; };; void foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conserv",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64691,Modifiability,variab,variables,64691,".bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:2622,Performance,optimiz,optimized,2622,"ultiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. /",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3986,Performance,load,load,3986,"ns should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	incl _foo; LBB16_2:	#cond_next. emit:; 	movl	_foo, %eax; 	cmpl	$1, %edi; 	sbbl	$-1, %eax; 	movl	%eax, _foo. //===---------------------------------------------------------------------===//. Combine: a = sin(x), b = cos(x) into a,b = sincos(x). Expand these to calls of sin/cos and stores:; double sincos(double x, double *sin, double *cos);; float sincosf(float x, float *sin, float *cos);; long double sincosl(long double x, long double *sin, long double *cos);. Doing so could allow SROA of the destination pointers. See also:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=17687. This is now easily doable with MRVs. We could even make an intrinsic for",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:7332,Performance,load,loads,7332,"realize that it is finite (if it were infinite, it would be undefined). Not; having this blocks Loop Idiom from matching strlen and friends. . void foo(char *C) {; int x = 0;; while (*C); ++x,++C;; }. //===---------------------------------------------------------------------===//. [LOOP RECOGNITION]. These idioms should be recognized as popcount (see PR1488):. unsigned countbits_slow(unsigned v) {; unsigned c;; for (c = 0; v; v >>= 1); c += v & 1;; return c;; }. unsigned int popcount(unsigned int input) {; unsigned int count = 0;; for (unsigned int i = 0; i < 4 * 8; i++); count += (input >> i) & i;; return count;; }. This should be recognized as CLZ: https://github.com/llvm/llvm-project/issues/64167. unsigned clz_a(unsigned a) {; int i;; for (i=0;i<32;i++); if (a & (1<<(31-i))); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8325,Performance,optimiz,optimization,8325,"eturn adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:10162,Performance,optimiz,optimized,10162,"fective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===---------------------------------------------------------------------===//. Tail call elim should be more aggressive, checking to see if the call is; followed by an uncond branch to an exit block. ; This testcase is due to tail-duplication not wanting to copy the return; ; instruction into the terminating blocks because there was other code; ; optimized out of the function after the taildup happened.; ; RUN: llvm-as < %s | opt -tailcallelim | llvm-dis | not grep call. define i32 @t4(i32 %a) {; entry:; 	%tmp.1 = and i32 %a, 1		; <i32> [#uses=1]; 	%tmp.2 = icmp ne i32 %tmp.1, 0		; <i1> [#uses=1]; 	br i1 %tmp.2, label %then.0, label %else.0. then.0:		; preds = %entry; 	%tmp.5 = add i32 %a, -1		; <i32> [#uses=1]; 	%tmp.3 = call i32 @t4( i32 %tmp.5 )		; <i32> [#uses=1]; 	br label %return. else.0:		; preds = %entry; 	%tmp.7 = icmp ne i32 %a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i32> [#uses=1]; 	%tmp.9 = call i32 @t4( i32 %tmp.11 )		; <i32> [#uses=1]; 	br label %return. return:		; preds = %then.1, %else.0, %then.0; 	%result.0 = phi i32 [ 0, %else.0 ], [ %tmp.3, %then.0 ],; [ %tmp.9, %then.1 ]; 	ret i32 %result.0; }. //===---------------------------------------------------------------------===//. Tail ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:11669,Performance,load,load,11669,"a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i32> [#uses=1]; 	%tmp.9 = call i32 @t4( i32 %tmp.11 )		; <i32> [#uses=1]; 	br label %return. return:		; preds = %then.1, %else.0, %then.0; 	%result.0 = phi i32 [ 0, %else.0 ], [ %tmp.3, %then.0 ],; [ %tmp.9, %then.1 ]; 	ret i32 %result.0; }. //===---------------------------------------------------------------------===//. Tail recursion elimination should handle:. int pow2m1(int n) {; if (n == 0); return 0;; return 2 * pow2m1 (n - 1) + 1;; }. Also, multiplies can be turned into SHL's, so they should be handled as if; they were associative. ""return foo() << 1"" can be tail recursion eliminated. //===---------------------------------------------------------------------===//. Argument promotion should promote arguments for recursive functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is l",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12661,Performance,load,loads,12661,"es=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13212,Performance,load,load,13212,"2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13234,Performance,load,load,13234,"2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13888,Performance,load,loads,13888,"ndle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip), %rax; 	movzwl	(%rax), %ecx; 	movzbl	2(%rax), %edx; 	shlq	$16, %rdx; 	orq	%rcx, %rdx; 	movzbl	3(%rax), %ecx; 	shlq	$24, %rcx; 	orq	%rdx, %rcx; 	movzbl	4(%rax), %eax; 	shlq	$32, %rax; 	orq	%rcx, %rax; 	ret. //===---------------------------------------------------------------------===//. We should add an FRINT node to the DAG to model targets that have legal; implementations of ceil/floor/rint. //===---------------------------------------------------------------------===//. Consider:. int test() {; long long input[8] = {1,0,1,0,1,0,1,0};; foo(input);; }. Clang compiles this into:. call void @llvm.memset.p0i8.i64(i8* %tmp, i8 0, i64 64, i32 16",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13906,Performance,load,loads,13906,"ndle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip), %rax; 	movzwl	(%rax), %ecx; 	movzbl	2(%rax), %edx; 	shlq	$16, %rdx; 	orq	%rcx, %rdx; 	movzbl	3(%rax), %ecx; 	shlq	$24, %rcx; 	orq	%rdx, %rcx; 	movzbl	4(%rax), %eax; 	shlq	$32, %rax; 	orq	%rcx, %rax; 	ret. //===---------------------------------------------------------------------===//. We should add an FRINT node to the DAG to model targets that have legal; implementations of ceil/floor/rint. //===---------------------------------------------------------------------===//. Consider:. int test() {; long long input[8] = {1,0,1,0,1,0,1,0};; foo(input);; }. Clang compiles this into:. call void @llvm.memset.p0i8.i64(i8* %tmp, i8 0, i64 64, i32 16",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20837,Performance,perform,perform,20837,"6U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:21581,Performance,optimiz,optimized,21581,"e <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:21916,Performance,optimiz,optimize,21916,"form the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:21985,Performance,optimiz,optimized,21985," ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:22566,Performance,optimiz,optimize,22566,"th should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:22623,Performance,optimiz,optimized,22623,"------------------------------------------------------------------===//. From GCC Bug 20192:; #define PMD_MASK (~((1UL << 23) - 1)); void clear_pmd_range(unsigned long start, unsigned long end); {; if (!(start & ~PMD_MASK) && !(end & ~PMD_MASK)); f();; }; The expression should optimize to something like; ""!((start|end)&~PMD_MASK). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned int f(unsigned int i, unsigned int n) {++i; if (i == n) ++i; return; i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------==",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23248,Performance,optimiz,optimized,23248,"= i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimize",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23482,Performance,optimiz,optimized,23482,"x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===----",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23548,Performance,optimiz,optimize,23548,"x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===----",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23760,Performance,optimiz,optimized,23760,"d; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24006,Performance,optimiz,optimized,24006,"o a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===--------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24248,Performance,optimiz,optimized,24248,"clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===--------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24456,Performance,optimiz,optimized,24456,"ently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===-----------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24669,Performance,optimiz,optimized,24669,"<< 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===--------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:24926,Performance,optimiz,optimized,24926,"& 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsig",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25167,Performance,optimiz,optimized,25167,"(c|a) & b);}; Should fold to ""(~a & c) | (a & b)"". Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a,int b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. un",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25378,Performance,optimiz,optimized,25378,"b) {return (~(a|b))|a;}; Should fold to ""a|~b"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b) {return (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===----------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25594,Performance,optimiz,optimized,25594,"urn (a&&b) || (a&&!b);}; Should fold to ""a"". Currently not optimized with ""clang -emit-llvm-bc; | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===-------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:25807,Performance,optimiz,optimized,25807,"c) {return (a&&b) || (!a&&c);}; Should fold to ""a ? b : c"", or at least something sane. Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26052,Performance,optimiz,optimized,26052,"int b, int c) {return (a&&b) || (a&&c) || (a&&b&&c);}; Should fold to a && (b || c). Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x | ((x & 8) ^ 8);}; Should combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return x ^ ((x & 8) ^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26527,Performance,optimiz,optimized,26527,"^ 8);}; Should also combine to x | 8. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int x) {return ((x | -9) ^ 8) & x;}; Should combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimize",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26756,Performance,optimiz,optimized,26756,"ld combine to x & -9. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned a) {return a * 0x11111111 >> 28 & 1;}; Should combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %de",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:26986,Performance,optimiz,optimized,26986,"d combine to ""a * 0x88888888 >> 31"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(char* x) {if ((*x & 32) == 0) return b();}; There's an unnecessary zext in the generated code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27266,Performance,optimiz,optimized,27266," code with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned long long x) {return 40 * (x >> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#us",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27474,Performance,optimiz,optimized,27474,">> 1);}; Should combine to ""20 * (((unsigned)x) & -2)"". Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27535,Performance,optimiz,optimized,27535,"emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x - 10) < 0; }; Should combine to ""x <= 9"" (the sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int g(int x) { return (x + 10) < 0; }; Should combine to ""x < -10"" (the add has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:28808,Performance,load,load,28808," = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, nu",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:30267,Performance,load,load,30267,"partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:30428,Performance,load,load,30428," the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31194,Performance,load,load,31194," = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31795,Performance,load,load,31795,"%indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbol",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:32092,Performance,load,load,32092,"pletely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:32630,Performance,load,loads,32630,"q	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRAN",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33189,Performance,load,load,33189,"on-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33431,Performance,load,load,33431,"nk the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; stor",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33479,Performance,load,loadpre,33479,"---------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34574,Performance,load,loaded,34574,"checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANAL",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:35284,Performance,load,loads,35284,"itable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OV",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:35396,Performance,load,loads,35396,"low the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:35440,Performance,load,load,35440,"low the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36036,Performance,load,loads,36036,"dcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36486,Performance,load,loading,36486,"------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36814,Performance,optimiz,optimizations,36814,"alloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should turn these snprintf idioms into memcpy (GCC PR47917). char buf1[6], buf2[6], buf3[4], buf4[4];; int i;. int foo (void) {; int ret = snprintf (buf1, sizeof buf1, ""abcde"");; ret += snprintf (buf2, sizeof buf2, ""abcdef"") * 16;; ret += snprintf (buf3, sizeof buf3, ""%s"", i++ < 6 ? ""abc"" : ""def"") * 256;; ret += snprintf (buf4, sizeof buf4, ""%s"",",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:39496,Performance,load,load,39496,"memcpy.i32(i8* %endptr, ; i8* getelementptr ([5 x i8]* @""\01LC42"", i32 0, i32 0), i32 5, i32 1); %3074 = call i32 @strlen(i8* %endptr) nounwind readonly ; ; This is interesting for a couple reasons. First, in this:. The memcpy+strlen strlen can be replaced with:. %3074 = call i32 @strlen([5 x i8]* @""\01LC42"") nounwind readonly . Because the destination was just copied into the specified memory buffer. This,; in turn, can be constant folded to ""4"". In other code, it contains:. %endptr6978 = bitcast i8* %endptr69 to i32* ; store i32 7107374, i32* %endptr6978, align 1; %3167 = call i32 @strlen(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no sto",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:39546,Performance,load,load,39546,"memcpy.i32(i8* %endptr, ; i8* getelementptr ([5 x i8]* @""\01LC42"", i32 0, i32 0), i32 5, i32 1); %3074 = call i32 @strlen(i8* %endptr) nounwind readonly ; ; This is interesting for a couple reasons. First, in this:. The memcpy+strlen strlen can be replaced with:. %3074 = call i32 @strlen([5 x i8]* @""\01LC42"") nounwind readonly . Because the destination was just copied into the specified memory buffer. This,; in turn, can be constant folded to ""4"". In other code, it contains:. %endptr6978 = bitcast i8* %endptr69 to i32* ; store i32 7107374, i32* %endptr6978, align 1; %3167 = call i32 @strlen(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no sto",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:42395,Performance,load,load,42395,"----------===//. 186.crafty has this interesting pattern with the ""out.4543"" variable:. call void @llvm.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could be dagcombine'd to a sign-extending load with a shift.; For example, on x86 this currently gets this:. 	movb	(%eax), %al; 	sarb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should mak",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:42524,Performance,load,load,42524,"m.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could be dagcombine'd to a sign-extending load with a shift.; For example, on x86 this currently gets this:. 	movb	(%eax), %al; 	sarb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should make use of nocapture to make its alias analysis; stuff much more precise. //===-------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:43597,Performance,optimiz,optimized,43597,"rb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should make use of nocapture to make its alias analysis; stuff much more precise. //===---------------------------------------------------------------------===//. The following functions should be optimized to use a select instead of a; branch (from gcc PR40072):. char char_int(int m) {if(m>7) return 0; return m;}; int int_char(char m) {if(m>7) return 0; return m;}. //===---------------------------------------------------------------------===//. int func(int a, int b) { if (a & 0x80) b |= 0x80; else b &= ~0x80; return b; }. Generates this:. define i32 @func(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = and i32 %a, 128 ; <i32> [#uses=1]; %1 = icmp eq i32 %0, 0 ; <i1> [#uses=1]; %2 = or i32 %b, 128 ; <i32> [#uses=1]; %3 = and i32 %b, -129 ; <i32> [#uses=1]; %b_addr.0 = select i1 %1, i32 %3, i32 %2 ; <i32> [#uses=1]; ret i32 %b_addr.0; }. However, it's functionally equivalent to:. b = (b & ~0x80) | (a & 0x80);. Which generates this:. define i32 @func(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = and i32 %b, -129 ; <i32> [#uses=1]; %1 = and i32 %a, 128 ; <i32> [#uses=1]; %2 = or i32 %0, %1 ; <i32> [#uses=1]; ret i32 %2; }. This can be generalized for other forms:. b = (b & ~0x80) | ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:47678,Performance,optimiz,optimized,47678," SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int **p;; y.p = &i;; x.q = &j;; p = __builtin_memcpy (&x, &y, sizeof (int *));; return **p;; }. This can be seen at:; $ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -function-attrs -S. //===---------------------------------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i32 %x, -30; %cmp2 = icmp ugt i32 %sub, 9; %or = or i1 %cmp, %cmp2; ret i1 %or; }; This should be optimized to a single compare. Testcase derived from gcc. //===---------------------------------------------------------------------===//. Missed instcombine or reassociate transformation:; int a(int a, int b) { return (a==12)&(b>47)&(b<58); }. The sgt and slt should be combined into a single comparison. Testcase derived; from gcc. //===---------------------------------------------------------------------===//. Missed instcombine transformation:. %382 = srem i32 %tmp14.i, 64 ; [#uses=1]; %383 = zext i32 %382 to i64 ; [#uses=1]; %384 = shl i64 %381, %383 ; [#uses=1]; %385 = icmp slt i32 %tmp14.i, 64 ; [#uses=1]. The srem can be transformed to an and because if %tmp14.i is negative, the; shift is undefined. Testcase derived from 403.gcc. //===---------------------------------------------------------------------===//. This is a range comparison on a divided result (from 403.gcc):. %1337 = sdiv ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:51360,Performance,optimiz,optimizer,51360,"ses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; case it choses instead to keep the max operation obvious. //===---------------------------------------------------------------------===//. define void @a(i32 %x) nounwind {; entry:; switch i32 %x, label %if.end [; i32 0, label %if.then; i32 1, label %if.then; i32 2, label %if.then; i32 3, label %if.then; i32 5, label %if.then; ]; if.then:; tail call void @foo() nounwind; ret void; if.end:; ret void; }; declare void @foo(). Generated code on x86-64 (other platforms give similar results):; a:; 	cmpl	$5, %edi; 	ja	LBB2_2; 	cmpl	$4, %edi; 	jne	LBB2_3; .LBB0_2:; 	ret; .LBB0",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:54299,Performance,perform,performance,54299,"(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize which by its; current definition always folds to a constant. We also should make sure that; we remove checking in code like. char *p = malloc(strlen(s)+1);; __strcpy_chk(p, s, __builtin_object_size(p, 0));. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:54347,Performance,optimiz,optimizer,54347,"(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize which by its; current definition always folds to a constant. We also should make sure that; we remove checking in code like. char *p = malloc(strlen(s)+1);; __strcpy_chk(p, s, __builtin_object_size(p, 0));. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:55725,Performance,optimiz,optimize,55725,"----------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===---------------------------------------------------------------------===//. memcpyopt should turn this:. define i8* @test10(i32 %x) {; %alloc = call noalias i8* @malloc(i32 %x) nounwind; call void @llvm.memset.p0i8.i32(i8* %alloc, i8 0, i32 %x, i32 1, i1 false); ret i8* %alloc; }. into a call to calloc. We should make sure that we analyze calloc as; aggressively as malloc though. //===---------------------------------------------------------------------===//. clang -O3 doesn't optimize this:. void f1(int* begin, int* end) {; std::fill(begin, end, 0);; }. into a memset. This is PR8942. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(int N) {; std::vector<int> v(N);. extern void sink(void*); sink(&v);; }. into. define void @_Z1fi(i32 %N) nounwind {; entry:; %v2 = alloca [3 x i32*], align 8; %v2.sub = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 0; %tmpcast = bitcast [3 x i32*]* %v2 to %""class.std::vector""*; %conv = sext i32 %N to i64; store i32* null, i32** %v2.sub, align 8, !tbaa !0; %tmp3.i.i.i.i.i = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 1; store i32* null, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0; %tmp4.i.i.i.i.i = getelementptr inbounds [3 x i32*]* %v2, i64 0, i64 2; store i32* null, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0; %cmp.i.i.i.i = icmp eq i32 %N, 0; br i1 %cmp.i.i.i.i, label %_ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.thread.i.i, la",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:59366,Performance,cache,cache,59366,"-------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(char* a, int n) {; __builtin_memset(a, 0, n);; for (int i = 0; i < n; ++i); a[i] = 0;; }. into:. define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {; entry:; %conv = sext i32 %n to i64; tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false); %cmp8 = icmp sgt i32 %n, 0; br i1 %cmp8, label %for.body.lr.ph, label %for.end. for.body.lr.ph: ; preds = %entry; %tmp10 = add i32 %n, -1; %tmp11 = zext i32 %tmp10 to i64; %tmp12 = add i64 %tmp11, 1; call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false); ret void. for.end: ; preds = %entry; ret void; }. This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold; the two memset's together. The issue with the addition only occurs in 64-bit mode, and appears to be at; least partially caused by Scalar Evolution not keeping its cache updated: it; returns the ""wrong"" result immediately after indvars runs, but figures out the; expected result if it is run from scratch on IR resulting from running indvars. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. struct S {; unsigned short m1, m2;; unsigned char m3, m4;; };. void f(int N) {; std::vector<S> v(N);; extern void sink(void*); sink(&v);; }. into poor code for zero-initializing 'v' when N is >0. The problem is that; S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and; 4 stores on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:60598,Performance,load,load,60598,"-===//. clang -O3 -fno-exceptions currently compiles this code:. struct S {; unsigned short m1, m2;; unsigned char m3, m4;; };. void f(int N) {; std::vector<S> v(N);; extern void sink(void*); sink(&v);; }. into poor code for zero-initializing 'v' when N is >0. The problem is that; S is only 6 bytes, but each element is 8 byte-aligned. We generate a loop and; 4 stores on each iteration. If the struct were 8 bytes, this gets turned into; a memset. In order to handle this we have to:; A) Teach clang to generate metadata for memsets of structs that have holes in; them.; B) Teach clang to use such a memset for zero init of this struct (since it has; a hole), instead of doing elementwise zeroing. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:61371,Performance,optimiz,optimizing,61371,". clang -O3 currently compiles this code:. extern const int magic;; double f() { return 0.0 * magic; }. into. @magic = external constant i32. define double @_Z1fv() nounwind readnone {; entry:; %tmp = load i32* @magic, align 4, !tbaa !0; %conv = sitofp i32 %tmp to double; %mul = fmul double %conv, 0.000000e+00; ret double %mul; }. We should be able to fold away this fmul to 0.0. More generally, fmul(x,0.0); can be folded to 0.0 if we can prove that the LHS is not -0.0, not a NaN, and; not an INF. The CannotBeNegativeZero predicate in value tracking should be; extended to support general ""fpclassify"" operations that can return ; yes/no/unknown for each of these predicates. In this predicate, we know that uitofp is trivially never NaN or -0.0, and; we know that it isn't +/-Inf if the floating point type has enough exponent bits; to represent the largest integer value as < inf. //===---------------------------------------------------------------------===//. When optimizing a transformation that can change the sign of 0.0 (such as the; 0.0*val -> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. str",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:62876,Performance,load,load,62876," 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. struct x {; char a;; int b[4];; };; void foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this becaus",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64015,Performance,perform,performance,64015,"160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64738,Performance,perform,performance,64738,".bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:66189,Performance,load,load,66189,"\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===---------------------------------------------------------------------===//. This code:. typedef struct {; int f1:1;; int f2:1;; int f3:1;; int f4:29;; } t1;. typedef struct {; int f1:1;; int f2:1;; int f3:30;; } t2;. t1 s1;; t2 s2;. void func1(void); {; s1.f1 = s2.f1;; s1.f2 = s2.f2;; }. Compiles into this IR (on x86-64 at least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===---------------------------------------------------------------------===//. Machine level code hoisting can be useful in some cases. For example, PR9408; is about:. typedef union {; void (*f1)(int);; void (*f2)(long);; } funcs;. void foo(funcs f, int which) {; int a = 5;; if (which) {; f.f1(a);; } else {; f.f2(a);; }; }. which we compile to:. foo: # @foo; # %bb.0: # %entry; pushq %rbp; movq %rsp, %rbp; testl %esi, %esi; movq %rdi, %rax; je .LBB0_2; # %bb.1: # %if.then; movl $5, %edi; callq *%rax; popq %rbp; ret; .LBB0_2: # %if.else; movl $5, %edi; callq *%rax; popq %rbp; r",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:66279,Performance,load,load,66279,"--===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits shrinks the ""and"" constant to 2 but instcombine misses the; icmp transform. //===---------------------------------------------------------------------===//. This code:. typedef struct {; int f1:1;; int f2:1;; int f3:1;; int f4:29;; } t1;. typedef struct {; int f1:1;; int f2:1;; int f3:30;; } t2;. t1 s1;; t2 s2;. void func1(void); {; s1.f1 = s2.f1;; s1.f2 = s2.f2;; }. Compiles into this IR (on x86-64 at least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===---------------------------------------------------------------------===//. Machine level code hoisting can be useful in some cases. For example, PR9408; is about:. typedef union {; void (*f1)(int);; void (*f2)(long);; } funcs;. void foo(funcs f, int which) {; int a = 5;; if (which) {; f.f1(a);; } else {; f.f2(a);; }; }. which we compile to:. foo: # @foo; # %bb.0: # %entry; pushq %rbp; movq %rsp, %rbp; testl %esi, %esi; movq %rdi, %rax; je .LBB0_2; # %bb.1: # %if.then; movl $5, %edi; callq *%rax; popq %rbp; ret; .LBB0_2: # %if.else; movl $5, %edi; callq *%rax; popq %rbp; ret. Note that bb1 and bb2 are the same. This doesn't happen at the IR level; because one call is passing an i32 and the o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:155,Safety,detect,detection,155,"Target Independent Opportunities:. //===---------------------------------------------------------------------===//. We should recognized various ""overflow detection"" idioms and translate them into; llvm.uadd.with.overflow and similar intrinsics. Here is a multiply idiom:. unsigned int mul(unsigned int a,unsigned int b) {; if ((unsigned long long)a*b>0xffffffff); exit(0);; return a*b;; }. The legalization code for mul-with-overflow needs to be made more robust before; this can be implemented though. //===---------------------------------------------------------------------===//. Get the C front-end to expand hypot(x,y) -> llvm.sqrt(x*x+y*y) when errno and; precision don't matter (ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:743,Safety,safe,safe,743,"Target Independent Opportunities:. //===---------------------------------------------------------------------===//. We should recognized various ""overflow detection"" idioms and translate them into; llvm.uadd.with.overflow and similar intrinsics. Here is a multiply idiom:. unsigned int mul(unsigned int a,unsigned int b) {; if ((unsigned long long)a*b>0xffffffff); exit(0);; return a*b;; }. The legalization code for mul-with-overflow needs to be made more robust before; this can be implemented though. //===---------------------------------------------------------------------===//. Get the C front-end to expand hypot(x,y) -> llvm.sqrt(x*x+y*y) when errno and; precision don't matter (ffastmath). Misc/mandel will like this. :) This isn't; safe in general, even on darwin. See the libm implementation of hypot for; examples (which special case when x/y are exactly zero to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:7804,Safety,avoid,avoids,7804,"ned int count = 0;; for (unsigned int i = 0; i < 4 * 8; i++); count += (input >> i) & i;; return count;; }. This should be recognized as CLZ: https://github.com/llvm/llvm-project/issues/64167. unsigned clz_a(unsigned a) {; int i;; for (i=0;i<32;i++); if (a & (1<<(31-i))); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18682,Safety,safe,safely,18682," or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by constant can be simplified (according to GCC PR12849) from; being a mulhi to being a mul lo (cheaper). Testcase:. void bar(unsigned n) {; if (n % 3 == 0); true();; }. This is equivalent to the f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:28581,Safety,redund,redundant,28581,"-------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31184,Safety,redund,redundant,31184," = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-c",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:31375,Safety,redund,redundant,31375,"p10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i64 %indvar, 1; %exitcond = icmp eq i64 %indvar.next, %tmp22; br i1 %exitcond, label %for.cond.for.end_crit_edge, label %for.body. It is good that we hoisted the reloads of numf2's, and Y out of the loop and; sunk the store to winner out. However, this is awful on several levels: the conditional truncate in the loop; (-indvars at fault? why can't we completely promote the IV to i64?). Beyond that, we have a partially redundant load in the loop: if ""winner"" (aka ; %i.01718) isn't updated, we reload Y[winner].y the next time through the loop.; Similarly, the addressing that feeds it (including the sext) is redundant. In; the end we get this generated assembly:. LBB0_2: ## %for.body; ## =>This Inner Loop Header: Depth=1; 	movsd	(%rdi), %xmm0; 	movslq	%edx, %r8; 	shlq	$4, %r8; 	ucomisd	(%rcx,%r8), %xmm0; 	jbe	LBB0_4; 	movl	%esi, %edx; LBB0_4: ## %for.inc; 	addq	$16, %rdi; 	incq	%rsi; 	cmpq	%rsi, %rax; 	jne	LBB0_2. All things considered this isn't too bad, but we shouldn't need the movslq or; the shlq instruction, or the load folded into ucomisd every time through the; loop. On an x86-specific topic, if the loop can't be restructure, the movl should be a; cmov. //===---------------------------------------------------------------------===//. [STORE SINKING]. GCC PR37810 is an interesting case where we should sink load/store reload; into the if block and outside the loop, so we don't reload/store it on the; non-call path. for () {; *P += 1;; if (); call();; else; ...; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33230,Safety,redund,redundant,33230,"..; ->; tmp = *P; for () {; tmp += 1;; if () {; *P = tmp;; call();; tmp = *P;; } else ...; }; *P = tmp;. We now hoist the reload after the call (Transforms/GVN/lpre-call-wrap.ll), but; we don't sink the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:39356,Safety,redund,redundant,39356,"0, i32 0; %3073 = call i8* @strcpy(i8* %3072, i8* %3071) nounwind; %strlen = call i32 @strlen(i8* %3072) ; uses = 1; %endptr = getelementptr [100 x i8]* %tempString, i32 0, i32 %strlen; call void @llvm.memcpy.i32(i8* %endptr, ; i8* getelementptr ([5 x i8]* @""\01LC42"", i32 0, i32 0), i32 5, i32 1); %3074 = call i32 @strlen(i8* %endptr) nounwind readonly ; ; This is interesting for a couple reasons. First, in this:. The memcpy+strlen strlen can be replaced with:. %3074 = call i32 @strlen([5 x i8]* @""\01LC42"") nounwind readonly . Because the destination was just copied into the specified memory buffer. This,; in turn, can be constant folded to ""4"". In other code, it contains:. %endptr6978 = bitcast i8* %endptr69 to i32* ; store i32 7107374, i32* %endptr6978, align 1; %3167 = call i32 @strlen(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:40138,Safety,redund,redundant,40138,"(i8* %endptr69) nounwind readonly . Which could also be constant folded. Whatever is producing this should probably; be fixed to leave this as a memcpy from a string. Further, eon also has an interesting partially redundant strlen call:. bb8: ; preds = %_ZN18eonImageCalculatorC1Ev.exit; %682 = getelementptr i8** %argv, i32 6 ; <i8**> [#uses=2]; %683 = load i8** %682, align 4 ; <i8*> [#uses=4]; %684 = load i8* %683, align 1 ; <i8> [#uses=1]; %685 = icmp eq i8 %684, 0 ; <i1> [#uses=1]; br i1 %685, label %bb10, label %bb9. bb9: ; preds = %bb8; %686 = call i32 @strlen(i8* %683) nounwind readonly ; %687 = icmp ugt i32 %686, 254 ; <i1> [#uses=1]; br i1 %687, label %bb10, label %bb11. bb10: ; preds = %bb9, %bb8; %688 = call i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:40891,Safety,redund,redundant,40891," i32 @strlen(i8* %683) nounwind readonly . This could be eliminated by doing the strlen once in bb8, saving code size and; improving perf on the bb8->9->10 path. //===---------------------------------------------------------------------===//. I see an interesting fully redundant call to strlen left in 186.crafty:InputMove; which looks like:; %movetext11 = getelementptr [128 x i8]* %movetext, i32 0, i32 0 ; . bb62: ; preds = %bb55, %bb53; %promote.0 = phi i32 [ %169, %bb55 ], [ 0, %bb53 ] ; %171 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1; %172 = add i32 %171, -1 ; <i32> [#uses=1]; %173 = getelementptr [128 x i8]* %movetext, i32 0, i32 %172 . ... no stores ...; br i1 %or.cond, label %bb65, label %bb72. bb65: ; preds = %bb62; store i8 0, i8* %173, align 1; br label %bb72. bb72: ; preds = %bb65, %bb62; %trank.1 = phi i32 [ %176, %bb65 ], [ -1, %bb62 ] ; %177 = call i32 @strlen(i8* %movetext11) nounwind readonly align 1. Note that on the bb62->bb72 path, that the %177 strlen call is partially; redundant with the %171 call. At worst, we could shove the %177 strlen call; up into the bb65 block moving it out of the bb62->bb72 path. However, note; that bb65 stores to the string, zeroing out the last byte. This means that on; that path the value of %177 is actually just %171-1. A sub is cheaper than a; strlen!. This pattern repeats several times, basically doing:. A = strlen(P);; P[A-1] = 0;; B = strlen(P);; where it is ""obvious"" that B = A-1. //===---------------------------------------------------------------------===//. 186.crafty has this interesting pattern with the ""out.4543"" variable:. call void @llvm.memcpy.i32(; i8* getelementptr ([10 x i8]* @out.4543, i32 0, i32 0),; i8* getelementptr ([7 x i8]* @""\01LC28700"", i32 0, i32 0), i32 7, i32 1) ; %101 = call@printf(i8* ... @out.4543, i32 0, i32 0)) nounwind . It is basically doing:. memcpy(globalarray, ""string"");; printf(..., globalarray);; ; Anyway, by knowing that printf just reads the memory and forw",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:63413,Safety,avoid,avoids,63413," foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; bot",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8106,Security,hash,hash,8106,")); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13042,Security,hash,hash,13042,"der this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:13084,Security,hash,hash,13084,"void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] ^= (1ULL << 63);; }. and I suspect other processors are similar. On X86 in particular this is a; big win because doing this with integers allows the use of read/modify/write; instructions. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey (",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:54236,Security,secur,security,54236,"(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize which by its; current definition always folds to a constant. We also should make sure that; we remove checking in code like. char *p = malloc(strlen(s)+1);; __strcpy_chk(p, s, __builtin_object_size(p, 0));. //===---------------------------------------------------------------------===//. clang -O3 currently compiles this code. int g(unsigned int a) {; unsigned int c[100];; c[10] = a;; c[11] = a;; unsigned int b = c[10] + c[11];; if(b > a*2) a = 4;; else a = 8;; return a + 7;; }. into. define i32 @g(i32 a) nounwind readnone {; %add = shl i32 %a, 1; %mul = shl i32 %a, 1; %cmp = icmp ugt i32 %add, %mul; %a.addr.0 = select i1 %cmp, i32 11, i32 15; ret i32 %a.addr.0; }. The icmp should fold to false. This CSE opportunity is only available; after GVN and InstCombine have run. //===------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:1860,Testability,test,testcase,1860,"o to get signed zeros etc; right). //===---------------------------------------------------------------------===//. On targets with expensive 64-bit multiply, we could LSR this:. for (i = ...; ++i) {; x = 1ULL << i;. into:; long long tmp = 1;; for (i = ...; ++i, tmp+=tmp); x = tmp;. This would be a win on ppc32, but not x86 or ppc64. //===---------------------------------------------------------------------===//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't thi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:8121,Testability,test,tests,8121,")); return i;; return 32;; }. This sort of thing should be added to the loop idiom pass. //===---------------------------------------------------------------------===//. These should turn into single 16-bit (unaligned?) loads on little/big endian; processors. unsigned short read_16_le(const unsigned char *adr) {; return adr[0] | (adr[1] << 8);; }; unsigned short read_16_be(const unsigned char *adr) {; return (adr[0] << 8) | adr[1];; }. //===---------------------------------------------------------------------===//. -instcombine should handle this transform:; icmp pred (sdiv X / C1 ), C2; when X, C1, and C2 are unsigned. Similarly for udiv and signed operands. . Currently InstCombine avoids this transform but will do it when the signs of; the operands and the sign of the divide match. See the FIXME in ; InstructionCombining.cpp in the visitSetCondInst method after the switch case ; for Instruction::UDiv (around line 4447) for more details. The SingleSource/Benchmarks/Shootout-C++/hash and hash2 tests have examples of; this construct. . //===---------------------------------------------------------------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:9078,Testability,test,test,9078,"-------------===//. [LOOP OPTIMIZATION]. SingleSource/Benchmarks/Misc/dt.c shows several interesting optimization; opportunities in its double_array_divs_variable function: it needs loop; interchange, memory promotion (which LICM already does), vectorization and; variable trip count loop unrolling (since it has a constant trip count). ICC; apparently produces this very nice code with -ffast-math:. ..B1.70: # Preds ..B1.70 ..B1.69; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; mulpd %xmm0, %xmm1 #108.2; addl $8, %edx #; cmpl $131072, %edx #108.2; jb ..B1.70 # Prob 99% #108.2. It would be better to count down to zero, but this is a lot better than what we; do. //===---------------------------------------------------------------------===//. Consider:. typedef unsigned U32;; typedef unsigned long long U64;; int test (U32 *inst, U64 *regs) {; U64 effective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===---------------------------------------------------------------------===//. Tail call elim should be more aggressive, checking to see if the call is; followed by an uncond branch to an exit block. ; This testcase is due to tail-duplication not wanting to copy the return; ; instruction into the terminating blocks because there was other code; ; optimized out of the function after the taildup happened.; ; R",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:10020,Testability,test,testcase,10020,"fective_addr2;; U32 temp = *inst;; int r1 = (temp >> 20) & 0xf;; int b2 = (temp >> 16) & 0xf;; effective_addr2 = temp & 0xfff;; if (b2) effective_addr2 += regs[b2];; b2 = (temp >> 12) & 0xf;; if (b2) effective_addr2 += regs[b2];; effective_addr2 &= regs[4];; if ((effective_addr2 & 3) == 0); return 1;; return 0;; }. Note that only the low 2 bits of effective_addr2 are used. On 32-bit systems,; we don't eliminate the computation of the top half of effective_addr2 because; we don't have whole-function selection dags. On x86, this means we use one; extra register for the function when effective_addr2 is declared as U64 than; when it is declared U32. PHI Slicing could be extended to do this. //===---------------------------------------------------------------------===//. Tail call elim should be more aggressive, checking to see if the call is; followed by an uncond branch to an exit block. ; This testcase is due to tail-duplication not wanting to copy the return; ; instruction into the terminating blocks because there was other code; ; optimized out of the function after the taildup happened.; ; RUN: llvm-as < %s | opt -tailcallelim | llvm-dis | not grep call. define i32 @t4(i32 %a) {; entry:; 	%tmp.1 = and i32 %a, 1		; <i32> [#uses=1]; 	%tmp.2 = icmp ne i32 %tmp.1, 0		; <i1> [#uses=1]; 	br i1 %tmp.2, label %then.0, label %else.0. then.0:		; preds = %entry; 	%tmp.5 = add i32 %a, -1		; <i32> [#uses=1]; 	%tmp.3 = call i32 @t4( i32 %tmp.5 )		; <i32> [#uses=1]; 	br label %return. else.0:		; preds = %entry; 	%tmp.7 = icmp ne i32 %a, 0		; <i1> [#uses=1]; 	br i1 %tmp.7, label %then.1, label %return. then.1:		; preds = %else.0; 	%tmp.11 = add i32 %a, -2		; <i32> [#uses=1]; 	%tmp.9 = call i32 @t4( i32 %tmp.11 )		; <i32> [#uses=1]; 	br label %return. return:		; preds = %then.1, %else.0, %then.0; 	%result.0 = phi i32 [ 0, %else.0 ], [ %tmp.3, %then.0 ],; [ %tmp.9, %then.1 ]; 	ret i32 %result.0; }. //===---------------------------------------------------------------------===//. Tail ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12075,Testability,assert,assert,12075,"-------------------------------===//. Tail recursion elimination should handle:. int pow2m1(int n) {; if (n == 0); return 0;; return 2 * pow2m1 (n - 1) + 1;; }. Also, multiplies can be turned into SHL's, so they should be handled as if; they were associative. ""return foo() << 1"" can be tail recursion eliminated. //===---------------------------------------------------------------------===//. Argument promotion should promote arguments for recursive functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburt",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12105,Testability,assert,assert,12105,"---===//. Tail recursion elimination should handle:. int pow2m1(int n) {; if (n == 0); return 0;; return 2 * pow2m1 (n - 1) + 1;; }. Also, multiplies can be turned into SHL's, so they should be handled as if; they were associative. ""return foo() << 1"" can be tail recursion eliminated. //===---------------------------------------------------------------------===//. Argument promotion should promote arguments for recursive functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:12545,Testability,assert,assertion,12545,"functions, like ; this:. ; RUN: llvm-as < %s | opt -argpromotion | llvm-dis | grep x.val. define internal i32 @foo(i32* %x) {; entry:; 	%tmp = load i32* %x		; <i32> [#uses=0]; 	%tmp.foo = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp.foo; }. define i32 @bar(i32* %x) {; entry:; 	%tmp3 = call i32 @foo( i32* %x )		; <i32> [#uses=1]; 	ret i32 %tmp3; }. //===---------------------------------------------------------------------===//. We should investigate an instruction sinking pass. Consider this silly; example in pic mode:. #include <assert.h>; void foo(int x) {; assert(x);; //...; }. we compile this to:; _foo:; 	subl	$28, %esp; 	call	""L1$pb""; ""L1$pb"":; 	popl	%eax; 	cmpl	$0, 32(%esp); 	je	LBB1_2	# cond_true; LBB1_1:	# return; 	# ...; 	addl	$28, %esp; 	ret; LBB1_2:	# cond_true; ... The PIC base computation (call+popl) is only used on one path through the ; code, but is currently always computed in the entry block. It would be ; better to sink the picbase computation down into the block for the ; assertion, as it is the only one that uses it. This happens for a lot of ; code with early outs. Another example is loads of arguments, which are usually emitted into the ; entry block on targets like x86. If not used in all paths through a ; function, they should be sunk into the ones that do. In this case, whole-function-isel would also handle this. //===---------------------------------------------------------------------===//. Investigate lowering of sparse switch statements into perfect hash tables:; http://burtleburtle.net/bob/hash/perfect.html. //===---------------------------------------------------------------------===//. We should turn things like ""load+fabs+store"" and ""load+fneg+store"" into the; corresponding integer operations. On a yonah, this loop:. double a[256];; void foo() {; int i, b;; for (b = 0; b < 10000000; b++); for (i = 0; i < 256; i++); a[i] = -a[i];; }. is twice as slow as this loop:. long long a[256];; void foo() {; int i, b;; for (b = 0; b",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:14732,Testability,test,test,14732,"ns. //===---------------------------------------------------------------------===//. DAG Combiner should try to combine small loads into larger loads when ; profitable. For example, we compile this C++ example:. struct THotKey { short Key; bool Control; bool Shift; bool Alt; };; extern THotKey m_HotKey;; THotKey GetHotKey () { return m_HotKey; }. into (-m64 -O3 -fno-exceptions -static -fomit-frame-pointer):. __Z9GetHotKeyv: ## @_Z9GetHotKeyv; 	movq	_m_HotKey@GOTPCREL(%rip), %rax; 	movzwl	(%rax), %ecx; 	movzbl	2(%rax), %edx; 	shlq	$16, %rdx; 	orq	%rcx, %rdx; 	movzbl	3(%rax), %ecx; 	shlq	$24, %rcx; 	orq	%rdx, %rcx; 	movzbl	4(%rax), %eax; 	shlq	$32, %rax; 	orq	%rcx, %rax; 	ret. //===---------------------------------------------------------------------===//. We should add an FRINT node to the DAG to model targets that have legal; implementations of ceil/floor/rint. //===---------------------------------------------------------------------===//. Consider:. int test() {; long long input[8] = {1,0,1,0,1,0,1,0};; foo(input);; }. Clang compiles this into:. call void @llvm.memset.p0i8.i64(i8* %tmp, i8 0, i64 64, i32 16, i1 false); %0 = getelementptr [8 x i64]* %input, i64 0, i64 0; store i64 1, i64* %0, align 16; %1 = getelementptr [8 x i64]* %input, i64 0, i64 2; store i64 1, i64* %1, align 16; %2 = getelementptr [8 x i64]* %input, i64 0, i64 4; store i64 1, i64* %2, align 16; %3 = getelementptr [8 x i64]* %input, i64 0, i64 6; store i64 1, i64* %3, align 16. Which gets codegen'd into:. 	pxor	%xmm0, %xmm0; 	movaps	%xmm0, -16(%rbp); 	movaps	%xmm0, -32(%rbp); 	movaps	%xmm0, -48(%rbp); 	movaps	%xmm0, -64(%rbp); 	movq	$1, -64(%rbp); 	movq	$1, -48(%rbp); 	movq	$1, -32(%rbp); 	movq	$1, -16(%rbp). It would be better to have 4 movq's of 0 instead of the movaps's. //===---------------------------------------------------------------------===//. http://llvm.org/PR717:. The following code should compile into ""ret int undef"". Instead, LLVM; produces ""ret int 0"":. int f() {; int x = 4;; in",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:16779,Testability,test,testcases,16779,"return y;; }. //===---------------------------------------------------------------------===//. The loop unroller should partially unroll loops (instead of peeling them); when code growth isn't too bad and when an unroll count allows simplification; of some code within the loop. One trivial example is:. #include <stdio.h>; int main() {; int nRet = 17;; int nLoop;; for ( nLoop = 0; nLoop < 1000; nLoop++ ) {; if ( nLoop & 1 ); nRet += 2;; else; nRet -= 1;; }; return nRet;; }. Unrolling by 2 would eliminate the '&1' in both copies, leading to a net; reduction in code size. The resultant code would then also be suitable for; exit value computation. //===---------------------------------------------------------------------===//. We miss a bunch of rotate opportunities on various targets, including ppc, x86,; etc. On X86, we miss a bunch of 'rotate by variable' cases because the rotate; matching code in dag combine doesn't look through truncates aggressively ; enough. Here are some testcases reduces from GCC PR17886:. unsigned long long f5(unsigned long long x, unsigned long long y) {; return (x << 8) | ((y >> 48) & 0xffull);; }; unsigned long long f6(unsigned long long x, unsigned long long y, int z) {; switch(z) {; case 1:; return (x << 8) | ((y >> 48) & 0xffull);; case 2:; return (x << 16) | ((y >> 40) & 0xffffull);; case 3:; return (x << 24) | ((y >> 32) & 0xffffffull);; case 4:; return (x << 32) | ((y >> 24) & 0xffffffffull);; default:; return (x << 40) | ((y >> 16) & 0xffffffffffull);; }; }. //===---------------------------------------------------------------------===//. This (and similar related idioms):. unsigned int foo(unsigned char i) {; return i | (i<<8) | (i<<16) | (i<<24);; } . compiles into:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20616,Testability,test,test,20616,". void bar(unsigned n) {; if (n % 3 == 0); true();; }. This is equivalent to the following, where 2863311531 is the multiplicative; inverse of 3, and 1431655766 is ((2^32)-1)/3+1:; void bar(unsigned n) {; if (n * 2863311531U < 1431655766U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20643,Testability,test,test,20643,". void bar(unsigned n) {; if (n % 3 == 0); true();; }. This is equivalent to the following, where 2863311531 is the multiplicative; inverse of 3, and 1431655766 is ((2^32)-1)/3+1:; void bar(unsigned n) {; if (n * 2863311531U < 1431655766U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:20672,Testability,test,test,20672,"his is equivalent to the following, where 2863311531 is the multiplicative; inverse of 3, and 1431655766 is ((2^32)-1)/3+1:; void bar(unsigned n) {; if (n * 2863311531U < 1431655766U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuff from this example (see PR1604): . #include <cstdio>; struct test {; int val;; virtual ~test() {}; };. int main() {; test t;; std::scanf(""%d"", &t.val);; std::printf(""%d\n"", t.val);; }. //===---------------------------------------------------------------------===//. These functions perform the same computation, but produce different assembly. define i8 @select(i8 %x) readnone nounwind {; %A = icmp ult i8 %x, 250; %B = select i1 %A, i8 0, i8 1; ret i8 %B ; }. define i8 @addshr(i8 %x) readnone nounwind {; %A = zext i8 %x to i9; %B = add i9 %A, 6 ;; 256 - 250 == 6; %C = lshr i9 %B, 8; %D = trunc i9 %C to i8; ret i8 %D; }. //===---------------------------------------------------------------------===//. From gcc bug 24696:; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) || ((b & (c - 1)) != 0);; }; int; f (unsigned long a, unsigned long b, unsigned long c); {; return ((a & (c - 1)) != 0) | ((b & (c - 1)) != 0);; }; Both should combine to ((a|b) & (c-1)) != 0. Currently not optimized with; ""clang -emit-llvm-bc | opt -O3"". //===--------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:29497,Testability,test,test,29497,"ction GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33440,Testability,test,testcases,33440,"nk the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; stor",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33453,Testability,test,testsuite,33453,"nk the store. We need partially dead store sinking. //===---------------------------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; stor",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:33500,Testability,test,testsuite,33500,"---------------------------------------------------===//. [LOAD PRE CRIT EDGE SPLITTING]. GCC PR37166: Sinking of loads prevents SROA'ing the ""g"" struct on the stack; leading to excess stack traffic. This could be handled by GVN with some crazy; symbolic phi translation. The code we get looks like (g is on the stack):. bb2:		; preds = %bb1; ..; 	%9 = getelementptr %struct.f* %g, i32 0, i32 0		; 	store i32 %8, i32* %9, align bel %bb3. bb3:		; preds = %bb1, %bb2, %bb; 	%c_addr.0 = phi %struct.f* [ %g, %bb2 ], [ %c, %bb ], [ %c, %bb1 ]; 	%b_addr.0 = phi %struct.f* [ %b, %bb2 ], [ %g, %bb ], [ %b, %bb1 ]; 	%10 = getelementptr %struct.f* %c_addr.0, i32 0, i32 0; 	%11 = load i32* %10, align 4. %11 is partially redundant, an in BB2 it should have the value %8. GCC PR33344 and PR35287 are similar cases. //===---------------------------------------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34304,Testability,benchmark,benchmark,34304,"----------------------------------------===//. [LOAD PRE]. There are many load PRE testcases in testsuite/gcc.dg/tree-ssa/loadpre* in the; GCC testsuite, ones we don't get yet are (checked through loadpre25):. [CRIT EDGE BREAKING]; predcom-4.c. [PRE OF READONLY CALL]; loadpre5.c. [TURN SELECT INTO BRANCH]; loadpre14.c loadpre15.c . actually a conditional increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34714,Testability,test,testcases,34714,"nal increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34727,Testability,test,testsuite,34727,"nal increment: loadpre18.c loadpre19.c. //===---------------------------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34777,Testability,test,testsuite,34777,"-------------------------------------------------===//. [LOAD PRE / STORE SINKING / SPEC HACK]. This is a chunk of code from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; re",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34905,Testability,test,testsuite,34905,"e from 456.hmmer:. int f(int M, int *mc, int *mpp, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:34954,Testability,test,testsuite,34954,"p, int *tpmm, int *ip, int *tpim, int *dpp,; int *tpdm, int xmb, int *bp, int *ms) {; int k, sc;; for (k = 1; k <= M; k++) {; mc[k] = mpp[k-1] + tpmm[k-1];; if ((sc = ip[k-1] + tpim[k-1]) > mc[k]) mc[k] = sc;; if ((sc = dpp[k-1] + tpdm[k-1]) > mc[k]) mc[k] = sc;; if ((sc = xmb + bp[k]) > mc[k]) mc[k] = sc;; mc[k] += ms[k];; }; }. It is very profitable for this benchmark to turn the conditional stores to mc[k]; into a conditional move (select instr in IR) and allow the final store to do the; store. See GCC PR27313 for more details. Note that this is valid to xform even; with the new C++ memory model, since mc[k] is previously loaded and later; stored. //===---------------------------------------------------------------------===//. [SCALAR PRE]; There are many PRE testcases in testsuite/gcc.dg/tree-ssa/ssa-pre-*.c in the; GCC testsuite. //===---------------------------------------------------------------------===//. There are some interesting cases in testsuite/gcc.dg/tree-ssa/pred-comm* in the; GCC testsuite. For example, we get the first example in predcom-1.c, but ; miss the second one:. unsigned fib[1000];; unsigned avg[1000];. __attribute__ ((noinline)); void count_averages(int n) {; int i;; for (i = 1; i < n; i++); avg[i] = (((unsigned long) fib[i - 1] + fib[i] + fib[i + 1]) / 3) & 0xffff;; }. which compiles into two loads instead of one in the loop. predcom-2.c is the same as predcom-1.c. predcom-3.c is very similar but needs loads feeding each other instead of; store->load. //===---------------------------------------------------------------------===//. [ALIAS ANALYSIS]. Type based alias analysis:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=14705. We should do better analysis of posix_memalign. At the least it should; no-capture its pointer argument, at best, we should know that the out-value; result doesn't point to anything (like malloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:42807,Testability,test,test,42807,"d forward substituting; the string directly into the printf, this eliminates reads from globalarray.; Since this pattern occurs frequently in crafty (due to the ""DisplayTime"" and; other similar functions) there are many stores to ""out"". Once all the printfs; stop using ""out"", all that is left is the memcpy's into it. This should allow; globalopt to remove the ""stored only"" global. //===---------------------------------------------------------------------===//. This code:. define inreg i32 @foo(i8* inreg %p) nounwind {; %tmp0 = load i8* %p; %tmp1 = ashr i8 %tmp0, 5; %tmp2 = sext i8 %tmp1 to i32; ret i32 %tmp2; }. could be dagcombine'd to a sign-extending load with a shift.; For example, on x86 this currently gets this:. 	movb	(%eax), %al; 	sarb	$5, %al; 	movsbl	%al, %eax. while it could get this:. 	movsbl	(%eax), %eax; 	sarl	$5, %eax. //===---------------------------------------------------------------------===//. GCC PR31029:. int test(int x) { return 1-x == x; } // --> return false; int test2(int x) { return 2-x == x; } // --> return x == 1 ?. Always foldable for odd constants, what is the rule for even?. //===---------------------------------------------------------------------===//. PR 3381: GEP to field of size 0 inside a struct could be turned into GEP; for next field in struct (which is at same address). For example: store of float into { {{}}, float } could be turned into a store to; the float directly. //===---------------------------------------------------------------------===//. The arg promotion pass should make use of nocapture to make its alias analysis; stuff much more precise. //===---------------------------------------------------------------------===//. The following functions should be optimized to use a select instead of a; branch (from gcc PR40072):. char char_int(int m) {if(m>7) return 0; return m;}; int int_char(char m) {if(m>7) return 0; return m;}. //===---------------------------------------------------------------------===//. int func(int",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:45858,Testability,test,test,45858,") | (a & 0x40);; b = (b & ~0x80) | (a & 0x80);; return (b);; }. define zeroext i8 @p1(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/mem",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46073,Testability,test,test,46073,"; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %1 = and i8 %a, -64 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46634,Testability,log,logic,46634," IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int **p;; y.p = &i;; x.q = &j;; p = __builtin_memcpy (&x, &y, sizeof (int *));; return **p;; }. This can be seen at:; $ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -function-attrs -S. //===---------------------------------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i3",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:48861,Testability,test,testcase,48861,"n:; int a(int a, int b) { return (a==12)&(b>47)&(b<58); }. The sgt and slt should be combined into a single comparison. Testcase derived; from gcc. //===---------------------------------------------------------------------===//. Missed instcombine transformation:. %382 = srem i32 %tmp14.i, 64 ; [#uses=1]; %383 = zext i32 %382 to i64 ; [#uses=1]; %384 = shl i64 %381, %383 ; [#uses=1]; %385 = icmp slt i32 %tmp14.i, 64 ; [#uses=1]. The srem can be transformed to an and because if %tmp14.i is negative, the; shift is undefined. Testcase derived from 403.gcc. //===---------------------------------------------------------------------===//. This is a range comparison on a divided result (from 403.gcc):. %1337 = sdiv i32 %1336, 8 ; [#uses=1]; %.off.i208 = add i32 %1336, 7 ; [#uses=1]; %1338 = icmp ult i32 %.off.i208, 15 ; [#uses=1]; ; We already catch this (removing the sdiv) if there isn't an add, we should; handle the 'add' as well. This is a common idiom with it's builtin_alloca code.; C testcase:. int a(int x) { return (unsigned)(x/16+7) < 15; }. Another similar case involves truncations on 64-bit targets:. %361 = sdiv i64 %.046, 8 ; [#uses=1]; %362 = trunc i64 %361 to i32 ; [#uses=2]; ...; %367 = icmp eq i32 %362, 0 ; [#uses=1]. //===---------------------------------------------------------------------===//. Missed instcombine/dagcombine transformation:; define void @lshift_lt(i8 zeroext %a) nounwind {; entry:; %conv = zext i8 %a to i32; %shl = shl i32 %conv, 3; %cmp = icmp ult i32 %shl, 33; br i1 %cmp, label %if.then, label %if.end. if.then:; tail call void @bar() nounwind; ret void. if.end:; ret void; }; declare void @bar() nounwind. The shift should be eliminated. Testcase derived from gcc. //===---------------------------------------------------------------------===//. These compile into different code, one gets recognized as a switch and the; other doesn't due to phase ordering issues (PR6212):. int test1(int mainType, int subType) {; if (mainType == 7); subType = ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:50225,Testability,test,test,50225,"tion:; define void @lshift_lt(i8 zeroext %a) nounwind {; entry:; %conv = zext i8 %a to i32; %shl = shl i32 %conv, 3; %cmp = icmp ult i32 %shl, 33; br i1 %cmp, label %if.then, label %if.end. if.then:; tail call void @bar() nounwind; ret void. if.end:; ret void; }; declare void @bar() nounwind. The shift should be eliminated. Testcase derived from gcc. //===---------------------------------------------------------------------===//. These compile into different code, one gets recognized as a switch and the; other doesn't due to phase ordering issues (PR6212):. int test1(int mainType, int subType) {; if (mainType == 7); subType = 4;; else if (mainType == 9); subType = 6;; else if (mainType == 11); subType = 9;; return subType;; }. int test2(int mainType, int subType) {; if (mainType == 7); subType = 4;; if (mainType == 9); subType = 6;; if (mainType == 11); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:51085,Testability,test,testcase,51085,"); subType = 9;; return subType;; }. //===---------------------------------------------------------------------===//. The following test case (from PR6576):. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %cond1 = icmp eq i32 %b, 0 ; <i1> [#uses=1]; br i1 %cond1, label %exit, label %bb.nph; bb.nph: ; preds = %entry; %tmp = mul i32 %b, %a ; <i32> [#uses=1]; ret i32 %tmp; exit: ; preds = %entry; ret i32 0; }. could be reduced to:. define i32 @mul(i32 %a, i32 %b) nounwind readnone {; entry:; %tmp = mul i32 %b, %a; ret i32 %tmp; }. //===---------------------------------------------------------------------===//. We should use DSE + llvm.lifetime.end to delete dead vtable pointer updates.; See GCC PR34949. Another interesting case is that something related could be used for variables; that go const after their ctor has finished. In these cases, globalopt (which; can statically run the constructor) could mark the global const (so it gets put; in the readonly section). A testcase would be:. #include <complex>; using namespace std;; const complex<char> should_be_in_rodata (42,-42);; complex<char> should_be_in_data (42,-42);; complex<char> should_be_in_bss;. Where we currently evaluate the ctors but the globals don't become const because; the optimizer doesn't know they ""become const"" after the ctor is done. See; GCC PR4131 for more examples. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; case it choses instead to keep the max operation obvious. //===---------------------------------------------------------------------===//. define void @a(i32 %x) nounwind {; entry:; switch i32 %x, label %if.end [; i32 0, label %if.then; i32 1, label %if.then; i32 2, label %if.then; i32 3, label %if.then; i32 5, label",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:53287,Testability,test,test,53287,"LBB0_2:; 	ret; .LBB0_3:; 	jmp	foo # TAILCALL. If we wanted to be really clever, we could simplify the whole thing to; something like the following, which eliminates a branch:; 	xorl $1, %edi; 	cmpl	$4, %edi; 	ja	.LBB0_2; 	ret; .LBB0_2:; 	jmp	foo # TAILCALL. //===---------------------------------------------------------------------===//. We compile this:. int foo(int a) { return (a & (~15)) / 16; }. Into:. define i32 @foo(i32 %a) nounwind readnone ssp {; entry:; %and = and i32 %a, -16; %div = sdiv i32 %and, 16; ret i32 %div; }. but this code (X & -A)/A is X >> log2(A) when A is a power of 2, so this case; should be instcombined into just ""a >> 4"". We do get this at the codegen level, so something knows about it, but ; instcombine should catch it earlier:. _foo: ## @foo; ## %bb.0: ## %entry; 	movl	%edi, %eax; 	sarl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. This code (from GCC PR28685):. int test(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing mor",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:53408,Testability,test,test,53408,"e the following, which eliminates a branch:; 	xorl $1, %edi; 	cmpl	$4, %edi; 	ja	.LBB0_2; 	ret; .LBB0_2:; 	jmp	foo # TAILCALL. //===---------------------------------------------------------------------===//. We compile this:. int foo(int a) { return (a & (~15)) / 16; }. Into:. define i32 @foo(i32 %a) nounwind readnone ssp {; entry:; %and = and i32 %a, -16; %div = sdiv i32 %and, 16; ret i32 %div; }. but this code (X & -A)/A is X >> log2(A) when A is a power of 2, so this case; should be instcombined into just ""a >> 4"". We do get this at the codegen level, so something knows about it, but ; instcombine should catch it earlier:. _foo: ## @foo; ## %bb.0: ## %entry; 	movl	%edi, %eax; 	sarl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. This code (from GCC PR28685):. int test(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %cmp = icmp slt i32 %a, %b; br i1 %cmp, label %return, label %if.end. if.end: ; preds = %entry; %cmp5 = icmp eq i32 %a, %b; %conv6 = zext i1 %cmp5 to i32; ret i32 %conv6. return: ; preds = %entry; ret i32 1; }. it could be:. define i32 @test__(i32 %a, i32 %b) nounwind readnone ssp {; entry:; %0 = icmp sle i32 %a, %b; %retval = zext i1 %0 to i32; ret i32 %retval; }. //===---------------------------------------------------------------------===//. This code can be seen in viterbi:. %64 = call noalias i8* @malloc(i64 %62) nounwind; ...; %67 = call i64 @llvm.objectsize.i64(i8* %64, i1 false) nounwind; %68 = call i8* @__memset_chk(i8* %64, i32 0, i64 %62, i64 %67) nounwind. llvm.objectsize.i64 should be taught about malloc/calloc, allowing it to; fold to %62. This is a security win (overflows of malloc will get caught); and also a performance win by exposing more memsets to the optimizer. This occurs several times in viterbi. Note that this would change the semantics of @llvm.objectsize whi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:62462,Testability,test,testfunc,62462,"-> 0.0 transformation above), it might be provable that the sign of the; expression doesn't matter. For example, by the above rules, we can't transform; fmul(sitofp(x), 0.0) into 0.0, because x might be -1 and the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. struct x {; char a;; int b[4];; };; void foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some im",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:62661,Testability,test,testfunc,62661," the result of the; expression is defined to be -0.0. If we look at the uses of the fmul for example, we might be able to prove that; all uses don't care about the sign of zero. For example, if we have:. fadd(fmul(sitofp(x), 0.0), 2.0). Since we know that x+2.0 doesn't care about the sign of any zeros in X, we can; transform the fmul to 0.0, and then the fadd to 2.0. //===---------------------------------------------------------------------===//. We should enhance memcpy/memcpy/memset to allow a metadata node on them; indicating that some bytes of the transfer are undefined. This is useful for; frontends like clang when lowering struct copies, when some elements of the; struct are undefined. Consider something like this:. struct x {; char a;; int b[4];; };; void foo(struct x*P);; struct x testfunc() {; struct x V1, V2;; foo(&V1);; V2 = V1;. return V2;; }. We currently compile this to:; $ clang t.c -S -o - -O0 -emit-llvm | opt -sroa -S. %struct.x = type { i8, [4 x i32] }. define void @testfunc(%struct.x* sret %agg.result) nounwind ssp {; entry:; %V1 = alloca %struct.x, align 4; call void @foo(%struct.x* %V1); %tmp1 = bitcast %struct.x* %V1 to i8*; %0 = bitcast %struct.x* %V1 to i160*; %srcval1 = load i160* %0, align 4; %tmp2 = bitcast %struct.x* %agg.result to i8*; %1 = bitcast %struct.x* %agg.result to i160*; store i160 %srcval1, i160* %1, align 4; ret void; }. This happens because SRoA sees that the temp alloca has is being memcpy'd into; and out of and it has holes and it has to be conservative. If we knew about the; holes, then this could be much much better. Having information about these holes would also improve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64340,Testability,benchmark,benchmarks,64340,"ve memcpy (etc) lowering at; llc time when it gets inlined, because we can use smaller transfers. This also; avoids partial register stalls in some important cases. //===---------------------------------------------------------------------===//. We don't fold (icmp (add) (add)) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:66985,Testability,test,testl,66985," least):. %struct.t1 = type { i8, [3 x i8] }; @s2 = global %struct.t1 zeroinitializer, align 4; @s1 = global %struct.t1 zeroinitializer, align 4; define void @func1() nounwind ssp noredzone {; entry:; %0 = load i32* bitcast (%struct.t1* @s2 to i32*), align 4; %bf.val.sext5 = and i32 %0, 1; %1 = load i32* bitcast (%struct.t1* @s1 to i32*), align 4; %2 = and i32 %1, -4; %3 = or i32 %2, %bf.val.sext5; %bf.val.sext26 = and i32 %0, 2; %4 = or i32 %3, %bf.val.sext26; store i32 %4, i32* bitcast (%struct.t1* @s1 to i32*), align 4; ret void; }. The two or/and's should be merged into one each. //===---------------------------------------------------------------------===//. Machine level code hoisting can be useful in some cases. For example, PR9408; is about:. typedef union {; void (*f1)(int);; void (*f2)(long);; } funcs;. void foo(funcs f, int which) {; int a = 5;; if (which) {; f.f1(a);; } else {; f.f2(a);; }; }. which we compile to:. foo: # @foo; # %bb.0: # %entry; pushq %rbp; movq %rsp, %rbp; testl %esi, %esi; movq %rdi, %rax; je .LBB0_2; # %bb.1: # %if.then; movl $5, %edi; callq *%rax; popq %rbp; ret; .LBB0_2: # %if.else; movl $5, %edi; callq *%rax; popq %rbp; ret. Note that bb1 and bb2 are the same. This doesn't happen at the IR level; because one call is passing an i32 and the other is passing an i64. //===---------------------------------------------------------------------===//. I see this sort of pattern in 176.gcc in a few places (e.g. the start of; store_bit_field). The rem should be replaced with a multiply and subtract:. %3 = sdiv i32 %A, %B; %4 = srem i32 %A, %B. Similarly for udiv/urem. Note that this shouldn't be done on X86 or ARM,; which can do this in a single operation (instruction or libcall). It is; probably best to do this in the code generator. //===---------------------------------------------------------------------===//. unsigned foo(unsigned x, unsigned y) { return (x & y) == 0 || x == 0; }; should fold to (x & y) == 0. //===-----------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:2281,Usability,simpl,simple,2281,"//. Shrink: (setlt (loadi32 P), 0) -> (setlt (loadi8 Phi), 0). //===---------------------------------------------------------------------===//. Reassociate should turn things like:. int factorial(int X) {; return X*X*X*X*X*X*X*X;; }. into llvm.powi calls, allowing the code generator to produce balanced; multiplication trees. First, the intrinsic needs to be extended to support integers, and second the; code generator needs to be enhanced to lower these to multiplication trees. //===---------------------------------------------------------------------===//. Interesting? testcase for add/shift/mul reassoc:. int bar(int x, int y) {; return x*x*x+y+x*x*x*x*x*y*y*y*y;; }; int foo(int z, int n) {; return bar(z, n) + bar(2*z, 2*n);; }. This is blocked on not handling X*X*X -> powi(X, 3) (see note above). The issue; is that we end up getting t = 2*X s = t*t and don't turn this into 4*X*X,; which is the same number of multiplies and is canonical, because the 2*X has; multiple uses. Here's a simple example:. define i32 @test15(i32 %X1) {; %B = mul i32 %X1, 47 ; X1*47; %C = mul i32 %B, %B; ret i32 %C; }. //===---------------------------------------------------------------------===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===----------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3464,Usability,simpl,simplify,3464,"--===//. Reassociate should handle the example in GCC PR16157:. extern int a0, a1, a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	incl _foo; LBB16_2:	#cond_next. emit:; 	movl	_foo, %eax; 	cmpl	$1, %edi; 	sbbl	$-1, %eax; 	movl	%eax, _foo. //===------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:3562,Usability,simpl,simplify,3562,", a2, a3, a4; extern int b0, b1, b2, b3, b4; ; void f () { /* this can be optimized to four additions... */ ; b4 = a4 + a3 + a2 + a1 + a0; ; b3 = a3 + a2 + a1 + a0; ; b2 = a2 + a1 + a0; ; b1 = a1 + a0; ; } . This requires reassociating to forms of expressions that are already available,; something that reassoc doesn't think about yet. //===---------------------------------------------------------------------===//. These two functions should generate the same code on big-endian systems:. int g(int *j,int *l) { return memcmp(j,l,4); }; int h(int *j, int *l) { return *j - *l; }. this could be done in SelectionDAGISel.cpp, along with other special cases,; for 1,2,4,8 bytes. //===---------------------------------------------------------------------===//. It would be nice to revert this patch:; http://lists.llvm.org/pipermail/llvm-commits/Week-of-Mon-20060213/031986.html. And teach the dag combiner enough to simplify the code expanded before ; legalize. It seems plausible that this knowledge would let it simplify other; stuff too. //===---------------------------------------------------------------------===//. For vector types, DataLayout.cpp::getTypeInfo() returns alignment that is equal; to the type size. It works but can be overly conservative as the alignment of; specific vector types are target dependent. //===---------------------------------------------------------------------===//. We should produce an unaligned load from code like this:. v4sf example(float *P) {; return (v4sf){P[0], P[1], P[2], P[3] };; }. //===---------------------------------------------------------------------===//. Add support for conditional increments, and other related patterns. Instead; of:. 	movl 136(%esp), %eax; 	cmpl $0, %eax; 	je LBB16_2	#cond_next; LBB16_1:	#cond_true; 	incl _foo; LBB16_2:	#cond_next. emit:; 	movl	_foo, %eax; 	cmpl	$1, %edi; 	sbbl	$-1, %eax; 	movl	%eax, _foo. //===---------------------------------------------------------------------===//. Combine: a = sin(x), b = cos(",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:16022,Usability,simpl,simplification,16022,"align 16; %1 = getelementptr [8 x i64]* %input, i64 0, i64 2; store i64 1, i64* %1, align 16; %2 = getelementptr [8 x i64]* %input, i64 0, i64 4; store i64 1, i64* %2, align 16; %3 = getelementptr [8 x i64]* %input, i64 0, i64 6; store i64 1, i64* %3, align 16. Which gets codegen'd into:. 	pxor	%xmm0, %xmm0; 	movaps	%xmm0, -16(%rbp); 	movaps	%xmm0, -32(%rbp); 	movaps	%xmm0, -48(%rbp); 	movaps	%xmm0, -64(%rbp); 	movq	$1, -64(%rbp); 	movq	$1, -48(%rbp); 	movq	$1, -32(%rbp); 	movq	$1, -16(%rbp). It would be better to have 4 movq's of 0 instead of the movaps's. //===---------------------------------------------------------------------===//. http://llvm.org/PR717:. The following code should compile into ""ret int undef"". Instead, LLVM; produces ""ret int 0"":. int f() {; int x = 4;; int y;; if (x == 3) y = 0;; return y;; }. //===---------------------------------------------------------------------===//. The loop unroller should partially unroll loops (instead of peeling them); when code growth isn't too bad and when an unroll count allows simplification; of some code within the loop. One trivial example is:. #include <stdio.h>; int main() {; int nRet = 17;; int nLoop;; for ( nLoop = 0; nLoop < 1000; nLoop++ ) {; if ( nLoop & 1 ); nRet += 2;; else; nRet -= 1;; }; return nRet;; }. Unrolling by 2 would eliminate the '&1' in both copies, leading to a net; reduction in code size. The resultant code would then also be suitable for; exit value computation. //===---------------------------------------------------------------------===//. We miss a bunch of rotate opportunities on various targets, including ppc, x86,; etc. On X86, we miss a bunch of 'rotate by variable' cases because the rotate; matching code in dag combine doesn't look through truncates aggressively ; enough. Here are some testcases reduces from GCC PR17886:. unsigned long long f5(unsigned long long x, unsigned long long y) {; return (x << 8) | ((y >> 48) & 0xffull);; }; unsigned long long f6(unsigned long long x, u",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18461,Usability,simpl,simplifications,18461,"o:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by const",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:18480,Usability,simpl,simplify,18480,"o:. define i32 @foo(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %shl5 = shl i32 %conv, 16; %shl9 = shl i32 %conv, 24; %or = or i32 %shl9, %conv; %or6 = or i32 %or, %shl5; %or10 = or i32 %or6, %shl; ret i32 %or10; }. it would be better as:. unsigned int bar(unsigned char i) {; unsigned int j=i | (i << 8); ; return j | (j<<16);; }. aka:. define i32 @bar(i8 zeroext %i) nounwind readnone ssp noredzone {; entry:; %conv = zext i8 %i to i32; %shl = shl i32 %conv, 8; %or = or i32 %shl, %conv; %shl5 = shl i32 %or, 16; %or6 = or i32 %shl5, %or; ret i32 %or6; }. or even i*0x01010101, depending on the speed of the multiplier. The best way to; handle this is to canonicalize it to a multiply in IR and have codegen handle; lowering multiplies to shifts on cpus where shifts are faster. //===---------------------------------------------------------------------===//. We do a number of simplifications in simplify libcalls to strength reduce; standard library functions, but we don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by const",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:19527,Usability,simpl,simplified,19527,"don't currently merge them together. For; example, it is useful to merge memcpy(a,b,strlen(b)) -> strcpy. This can only; be done safely if ""b"" isn't modified between the strlen and memcpy of course. //===---------------------------------------------------------------------===//. We compile this program: (from GCC PR11680); http://gcc.gnu.org/bugzilla/attachment.cgi?id=4487. Into code that runs the same speed in fast/slow modes, but both modes run 2x; slower than when compile with GCC (either 4.0 or 4.2):. $ llvm-g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 1.821u 0.003s 0:01.82 100.0%	0+0k 0+0io 0pf+0w. $ g++ perf.cpp -O3 -fno-exceptions; $ time ./a.out fast; 0.821u 0.001s 0:00.82 100.0%	0+0k 0+0io 0pf+0w. It looks like we are making the same inlining decisions, so this may be raw; codegen badness or something else (haven't investigated). //===---------------------------------------------------------------------===//. Divisibility by constant can be simplified (according to GCC PR12849) from; being a mulhi to being a mul lo (cheaper). Testcase:. void bar(unsigned n) {; if (n % 3 == 0); true();; }. This is equivalent to the following, where 2863311531 is the multiplicative; inverse of 3, and 1431655766 is ((2^32)-1)/3+1:; void bar(unsigned n) {; if (n * 2863311531U < 1431655766U); true();; }. The same transformation can work with an even modulo with the addition of a; rotate: rotate the result of the multiply to the right by the number of bits; which need to be zero for the condition to be true, and shrink the compare RHS; by the same amount. Unless the target supports rotates, though, that; transformation probably isn't worthwhile. The transformation can also easily be made to work with non-zero equality; comparisons: just transform, for example, ""n % 3 == 1"" to ""(n-1) % 3 == 0"". //===---------------------------------------------------------------------===//. Better mod/ref analysis for scanf would allow us to eliminate the vtable and a; bunch of other stuf",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:23183,Usability,simpl,simplify,23183,"i;}; unsigned int f2(unsigned int i, unsigned int n) {++i; i += i == n; return i;}; These should combine to the same thing. Currently, the first function; produces better code on X86. //===---------------------------------------------------------------------===//. From GCC Bug 15784:; #define abs(x) x>0?x:-x; int f(int x, int y); {; return (abs(x)) >= 0;; }; This should optimize to x == INT_MIN. (With -fwrapv.) Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 14753:; void; rotate_cst (unsigned int a); {; a = (a << 10) | (a >> 22);; if (a == 123); bar ();; }; void; minus_cst (unsigned int a); {; unsigned int tem;. tem = 20 - a;; if (tem == 5); bar ();; }; void; mask_gt (unsigned int a); {; /* This is equivalent to a > 15. */; if ((a & ~7) > 8); bar ();; }; void; rshift_gt (unsigned int a); {; /* This is equivalent to a > 23. */; if ((a >> 2) > 5); bar ();; }. All should simplify to a single comparison. All of these are; currently not optimized with ""clang -emit-llvm-bc | opt; -O3"". //===---------------------------------------------------------------------===//. From GCC Bug 32605:; int c(int* x) {return (char*)x+2 == (char*)x;}; Should combine to 0. Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"" (although llc can optimize it). //===---------------------------------------------------------------------===//. int a(unsigned b) {return ((b << 31) | (b << 30)) >> 31;}; Should be combined to ""((b >> 1) | b) & 1"". Currently not optimized; with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned a(unsigned x, unsigned y) { return x | (y & 1) | (y & 2);}; Should combine to ""x | (y & 3)"". Currently not optimized with ""clang; -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int a(int a, int b, int c) {return (~a & c) | ((c|a) & b)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:27969,Usability,simpl,simplified,27969,"mized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:28051,Usability,simpl,simplifications,28051,"----------------------------------===//. int f(int i, int j) { return i < j + 1; }; int g(int i, int j) { return j > i - 1; }; Should combine to ""i <= j"" (the add/sub has nsw). Currently not; optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. unsigned f(unsigned x) { return ((x & 7) + 1) & 15; }; The & 15 part should be optimized away, it doesn't change the result. Currently; not optimized with ""clang -emit-llvm-bc | opt -O3"". //===---------------------------------------------------------------------===//. This was noticed in the entryblock for grokdeclarator in 403.gcc:. %tmp = icmp eq i32 %decl_context, 4 ; %decl_context_addr.0 = select i1 %tmp, i32 3, i32 %decl_context ; %tmp1 = icmp eq i32 %decl_context_addr.0, 1 ; %decl_context_addr.1 = select i1 %tmp1, i32 0, i32 %decl_context_addr.0. tmp1 should be simplified to something like:; (!tmp || decl_context == 1). This allows recursive simplifications, tmp1 is used all over the place in; the function, e.g. by:. %tmp23 = icmp eq i32 %decl_context_addr.1, 0 ; <i1> [#uses=1]; %tmp24 = xor i1 %tmp1, true ; <i1> [#uses=1]; %or.cond8 = and i1 %tmp23, %tmp24 ; <i1> [#uses=1]. later. //===---------------------------------------------------------------------===//. [STORE SINKING]. Store sinking: This code:. void f (int n, int *cond, int *res) {; int i;; *res = 0;; for (i = 0; i < n; i++); if (*cond); *res ^= 234; /* (*) */; }. On this function GVN hoists the fully redundant value of *res, but nothing; moves the store out. This gives us this code:. bb:		; preds = %bb2, %entry; 	%.rle = phi i32 [ 0, %entry ], [ %.rle6, %bb2 ]	; 	%i.05 = phi i32 [ 0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%ind",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:29796,Usability,simpl,simple,29796,"0, %entry ], [ %indvar.next, %bb2 ]; 	%1 = load i32* %cond, align 4; 	%2 = icmp eq i32 %1, 0; 	br i1 %2, label %bb2, label %bb1. bb1:		; preds = %bb; 	%3 = xor i32 %.rle, 234	; 	store i32 %3, i32* %res, align 4; 	br label %bb2. bb2:		; preds = %bb, %bb1; 	%.rle6 = phi i32 [ %3, %bb1 ], [ %.rle, %bb ]	; 	%indvar.next = add i32 %i.05, 1	; 	%exitcond = icmp eq i32 %indvar.next, %n; 	br i1 %exitcond, label %return, label %bb. DSE should sink partially dead stores to get the store out of the loop. Here's another partial dead case:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=12395. //===---------------------------------------------------------------------===//. Scalar PRE hoists the mul in the common block up to the else:. int test (int a, int b, int c, int g) {; int d, e;; if (a); d = b * c;; else; d = b - c;; e = b * c + g;; return d + e;; }. It would be better to do the mul once to reduce codesize above the if.; This is GCC PR38204. //===---------------------------------------------------------------------===//; This simple function from 179.art:. int winner, numf2s;; struct { double y; int reset; } *Y;. void find_match() {; int i;; winner = 0;; for (i=0;i<numf2s;i++); if (Y[i].y > Y[winner].y); winner =i;; }. Compiles into (with clang TBAA):. for.body: ; preds = %for.inc, %bb.nph; %indvar = phi i64 [ 0, %bb.nph ], [ %indvar.next, %for.inc ]; %i.01718 = phi i32 [ 0, %bb.nph ], [ %i.01719, %for.inc ]; %tmp4 = getelementptr inbounds %struct.anon* %tmp3, i64 %indvar, i32 0; %tmp5 = load double* %tmp4, align 8, !tbaa !4; %idxprom7 = sext i32 %i.01718 to i64; %tmp10 = getelementptr inbounds %struct.anon* %tmp3, i64 %idxprom7, i32 0; %tmp11 = load double* %tmp10, align 8, !tbaa !4; %cmp12 = fcmp ogt double %tmp5, %tmp11; br i1 %cmp12, label %if.then, label %for.inc. if.then: ; preds = %for.body; %i.017 = trunc i64 %indvar to i32; br label %for.inc. for.inc: ; preds = %for.body, %if.then; %i.01719 = phi i32 [ %i.01718, %for.body ], [ %i.017, %if.then ]; %indvar.next = add i6",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:36779,Usability,simpl,simplifylibcalls,36779,"alloc). One example of this is in; SingleSource/Benchmarks/Misc/dt.c. //===---------------------------------------------------------------------===//. Interesting missed case because of control flow flattening (should be 2 loads):; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=26629; With: llvm-gcc t2.c -S -o - -O0 -emit-llvm | llvm-as | ; opt -mem2reg -gvn -instcombine | llvm-dis; we miss it because we need 1) CRIT EDGE 2) MULTIPLE DIFFERENT; VALS PRODUCED BY ONE BLOCK OVER DIFFERENT PATHS. //===---------------------------------------------------------------------===//. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19633; We could eliminate the branch condition here, loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should turn these snprintf idioms into memcpy (GCC PR47917). char buf1[6], buf2[6], buf3[4], buf4[4];; int i;. int foo (void) {; int ret = snprintf (buf1, sizeof buf1, ""abcde"");; ret += snprintf (buf2, sizeof buf2, ""abcdef"") * 16;; ret += snprintf (buf3, sizeof buf3, ""%s"", i++ < 6 ? ""abc"" : ""def"") * 256;; ret += snprintf (buf4, sizeof buf4, ""%s"",",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:37448,Usability,simpl,simplifylibcalls,37448,"loading from null is undefined:. struct S { int w, x, y, z; };; struct T { int r; struct S s; };; void bar (struct S, int);; void foo (int a, struct T b); {; struct S *c = 0;; if (a); c = &b.s;; bar (*c, a);; }. //===---------------------------------------------------------------------===//. simplifylibcalls should do several optimizations for strspn/strcspn:. strcspn(x, ""a"") -> inlined loop for up to 3 letters (similarly for strspn):. size_t __strcspn_c3 (__const char *__s, int __reject1, int __reject2,; int __reject3) {; register size_t __result = 0;; while (__s[__result] != '\0' && __s[__result] != __reject1 &&; __s[__result] != __reject2 && __s[__result] != __reject3); ++__result;; return __result;; }. This should turn into a switch on the character. See PR3253 for some notes on; codegen. 456.hmmer apparently uses strcspn and strspn a lot. 471.omnetpp uses strspn. //===---------------------------------------------------------------------===//. simplifylibcalls should turn these snprintf idioms into memcpy (GCC PR47917). char buf1[6], buf2[6], buf3[4], buf4[4];; int i;. int foo (void) {; int ret = snprintf (buf1, sizeof buf1, ""abcde"");; ret += snprintf (buf2, sizeof buf2, ""abcdef"") * 16;; ret += snprintf (buf3, sizeof buf3, ""%s"", i++ < 6 ? ""abc"" : ""def"") * 256;; ret += snprintf (buf4, sizeof buf4, ""%s"", i++ > 10 ? ""abcde"" : ""defgh"")*4096;; return ret;; }. //===---------------------------------------------------------------------===//. ""gas"" uses this idiom:; else if (strchr (""+-/*%|&^:[]()~"", *intel_parser.op_string)); ..; else if (strchr (""<>"", *intel_parser.op_string). Those should be turned into a switch. SimplifyLibCalls only gets the second; case. //===---------------------------------------------------------------------===//. 252.eon contains this interesting code:. %3072 = getelementptr [100 x i8]* %tempString, i32 0, i32 0; %3073 = call i8* @strcpy(i8* %3072, i8* %3071) nounwind; %strlen = call i32 @strlen(i8* %3072) ; uses = 1; %endptr = getelementptr [1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46183,Usability,simpl,simple,46183," ret i8 %2; }. define zeroext i8 @p2(i8 zeroext %b, i8 zeroext %a) nounwind readnone ssp {; entry:; %0 = and i8 %b, 63 ; <i8> [#uses=1]; %.masked = and i8 %a, 64 ; <i8> [#uses=1]; %1 = and i8 %a, -128 ; <i8> [#uses=1]; %2 = or i8 %1, %0 ; <i8> [#uses=1]; %3 = or i8 %2, %.masked ; <i8> [#uses=1]; ret i8 %3; }. //===---------------------------------------------------------------------===//. IPSCCP does not currently propagate argument dependent constants through; functions where it does not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:46687,Usability,simpl,simple,46687,"oes not not all of the callers. This includes functions; with normal external linkage as well as templates, C99 inline functions etc.; Specifically, it does nothing to:. define i32 @test(i32 %x, i32 %y, i32 %z) nounwind {; entry:; %0 = add nsw i32 %y, %z ; %1 = mul i32 %0, %x ; %2 = mul i32 %y, %z ; %3 = add nsw i32 %1, %2 ; ret i32 %3; }. define i32 @test2() nounwind {; entry:; %0 = call i32 @test(i32 1, i32 2, i32 4) nounwind; ret i32 %0; }. It would be interesting extend IPSCCP to be able to handle simple cases like; this, where all of the arguments to a call are constant. Because IPSCCP runs; before inlining, trivial templates and inline functions are not yet inlined.; The results for a function + set of constant arguments should be memoized in a; map. //===---------------------------------------------------------------------===//. The libcall constant folding stuff should be moved out of SimplifyLibcalls into; libanalysis' constantfolding logic. This would allow IPSCCP to be able to; handle simple things like this:. static int foo(const char *X) { return strlen(X); }; int bar() { return foo(""abcd""); }. //===---------------------------------------------------------------------===//. function-attrs doesn't know much about memcpy/memset. This function should be; marked readnone rather than readonly, since it only twiddles local memory, but; function-attrs doesn't handle memset/memcpy/memmove aggressively:. struct X { int *p; int *q; };; int foo() {; int i = 0, j = 1;; struct X x, y;; int **p;; y.p = &i;; x.q = &j;; p = __builtin_memcpy (&x, &y, sizeof (int *));; return **p;; }. This can be seen at:; $ clang t.c -S -o - -mkernel -O0 -emit-llvm | opt -function-attrs -S. //===---------------------------------------------------------------------===//. Missed instcombine transformation:; define i1 @a(i32 %x) nounwind readnone {; entry:; %cmp = icmp eq i32 %x, 30; %sub = add i32 %x, -30; %cmp2 = icmp ugt i32 %sub, 9; %or = or i1 %cmp, %cmp2; ret i1 %or; }; This should b",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:52419,Usability,simpl,simplify,52419,"s. //===---------------------------------------------------------------------===//. In this code:. long foo(long x) {; return x > 1 ? x : 1;; }. LLVM emits a comparison with 1 instead of 0. 0 would be equivalent; and cheaper on most targets. LLVM prefers comparisons with zero over non-zero in general, but in this; case it choses instead to keep the max operation obvious. //===---------------------------------------------------------------------===//. define void @a(i32 %x) nounwind {; entry:; switch i32 %x, label %if.end [; i32 0, label %if.then; i32 1, label %if.then; i32 2, label %if.then; i32 3, label %if.then; i32 5, label %if.then; ]; if.then:; tail call void @foo() nounwind; ret void; if.end:; ret void; }; declare void @foo(). Generated code on x86-64 (other platforms give similar results):; a:; 	cmpl	$5, %edi; 	ja	LBB2_2; 	cmpl	$4, %edi; 	jne	LBB2_3; .LBB0_2:; 	ret; .LBB0_3:; 	jmp	foo # TAILCALL. If we wanted to be really clever, we could simplify the whole thing to; something like the following, which eliminates a branch:; 	xorl $1, %edi; 	cmpl	$4, %edi; 	ja	.LBB0_2; 	ret; .LBB0_2:; 	jmp	foo # TAILCALL. //===---------------------------------------------------------------------===//. We compile this:. int foo(int a) { return (a & (~15)) / 16; }. Into:. define i32 @foo(i32 %a) nounwind readnone ssp {; entry:; %and = and i32 %a, -16; %div = sdiv i32 %and, 16; ret i32 %div; }. but this code (X & -A)/A is X >> log2(A) when A is a power of 2, so this case; should be instcombined into just ""a >> 4"". We do get this at the codegen level, so something knows about it, but ; instcombine should catch it earlier:. _foo: ## @foo; ## %bb.0: ## %entry; 	movl	%edi, %eax; 	sarl	$4, %eax; 	ret. //===---------------------------------------------------------------------===//. This code (from GCC PR28685):. int test(int a, int b) {; int lt = a < b;; int eq = a == b;; if (lt); return 1;; return eq;; }. Is compiled to:. define i32 @test(i32 %a, i32 %b) nounwind readnone ssp {; entry",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:58227,Usability,simpl,simplified,58227,"en.i.i.i.i.i, label %_ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.i.i. if.then.i.i.i.i.i: ; preds = %cond.true.i.i.i.i; call void @_ZSt17__throw_bad_allocv() noreturn nounwind; unreachable. _ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.i.i: ; preds = %cond.true.i.i.i.i; %mul.i.i.i.i.i = shl i64 %conv, 2; %call3.i.i.i.i.i = call noalias i8* @_Znwm(i64 %mul.i.i.i.i.i) nounwind; %0 = bitcast i8* %call3.i.i.i.i.i to i32*; store i32* %0, i32** %v2.sub, align 8, !tbaa !0; store i32* %0, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0; %add.ptr.i.i.i = getelementptr inbounds i32* %0, i64 %conv; store i32* %add.ptr.i.i.i, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0; call void @llvm.memset.p0i8.i64(i8* %call3.i.i.i.i.i, i8 0, i64 %mul.i.i.i.i.i, i32 4, i1 false); br label %_ZNSt6vectorIiSaIiEEC1EmRKiRKS0_.exit. This is just the handling the construction of the vector. Most surprising here; is the fact that all three null stores in %entry are dead (because we do no; cross-block DSE). Also surprising is that %conv isn't simplified to 0 in %....exit.thread.i.i.; This is a because the client of LazyValueInfo doesn't simplify all instruction; operands, just selected ones. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(char* a, int n) {; __builtin_memset(a, 0, n);; for (int i = 0; i < n; ++i); a[i] = 0;; }. into:. define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {; entry:; %conv = sext i32 %n to i64; tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false); %cmp8 = icmp sgt i32 %n, 0; br i1 %cmp8, label %for.body.lr.ph, label %for.end. for.body.lr.ph: ; preds = %entry; %tmp10 = add i32 %n, -1; %tmp11 = zext i32 %tmp10 to i64; %tmp12 = add i64 %tmp11, 1; call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false); ret void. for.end: ; preds = %entry; ret void; }. This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold; the two memset's ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:58323,Usability,simpl,simplify,58323,"true.i.i.i.i; call void @_ZSt17__throw_bad_allocv() noreturn nounwind; unreachable. _ZNSt12_Vector_baseIiSaIiEEC2EmRKS0_.exit.i.i: ; preds = %cond.true.i.i.i.i; %mul.i.i.i.i.i = shl i64 %conv, 2; %call3.i.i.i.i.i = call noalias i8* @_Znwm(i64 %mul.i.i.i.i.i) nounwind; %0 = bitcast i8* %call3.i.i.i.i.i to i32*; store i32* %0, i32** %v2.sub, align 8, !tbaa !0; store i32* %0, i32** %tmp3.i.i.i.i.i, align 8, !tbaa !0; %add.ptr.i.i.i = getelementptr inbounds i32* %0, i64 %conv; store i32* %add.ptr.i.i.i, i32** %tmp4.i.i.i.i.i, align 8, !tbaa !0; call void @llvm.memset.p0i8.i64(i8* %call3.i.i.i.i.i, i8 0, i64 %mul.i.i.i.i.i, i32 4, i1 false); br label %_ZNSt6vectorIiSaIiEEC1EmRKiRKS0_.exit. This is just the handling the construction of the vector. Most surprising here; is the fact that all three null stores in %entry are dead (because we do no; cross-block DSE). Also surprising is that %conv isn't simplified to 0 in %....exit.thread.i.i.; This is a because the client of LazyValueInfo doesn't simplify all instruction; operands, just selected ones. //===---------------------------------------------------------------------===//. clang -O3 -fno-exceptions currently compiles this code:. void f(char* a, int n) {; __builtin_memset(a, 0, n);; for (int i = 0; i < n; ++i); a[i] = 0;; }. into:. define void @_Z1fPci(i8* nocapture %a, i32 %n) nounwind {; entry:; %conv = sext i32 %n to i64; tail call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %conv, i32 1, i1 false); %cmp8 = icmp sgt i32 %n, 0; br i1 %cmp8, label %for.body.lr.ph, label %for.end. for.body.lr.ph: ; preds = %entry; %tmp10 = add i32 %n, -1; %tmp11 = zext i32 %tmp10 to i64; %tmp12 = add i64 %tmp11, 1; call void @llvm.memset.p0i8.i64(i8* %a, i8 0, i64 %tmp12, i32 1, i1 false); ret void. for.end: ; preds = %entry; ret void; }. This shouldn't need the ((zext (%n - 1)) + 1) game, and it should ideally fold; the two memset's together. The issue with the addition only occurs in 64-bit mode, and appears to be at; least partially ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt:64574,Usability,simpl,simplifying,64574,")) unless the two adds only have a single use.; There are a lot of cases that we're refusing to fold in (e.g.) 256.bzip2, for; example:. %indvar.next90 = add i64 %indvar89, 1 ;; Has 2 uses; %tmp96 = add i64 %tmp95, 1 ;; Has 1 use; %exitcond97 = icmp eq i64 %indvar.next90, %tmp96. We don't fold this because we don't want to introduce an overlapped live range; of the ivar. However if we can make this more aggressive without causing; performance issues in two ways:. 1. If *either* the LHS or RHS has a single use, we can definitely do the; transformation. In the overlapping liverange case we're trading one register; use for one fewer operation, which is a reasonable trade. Before doing this; we should verify that the llc output actually shrinks for some benchmarks.; 2. If both ops have multiple uses, we can still fold it if the operations are; both sinkable to *after* the icmp (e.g. in a subsequent block) which doesn't; increase register pressure. There are a ton of icmp's we aren't simplifying because of the reg pressure; concern. Care is warranted here though because many of these are induction; variables and other cases that matter a lot to performance, like the above.; Here's a blob of code that you can drop into the bottom of visitICmp to see some; missed cases:. { Value *A, *B, *C, *D;; if (match(Op0, m_Add(m_Value(A), m_Value(B))) && ; match(Op1, m_Add(m_Value(C), m_Value(D))) &&; (A == C || A == D || B == C || B == D)) {; errs() << ""OP0 = "" << *Op0 << "" U="" << Op0->getNumUses() << ""\n"";; errs() << ""OP1 = "" << *Op1 << "" U="" << Op1->getNumUses() << ""\n"";; errs() << ""CMP = "" << I << ""\n\n"";; }; }. //===---------------------------------------------------------------------===//. define i1 @test1(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ult i32 %and, 2; ret i1 %cmp; }. Can be folded to (x & 2) == 0. define i1 @test2(i32 %x) nounwind {; %and = and i32 %x, 3; %cmp = icmp ugt i32 %and, 1; ret i1 %cmp; }. Can be folded to (x & 2) != 0. SimplifyDemandedBits sh",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt:313,Integrability,depend,dependency,313,"# Ensure that libLLVMTargetParser does not carry any static global initializer.; # ManagedStatic can be used to enable lazy-initialization of globals.; #; # HAS_WERROR_GLOBAL_CTORS and LLVM_HAS_NOGLOBAL_CTOR_MUTEX should have been set; # by llvm/lib/Support/CMakeLists.txt (which provides the required Support; # dependency).; if (HAS_WERROR_GLOBAL_CTORS AND NOT LLVM_HAS_NOGLOBAL_CTOR_MUTEX); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Werror=global-constructors""); endif(). # Solaris code uses kstat, so specify dependency explicitly for shared builds.; if (${CMAKE_SYSTEM_NAME} MATCHES ""SunOS""); set(system_libs kstat); endif(). add_llvm_component_library(LLVMTargetParser; AArch64TargetParser.cpp; ARMTargetParserCommon.cpp; ARMTargetParser.cpp; CSKYTargetParser.cpp; Host.cpp; LoongArchTargetParser.cpp; RISCVTargetParser.cpp; SubtargetFeature.cpp; TargetParser.cpp; Triple.cpp; X86TargetParser.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows. LINK_LIBS; ${system_libs}. LINK_COMPONENTS; Support. DEPENDS; RISCVTargetParserTableGen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt:512,Integrability,depend,dependency,512,"# Ensure that libLLVMTargetParser does not carry any static global initializer.; # ManagedStatic can be used to enable lazy-initialization of globals.; #; # HAS_WERROR_GLOBAL_CTORS and LLVM_HAS_NOGLOBAL_CTOR_MUTEX should have been set; # by llvm/lib/Support/CMakeLists.txt (which provides the required Support; # dependency).; if (HAS_WERROR_GLOBAL_CTORS AND NOT LLVM_HAS_NOGLOBAL_CTOR_MUTEX); SET(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} -Werror=global-constructors""); endif(). # Solaris code uses kstat, so specify dependency explicitly for shared builds.; if (${CMAKE_SYSTEM_NAME} MATCHES ""SunOS""); set(system_libs kstat); endif(). add_llvm_component_library(LLVMTargetParser; AArch64TargetParser.cpp; ARMTargetParserCommon.cpp; ARMTargetParser.cpp; CSKYTargetParser.cpp; Host.cpp; LoongArchTargetParser.cpp; RISCVTargetParser.cpp; SubtargetFeature.cpp; TargetParser.cpp; Triple.cpp; X86TargetParser.cpp. ADDITIONAL_HEADER_DIRS; Unix; Windows. LINK_LIBS; ${system_libs}. LINK_COMPONENTS; Support. DEPENDS; RISCVTargetParserTableGen; ); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TargetParser/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt:538,Deployability,configurat,configuration,538,"include(GetLibraryName). if(LLVM_ENABLE_LIBXML2); set(imported_libs LibXml2::LibXml2); endif(). add_llvm_component_library(LLVMWindowsManifest; WindowsManifestMerger.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/WindowsManifest; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${imported_libs}. LINK_COMPONENTS; Support; ). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_LIBXML2); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION_${build_type}); endif(); if(NOT libxml2_library); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION); endif(); get_library_name(${libxml2_library} libxml2_library); set_property(TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS ${libxml2_library}); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt:364,Modifiability,config,config,364,"include(GetLibraryName). if(LLVM_ENABLE_LIBXML2); set(imported_libs LibXml2::LibXml2); endif(). add_llvm_component_library(LLVMWindowsManifest; WindowsManifestMerger.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/WindowsManifest; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${imported_libs}. LINK_COMPONENTS; Support; ). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_LIBXML2); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION_${build_type}); endif(); if(NOT libxml2_library); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION); endif(); get_library_name(${libxml2_library} libxml2_library); set_property(TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS ${libxml2_library}); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt:395,Modifiability,config,config,395,"include(GetLibraryName). if(LLVM_ENABLE_LIBXML2); set(imported_libs LibXml2::LibXml2); endif(). add_llvm_component_library(LLVMWindowsManifest; WindowsManifestMerger.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/WindowsManifest; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${imported_libs}. LINK_COMPONENTS; Support; ). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_LIBXML2); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION_${build_type}); endif(); if(NOT libxml2_library); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION); endif(); get_library_name(${libxml2_library} libxml2_library); set_property(TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS ${libxml2_library}); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt:538,Modifiability,config,configuration,538,"include(GetLibraryName). if(LLVM_ENABLE_LIBXML2); set(imported_libs LibXml2::LibXml2); endif(). add_llvm_component_library(LLVMWindowsManifest; WindowsManifestMerger.cpp. ADDITIONAL_HEADER_DIRS; ${LLVM_MAIN_INCLUDE_DIR}/llvm/WindowsManifest; ${Backtrace_INCLUDE_DIRS}. LINK_LIBS; ${imported_libs}. LINK_COMPONENTS; Support; ). # This block is only needed for llvm-config. When we deprecate llvm-config and; # move to using CMake export, this block can be removed.; if(LLVM_ENABLE_LIBXML2); # CMAKE_BUILD_TYPE is only meaningful to single-configuration generators.; if(CMAKE_BUILD_TYPE); string(TOUPPER ${CMAKE_BUILD_TYPE} build_type); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION_${build_type}); endif(); if(NOT libxml2_library); get_property(libxml2_library TARGET LibXml2::LibXml2 PROPERTY LOCATION); endif(); get_library_name(${libxml2_library} libxml2_library); set_property(TARGET LLVMWindowsManifest PROPERTY LLVM_SYSTEM_LIBS ${libxml2_library}); endif(); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/WindowsManifest/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt:557,Integrability,message,message,557,"include_directories( ${CMAKE_CURRENT_SOURCE_DIR}/.. ). if(NOT DEFINED ITTAPI_GIT_REPOSITORY); set(ITTAPI_GIT_REPOSITORY https://github.com/intel/ittapi.git); endif(). if(NOT DEFINED ITTAPI_GIT_TAG); set(ITTAPI_GIT_TAG v3.18.12); endif(). if(NOT DEFINED ITTAPI_SOURCE_DIR); set(ITTAPI_SOURCE_DIR ${PROJECT_BINARY_DIR}); endif(). if(NOT EXISTS ${ITTAPI_SOURCE_DIR}/ittapi); execute_process(COMMAND ${GIT_EXECUTABLE} clone ${ITTAPI_GIT_REPOSITORY}; WORKING_DIRECTORY ${ITTAPI_SOURCE_DIR}; RESULT_VARIABLE GIT_CLONE_RESULT); if(NOT GIT_CLONE_RESULT EQUAL ""0""); message(FATAL_ERROR ""git clone ${ITTAPI_GIT_REPOSITORY} failed with ${GIT_CLONE_RESULT}, please clone ${ITTAPI_GIT_REPOSITORY}""); endif(); endif(). execute_process(COMMAND ${GIT_EXECUTABLE} checkout ${ITTAPI_GIT_TAG}; WORKING_DIRECTORY ${ITTAPI_SOURCE_DIR}/ittapi; RESULT_VARIABLE GIT_CHECKOUT_RESULT); if(NOT GIT_CHECKOUT_RESULT EQUAL ""0""); message(FATAL_ERROR ""git checkout ${ITTAPI_GIT_TAG} failed with ${GIT_CHECKOUT_RESULT}, please checkout ${ITTAPI_GIT_TAG} at ${ITTAPI_SOURCE_DIR}/ittapi""); endif(). include_directories( ${ITTAPI_SOURCE_DIR}/ittapi/include/ ). if( HAVE_LIBDL ); set(LLVM_INTEL_JIT_LIBS ${CMAKE_DL_LIBS}); endif(). set(LLVM_INTEL_JIT_LIBS ${LLVM_PTHREAD_LIB} ${LLVM_INTEL_JIT_LIBS}). add_llvm_component_library(LLVMIntelJITEvents; IntelJITEventListener.cpp; jitprofiling.c; ${ITTAPI_SOURCE_DIR}/ittapi/src/ittnotify/ittnotify_static.c. LINK_LIBS ${LLVM_INTEL_JIT_LIBS}. LINK_COMPONENTS; CodeGen; Core; DebugInfoDWARF; Support; Object; ExecutionEngine; ). add_dependencies(LLVMIntelJITEvents LLVMCodeGen); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt:899,Integrability,message,message,899,"include_directories( ${CMAKE_CURRENT_SOURCE_DIR}/.. ). if(NOT DEFINED ITTAPI_GIT_REPOSITORY); set(ITTAPI_GIT_REPOSITORY https://github.com/intel/ittapi.git); endif(). if(NOT DEFINED ITTAPI_GIT_TAG); set(ITTAPI_GIT_TAG v3.18.12); endif(). if(NOT DEFINED ITTAPI_SOURCE_DIR); set(ITTAPI_SOURCE_DIR ${PROJECT_BINARY_DIR}); endif(). if(NOT EXISTS ${ITTAPI_SOURCE_DIR}/ittapi); execute_process(COMMAND ${GIT_EXECUTABLE} clone ${ITTAPI_GIT_REPOSITORY}; WORKING_DIRECTORY ${ITTAPI_SOURCE_DIR}; RESULT_VARIABLE GIT_CLONE_RESULT); if(NOT GIT_CLONE_RESULT EQUAL ""0""); message(FATAL_ERROR ""git clone ${ITTAPI_GIT_REPOSITORY} failed with ${GIT_CLONE_RESULT}, please clone ${ITTAPI_GIT_REPOSITORY}""); endif(); endif(). execute_process(COMMAND ${GIT_EXECUTABLE} checkout ${ITTAPI_GIT_TAG}; WORKING_DIRECTORY ${ITTAPI_SOURCE_DIR}/ittapi; RESULT_VARIABLE GIT_CHECKOUT_RESULT); if(NOT GIT_CHECKOUT_RESULT EQUAL ""0""); message(FATAL_ERROR ""git checkout ${ITTAPI_GIT_TAG} failed with ${GIT_CHECKOUT_RESULT}, please checkout ${ITTAPI_GIT_TAG} at ${ITTAPI_SOURCE_DIR}/ittapi""); endif(). include_directories( ${ITTAPI_SOURCE_DIR}/ittapi/include/ ). if( HAVE_LIBDL ); set(LLVM_INTEL_JIT_LIBS ${CMAKE_DL_LIBS}); endif(). set(LLVM_INTEL_JIT_LIBS ${LLVM_PTHREAD_LIB} ${LLVM_INTEL_JIT_LIBS}). add_llvm_component_library(LLVMIntelJITEvents; IntelJITEventListener.cpp; jitprofiling.c; ${ITTAPI_SOURCE_DIR}/ittapi/src/ittnotify/ittnotify_static.c. LINK_LIBS ${LLVM_INTEL_JIT_LIBS}. LINK_COMPONENTS; CodeGen; Core; DebugInfoDWARF; Support; Object; ExecutionEngine; ). add_dependencies(LLVMIntelJITEvents LLVMCodeGen); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:475,Availability,avail,available,475,"set(LLVM_BLAKE3_FILES; blake3.c; blake3_dispatch.c; blake3_portable.c; blake3_neon.c; ). if (LLVM_DISABLE_ASSEMBLY_FILES); set(CAN_USE_ASSEMBLER FALSE); else(); set(CAN_USE_ASSEMBLER TRUE); endif(). macro(disable_blake3_x86_simd); add_compile_definitions(BLAKE3_NO_AVX512 BLAKE3_NO_AVX2 BLAKE3_NO_SSE41 BLAKE3_NO_SSE2); endmacro(). # The BLAKE3 team recommends using the assembly versions, from the README:; #; # ""For each of the x86 SIMD instruction sets, four versions are available:; # three flavors of assembly (Unix, Windows MSVC, and Windows GNU) and one; # version using C intrinsics. The assembly versions are generally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:1925,Deployability,configurat,configuration,1925,"nerally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but their source is guarded with architecture ""#ifdef"" checks.; list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_unix.S; blake3_sse41_x86-64_unix.S; blake3_avx2_x86-64_unix.S; blake3_avx512_x86-64_unix.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_unix.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); endif(); else(); # CAN_USE_ASSEMBLER == FALSE; disable_blake3_x86_simd(); endif(). add_library(LLVMSupportBlake3 OBJECT EXCLUDE_FROM_ALL ${LLVM_BLAKE3_FILES}); llvm_update_compile_flags(LLVMSupportBlake3); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:1925,Modifiability,config,configuration,1925,"nerally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but their source is guarded with architecture ""#ifdef"" checks.; list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_unix.S; blake3_sse41_x86-64_unix.S; blake3_avx2_x86-64_unix.S; blake3_avx512_x86-64_unix.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_unix.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); endif(); else(); # CAN_USE_ASSEMBLER == FALSE; disable_blake3_x86_simd(); endif(). add_library(LLVMSupportBlake3 OBJECT EXCLUDE_FROM_ALL ${LLVM_BLAKE3_FILES}); llvm_update_compile_flags(LLVMSupportBlake3); ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:647,Performance,perform,perform,647,"set(LLVM_BLAKE3_FILES; blake3.c; blake3_dispatch.c; blake3_portable.c; blake3_neon.c; ). if (LLVM_DISABLE_ASSEMBLY_FILES); set(CAN_USE_ASSEMBLER FALSE); else(); set(CAN_USE_ASSEMBLER TRUE); endif(). macro(disable_blake3_x86_simd); add_compile_definitions(BLAKE3_NO_AVX512 BLAKE3_NO_AVX2 BLAKE3_NO_SSE41 BLAKE3_NO_SSE2); endmacro(). # The BLAKE3 team recommends using the assembly versions, from the README:; #; # ""For each of the x86 SIMD instruction sets, four versions are available:; # three flavors of assembly (Unix, Windows MSVC, and Windows GNU) and one; # version using C intrinsics. The assembly versions are generally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt:668,Performance,perform,perform,668,"set(LLVM_BLAKE3_FILES; blake3.c; blake3_dispatch.c; blake3_portable.c; blake3_neon.c; ). if (LLVM_DISABLE_ASSEMBLY_FILES); set(CAN_USE_ASSEMBLER FALSE); else(); set(CAN_USE_ASSEMBLER TRUE); endif(). macro(disable_blake3_x86_simd); add_compile_definitions(BLAKE3_NO_AVX512 BLAKE3_NO_AVX2 BLAKE3_NO_SSE41 BLAKE3_NO_SSE2); endmacro(). # The BLAKE3 team recommends using the assembly versions, from the README:; #; # ""For each of the x86 SIMD instruction sets, four versions are available:; # three flavors of assembly (Unix, Windows MSVC, and Windows GNU) and one; # version using C intrinsics. The assembly versions are generally; # preferred. They perform better, they perform more consistently across; # different compilers, and they build more quickly."". if (CAN_USE_ASSEMBLER); if (MSVC); check_symbol_exists(_M_X64 """" IS_X64); if (IS_X64); enable_language(ASM_MASM); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_msvc.asm; blake3_sse41_x86-64_windows_msvc.asm; blake3_avx2_x86-64_windows_msvc.asm; blake3_avx512_x86-64_windows_msvc.asm; ); else(); disable_blake3_x86_simd(); endif(); elseif(WIN32 OR CYGWIN); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64); list(APPEND LLVM_BLAKE3_FILES; blake3_sse2_x86-64_windows_gnu.S; blake3_sse41_x86-64_windows_gnu.S; blake3_avx2_x86-64_windows_gnu.S; blake3_avx512_x86-64_windows_gnu.S; ); # Clang before 7 needs -mavx512vl to assemble some instructions.; set_source_files_properties(blake3_avx512_x86-64_windows_gnu.S; PROPERTIES COMPILE_OPTIONS ""-mavx512vl""); else(); disable_blake3_x86_simd(); endif(); else(); check_symbol_exists(__x86_64__ """" IS_X64); if (IS_X64 OR CMAKE_OSX_ARCHITECTURES MATCHES ""x86_64""); # In a macOS Universal build (setting CMAKE_OSX_ARCHITECTURES to multiple; # values), compilation of the source files will target multiple architectures; # (each source file is internally compiled once for each architecture).; # To accomodate this configuration we include these assembly files without a; # CMake check but t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BLAKE3/CMakeLists.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:473,Availability,failure,failure,473,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:5674,Deployability,toggle,toggle,5674,"-----------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb instructions clobber condition code. //===---------------------------------------------------------------------===//. Thumb load / store address mode offsets are scaled. The values kept in the; instruction operands are pre-scale values. This probably ought to be changed; to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions. //===---------------------------------------------------------------------===//. We need to make (some of the) Thumb1 instructions predicable. That will allow; shrinking of predicated Thumb2 instructions. To allow this, we need to be able; to toggle the 's' bit since they do not set CPSR when they are inside IT blocks. //===---------------------------------------------------------------------===//. Make use of hi register variants of cmp: tCMPhir / tCMPZhir. //===---------------------------------------------------------------------===//. Thumb1 immediate field sometimes keep pre-scaled values. See; ThumbRegisterInfo::eliminateFrameIndex. This is inconsistent from ARM and; Thumb2. //===---------------------------------------------------------------------===//. Rather than having tBR_JTr print a "".align 2"" and constant island pass pad it,; add a target specific ALIGN instruction instead. That way, getInstSizeInBytes; won't have to over-estimate. It can also be used for loop alignment pass. //===---------------------------------------------------------------------===//. We generate conditional code for icmp when we don't need to. This code:. int foo(int s) {; return s == 1;; }. produces:. foo:; cmp r0, #1; mov.w r0, #0; it eq; moveq r0, #1; bx l",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:600,Performance,load,load,600,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:653,Performance,load,load,653,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:2034,Performance,load,load,2034,"; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also does something similar to constpool load:; LPCRELL0:; ldr r0, LCPI1_0; =>; ldr r0, pc, #((LCPI1_0-(LPCRELL0+4))&0xfffffffc). //===---------------------------------------------------------------------===//. We compile the following:. define i16 @func_entry_2E_ce(i32 %i) {; switch i32 %i, label %bb12.exitStub [; i32 0, label %bb4.exitStub; i32 1, label %bb9.exitStub; i32 2, label %bb4.exitStub; i32 3, label %bb4.exitStub; i32 7, label %bb9.exitStub; i32 8, label %bb.exitStub; i32 9, label %bb9.exitStub; ]. bb12.exitStub:; ret i16 0. bb4.exitStub:; ret i16 1. bb9.exitStub:; ret i16 2. bb.exitStub:; ret i16 3; }. into:. _func_entry_2E_ce:; mov r2, #1; lsl r2, r0; cmp r0, #9; bhi LBB1_4 @bb12.exitStub; LBB1_1: @newFuncRoot; mov r1, #13; tst r2, r1; bne LBB1_5 @bb4.exitStub; LBB1_2: @newFuncRoot; ldr r1, LCPI1_0; tst r2, r1; bne LBB1_6 @bb9.exitStub; LBB1_3: @newFuncRoot; mov r1, #1; lsl r1, r1, #8; tst r2, r1; bne LBB1_7 @bb.exitStub; LBB1_4: @bb12.exitStub; mov r0, #0; bx lr; LBB1_5: @bb4.exitStub; mov r0, #1; bx lr; LBB1_6: @bb9.exitStub",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:4100,Performance,load,load,4100,"@bb.exitStub; mov r0, #3; bx lr; LBB1_8:; .align 2; LCPI1_0:; .long 642. gcc compiles to:. 	cmp	r0, #9; 	@ lr needed for prologue; 	bhi	L2; 	ldr	r3, L11; 	mov	r2, #1; 	mov	r1, r2, asl r0; 	ands	r0, r3, r2, asl r0; 	movne	r0, #2; 	bxne	lr; 	tst	r1, #13; 	beq	L9; L3:; 	mov	r0, r2; 	bx	lr; L9:; 	tst	r1, #256; 	movne	r0, #3; 	bxne	lr; L2:; 	mov	r0, #0; 	bx	lr; L12:; 	.align 2; L11:; 	.long	642; . GCC is doing a couple of clever things here:; 1. It is predicating one of the returns. This isn't a clear win though: in; cases where that return isn't taken, it is replacing one condbranch with; two 'ne' predicated instructions.; 2. It is sinking the shift of ""1 << i"" into the tst, and using ands instead of; tst. This will probably require whole function isel.; 3. GCC emits:; 	tst	r1, #256; we emit:; mov r1, #1; lsl r1, r1, #8; tst r2, r1. //===---------------------------------------------------------------------===//. When spilling in thumb mode and the sp offset is too large to fit in the ldr /; str offset field, we load the offset from a constpool entry and add it to sp:. ldr r2, LCPI; add r2, sp; ldr r2, [r2]. These instructions preserve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:5203,Performance,load,load,5203,"rve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb instructions clobber condition code. //===---------------------------------------------------------------------===//. Thumb load / store address mode offsets are scaled. The values kept in the; instruction operands are pre-scale values. This probably ought to be changed; to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions. //===---------------------------------------------------------------------===//. We need to make (some of the) Thumb1 instructions predicable. That will allow; shrinking of predicated Thumb2 instructions. To allow this, we need to be able; to toggle the 's' bit since they do not set CPSR when they are inside IT blocks. //===---------------------------------------------------------------------===//. Make use of hi register variants of cmp: tCMPhir / tCMPZhir. //===---------------------------------------------------------------------===//. Thumb1 immediate field sometimes keep pre-scaled values. See; ThumbRegisterInfo::eliminateFrameIndex. This is inconsistent from ARM and; Thumb2. //===---------------------------------------------------------------------===//. Rather than having tB",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:466,Safety,abort,abort,466,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:5354,Safety,avoid,avoid,5354,"e if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb instructions clobber condition code. //===---------------------------------------------------------------------===//. Thumb load / store address mode offsets are scaled. The values kept in the; instruction operands are pre-scale values. This probably ought to be changed; to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions. //===---------------------------------------------------------------------===//. We need to make (some of the) Thumb1 instructions predicable. That will allow; shrinking of predicated Thumb2 instructions. To allow this, we need to be able; to toggle the 's' bit since they do not set CPSR when they are inside IT blocks. //===---------------------------------------------------------------------===//. Make use of hi register variants of cmp: tCMPhir / tCMPZhir. //===---------------------------------------------------------------------===//. Thumb1 immediate field sometimes keep pre-scaled values. See; ThumbRegisterInfo::eliminateFrameIndex. This is inconsistent from ARM and; Thumb2. //===---------------------------------------------------------------------===//. Rather than having tBR_JTr print a "".align 2"" and constant island pass pad it,; add a target specific ALIGN instruction instead. That way, getInstSizeInBytes; won't have to",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:489,Testability,assert,asserts,489,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend (Thumb specific).; //===---------------------------------------------------------------------===//. * Add support for compiling functions in both ARM and Thumb mode, then taking; the smallest. * Add support for compiling individual basic blocks in thumb mode, when in a ; larger ARM function. This can be used for presumed cold code, like paths; to abort (failure path of asserts), EH handling code, etc. * Thumb doesn't have normal pre/post increment addressing modes, but you can; load/store 32-bit integers with pre/postinc by using load/store multiple; instrs with a single register. * Make better use of high registers r8, r10, r11, r12 (ip). Some variants of add; and cmp instructions can use high registers. Also, we can use them as; temporaries to spill values into. * In thumb mode, short, byte, and bool preferred alignments are currently set; to 4 to accommodate ISA restriction (i.e. add sp, #imm, imm must be multiple; of 4). //===---------------------------------------------------------------------===//. Potential jumptable improvements:. * If we know function size is less than (1 << 16) * 2 bytes, we can use 16-bit; jumptable entries (e.g. (L1 - L2) >> 1). Or even smaller entries if the; function is even smaller. This also applies to ARM. * Thumb jumptable codegen can improve given some help from the assembler. This; is what we generate right now:. 	.set PCRELV0, (LJTI1_0_0-(LPCRELL0+4)); LPCRELL0:; 	mov r1, #PCRELV0; 	add r1, pc; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3; ... Note there is another pc relative add that we can take advantage of.; add r1, pc, #imm_8 * 4. We should be able to generate:. LPCRELL0:; 	add r1, LJTI1_0_0; 	ldr r0, [r0, r1]; 	mov pc, r0 ; 	.align	2; LJTI1_0_0:; 	.long	 LBB1_3. if the assembler can translate the add to:; add r1, pc, #((LJTI1_0_0-(LPCRELL0+4))&0xfffffffc). Note the assembler also doe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:4754,Testability,test,test,4754,"d using ands instead of; tst. This will probably require whole function isel.; 3. GCC emits:; 	tst	r1, #256; we emit:; mov r1, #1; lsl r1, r1, #8; tst r2, r1. //===---------------------------------------------------------------------===//. When spilling in thumb mode and the sp offset is too large to fit in the ldr /; str offset field, we load the offset from a constpool entry and add it to sp:. ldr r2, LCPI; add r2, sp; ldr r2, [r2]. These instructions preserve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets. See; oggenc for an example. //===---------------------------------------------------------------------===//. Poor codegen test/CodeGen/ARM/select.ll f7:. 	ldr r5, LCPI1_0; LPC0:; 	add r5, pc; 	ldr r6, LCPI1_1; 	ldr r2, LCPI1_2; 	mov r3, r6; 	mov lr, pc; 	bx r5. //===---------------------------------------------------------------------===//. Make register allocator / spiller smarter so we can re-materialize ""mov r, imm"",; etc. Almost all Thumb instructions clobber condition code. //===---------------------------------------------------------------------===//. Thumb load / store address mode offsets are scaled. The values kept in the; instruction operands are pre-scale values. This probably ought to be changed; to avoid extra work when we convert Thumb2 instructions to Thumb1 instructions. //===---------------------------------------------------------------------===//. We need to make (some of the) Thumb1 instructions predicable. That will allow; shrinking of predicated Thumb2 instructions. To allow this, we need to be able; to toggle the 's' bit since they do not set CPSR when they are inside IT blocks. //===--",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt:3573,Usability,clear,clear,3573,"try_2E_ce:; mov r2, #1; lsl r2, r0; cmp r0, #9; bhi LBB1_4 @bb12.exitStub; LBB1_1: @newFuncRoot; mov r1, #13; tst r2, r1; bne LBB1_5 @bb4.exitStub; LBB1_2: @newFuncRoot; ldr r1, LCPI1_0; tst r2, r1; bne LBB1_6 @bb9.exitStub; LBB1_3: @newFuncRoot; mov r1, #1; lsl r1, r1, #8; tst r2, r1; bne LBB1_7 @bb.exitStub; LBB1_4: @bb12.exitStub; mov r0, #0; bx lr; LBB1_5: @bb4.exitStub; mov r0, #1; bx lr; LBB1_6: @bb9.exitStub; mov r0, #2; bx lr; LBB1_7: @bb.exitStub; mov r0, #3; bx lr; LBB1_8:; .align 2; LCPI1_0:; .long 642. gcc compiles to:. 	cmp	r0, #9; 	@ lr needed for prologue; 	bhi	L2; 	ldr	r3, L11; 	mov	r2, #1; 	mov	r1, r2, asl r0; 	ands	r0, r3, r2, asl r0; 	movne	r0, #2; 	bxne	lr; 	tst	r1, #13; 	beq	L9; L3:; 	mov	r0, r2; 	bx	lr; L9:; 	tst	r1, #256; 	movne	r0, #3; 	bxne	lr; L2:; 	mov	r0, #0; 	bx	lr; L12:; 	.align 2; L11:; 	.long	642; . GCC is doing a couple of clever things here:; 1. It is predicating one of the returns. This isn't a clear win though: in; cases where that return isn't taken, it is replacing one condbranch with; two 'ne' predicated instructions.; 2. It is sinking the shift of ""1 << i"" into the tst, and using ands instead of; tst. This will probably require whole function isel.; 3. GCC emits:; 	tst	r1, #256; we emit:; mov r1, #1; lsl r1, r1, #8; tst r2, r1. //===---------------------------------------------------------------------===//. When spilling in thumb mode and the sp offset is too large to fit in the ldr /; str offset field, we load the offset from a constpool entry and add it to sp:. ldr r2, LCPI; add r2, sp; ldr r2, [r2]. These instructions preserve the condition code which is important if the spill; is between a cmp and a bcc instruction. However, we can use the (potentially); cheaper sequence if we know it's ok to clobber the condition register. add r2, sp, #255 * 4; add r2, #132; ldr r2, [r2, #7 * 4]. This is especially bad when dynamic alloca is used. The all fixed size stack; objects are referenced off the frame pointer with negative offsets.",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README-Thumb.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19313,Availability,redundant,redundant,19313,"--------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:21536,Availability,robust,robust,21536,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:22036,Availability,error,errors,22036,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9553,Deployability,update,update,9553," However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768))? (LineLen) : (32768))); Line[i++] = Stack[--StackPtr];; if (LineLen > 32768); {; while (StackPtr != 0 && i < LineLen); {; i++;; --StackPtr;; }; }; }; return StackPtr;; }. //===---------------------------------------------------------------------===//. This should compile to the mlas instruction:; int mlas(int x, int y, int z) { return ((x * y + z) < 0) ? 7 : 13; }. //===---------------------------------------------------------------------===//. At some point, we should triage these to see if they still apply to us",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:16878,Deployability,patch,patches,16878,":. _foo:; 	and r1, r0, #127; 	ldr r2, LCPI1_0; 	ldr r2, [r2]; 	ldr r1, [r2, +r1, lsl #2]; 	mov r2, r1, lsr #2; 	tst r0, #128; 	moveq r2, r1; 	ldr r0, LCPI1_1; 	and r0, r2, r0; 	bx lr. It would be better to do something like this, to fold the shift into the; conditional move:. 	and r1, r0, #127; 	ldr r2, LCPI1_0; 	ldr r2, [r2]; 	ldr r1, [r2, +r1, lsl #2]; 	tst r0, #128; 	movne r1, r1, lsr #2; 	ldr r0, LCPI1_1; 	and r0, r1, r0; 	bx lr. it saves an instruction and a register. //===---------------------------------------------------------------------===//. It might be profitable to cse MOVi16 if there are lots of 32-bit immediates; with the same bottom half. //===---------------------------------------------------------------------===//. Robert Muth started working on an alternate jump table implementation that; does not put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the followin",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1097,Energy Efficiency,reduce,reduces,1097,"------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===-------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:2916,Energy Efficiency,power,power,2916,"rocess the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:3476,Energy Efficiency,power,power,3476,"i:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }. llvm code (-O2); ldrb r3, [r1, #+6]; ldr r2, [r1]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, and possibly speed as well (we don't have; a good way to measure on ARM). //===---------------------------------------------------------------------===//. * Consider this silly example:. double bar(double x) {; double r = foo(3.1);; return x+r;; }. _bar:; stmfd sp!, {r4, r5, r7, l",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:4904,Energy Efficiency,schedul,scheduled,4904,"]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, and possibly speed as well (we don't have; a good way to measure on ARM). //===---------------------------------------------------------------------===//. * Consider this silly example:. double bar(double x) {; double r = foo(3.1);; return x+r;; }. _bar:; stmfd sp!, {r4, r5, r7, lr}; add r7, sp, #8; mov r4, r0; mov r5, r1; fldd d0, LCPI1_0; fmrrd r0, r1, d0; bl _foo; fmdrr d0, r4, r5; fmsr s2, r0; fsitod d1, s2; faddd d0, d1, d0; fmrrd r0, r1, d0; ldmfd sp!, {r4, r5, r7, pc}. Ignore the prologue and epilogue stuff for a second. Note; 	mov r4, r0; 	mov r5, r1; the copys to callee-save registers and the fact they are only being used by the; fmdrr instruction. It would have been better had the fmdrr been scheduled; before the call and place the result in a callee-save DPR register. The two; mov ops would not have been necessary. //===---------------------------------------------------------------------===//. Calling convention related stuff:. * gcc's parameter passing implementation is terrible and we suffer as a result:. e.g.; struct s {; double d1;; int s1;; };. void foo(struct s S) {; printf(""%g, %d\n"", S.d1, S.s1);; }. 'S' is passed via registers r0, r1, r2. But gcc stores them to the stack, and; then reload them to r1, r2, and r3 before issuing the call (r0 contains the; address of the format string):. 	stmfd	sp!, {r7, lr}; 	add	r7, sp, #0; 	sub	sp, sp, #12; 	stmia	sp, {r0, r1, r2}; 	ldmia	sp, {r1-r2}; 	ldr	r0, L5; 	ldr	r3, [sp, #8]; L2:; 	add	r0, pc, r0; 	bl	L_printf$stub. Instead of a stmia, ldmia, and a ldr, wouldn't it be better to do three moves?. * Return an aggregate type is even worse:. e.g.; struct s foo(void) {; struct s S = {1.1, 2};; return S;; }. 	mov	ip, r0; 	ldr	r0, L5; 	sub	sp, sp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:7368,Energy Efficiency,allocate,allocate,7368," some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is translated; into a number of load and stores, or 2) custom lower memcpy (of small size) to; be ldmia / stmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/st",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8704,Energy Efficiency,schedul,scheduling,8704,"--------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single imm",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:950,Modifiability,extend,extended,950,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend.; //===---------------------------------------------------------------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===----------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1052,Modifiability,extend,extending,1052,"----------------------------------------------------===//; // Random ideas for the ARM backend.; //===---------------------------------------------------------------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===-------------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:3201,Modifiability,enhance,enhancement,3201,"-------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }. llvm code (-O2); ldrb r3, [r1, #+6]; ldr r2, [r1]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, an",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:12173,Modifiability,extend,extend,12173,".cgi?id=9760; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9759; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9703; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9702; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=9663. http://www.inf.u-szeged.hu/gcc-arm/; http://citeseer.ist.psu.edu/debus04linktime.html. //===---------------------------------------------------------------------===//. gcc generates smaller code for this function at -O2 or -Os:. void foo(signed char* p) {; if (*p == 3); bar();; else if (*p == 4); baz();; else if (*p == 5); quux();; }. llvm decides it's a good idea to turn the repeated if...else into a; binary tree, as if it were a switch; the resulting code requires -1; compare-and-branches when *p<=2 or *p==5, the same number if *p==4; or *p>6, and +1 if *p==3. So it should be a speed win; (on balance). However, the revised code is larger, with 4 conditional; branches instead of 3. More seriously, there is a byte->word extend before; each comparison, where there should be only one, and the condition codes; are not remembered when the same two values are compared twice. //===---------------------------------------------------------------------===//. More LSR enhancements possible:. 1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged; in a load / store.; 2. Allow iv reuse even when a type conversion is required. For example, i8; and i32 load / store addressing modes are identical. //===---------------------------------------------------------------------===//. This:. int foo(int a, int b, int c, int d) {; long long acc = (long long)a * (long long)b;; acc += (long long)c * (long long)d;; return (int)(acc >> 32);; }. Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies; two signed 32-bit values to produce a 64-bit value, and accumulates this with; a 64-bit value. We currently get this with both v4 and v6:. _foo:; smull r1, r0, r1, r0; smull r3, r2, r3, r2; adds r3, r3, r1; adc r0, r2, r0; bx lr. //===--------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:12416,Modifiability,enhance,enhancements,12416,"how_bug.cgi?id=9663. http://www.inf.u-szeged.hu/gcc-arm/; http://citeseer.ist.psu.edu/debus04linktime.html. //===---------------------------------------------------------------------===//. gcc generates smaller code for this function at -O2 or -Os:. void foo(signed char* p) {; if (*p == 3); bar();; else if (*p == 4); baz();; else if (*p == 5); quux();; }. llvm decides it's a good idea to turn the repeated if...else into a; binary tree, as if it were a switch; the resulting code requires -1; compare-and-branches when *p<=2 or *p==5, the same number if *p==4; or *p>6, and +1 if *p==3. So it should be a speed win; (on balance). However, the revised code is larger, with 4 conditional; branches instead of 3. More seriously, there is a byte->word extend before; each comparison, where there should be only one, and the condition codes; are not remembered when the same two values are compared twice. //===---------------------------------------------------------------------===//. More LSR enhancements possible:. 1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged; in a load / store.; 2. Allow iv reuse even when a type conversion is required. For example, i8; and i32 load / store addressing modes are identical. //===---------------------------------------------------------------------===//. This:. int foo(int a, int b, int c, int d) {; long long acc = (long long)a * (long long)b;; acc += (long long)c * (long long)d;; return (int)(acc >> 32);; }. Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies; two signed 32-bit values to produce a 64-bit value, and accumulates this with; a 64-bit value. We currently get this with both v4 and v6:. _foo:; smull r1, r0, r1, r0; smull r3, r2, r3, r2; adds r3, r3, r1; adc r0, r2, r0; bx lr. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19668,Modifiability,rewrite,rewrite,19668," r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. W",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:20811,Modifiability,portab,portable,20811,"ile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:580,Performance,optimiz,optimization,580,"//===---------------------------------------------------------------------===//; // Random ideas for the ARM backend.; //===---------------------------------------------------------------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===----------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1272,Performance,load,load,1272,"------------===//. Reimplement 'select' in terms of 'SEL'. * We would really like to support UXTAB16, but we need to prove that the; add doesn't need to overflow between the two 16-bit chunks. * Implement pre/post increment support. (e.g. PR935); * Implement smarter constant generation for binops with large immediates. A few ARMv6T2 ops should be pattern matched: BFI, SBFX, and UBFX. Interesting optimization for PIC codegen on arm-linux:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=43129. //===---------------------------------------------------------------------===//. Crazy idea: Consider code that uses lots of 8-bit or 16-bit values. By the; time regalloc happens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===-------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1792,Performance,queue,queue,1792,"pens, these values are now in a 32-bit register, usually with; the top-bits known to be sign or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:2823,Performance,load,load,2823,"rocess the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:2836,Performance,optimiz,optimizations,2836,"rocess the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:3517,Performance,load,load,3517,"-------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4]; 	fstd d0, [r4, #+32]; 	fldd d0, [r4, #+8]; 	fstd d0, [r4, #+40]; 	fldd d0, [r4, #+16]; 	fstd d0, [r4, #+48]; 	fldd d0, [r4, #+24]; 	fstd d0, [r4, #+56]. If we can spare the registers, it would be better to use fldm and fstm here.; Need major register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }. llvm code (-O2); ldrb r3, [r1, #+6]; ldr r2, [r1]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, and possibly speed as well (we don't have; a good way to measure on ARM). //===---------------------------------------------------------------------===//. * Consider this silly example:. double bar(double x) {; double r = foo(3.1);; return x+r;; }. _bar:; stmfd sp!, {r4, r5, r7, lr}; add r7, sp, #8; mov r4, r0; mov r5, r1; fldd d0, LCPI1_0; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:6184,Performance,load,loads,6184,"rrible and we suffer as a result:. e.g.; struct s {; double d1;; int s1;; };. void foo(struct s S) {; printf(""%g, %d\n"", S.d1, S.s1);; }. 'S' is passed via registers r0, r1, r2. But gcc stores them to the stack, and; then reload them to r1, r2, and r3 before issuing the call (r0 contains the; address of the format string):. 	stmfd	sp!, {r7, lr}; 	add	r7, sp, #0; 	sub	sp, sp, #12; 	stmia	sp, {r0, r1, r2}; 	ldmia	sp, {r1-r2}; 	ldr	r0, L5; 	ldr	r3, [sp, #8]; L2:; 	add	r0, pc, r0; 	bl	L_printf$stub. Instead of a stmia, ldmia, and a ldr, wouldn't it be better to do three moves?. * Return an aggregate type is even worse:. e.g.; struct s foo(void) {; struct s S = {1.1, 2};; return S;; }. 	mov	ip, r0; 	ldr	r0, L5; 	sub	sp, sp, #12; L2:; 	add	r0, pc, r0; 	@ lr needed for prologue; 	ldmia	r0, {r0, r1, r2}; 	stmia	sp, {r0, r1, r2}; 	stmia	ip, {r0, r1, r2}; 	mov	r0, ip; 	add	sp, sp, #12; 	bx	lr. r0 (and later ip) is the hidden parameter from caller to store the value in. The; first ldmia loads the constants into r0, r1, r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is trans",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:7110,Performance,load,loads,7110," (and later ip) is the hidden parameter from caller to store the value in. The; first ldmia loads the constants into r0, r1, r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is translated; into a number of load and stores, or 2) custom lower memcpy (of small size) to; be ldmia / stmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:7217,Performance,load,load,7217," r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; 	cast %struct.s* %S to sbyte*		; <sbyte*>:0 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %0, sbyte* cast ({ double, int }* %C.0.904 to sbyte*), uint 12, uint 4 ); 	cast %struct.s* %agg.result to sbyte*		; <sbyte*>:1 [#uses=2]; 	call void %llvm.memcpy.i32( sbyte* %1, sbyte* %0, uint 12, uint 0 ); 	cast %struct.s* %memtmp to sbyte*		; <sbyte*>:2 [#uses=1]; 	call void %llvm.memcpy.i32( sbyte* %2, sbyte* %1, uint 12, uint 0 ); 	ret void; }. llc ends up issuing two memcpy's (the first memcpy becomes 3 loads from; constantpool). Perhaps we should 1) fix llvm-gcc so the memcpy is translated; into a number of load and stores, or 2) custom lower memcpy (of small size) to; be ldmia / stmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8212,Performance,load,load,8212,"mber of load and stores, or 2) custom lower memcpy (of small size) to; be ldmia / stmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===----------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8272,Performance,load,load,8272,"tmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tr",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8346,Performance,load,load,8346,"tmia. I think option 2 is better but the current register; allocator cannot allocate a chunk of registers at a time. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tr",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8423,Performance,perform,performing,8423,"ime. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8438,Performance,optimiz,optimization,8438,"ime. A feasible temporary solution is to use specific physical registers at the; lowering time for small (<= 4 words?) transfer size. * ARM CSRet calling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8566,Performance,optimiz,optimization,8566,"ling convention requires the hidden argument to be returned by; the callee. //===---------------------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to u",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8629,Performance,perform,performance,8629,"--------------------------------------------------------===//. We can definitely do a better job on BB placements to eliminate some branches.; It's very common to see llvm generated assembly code that looks like this:. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single imm",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8833,Performance,load,load,8833,":. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:8871,Performance,load,load,8871,":. LBB3:; ...; LBB4:; ...; beq LBB3; b LBB2. If BB4 is the only predecessor of BB3, then we can emit BB3 after BB4. We can; then eliminate beq and turn the unconditional branch to LBB2 to a bne. See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9063,Performance,load,loads,9063," See McCat/18-imp/ComputeBoundingBoxes for an example. //===---------------------------------------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9163,Performance,load,load,9163,"--------------------------------------===//. Pre-/post- indexed load / stores:. 1) We should not make the pre/post- indexed load/store transform if the base ptr; is guaranteed to be live beyond the load/store. This can happen if the base; ptr is live out of the block we are performing the optimization. e.g. mov r1, r2; ldr r3, [r1], #4; ... vs. ldr r3, [r2]; add r1, r2, #4; ... In most cases, this is just a wasted optimization. However, sometimes it can; negatively impact the performance because two-address code is more restrictive; when it comes to scheduling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768))? (LineLen) : (32768))); Line[i++] = Stack[--StackPtr];; if (LineLen > 32768); {; while (S",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:9709,Performance,load,load,9709,"duling. Unfortunately, liveout information is currently unavailable during DAG combine; time. 2) Consider spliting a indexed load / store into a pair of add/sub + load/store; to solve #1 (in TwoAddressInstructionPass.cpp). 3) Enhance LSR to generate more opportunities for indexed ops. 4) Once we added support for multiple result patterns, write indexed loads; patterns instead of C++ instruction selection code. 5) Use VLDM / VSTM to emulate indexed FP load / store. //===---------------------------------------------------------------------===//. Implement support for some more tricky ways to materialize immediates. For; example, to get 0xffff8000, we can use:. mov r9, #&3f8000; sub r9, r9, #&400000. //===---------------------------------------------------------------------===//. We sometimes generate multiple add / sub instructions to update sp in prologue; and epilogue if the inc / dec value is too large to fit in a single immediate; operand. In some cases, perhaps it might be better to load the value from a; constantpool instead. //===---------------------------------------------------------------------===//. GCC generates significantly better code for this function. int foo(int StackPtr, unsigned char *Line, unsigned char *Stack, int LineLen) {; int i = 0;. if (StackPtr != 0) {; while (StackPtr != 0 && i < (((LineLen) < (32768))? (LineLen) : (32768))); Line[i++] = Stack[--StackPtr];; if (LineLen > 32768); {; while (StackPtr != 0 && i < LineLen); {; i++;; --StackPtr;; }; }; }; return StackPtr;; }. //===---------------------------------------------------------------------===//. This should compile to the mlas instruction:; int mlas(int x, int y, int z) { return ((x * y + z) < 0) ? 7 : 13; }. //===---------------------------------------------------------------------===//. At some point, we should triage these to see if they still apply to us:. http://gcc.gnu.org/bugzilla/show_bug.cgi?id=19598; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=18560; http://gcc.gnu.org/bugzi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:12524,Performance,load,load,12524,"teseer.ist.psu.edu/debus04linktime.html. //===---------------------------------------------------------------------===//. gcc generates smaller code for this function at -O2 or -Os:. void foo(signed char* p) {; if (*p == 3); bar();; else if (*p == 4); baz();; else if (*p == 5); quux();; }. llvm decides it's a good idea to turn the repeated if...else into a; binary tree, as if it were a switch; the resulting code requires -1; compare-and-branches when *p<=2 or *p==5, the same number if *p==4; or *p>6, and +1 if *p==3. So it should be a speed win; (on balance). However, the revised code is larger, with 4 conditional; branches instead of 3. More seriously, there is a byte->word extend before; each comparison, where there should be only one, and the condition codes; are not remembered when the same two values are compared twice. //===---------------------------------------------------------------------===//. More LSR enhancements possible:. 1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged; in a load / store.; 2. Allow iv reuse even when a type conversion is required. For example, i8; and i32 load / store addressing modes are identical. //===---------------------------------------------------------------------===//. This:. int foo(int a, int b, int c, int d) {; long long acc = (long long)a * (long long)b;; acc += (long long)c * (long long)d;; return (int)(acc >> 32);; }. Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies; two signed 32-bit values to produce a 64-bit value, and accumulates this with; a 64-bit value. We currently get this with both v4 and v6:. _foo:; smull r1, r0, r1, r0; smull r3, r2, r3, r2; adds r3, r3, r1; adc r0, r2, r0; bx lr. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:12623,Performance,load,load,12623," code for this function at -O2 or -Os:. void foo(signed char* p) {; if (*p == 3); bar();; else if (*p == 4); baz();; else if (*p == 5); quux();; }. llvm decides it's a good idea to turn the repeated if...else into a; binary tree, as if it were a switch; the resulting code requires -1; compare-and-branches when *p<=2 or *p==5, the same number if *p==4; or *p>6, and +1 if *p==3. So it should be a speed win; (on balance). However, the revised code is larger, with 4 conditional; branches instead of 3. More seriously, there is a byte->word extend before; each comparison, where there should be only one, and the condition codes; are not remembered when the same two values are compared twice. //===---------------------------------------------------------------------===//. More LSR enhancements possible:. 1. Teach LSR about pre- and post- indexed ops to allow iv increment be merged; in a load / store.; 2. Allow iv reuse even when a type conversion is required. For example, i8; and i32 load / store addressing modes are identical. //===---------------------------------------------------------------------===//. This:. int foo(int a, int b, int c, int d) {; long long acc = (long long)a * (long long)b;; acc += (long long)c * (long long)d;; return (int)(acc >> 32);; }. Should compile to use SMLAL (Signed Multiply Accumulate Long) which multiplies; two signed 32-bit values to produce a 64-bit value, and accumulates this with; a 64-bit value. We currently get this with both v4 and v6:. _foo:; smull r1, r0, r1, r0; smull r3, r2, r3, r2; adds r3, r3, r1; adc r0, r2, r0; bx lr. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:. _Z8full_addjj:; 	adds	r2, r1, r2; 	movcc	r1, #0; 	movcs	r1, #1; 	str	r2, [r0, #0]; 	strb	r1, [r0, #4]",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:17298,Performance,load,load,17298,"r1, r1, lsr #2; 	ldr r0, LCPI1_1; 	and r0, r1, r0; 	bx lr. it saves an instruction and a register. //===---------------------------------------------------------------------===//. It might be profitable to cse MOVi16 if there are lots of 32-bit immediates; with the same bottom half. //===---------------------------------------------------------------------===//. Robert Muth started working on an alternate jump table implementation that; does not put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:18198,Performance,load,loaded,18198,"==//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19410,Performance,load,loading,19410," r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. W",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19910,Performance,optimiz,optimized,19910,"-------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM whe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:19313,Safety,redund,redundant,19313,"--------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:4101,Testability,benchmark,benchmark,4101,"or register allocator enhancement though. 2) Can we recognize the relative position of constantpool entries? i.e. Treat. 	ldr r0, LCPI17_3; 	ldr r1, LCPI17_4; 	ldr r2, LCPI17_5. as; 	ldr r0, LCPI17; 	ldr r1, LCPI17+4; 	ldr r2, LCPI17+8. Then the ldr's can be combined into a single ldm. See Olden/power. Note for ARM v4 gcc uses ldmia to load a pair of 32-bit values to represent a; double 64-bit FP constant:. 	adr	r0, L6; 	ldmia	r0, {r0-r1}. 	.align 2; L6:; 	.long	-858993459; 	.long	1074318540. 3) struct copies appear to be done field by field; instead of by words, at least sometimes:. struct foo { int x; short s; char c1; char c2; };; void cpy(struct foo*a, struct foo*b) { *a = *b; }. llvm code (-O2); ldrb r3, [r1, #+6]; ldr r2, [r1]; ldrb r12, [r1, #+7]; ldrh r1, [r1, #+4]; str r2, [r0]; strh r1, [r0, #+4]; strb r3, [r0, #+6]; strb r12, [r0, #+7]; gcc code (-O2); ldmia r1, {r1-r2}; stmia r0, {r1-r2}. In this benchmark poor handling of aggregate copies has shown up as; having a large effect on size, and possibly speed as well (we don't have; a good way to measure on ARM). //===---------------------------------------------------------------------===//. * Consider this silly example:. double bar(double x) {; double r = foo(3.1);; return x+r;; }. _bar:; stmfd sp!, {r4, r5, r7, lr}; add r7, sp, #8; mov r4, r0; mov r5, r1; fldd d0, LCPI1_0; fmrrd r0, r1, d0; bl _foo; fmdrr d0, r4, r5; fmsr s2, r0; fsitod d1, s2; faddd d0, d1, d0; fmrrd r0, r1, d0; ldmfd sp!, {r4, r5, r7, pc}. Ignore the prologue and epilogue stuff for a second. Note; 	mov r4, r0; 	mov r5, r1; the copys to callee-save registers and the fact they are only being used by the; fmdrr instruction. It would have been better had the fmdrr been scheduled; before the call and place the result in a callee-save DPR register. The two; mov ops would not have been necessary. //===---------------------------------------------------------------------===//. Calling convention related stuff:. * gcc's parameter passing implem",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:5688,Testability,stub,stub,5688,"s2; faddd d0, d1, d0; fmrrd r0, r1, d0; ldmfd sp!, {r4, r5, r7, pc}. Ignore the prologue and epilogue stuff for a second. Note; 	mov r4, r0; 	mov r5, r1; the copys to callee-save registers and the fact they are only being used by the; fmdrr instruction. It would have been better had the fmdrr been scheduled; before the call and place the result in a callee-save DPR register. The two; mov ops would not have been necessary. //===---------------------------------------------------------------------===//. Calling convention related stuff:. * gcc's parameter passing implementation is terrible and we suffer as a result:. e.g.; struct s {; double d1;; int s1;; };. void foo(struct s S) {; printf(""%g, %d\n"", S.d1, S.s1);; }. 'S' is passed via registers r0, r1, r2. But gcc stores them to the stack, and; then reload them to r1, r2, and r3 before issuing the call (r0 contains the; address of the format string):. 	stmfd	sp!, {r7, lr}; 	add	r7, sp, #0; 	sub	sp, sp, #12; 	stmia	sp, {r0, r1, r2}; 	ldmia	sp, {r1-r2}; 	ldr	r0, L5; 	ldr	r3, [sp, #8]; L2:; 	add	r0, pc, r0; 	bl	L_printf$stub. Instead of a stmia, ldmia, and a ldr, wouldn't it be better to do three moves?. * Return an aggregate type is even worse:. e.g.; struct s foo(void) {; struct s S = {1.1, 2};; return S;; }. 	mov	ip, r0; 	ldr	r0, L5; 	sub	sp, sp, #12; L2:; 	add	r0, pc, r0; 	@ lr needed for prologue; 	ldmia	r0, {r0, r1, r2}; 	stmia	sp, {r0, r1, r2}; 	stmia	ip, {r0, r1, r2}; 	mov	r0, ip; 	add	sp, sp, #12; 	bx	lr. r0 (and later ip) is the hidden parameter from caller to store the value in. The; first ldmia loads the constants into r0, r1, r2. The last stmia stores r0, r1,; r2 into the address passed in. However, there is one additional stmia that; stores r0, r1, and r2 to some stack location. The store is dead. The llvm-gcc generated code looks like this:. csretcc void %foo(%struct.s* %agg.result) {; entry:; 	%S = alloca %struct.s, align 4		; <%struct.s*> [#uses=1]; 	%memtmp = alloca %struct.s		; <%struct.s*> [#uses=1]; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:17203,Testability,test,test,17203,"	ldr r2, [r2]; 	ldr r1, [r2, +r1, lsl #2]; 	tst r0, #128; 	movne r1, r1, lsr #2; 	ldr r0, LCPI1_1; 	and r0, r1, r0; 	bx lr. it saves an instruction and a register. //===---------------------------------------------------------------------===//. It might be profitable to cse MOVi16 if there are lots of 32-bit immediates; with the same bottom half. //===---------------------------------------------------------------------===//. Robert Muth started working on an alternate jump table implementation that; does not put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:17586,Testability,log,logic,17586,"--------------------------------------------------------------===//. Robert Muth started working on an alternate jump table implementation that; does not put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:17788,Testability,test,testcase,17788,"t put the tables in-line in the text. This is more like the llvm; default jump table implementation. This might be useful sometime. Several; revisions of patches are on the mailing list, beginning at:; http://lists.llvm.org/pipermail/llvm-dev/2009-June/022763.html. //===---------------------------------------------------------------------===//. Make use of the ""rbit"" instruction. //===---------------------------------------------------------------------===//. Take a look at test/CodeGen/Thumb2/machine-licm.ll. ARM should be taught how; to licm and cse the unnecessary load from cp#1. //===---------------------------------------------------------------------===//. The CMN instruction sets the flags like an ADD instruction, while CMP sets; them like a subtract. Therefore to be able to use CMN for comparisons other; than the Z bit, we'll need additional logic to reverse the conditionals; associated with the comparison. Perhaps a pseudo-instruction for the comparison,; with a post-codegen pass to clean up and handle the condition codes?; See PR5694 for testcase. //===---------------------------------------------------------------------===//. Given the following on armv5:; int test1(int A, int B) {; return (A&-8388481)|(B&8388480);; }. We currently generate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:20407,Testability,test,test,20407,"vt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===-----------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:20424,Testability,test,test,20424,"vt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; movw r0, #32796 @ <= this MOVW is not needed, value is there already; movt r0, #20482; str r1, [r0]. //===---------------------------------------------------------------------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===-----------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:21505,Testability,test,test,21505,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:21622,Testability,test,tests,21622,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:22016,Testability,test,test,22016,"----------===//. Improve codegen for select's:; if (x != 0) x = 1; if (x == 1) x = 1. ARM codegen used to look like this:; mov r1, r0; cmp r1, #1; mov r0, #0; moveq r0, #1. The naive lowering select between two different values. It should recognize the; test is equality test so it's more a conditional move rather than a select:; cmp r0, #1; movne r0, #0. Currently this is a ARM specific dag combine. We probably should make it into a; target-neutral one. //===---------------------------------------------------------------------===//. Optimize unnecessary checks for zero with __builtin_clz/ctz. Those builtins; are specified to be undefined at zero, so portable code must check for zero; and handle it as a special case. That is unnecessary on ARM where those; operations are implemented in a way that is well-defined for zero. For; example:. int f(int x) { return x ? __builtin_clz(x) : sizeof(int)*8; }. should just be implemented with a CLZ instruction. Since there are other; targets, e.g., PPC, that share this behavior, it would be best to implement; this in a target-independent way: we should probably fold that (when using; ""undefined at zero"" semantics) to set the ""defined at zero"" bit and have; the code generator expand out the right code. //===---------------------------------------------------------------------===//. Clean up the test/MC/ARM files to have more robust register choices. R0 should not be used as a register operand in the assembler tests as it's then; not possible to distinguish between a correct encoding and a missing operand; encoding, as zero is the default value for the binary encoder.; e.g.,; add r0, r0 // bad; add r3, r5 // good. Register operands should be distinct. That is, when the encoding does not; require two syntactical operands to refer to the same register, two different; registers should be used in the test so as to catch errors where the; operands are swapped in the encoding.; e.g.,; subs.w r1, r1, r1 // bad; subs.w r1, r2, r3 // good. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:1933,Usability,simpl,simply,1933,"gn or zero extended. If spilled, we should be able; to spill these to a 8-bit or 16-bit stack slot, zero or sign extending as part; of the reload. Doing this reduces the size of the stack frame (important for thumb etc), and; also increases the likelihood that we will be able to reload multiple values; from the stack with a single load. //===---------------------------------------------------------------------===//. The constant island pass is in good shape. Some cleanups might be desirable,; but there is unlikely to be much improvement in the generated code. 1. There may be some advantage to trying to be smarter about the initial; placement, rather than putting everything at the end. 2. There might be some compile-time efficiency to be had by representing; consecutive islands as a single block rather than multiple blocks. 3. Use a priority queue to sort constant pool users in inverse order of; position so we always process the one closed to the end of functions; first. This may simply CreateNewWater. //===---------------------------------------------------------------------===//. Eliminate copysign custom expansion. We are still generating crappy code with; default expansion + if-conversion. //===---------------------------------------------------------------------===//. Eliminate one instruction from:. define i32 @_Z6slow4bii(i32 %x, i32 %y) {; %tmp = icmp sgt i32 %x, %y; %retval = select i1 %tmp, i32 %x, i32 %y; ret i32 %retval; }. __Z6slow4bii:; cmp r0, r1; movgt r1, r0; mov r0, r1; bx lr; =>. __Z6slow4bii:; cmp r0, r1; movle r0, r1; bx lr. //===---------------------------------------------------------------------===//. Implement long long ""X-3"" with instructions that fold the immediate in. These; were disabled due to badness with the ARM carry flag on subtracts. //===---------------------------------------------------------------------===//. More load / store optimizations:; 1) Better representation for block transfer? This is from Olden/power:. 	fldd d0, [r4];",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt:18993,Usability,simpl,simple,18993,"enerate:; 	ldr	r2, .LCPI0_0; 	and	r0, r0, r2; 	ldr	r2, .LCPI0_1; 	and	r1, r1, r2; 	orr	r0, r1, r0; 	bx	lr. We should be able to replace the second ldr+and with a bic (i.e. reuse the; constant which was already loaded). Not sure what's necessary to do that. //===---------------------------------------------------------------------===//. The code generated for bswap on armv4/5 (CPUs without rev) is less than ideal:. int a(int x) { return __builtin_bswap32(x); }. a:; 	mov	r1, #255, 24; 	mov	r2, #255, 16; 	and	r1, r1, r0, lsr #8; 	and	r2, r2, r0, lsl #8; 	orr	r1, r1, r0, lsr #24; 	orr	r0, r2, r0, lsl #24; 	orr	r0, r0, r1; 	bx	lr. Something like the following would be better (fewer instructions/registers):; 	eor r1, r0, r0, ror #16; 	bic r1, r1, #0xff0000; 	mov r1, r1, lsr #8; 	eor r0, r1, r0, ror #8; 	bx	lr. A custom Thumb version would also be a slight improvement over the generic; version. //===---------------------------------------------------------------------===//. Consider the following simple C code:. void foo(unsigned char *a, unsigned char *b, int *c) {; if ((*a | *b) == 0) *c = 0;; }. currently llvm-gcc generates something like this (nice branchless code I'd say):. ldrb r0, [r0]; ldrb r1, [r1]; orr r0, r1, r0; tst r0, #255; moveq r0, #0; streq r0, [r2]; bx lr. Note that both ""tst"" and ""moveq"" are redundant. //===---------------------------------------------------------------------===//. When loading immediate constants with movt/movw, if there are multiple; constants needed with the same low 16 bits, and those values are not live at; the same time, it would be possible to use a single movw instruction, followed; by multiple movt instructions to rewrite the high bits to different values.; For example:. volatile store i32 -1, i32* inttoptr (i32 1342210076 to i32*), align 4,; !tbaa; !0; volatile store i32 -1, i32* inttoptr (i32 1342341148 to i32*), align 4,; !tbaa; !0. is compiled and optimized to:. movw r0, #32796; mov.w r1, #-1; movt r0, #20480; str r1, [r0]; m",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/ARM/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt:2368,Availability,mask,masks,2368,"MipsISD node. ilvl.d, pckev.d:; It is not possible to emit ilvl.d, or pckev.d since ilvev.d covers the; same shuffle. ilvev.d will be emitted instead. ilvr.d, ilvod.d, pckod.d:; It is not possible to emit ilvr.d, or pckod.d since ilvod.d covers the; same shuffle. ilvod.d will be emitted instead. splat.[bhwd]; The intrinsic will work as expected. However, unlike other intrinsics; it lowers directly to MipsISD::VSHF instead of using common IR. splati.w:; It is not possible to emit splati.w since shf.w covers the same cases.; shf.w will be emitted instead. copy_s.w:; On MIPS32, the copy_u.d intrinsic will emit this instruction instead of; copy_u.w. This is semantically equivalent since the general-purpose; register file is 32-bits wide. binsri.[bhwd], binsli.[bhwd]:; These two operations are equivalent to each other with the operands; swapped and condition inverted. The compiler may use either one as; appropriate.; Furthermore, the compiler may use bsel.[bhwd] for some masks that do; not survive the legalization process (this is a bug and will be fixed). bmnz.v, bmz.v, bsel.v:; These three operations differ only in the operand that is tied to the; result and the order of the operands.; It is (currently) not possible to emit bmz.v, or bsel.v since bmnz.v is; the same operation and will be emitted instead.; In future, the compiler may choose between these three instructions; according to register allocation.; These three operations can be very confusing so here is a mapping; between the instructions and the vselect node in one place:; bmz.v wd, ws, wt/i8 -> (vselect wt/i8, wd, ws); bmnz.v wd, ws, wt/i8 -> (vselect wt/i8, ws, wd); bsel.v wd, ws, wt/i8 -> (vselect wd, wt/i8, ws). bmnzi.b, bmzi.b:; Like their non-immediate counterparts, bmnzi.v and bmzi.v are the same; operation with the operands swapped. bmnzi.v will (currently) be emitted; for both cases. bseli.v:; Unlike the non-immediate versions, bseli.v is distinguishable from; bmnzi.b and bmzi.b and can be emitted.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt:156,Energy Efficiency,reduce,reduce,156,"Code Generation Notes for MSA; =============================. Intrinsics are lowered to SelectionDAG nodes where possible in order to enable; optimisation, reduce the size of the ISel matcher, and reduce repetition in; the implementation. In a small number of cases, this can cause different; (semantically equivalent) instructions to be used in place of the requested; instruction, even when no optimisation has taken place. Instructions; ============. This section describes any quirks of instruction selection for MSA. For; example, two instructions might be equally valid for some given IR and one is; chosen in preference to the other. bclri.b:; It is not possible to emit bclri.b since andi.b covers exactly the; same cases. andi.b should use fractionally less power than bclri.b in; most hardware implementations so it is used in preference to bclri.b. vshf.w:; It is not possible to emit vshf.w when the shuffle description is; constant since shf.w covers exactly the same cases. shf.w is used; instead. It is also impossible for the shuffle description to be; unknown at compile-time due to the definition of shufflevector in; LLVM IR. vshf.[bhwd]; When the shuffle description describes a splat operation, splat.[bhwd]; instructions will be selected instead of vshf.[bhwd]. Unlike the ilv*,; and pck* instructions, this is matched from MipsISD::VSHF instead of; a special-case MipsISD node. ilvl.d, pckev.d:; It is not possible to emit ilvl.d, or pckev.d since ilvev.d covers the; same shuffle. ilvev.d will be emitted instead. ilvr.d, ilvod.d, pckod.d:; It is not possible to emit ilvr.d, or pckod.d since ilvod.d covers the; same shuffle. ilvod.d will be emitted instead. splat.[bhwd]; The intrinsic will work as expected. However, unlike other intrinsics; it lowers directly to MipsISD::VSHF instead of using common IR. splati.w:; It is not possible to emit splati.w since shf.w covers the same cases.; shf.w will be emitted instead. copy_s.w:; On MIPS32, the copy_u.d intrinsic will emit",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt:197,Energy Efficiency,reduce,reduce,197,"Code Generation Notes for MSA; =============================. Intrinsics are lowered to SelectionDAG nodes where possible in order to enable; optimisation, reduce the size of the ISel matcher, and reduce repetition in; the implementation. In a small number of cases, this can cause different; (semantically equivalent) instructions to be used in place of the requested; instruction, even when no optimisation has taken place. Instructions; ============. This section describes any quirks of instruction selection for MSA. For; example, two instructions might be equally valid for some given IR and one is; chosen in preference to the other. bclri.b:; It is not possible to emit bclri.b since andi.b covers exactly the; same cases. andi.b should use fractionally less power than bclri.b in; most hardware implementations so it is used in preference to bclri.b. vshf.w:; It is not possible to emit vshf.w when the shuffle description is; constant since shf.w covers exactly the same cases. shf.w is used; instead. It is also impossible for the shuffle description to be; unknown at compile-time due to the definition of shufflevector in; LLVM IR. vshf.[bhwd]; When the shuffle description describes a splat operation, splat.[bhwd]; instructions will be selected instead of vshf.[bhwd]. Unlike the ilv*,; and pck* instructions, this is matched from MipsISD::VSHF instead of; a special-case MipsISD node. ilvl.d, pckev.d:; It is not possible to emit ilvl.d, or pckev.d since ilvev.d covers the; same shuffle. ilvev.d will be emitted instead. ilvr.d, ilvod.d, pckod.d:; It is not possible to emit ilvr.d, or pckod.d since ilvod.d covers the; same shuffle. ilvod.d will be emitted instead. splat.[bhwd]; The intrinsic will work as expected. However, unlike other intrinsics; it lowers directly to MipsISD::VSHF instead of using common IR. splati.w:; It is not possible to emit splati.w since shf.w covers the same cases.; shf.w will be emitted instead. copy_s.w:; On MIPS32, the copy_u.d intrinsic will emit",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt:767,Energy Efficiency,power,power,767,"Code Generation Notes for MSA; =============================. Intrinsics are lowered to SelectionDAG nodes where possible in order to enable; optimisation, reduce the size of the ISel matcher, and reduce repetition in; the implementation. In a small number of cases, this can cause different; (semantically equivalent) instructions to be used in place of the requested; instruction, even when no optimisation has taken place. Instructions; ============. This section describes any quirks of instruction selection for MSA. For; example, two instructions might be equally valid for some given IR and one is; chosen in preference to the other. bclri.b:; It is not possible to emit bclri.b since andi.b covers exactly the; same cases. andi.b should use fractionally less power than bclri.b in; most hardware implementations so it is used in preference to bclri.b. vshf.w:; It is not possible to emit vshf.w when the shuffle description is; constant since shf.w covers exactly the same cases. shf.w is used; instead. It is also impossible for the shuffle description to be; unknown at compile-time due to the definition of shufflevector in; LLVM IR. vshf.[bhwd]; When the shuffle description describes a splat operation, splat.[bhwd]; instructions will be selected instead of vshf.[bhwd]. Unlike the ilv*,; and pck* instructions, this is matched from MipsISD::VSHF instead of; a special-case MipsISD node. ilvl.d, pckev.d:; It is not possible to emit ilvl.d, or pckev.d since ilvev.d covers the; same shuffle. ilvev.d will be emitted instead. ilvr.d, ilvod.d, pckod.d:; It is not possible to emit ilvr.d, or pckod.d since ilvod.d covers the; same shuffle. ilvod.d will be emitted instead. splat.[bhwd]; The intrinsic will work as expected. However, unlike other intrinsics; it lowers directly to MipsISD::VSHF instead of using common IR. splati.w:; It is not possible to emit splati.w since shf.w covers the same cases.; shf.w will be emitted instead. copy_s.w:; On MIPS32, the copy_u.d intrinsic will emit",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MSA.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:416,Integrability,depend,dependent,416,"MIPS Relocation Principles. In LLVM, there are several elements of the llvm::ISD::NodeType enum; that deal with addresses and/or relocations. These are defined in; include/llvm/Target/TargetSelectionDAG.td, namely:; GlobalAddress, GlobalTLSAddress, JumpTable, ConstantPool,; ExternalSymbol, BlockAddress; The MIPS backend uses several principles to handle these. 1. Code for lowering addresses references to machine dependent code is; factored into common code for generating different address forms and; is called by the relocation model specific lowering function, using; templated functions. For example:. // lib/Target/Mips/MipsISelLowering.cpp; SDValue MipsTargetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and r",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1234,Integrability,depend,depending,1234,"rgetSelectionDAG.td, namely:; GlobalAddress, GlobalTLSAddress, JumpTable, ConstantPool,; ExternalSymbol, BlockAddress; The MIPS backend uses several principles to handle these. 1. Code for lowering addresses references to machine dependent code is; factored into common code for generating different address forms and; is called by the relocation model specific lowering function, using; templated functions. For example:. // lib/Target/Mips/MipsISelLowering.cpp; SDValue MipsTargetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1536,Integrability,wrap,wrapper,1536," lowering function, using; templated functions. For example:. // lib/Target/Mips/MipsISelLowering.cpp; SDValue MipsTargetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1649,Integrability,wrap,wrapper,1649,"argetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoR",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1801,Integrability,depend,dependent,1801,"SelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1904,Modifiability,parameteriz,parameterized,1904,"the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3386,Modifiability,extend,extends,3386,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3865,Modifiability,extend,extended,3865,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1530,Performance,load,load,1530," lowering function, using; templated functions. For example:. // lib/Target/Mips/MipsISelLowering.cpp; SDValue MipsTargetLowering::; lowerJumpTable(SDValue Op, SelectionDAG &DAG) const. calls. template <class NodeTy> // lib/Target/Mips/MipsISelLowering.h; SDValue getAddrLocal(NodeTy *N, const SDLoc &DL, EVT Ty,; SelectionDAG &DAG, bool IsN32OrN64) const. which calls the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:1927,Performance,load,load,1927,"the overloaded function:. // lib/Target/Mips/MipsISelLowering.h; SDValue getTargetNode(JumpTableSDNode *N, EVT Ty, SelectionDAG &DAG,; unsigned Flag) const;. 2. Generic address nodes are lowered to some combination of target; independent and machine specific SDNodes (for example:; MipsISD::{Highest, Higher, Hi, Lo}) depending upon relocation model,; ABI, and compilation options. The choice of specific instructions that are to be used is delegated; to ISel which in turn relies on TableGen patterns to choose subtarget; specific instructions. For example, in getAddrLocal, the pseudo-code; generated is:. (add (load (wrapper $gp, %got(sym)), %lo(sym)). where ""%lo"" represents an instance of an SDNode with opcode; ""MipsISD::Lo"", ""wrapper"" indicates one with opcode ""MipsISD::Wrapper"",; and ""%got"" the global table pointer ""getGlobalReg(...)"". The ""add"" is; ""ISD::ADD"", not a target dependent one. 3. A TableGen multiclass pattern ""MipsHiLoRelocs"" is used to define a; template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3293,Performance,load,loads,3293,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3343,Performance,load,loads,3343,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3764,Performance,load,load,3764,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt:3809,Usability,clear,clear,3809,"template pattern parameterized over the load upper immediate; instruction, add operation, the zero register, and register class.; Here the instantiation of MipsHiLoRelocs in MipsInstrInfo.td is used; to MIPS32 to compute addresses for the static relocation model. // lib/Target/Mips/MipsInstrInfo.td; multiclass MipsHiLoRelocs<Instruction Lui, Instruction Addiu,; Register ZeroReg, RegisterOperand GPROpnd> {; def : MipsPat<(MipsHi tglobaladdr:$in), (Lui tglobaladdr:$in)>;; ...; def : MipsPat<(MipsLo tglobaladdr:$in), (Addiu ZeroReg, tglobaladdr:$in)>;; ...; def : MipsPat<(add GPROpnd:$hi, (MipsLo tglobaladdr:$lo)),; (Addiu GPROpnd:$hi, tglobaladdr:$lo)>;; ...; }; defm : MipsHiLoRelocs<LUi, ADDiu, ZERO, GPR32Opnd>;. // lib/Target/Mips/Mips64InstrInfo.td; defm : MipsHiLoRelocs<LUi64, DADDiu, ZERO_64, GPR64Opnd>, SYM_32;. The instantiation in Mips64InstrInfo.td is used for MIPS64 in ILP32; mode, as guarded by the predicate ""SYM_32"" and also for a submode of; LP64 where symbols are assumed to be 32 bits wide. More details on how multiclasses in TableGen work can be found in the; section ""Multiclass definitions and instances"" in the document; ""TableGen Language Introduction"". 4. Instruction definitions are multiply defined to cover the different; register classes. In some cases, such as LW/LW64, this also accounts; for the difference in the results of instruction execution. On MIPS32,; ""lw"" loads a 32 bit value from memory. On MIPS64, ""lw"" loads a 32 bit; value from memory and sign extends the value to 64 bits. // lib/Target/Mips/MipsInstrInfo.td; def LUi : MMRel, LoadUpper<""lui"", GPR32Opnd, uimm16_relaxed>, LUI_FM;; // lib/Target/Mips/Mips64InstrInfo.td; def LUi64 : LoadUpper<""lui"", GPR64Opnd, uimm16_64_relaxed>, LUI_FM;. defines two names ""LUi"" and ""LUi64"" with two different register; classes, but with the same encoding---""LUI_FM"". These instructions load a; 16-bit immediate into bits 31-16 and clear the lower 15 bits. On MIPS64,; the result is sign-extended to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/Relocation.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/README.txt:493,Availability,avail,available,493,"//===---------------------------------------------------------------------===//; // MSP430 backend.; //===---------------------------------------------------------------------===//. DISCLAIMER: This backend should be considered as highly experimental. I never; seen nor worked with this MCU, all information was gathered from datasheet; only. The original intention of making this backend was to write documentation; of form ""How to write backend for dummies"" :) Thes notes hopefully will be; available pretty soon. Some things are incomplete / not implemented yet (this list surely is not; complete as well):. 1. Verify, how stuff is handling implicit zext with 8 bit operands (this might; be modelled currently in improper way - should we need to mark the superreg as; def for every 8 bit instruction?). 2. Libcalls: multiplication, division, remainder. Note, that calling convention; for libcalls is incomptible with calling convention of libcalls of msp430-gcc; (these cannot be used though due to license restriction). 3. Implement multiplication / division by constant (dag combiner hook?). 4. Implement non-constant shifts. 5. Implement varargs stuff. 6. Verify and fix (if needed) how's stuff playing with i32 / i64. 7. Implement floating point stuff (softfp?). 8. Implement instruction encoding for (possible) direct code emission in the; future. 9. Since almost all instructions set flags - implement brcond / select in better; way (currently they emit explicit comparison). 10. Handle imm in comparisons in better way (see comment in MSP430InstrInfo.td). 11. Implement hooks for better memory op folding, etc. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/MSP430/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5762,Availability,mask,masked,5762,"-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo_0: ; entry; mflr r11; *** stw r11, 8(r1); bl ""L00000$pb""; ""L00000$pb"":; mflr r2; addis r2, r2, ha16(.CPI_foo_0-""L00000$pb""); lfs f0, lo16(.CPI_foo_0-""L00000$pb"")(r2); fadds f1, f1, f0; *** lwz r11, 8(r1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5847,Availability,mask,masked,5847,"int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo_0: ; entry; mflr r11; *** stw r11, 8(r1); bl ""L00000$pb""; ""L00000$pb"":; mflr r2; addis r2, r2, ha16(.CPI_foo_0-""L00000$pb""); lfs f0, lo16(.CPI_foo_0-""L00000$pb"")(r2); fadds f1, f1, f0; *** lwz r11, 8(r1); mtlr r11; blr. This is functional, but there is no reason to spill the LR register all the way; to the stack (the two marked in",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:11187,Availability,recover,recover,11187,",1; 	blr; ; which is more efficient and can use mfocr. See PR642 for some more context. //===---------------------------------------------------------------------===//. void foo(float *data, float d) {; long i;; for (i = 0; i < 8000; i++); data[i] = d;; }; void foo2(float *data, float d) {; long i;; data--;; for (i = 0; i < 8000; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB2_1	; bb; 	blr . The 'mr' could be eliminated to folding the add into the cmp better. //===---------------------------------------------------------------------===//; Codegen for the following (low-probability) case deteriorated considerably ; when the correctness fixes for unordered comparisons went in (PR 642, 58871).; It should be possible to recover the code quality described in the comments. ; RUN: llvm-as < %s | llc -march=ppc32 | grep or | count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to G",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2005,Deployability,patch,patches,2005,"nz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; v",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2779,Deployability,patch,patches,2779,"a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1; bgt cr0, LBB2_2 ; entry; LBB2_1: ; entry; mr r2, r3; LBB2_2: ; entry; stw r2, 0(r5); blr. GCC produces:. _g:; subf r4,r4,r3; srawi r2,r4,31; xor r0,r2,r4; subf r0,r2,r0; stw",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2659,Energy Efficiency,reduce,reduces,2659,"s r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:4310,Energy Efficiency,reduce,reduced,4310,"mpile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1; bgt cr0, LBB2_2 ; entry; LBB2_1: ; entry; mr r2, r3; LBB2_2: ; entry; stw r2, 0(r5); blr. GCC produces:. _g:; subf r4,r4,r3; srawi r2,r4,31; xor r0,r2,r4; subf r0,r2,r0; stw r0,0(r5); blr. ... which is much nicer. This theoretically may help improve twolf slightly (used in dimbox.c:142?). ===-------------------------------------------------------------------------===. PR5945: This: ; define i32 @clamp0g(i32 %a) {; entry:; %cmp = icmp slt i32 %a, 0; %sel = select i1 %cmp, i32 0, i32 %a; ret i32 %sel; }. Is compile to this with the PowerPC (32-bit) backend:. _clamp0g:; cmpwi cr0, r3, 0; li r2, 0; blt cr0, LBB1_2; ; %bb.1: ; %entry; mr r2, r3; LBB1_2: ; %entry; mr r3, r2; blr. This could be reduced to the much simpler:. _clamp0g:; srawi r2, r3, 31; andc r3, r3, r2; blr. ===-------------------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:4720,Energy Efficiency,reduce,reduce,4720,"r4,r4,r3; srawi r2,r4,31; xor r0,r2,r4; subf r0,r2,r0; stw r0,0(r5); blr. ... which is much nicer. This theoretically may help improve twolf slightly (used in dimbox.c:142?). ===-------------------------------------------------------------------------===. PR5945: This: ; define i32 @clamp0g(i32 %a) {; entry:; %cmp = icmp slt i32 %a, 0; %sel = select i1 %cmp, i32 0, i32 %a; ret i32 %sel; }. Is compile to this with the PowerPC (32-bit) backend:. _clamp0g:; cmpwi cr0, r3, 0; li r2, 0; blt cr0, LBB1_2; ; %bb.1: ; %entry; mr r2, r3; LBB1_2: ; %entry; mr r3, r2; blr. This could be reduced to the much simpler:. _clamp0g:; srawi r2, r3, 31; andc r3, r3, r2; blr. ===-------------------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 21",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:10225,Energy Efficiency,efficient,efficient,10225,"nsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:. __Z11no_overflowjj:; add r4,r3,r4; subfc r3,r3,r4; li r3,0; adde r3,r3,r3; blr. (or better) not:. __Z11no_overflowjj:; add r2, r4, r3; cmplw cr7, r2, r3; mfcr r2; rlwinm r2, r2, 29, 31, 31; xori r3, r2, 1; blr . //===---------------------------------------------------------------------===//. We compile some FP comparisons into an mfcr with two rlwinms and an or. For; example:; #include <math.h>; int test(double x, double y) { return islessequal(x, y);}; int test2(double x, double y) { return islessgreater(x, y);}; int test3(double x, double y) { return !islessequal(x, y);}. Compiles into (all three are similar, but the bits differ):. _test:; 	fcmpu cr7, f1, f2; 	mfcr r2; 	rlwinm r3, r2, 29, 31, 31; 	rlwinm r2, r2, 31, 31, 31; 	or r3, r2, r3; 	blr . GCC compiles this into:. _test:; 	fcmpu cr7,f1,f2; 	cror 30,28,30; 	mfcr r3; 	rlwinm r3,r3,31,1; 	blr; ; which is more efficient and can use mfocr. See PR642 for some more context. //===---------------------------------------------------------------------===//. void foo(float *data, float d) {; long i;; for (i = 0; i < 8000; i++); data[i] = d;; }; void foo2(float *data, float d) {; long i;; data--;; for (i = 0; i < 8000; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB2_1	; bb; 	blr . The 'mr' could be eliminated to folding the add into the cmp better. //===---------------------------------------------------------------------===//; Codegen for the following (low-probability) case deteriorated considerably ; when the correctness fixes for unordered comparisons went in (PR 642, 58871).; It should be possible to recov",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:14856,Energy Efficiency,schedul,scheduled,14856,"---------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can happen, and I don't think saving before the SP; decrement is safe on that target, as there is no red zone. This is currently; broken AFAIK, although it's not a target I can exercise.; The following demonstrates the problem:; extern void bar(char *p);; void foo() {; char x[100000];; bar(x);; __asm__("""" ::: ""cr2"");; }. //===-------------------------------------------------------------------------===; Naming convention for instruction formats is very haphazard.; We have agreed on a naming scheme as follows:. <INST_form>{_<OP_type><OP_len>}+. Where:; INST_form is the instruction format (X-form, etc.); OP_type is the operand type - one of OPC (opcode), RD (register destination),; RS (register source),; RDp (destination register pair),; RSp (source register pair), IM (immediate),; XO (extended opcode); OP_len is the length of the operand in bits. VSX register operands would be of length 6 (split across two fields),; condition register fields of length 3.; We would not need denote reserved fields in names of instruction formats. //===----------------------------------------------------------------------===//. Instruction fusion was introduced in ISA 2.06 and more opportunities added in; ISA 2.07. LLVM needs to add infrastructure to recognize fusion opportunities; and force instruction pairs to be scheduled together. -----------------------------------------------------------------------------. More general handling of any_extend and zero_extend:. See https://reviews.llvm.org/D24924#555306; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:1933,Modifiability,variab,variable,1933,"	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-wea",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:14335,Modifiability,extend,extended,14335,"---------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can happen, and I don't think saving before the SP; decrement is safe on that target, as there is no red zone. This is currently; broken AFAIK, although it's not a target I can exercise.; The following demonstrates the problem:; extern void bar(char *p);; void foo() {; char x[100000];; bar(x);; __asm__("""" ::: ""cr2"");; }. //===-------------------------------------------------------------------------===; Naming convention for instruction formats is very haphazard.; We have agreed on a naming scheme as follows:. <INST_form>{_<OP_type><OP_len>}+. Where:; INST_form is the instruction format (X-form, etc.); OP_type is the operand type - one of OPC (opcode), RD (register destination),; RS (register source),; RDp (destination register pair),; RSp (source register pair), IM (immediate),; XO (extended opcode); OP_len is the length of the operand in bits. VSX register operands would be of length 6 (split across two fields),; condition register fields of length 3.; We would not need denote reserved fields in names of instruction formats. //===----------------------------------------------------------------------===//. Instruction fusion was introduced in ISA 2.06 and more opportunities added in; ISA 2.07. LLVM needs to add infrastructure to recognize fusion opportunities; and force instruction pairs to be scheduled together. -----------------------------------------------------------------------------. More general handling of any_extend and zero_extend:. See https://reviews.llvm.org/D24924#555306; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:114,Performance,load,load,114,"//===- README.txt - Notes for improving PowerPC-specific code gen ---------===//. TODO:; * lmw/stmw pass a la arm load store optimizer for prolog/epilog. ===-------------------------------------------------------------------------===. This code:. unsigned add32carry(unsigned sum, unsigned x) {; unsigned z = sum + x;; if (sum + x < x); z++;; return z;; }. Should compile to something like:. 	addc r3,r3,r4; 	addze r3,r3. instead we get:. 	add r3, r4, r3; 	cmplw cr7, r3, r4; 	mfcr r4 ; 1; 	rlwinm r4, r4, 29, 31, 31; 	add r3, r3, r4. Ick. ===-------------------------------------------------------------------------===. We compile the hottest inner loop of viterbi to:. li r6, 0; b LBB1_84 ;bb432.i; LBB1_83: ;bb420.i; lbzx r8, r5, r7; addi r6, r7, 1; stbx r8, r4, r7; LBB1_84: ;bb432.i; mr r7, r6; cmplwi cr0, r7, 143; bne cr0, LBB1_83 ;bb420.i. The CBE manages to produce:. 	li r0, 143; 	mtctr r0; loop:; 	lbzx r2, r2, r11; 	stbx r0, r2, r9; 	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:125,Performance,optimiz,optimizer,125,"//===- README.txt - Notes for improving PowerPC-specific code gen ---------===//. TODO:; * lmw/stmw pass a la arm load store optimizer for prolog/epilog. ===-------------------------------------------------------------------------===. This code:. unsigned add32carry(unsigned sum, unsigned x) {; unsigned z = sum + x;; if (sum + x < x); z++;; return z;; }. Should compile to something like:. 	addc r3,r3,r4; 	addze r3,r3. instead we get:. 	add r3, r4, r3; 	cmplw cr7, r3, r4; 	mfcr r4 ; 1; 	rlwinm r4, r4, 29, 31, 31; 	add r3, r3, r4. Ick. ===-------------------------------------------------------------------------===. We compile the hottest inner loop of viterbi to:. li r6, 0; b LBB1_84 ;bb432.i; LBB1_83: ;bb420.i; lbzx r8, r5, r7; addi r6, r7, 1; stbx r8, r4, r7; LBB1_84: ;bb432.i; mr r7, r6; cmplwi cr0, r7, 143; bne cr0, LBB1_83 ;bb420.i. The CBE manages to produce:. 	li r0, 143; 	mtctr r0; loop:; 	lbzx r2, r2, r11; 	stbx r0, r2, r9; 	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2500,Performance,optimiz,optimization,2500,"PI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5397,Performance,load,load,5397,"--------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5499,Performance,load,load,5499,"int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant poo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:5602,Performance,load,load,5602,"TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* %b) {; entry:; %tmp = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp = load i32* %tmp ; <uint> [#uses=1]; %tmp3 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=1]; %tmp4 = load i32* %tmp3 ; <uint> [#uses=1]; %tmp8 = bitcast %struct.B* %b to i32* ; <uint*> [#uses=2]; %tmp9 = load i32* %tmp8 ; <uint> [#uses=1]; %tmp4.mask17 = shl i32 %tmp4, i8 1 ; <uint> [#uses=1]; %tmp1415 = and i32 %tmp4.mask17, 2147483648 ; <uint> [#uses=1]; %tmp.masked = and i32 %tmp, 2147483648 ; <uint> [#uses=1]; %tmp11 = or i32 %tmp1415, %tmp.masked ; <uint> [#uses=1]; %tmp12 = and i32 %tmp9, 2147483647 ; <uint> [#uses=1]; %tmp13 = or i32 %tmp12, %tmp11 ; <uint> [#uses=1]; store i32 %tmp13, i32* %tmp8; ret void; }. We emit:. _foo:; lwz r2, 0(r3); slwi r4, r2, 1; or r4, r4, r2; rlwimi r2, r4, 0, 0, 0; stw r2, 0(r3); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:7159,Performance,optimiz,optimization,7159,"); blr. We could collapse a bunch of those ORs and ANDs and generate the following; equivalent code:. _foo:; lwz r2, 0(r3); rlwinm r4, r2, 1, 0, 0; or r2, r2, r4; stw r2, 0(r3); blr. ===-------------------------------------------------------------------------===. Consider a function like this:. float foo(float X) { return X + 1234.4123f; }. The FP constant ends up in the constant pool, so we need to get the LR register.; This ends up producing code like this:. _foo:; .LBB_foo_0: ; entry; mflr r11; *** stw r11, 8(r1); bl ""L00000$pb""; ""L00000$pb"":; mflr r2; addis r2, r2, ha16(.CPI_foo_0-""L00000$pb""); lfs f0, lo16(.CPI_foo_0-""L00000$pb"")(r2); fadds f1, f1, f0; *** lwz r11, 8(r1); mtlr r11; blr. This is functional, but there is no reason to spill the LR register all the way; to the stack (the two marked instrs): spilling it to a GPR is quite enough. Implementing this will require some codegen improvements. Nate writes:. ""So basically what we need to support the ""no stack frame save and restore"" is a; generalization of the LR optimization to ""callee-save regs"". Currently, we have LR marked as a callee-save reg. The register allocator sees; that it's callee save, and spills it directly to the stack. Ideally, something like this would happen:. LR would be in a separate register class from the GPRs. The class of LR would be; marked ""unspillable"". When the register allocator came across an unspillable; reg, it would ask ""what is the best class to copy this into that I *can* spill""; If it gets a class back, which it will in this case (the gprs), it grabs a free; register of that class. If it is then later necessary to spill that reg, so be; it. ===-------------------------------------------------------------------------===. We compile this:; int test(_Bool X) {; return X ? 524288 : 0;; }. to: ; _test:; cmplwi cr0, r3, 0; lis r2, 8; li r3, 0; beq cr0, LBB1_2 ;entry; LBB1_1: ;entry; mr r3, r2; LBB1_2: ;entry; blr . instead of:; _test:; addic r2,r3,-1; subfe r0,r2,r3; slwi r3,r0,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12185,Performance,load,load,12185,"cribed in the comments. ; RUN: llvm-as < %s | llc -march=ppc32 | grep or | count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12286,Performance,load,load,12286,"| count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:1842,Safety,avoid,avoid,1842,"143; bne cr0, LBB1_83 ;bb420.i. The CBE manages to produce:. 	li r0, 143; 	mtctr r0; loop:; 	lbzx r2, r2, r11; 	stbx r0, r2, r9; 	addi r2, r2, 1; 	bdz later; 	b loop. This could be much better (bdnz instead of bdz) but it still beats us. If we; produced this with bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===----",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2631,Safety,avoid,avoiding,2631,"s r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:11187,Safety,recover,recover,11187,",1; 	blr; ; which is more efficient and can use mfocr. See PR642 for some more context. //===---------------------------------------------------------------------===//. void foo(float *data, float d) {; long i;; for (i = 0; i < 8000; i++); data[i] = d;; }; void foo2(float *data, float d) {; long i;; data--;; for (i = 0; i < 8000; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB2_1	; bb; 	blr . The 'mr' could be eliminated to folding the add into the cmp better. //===---------------------------------------------------------------------===//; Codegen for the following (low-probability) case deteriorated considerably ; when the correctness fixes for unordered comparisons went in (PR 642, 58871).; It should be possible to recover the code quality described in the comments. ; RUN: llvm-as < %s | llc -march=ppc32 | grep or | count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to G",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12307,Safety,avoid,avoid,12307,"| count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12533,Safety,safe,safe,12533,"1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:13606,Safety,safe,safe,13606," -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can happen, and I don't think saving before the SP; decrement is safe on that target, as there is no red zone. This is currently; broken AFAIK, although it's not a target I can exercise.; The following demonstrates the problem:; extern void bar(char *p);; void foo() {; char x[100000];; bar(x);; __asm__("""" ::: ""cr2"");; }. //===-------------------------------------------------------------------------===; Naming convention for instruction formats is very haphazard.; We have agreed on a naming scheme as follows:. <INST_form>{_<OP_type><OP_len>}+. Where:; INST_form is the instruction format (X-form, etc.); OP_type is the operand type - one of OPC (opcode), RD (register destination),; RS (register source),; RDp (destination register pair),; RSp (source register pair), IM (immediate),; XO (extended opcode); OP_len is the length of the operand in bits. VSX register operands would be of length 6 (split across two fields),; condition register fields of length 3.; We would not need denote reserved fields in names of instruction formats. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2644,Security,access,accesses,2644,"s r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ha16(_a); la r2, lo16(_a)(r2); lbz r2, 3(r2); stb r2, 0(r3); blr. Becomes. _foo:; lis r2, ha16(_a+3); lbz r2, lo16(_a+3)(r2); stb r2, 0(r3); blr. ===-------------------------------------------------------------------------===. We should compile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:2087,Testability,test,testf,2087,"th bdnz, the loop would be a single dispatch group. ===-------------------------------------------------------------------------===. Lump the constant pool for each function into ONE pic object, and reference; pieces of it as offsets from the start. For functions like this (contrived; to have lots of constants obviously):. double X(double Y) { return (Y*1.23 + 4.512)*2.34 + 14.38; }. We generate:. _X:; lis r2, ha16(.CPI_X_0); lfd f0, lo16(.CPI_X_0)(r2); lis r2, ha16(.CPI_X_1); lfd f2, lo16(.CPI_X_1)(r2); fmadd f0, f1, f0, f2; lis r2, ha16(.CPI_X_2); lfd f1, lo16(.CPI_X_2)(r2); lis r2, ha16(.CPI_X_3); lfd f2, lo16(.CPI_X_3)(r2); fmadd f1, f0, f1, f2; blr. It would be better to materialize .CPI_X into a register, then use immediates; off of the register to avoid the lis's. This is even more important in PIC ; mode. Note that this (and the static variable version) is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. Here's another example (the sgn function):; double testf(double a) {; return a == 0.0 ? 0.0 : (a > 0.0 ? 1.0 : -1.0);; }. it produces a BB like this:; LBB1_1: ; cond_true; lis r2, ha16(LCPI1_0); lfs f0, lo16(LCPI1_0)(r2); lis r2, ha16(LCPI1_1); lis r3, ha16(LCPI1_2); lfs f2, lo16(LCPI1_2)(r3); lfs f3, lo16(LCPI1_1)(r2); fsub f0, f0, f1; fsel f1, f0, f2, f3; blr . ===-------------------------------------------------------------------------===. PIC Code Gen IPO optimization:. Squish small scalar globals together into a single global struct, allowing the ; address of the struct to be CSE'd, avoiding PIC accesses (also reduces the size; of the GOT on targets with one). Note that this is discussed here for GCC:; http://gcc.gnu.org/ml/gcc-patches/2006-02/msg00133.html. ===-------------------------------------------------------------------------===. Fold add and sub with constant into non-extern, non-weak addresses so this:. static int a;; void bar(int b) { a = b; }; void foo(unsigned char *c) {; *c = a;; }. So that . _foo:; lis r2, ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:7888,Testability,test,test,7888," way; to the stack (the two marked instrs): spilling it to a GPR is quite enough. Implementing this will require some codegen improvements. Nate writes:. ""So basically what we need to support the ""no stack frame save and restore"" is a; generalization of the LR optimization to ""callee-save regs"". Currently, we have LR marked as a callee-save reg. The register allocator sees; that it's callee save, and spills it directly to the stack. Ideally, something like this would happen:. LR would be in a separate register class from the GPRs. The class of LR would be; marked ""unspillable"". When the register allocator came across an unspillable; reg, it would ask ""what is the best class to copy this into that I *can* spill""; If it gets a class back, which it will in this case (the gprs), it grabs a free; register of that class. If it is then later necessary to spill that reg, so be; it. ===-------------------------------------------------------------------------===. We compile this:; int test(_Bool X) {; return X ? 524288 : 0;; }. to: ; _test:; cmplwi cr0, r3, 0; lis r2, 8; li r3, 0; beq cr0, LBB1_2 ;entry; LBB1_1: ;entry; mr r3, r2; LBB1_2: ;entry; blr . instead of:; _test:; addic r2,r3,-1; subfe r0,r2,r3; slwi r3,r0,19; blr. This sort of thing occurs a lot due to globalopt. ===-------------------------------------------------------------------------===. We compile:. define i32 @bar(i32 %x) nounwind readnone ssp {; entry:; %0 = icmp eq i32 %x, 0 ; <i1> [#uses=1]; %neg = sext i1 %0 to i32 ; <i32> [#uses=1]; ret i32 %neg; }. to:. _bar:; 	cntlzw r2, r3; 	slwi r2, r2, 26; 	srawi r3, r2, 31; 	blr . it would be better to produce:. _bar: ; addic r3,r3,-1; subfe r3,r3,r3; blr. ===-------------------------------------------------------------------------===. We generate horrible ppc code for this:. #define N 2000000; double a[N],c[N];; void simpleloop() {; int j;; for (j=0; j<N; j++); c[j] = a[j];; }. LBB1_1: ;bb; lfdx f0, r3, r4; addi r5, r5, 1 ;; Extra IV for the exit value compare.; s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:9750,Testability,test,test,9750,"3, r4; addi r5, r5, 1 ;; Extra IV for the exit value compare.; stfdx f0, r2, r4; addi r4, r4, 8. xoris r6, r5, 30 ;; This is due to a large immediate.; cmplwi cr0, r6, 33920; bne cr0, LBB1_1. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; inline std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:. __Z11no_overflowjj:; add r4,r3,r4; subfc r3,r3,r4; li r3,0; adde r3,r3,r3; blr. (or better) not:. __Z11no_overflowjj:; add r2, r4, r3; cmplw cr7, r2, r3; mfcr r2; rlwinm r2, r2, 29, 31, 31; xori r3, r2, 1; blr . //===---------------------------------------------------------------------===//. We compile some FP comparisons into an mfcr with two rlwinms and an or. For; example:; #include <math.h>; int test(double x, double y) { return islessequal(x, y);}; int test2(double x, double y) { return islessgreater(x, y);}; int test3(double x, double y) { return !islessequal(x, y);}. Compiles into (all three are similar, but the bits differ):. _test:; 	fcmpu cr7, f1, f2; 	mfcr r2; 	rlwinm r3, r2, 29, 31, 31; 	rlwinm r2, r2, 31, 31, 31; 	or r3, r2, r3; 	blr . GCC compiles this into:. _test:; 	fcmpu cr7,f1,f2; 	cror 30,28,30; 	mfcr r3; 	rlwinm r3,r3,31,1; 	blr; ; which is more efficient and can use mfocr. See PR642 for some more context. //===---------------------------------------------------------------------===//. void foo(float *data, float d) {; long i;; for (i = 0; i < 8000; i++); data[i] = d;; }; void foo2(float *data, float d) {; long i;; data--;; for (i = 0; i < 8000; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:11450,Testability,test,test,11450,"0; i++) {; data[1] = d;; data++;; }; }. These compile to:. _foo:; 	li r2, 0; LBB1_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB1_1	; bb; 	blr ; _foo2:; 	li r2, 0; LBB2_1:	; bb; 	addi r4, r2, 4; 	stfsx f1, r3, r2; 	cmplwi cr0, r4, 32000; 	mr r2, r4; 	bne cr0, LBB2_1	; bb; 	blr . The 'mr' could be eliminated to folding the add into the cmp better. //===---------------------------------------------------------------------===//; Codegen for the following (low-probability) case deteriorated considerably ; when the correctness fixes for unordered comparisons went in (PR 642, 58871).; It should be possible to recover the code quality described in the comments. ; RUN: llvm-as < %s | llc -march=ppc32 | grep or | count 3; ; This should produce one 'or' or 'cror' instruction per function. ; RUN: llvm-as < %s | llc -march=ppc32 | grep mfcr | count 3; ; PR2964. define i32 @test(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ole double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test2(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not I",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:4330,Usability,simpl,simpler,4330,"mpile these two functions to the same thing:. #include <stdlib.h>; void f(int a, int b, int *P) {; *P = (a-b)>=0?(a-b):(b-a);; }; void g(int a, int b, int *P) {; *P = abs(a-b);; }. Further, they should compile to something better than:. _g:; subf r2, r4, r3; subfic r3, r2, 0; cmpwi cr0, r2, -1; bgt cr0, LBB2_2 ; entry; LBB2_1: ; entry; mr r2, r3; LBB2_2: ; entry; stw r2, 0(r5); blr. GCC produces:. _g:; subf r4,r4,r3; srawi r2,r4,31; xor r0,r2,r4; subf r0,r2,r0; stw r0,0(r5); blr. ... which is much nicer. This theoretically may help improve twolf slightly (used in dimbox.c:142?). ===-------------------------------------------------------------------------===. PR5945: This: ; define i32 @clamp0g(i32 %a) {; entry:; %cmp = icmp slt i32 %a, 0; %sel = select i1 %cmp, i32 0, i32 %a; ret i32 %sel; }. Is compile to this with the PowerPC (32-bit) backend:. _clamp0g:; cmpwi cr0, r3, 0; li r2, 0; blt cr0, LBB1_2; ; %bb.1: ; %entry; mr r2, r3; LBB1_2: ; %entry; mr r3, r2; blr. This could be reduced to the much simpler:. _clamp0g:; srawi r2, r3, 31; andc r3, r3, r2; blr. ===-------------------------------------------------------------------------===. int foo(int N, int ***W, int **TK, int X) {; int t, i;; ; for (t = 0; t < N; ++t); for (i = 0; i < 4; ++i); W[t / X][i][t % X] = TK[i][t];; ; return 5;; }. We generate relatively atrocious code for this loop compared to gcc. We could also strength reduce the rem and the div:; http://www.lcs.mit.edu/pubs/pdf/MIT-LCS-TM-600.pdf. ===-------------------------------------------------------------------------===. We generate ugly code for this:. void func(unsigned int *ret, float dx, float dy, float dz, float dw) {; unsigned code = 0;; if(dx < -dw) code |= 1;; if(dx > dw) code |= 2;; if(dy < -dw) code |= 4;; if(dy > dw) code |= 8;; if(dz < -dw) code |= 16;; if(dz > dw) code |= 32;; *ret = code;; }. ===-------------------------------------------------------------------------===. %struct.B = type { i8, [3 x i8] }. define void @bar(%struct.B* ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:8749,Usability,simpl,simpleloop,8749," spill that reg, so be; it. ===-------------------------------------------------------------------------===. We compile this:; int test(_Bool X) {; return X ? 524288 : 0;; }. to: ; _test:; cmplwi cr0, r3, 0; lis r2, 8; li r3, 0; beq cr0, LBB1_2 ;entry; LBB1_1: ;entry; mr r3, r2; LBB1_2: ;entry; blr . instead of:; _test:; addic r2,r3,-1; subfe r0,r2,r3; slwi r3,r0,19; blr. This sort of thing occurs a lot due to globalopt. ===-------------------------------------------------------------------------===. We compile:. define i32 @bar(i32 %x) nounwind readnone ssp {; entry:; %0 = icmp eq i32 %x, 0 ; <i1> [#uses=1]; %neg = sext i1 %0 to i32 ; <i32> [#uses=1]; ret i32 %neg; }. to:. _bar:; 	cntlzw r2, r3; 	slwi r2, r2, 26; 	srawi r3, r2, 31; 	blr . it would be better to produce:. _bar: ; addic r3,r3,-1; subfe r3,r3,r3; blr. ===-------------------------------------------------------------------------===. We generate horrible ppc code for this:. #define N 2000000; double a[N],c[N];; void simpleloop() {; int j;; for (j=0; j<N; j++); c[j] = a[j];; }. LBB1_1: ;bb; lfdx f0, r3, r4; addi r5, r5, 1 ;; Extra IV for the exit value compare.; stfdx f0, r2, r4; addi r4, r4, 8. xoris r6, r5, 30 ;; This is due to a large immediate.; cmplwi cr0, r6, 33920; bne cr0, LBB1_1. //===---------------------------------------------------------------------===//. This:; #include <algorithm>; inline std::pair<unsigned, bool> full_add(unsigned a, unsigned b); { return std::make_pair(a + b, a + b < a); }; bool no_overflow(unsigned a, unsigned b); { return !full_add(a, b).second; }. Should compile to:. __Z11no_overflowjj:; add r4,r3,r4; subfc r3,r3,r4; li r3,0; adde r3,r3,r3; blr. (or better) not:. __Z11no_overflowjj:; add r2, r4, r3; cmplw cr7, r2, r3; mfcr r2; rlwinm r2, r2, 29, 31, 31; xori r3, r2, 1; blr . //===---------------------------------------------------------------------===//. We compile some FP comparisons into an mfcr with two rlwinms and an or. For; example:; #include <math.h>; int test(dou",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt:12744,Usability,simpl,simple,12744,"fcmp one double %x, %y		; <i1> [#uses=1]; 	%tmp345 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp345; }. define i32 @test3(double %x, double %y) nounwind {; entry:; 	%tmp3 = fcmp ugt double %x, %y		; <i1> [#uses=1]; 	%tmp34 = zext i1 %tmp3 to i32		; <i32> [#uses=1]; 	ret i32 %tmp34; }. //===---------------------------------------------------------------------===//; for the following code:. void foo (float *__restrict__ a, int *__restrict__ b, int n) {; a[n] = b[n] * 2.321;; }. we load b[n] to GPR, then move it VSX register and convert it float. We should ; use vsx scalar integer load instructions to avoid direct moves. //===----------------------------------------------------------------------===//; ; RUN: llvm-as < %s | llc -march=ppc32 | not grep fneg. ; This could generate FSEL with appropriate flags (FSEL is not IEEE-safe, and ; ; should not be generated except with -enable-finite-only-fp-math or the like).; ; With the correctness fixes for PR642 (58871) LowerSELECT_CC would need to; ; recognize a more elaborate tree than a simple SETxx. define double @test_FNEG_sel(double %A, double %B, double %C) {; %D = fsub double -0.000000e+00, %A ; <double> [#uses=1]; %Cond = fcmp ugt double %D, -0.000000e+00 ; <i1> [#uses=1]; %E = select i1 %Cond, double %B, double %C ; <double> [#uses=1]; ret double %E; }. //===----------------------------------------------------------------------===//; The save/restore sequence for CR in prolog/epilog is terrible:; - Each CR subreg is saved individually, rather than doing one save as a unit.; - On Darwin, the save is done after the decrement of SP, which means the offset; from SP of the save slot can be too big for a store instruction, which means we; need an additional register (currently hacked in 96015+96020; the solution there; is correct, but poor).; - On SVR4 the same thing can happen, and I don't think saving before the SP; decrement is safe on that target, as there is no red zone. This is currently; broken AFAIK, althou",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:5323,Availability,mask,mask,5323,"the same peephole; applied to it as a predicate compare used by a br_cc. There should be no; mfcr here:. _foo:; mfspr r2, 256; oris r5, r2, 12288; mtspr 256, r5; li r5, 12; li r6, 3245; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; entry; mr r3, r6; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. CodeGen/PowerPC/vec_constants.ll has an and operation that should be; codegen'd to andc. The issue is that the 'all ones' build vector is; SelectNodeTo'd a VSPLTISB instruction node before the and/xor is selected; which prevents the vnot pattern from matching. //===----------------------------------------------------------------------===//. An alternative to the store/store/load approach for illegal insert element ; lowering would be:. 1. store element to any ol' slot; 2. lvx the slot; 3. lvsl 0; splat index; vcmpeq to generate a select mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x i8>, align 16; store <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:10169,Availability,avail,available,10169,"nce for implementing a shufflevector operation on; PowerPC. However, this was designed for big-endian code generation. We could; modify this program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2613,Energy Efficiency,schedul,scheduler,2613,"------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly awefu",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2884,Energy Efficiency,schedul,scheduling,2884,"aint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly aweful:. vector float test(float a, float b) {; return (vector float){ 0.0, a, 0.0, 0.0}; ; }. LCPI1_0: ; float; .space 4; .text; .globl _test; .align 4; _test:; mfspr r2,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2945,Energy Efficiency,schedul,scheduler,2945,"re of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly aweful:. vector float test(float a, float b) {; return (vector float){ 0.0, a, 0.0, 0.0}; ; }. LCPI1_0: ; float; .space 4; .text; .globl _test; .align 4; _test:; mfspr r2, 256; oris r3, r2, 4096; mtspr 256, r3; lis r3, ha16(LCPI1_0); addi r4, r1, -32; stfs f1, -16(r1); addi r5, r1, -16; lfs f0, lo16(LCPI",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:10862,Modifiability,extend,extending,10862,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:11044,Modifiability,extend,extending,11044,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:11226,Modifiability,extend,extending,11226,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1123,Performance,load,load,1123,"vector; registers, to generate better spill code. //===----------------------------------------------------------------------===//. The first should be a single lvx from the constant pool, the second should be ; a xor/stvx:. void foo(void) {; int x[8] __attribute__((aligned(128))) = { 1, 1, 1, 17, 1, 1, 1, 1 };; bar (x);; }. #include <string.h>; void foo(void) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //=",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1174,Performance,load,load,1174,"vector; registers, to generate better spill code. //===----------------------------------------------------------------------===//. The first should be a single lvx from the constant pool, the second should be ; a xor/stvx:. void foo(void) {; int x[8] __attribute__((aligned(128))) = { 1, 1, 1, 17, 1, 1, 1, 1 };; bar (x);; }. #include <string.h>; void foo(void) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //=",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1520,Performance,load,load,1520,"id) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1546,Performance,load,load,1546,"id) {; int x[8] __attribute__((aligned(128)));; memset (x, 0, sizeof (x));; bar (x);; }. //===----------------------------------------------------------------------===//. Altivec: Codegen'ing MUL with vector FMADD should add -0.0, not 0.0:; http://gcc.gnu.org/bugzilla/show_bug.cgi?id=8763. When -ffast-math is on, we can use 0.0. //===----------------------------------------------------------------------===//. Consider this:; v4f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:1990,Performance,load,load,1990,"f32 Vector;; v4f32 Vector2 = { Vector.X, Vector.X, Vector.X, Vector.X };. Since we know that ""Vector"" is 16-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:2083,Performance,load,load,2083,"-byte aligned and we know the element offset ; of "".X"", we should change the load into a lve*x instruction, instead of doing; a load/store/lve*x sequence. //===----------------------------------------------------------------------===//. Implement passing vectors by value into calls and receiving them as arguments. //===----------------------------------------------------------------------===//. GCC apparently tries to codegen { C1, C2, Variable, C3 } as a constant pool load; of C1/C2/C3, then a load and vperm of Variable. //===----------------------------------------------------------------------===//. We need a way to teach tblgen that some operands of an intrinsic are required to; be constants. The verifier should enforce this constraint. //===----------------------------------------------------------------------===//. We currently codegen SCALAR_TO_VECTOR as a store of the scalar to a 16-byte; aligned stack slot, followed by a load/vperm. We should probably just store it; to a scalar stack slot, then use lvsl/vperm to load it. If the value is already; in memory this is a big win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:5157,Performance,load,load,5157,"or float *y) {; if (vec_all_eq(*x,*y)) return 3245; ; else return 12;; }. A predicate compare being used in a select_cc should have the same peephole; applied to it as a predicate compare used by a br_cc. There should be no; mfcr here:. _foo:; mfspr r2, 256; oris r5, r2, 12288; mtspr 256, r5; li r5, 12; li r6, 3245; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; entry; mr r3, r6; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. CodeGen/PowerPC/vec_constants.ll has an and operation that should be; codegen'd to andc. The issue is that the 'all ones' build vector is; SelectNodeTo'd a VSPLTISB instruction node before the and/xor is selected; which prevents the vnot pattern from matching. //===----------------------------------------------------------------------===//. An alternative to the store/store/load approach for illegal insert element ; lowering would be:. 1. store element to any ol' slot; 2. lvx the slot; 3. lvsl 0; splat index; vcmpeq to generate a select mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:6509,Performance,load,load,6509," mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x i8>, align 16; store <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8 16>, <16 x i8>* %a, align 16; store <16 x i8> <i8 113, i8 114, i8 115, i8 116, i8 117, i8 118, i8 119, i8 120, i8 121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16; %0 = load <16 x i8>* %a, align 16; %1 = load <16 x i8>* %b, align 16; %2 = call <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8> %0, <16 x i8> %1); ret <16 x i8> %2; }. ; Function Attrs: nounwind readnone; declare <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8>, <16 x i8>) #1. Produces the following code with -mtriple=powerpc64-unknown-linux-gnu:; # %bb.0: # %entry; addis 3, 2, .LCPI0_0@toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:6544,Performance,load,load,6544," mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x i8>, align 16; store <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8 16>, <16 x i8>* %a, align 16; store <16 x i8> <i8 113, i8 114, i8 115, i8 116, i8 117, i8 118, i8 119, i8 120, i8 121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16; %0 = load <16 x i8>* %a, align 16; %1 = load <16 x i8>* %b, align 16; %2 = call <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8> %0, <16 x i8> %1); ret <16 x i8> %2; }. ; Function Attrs: nounwind readnone; declare <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8>, <16 x i8>) #1. Produces the following code with -mtriple=powerpc64-unknown-linux-gnu:; # %bb.0: # %entry; addis 3, 2, .LCPI0_0@toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===---------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:7910,Performance,load,load-hit-store,7910,"toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===----------------------------------------------------------------------===//. The following example is found in test/CodeGen/PowerPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this is a; poor choice, since we have to load the address into an index register. This; should be fixed for P7/P8. . //===----------------------------------------------------------------------===//. Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.; However, we could actually support both kinds on either endianness, if we check; for the appropriate shufflevector pattern for each case ... this would cause; some additional shufflevectors to be recognized and ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:8453,Performance,load,load,8453,"erPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this is a; poor choice, since we have to load the address into an index register. This; should be fixed for P7/P8. . //===----------------------------------------------------------------------===//. Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.; However, we could actually support both kinds on either endianness, if we check; for the appropriate shufflevector pattern for each case ... this would cause; some additional shufflevectors to be recognized and implemented via the; ""swapped"" form. //===----------------------------------------------------------------------===//. There is a utility program called PerfectShuffle that generates a table of the; shortest instruction sequence for implementing a shufflevector operation on; PowerPC. However, this was designed for big-endian code generation. We could; modify this program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===--------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:10300,Performance,load,load-hit-store,10300,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:5542,Safety,avoid,avoid,5542,"fp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; entry; mr r3, r6; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. CodeGen/PowerPC/vec_constants.ll has an and operation that should be; codegen'd to andc. The issue is that the 'all ones' build vector is; SelectNodeTo'd a VSPLTISB instruction node before the and/xor is selected; which prevents the vnot pattern from matching. //===----------------------------------------------------------------------===//. An alternative to the store/store/load approach for illegal insert element ; lowering would be:. 1. store element to any ol' slot; 2. lvx the slot; 3. lvsl 0; splat index; vcmpeq to generate a select mask; 4. lvsl slot + x; vperm to rotate result into correct slot; 5. vsel result together. //===----------------------------------------------------------------------===//. Should codegen branches on vec_any/vec_all to avoid mfcr. Two examples:. #include <altivec.h>; int f(vector float a, vector float b); {; int aa = 0;; if (vec_all_ge(a, b)); aa |= 0x1;; if (vec_any_ge(a,b)); aa |= 0x2;; return aa;; }. vector float f(vector float a, vector float b) { ; if (vec_any_eq(a, b)) ; return a; ; else ; return b; ; }. //===----------------------------------------------------------------------===//. We should do a little better with eliminating dead stores.; The stores to the stack are dead since %a and %b are not needed. ; Function Attrs: nounwind; define <16 x i8> @test_vpmsumb() #0 {; entry:; %a = alloca <16 x i8>, align 16; %b = alloca <16 x i8>, align 16; store <16 x i8> <i8 1, i8 2, i8 3, i8 4, i8 5, i8 6, i8 7, i8 8, i8 9, i8 10, i8 11, i8 12, i8 13, i8 14, i8 15, i8 16>, <16 x i8>* %a, align 16; store <16 x i8> <i8 113, i8 114, i8 115, i8 116, i8 117, i8 118, i8 119, i8 120, i8 121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16; %0 = load <16 x i8>*",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:7925,Safety,hazard,hazard,7925,"toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===----------------------------------------------------------------------===//. The following example is found in test/CodeGen/PowerPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this is a; poor choice, since we have to load the address into an index register. This; should be fixed for P7/P8. . //===----------------------------------------------------------------------===//. Right now, ShuffleKind 0 is supported only on BE, and ShuffleKind 2 only on LE.; However, we could actually support both kinds on either endianness, if we check; for the appropriate shufflevector pattern for each case ... this would cause; some additional shufflevectors to be recognized and ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:10315,Safety,hazard,hazards,10315,"is program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 3, using a direct move to a GPR, and; sign-extending the 16-bit result to 64 bits. v16i8 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 7, using a direct move to a GPR, and; sign-extending the 8-bit result to 64 bits.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:3063,Testability,test,test,3063," win. //===----------------------------------------------------------------------===//. extract_vector_elt of an arbitrary constant vector can be done with the ; following instructions:. vTemp = vec_splat(v0,2); // 2 is the element the src is in.; vec_ste(&destloc,0,vTemp);. We can do an arbitrary non-constant value by using lvsr/perm/ste. //===----------------------------------------------------------------------===//. If we want to tie instruction selection into the scheduler, we can do some; constant formation with different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly aweful:. vector float test(float a, float b) {; return (vector float){ 0.0, a, 0.0, 0.0}; ; }. LCPI1_0: ; float; .space 4; .text; .globl _test; .align 4; _test:; mfspr r2, 256; oris r3, r2, 4096; mtspr 256, r3; lis r3, ha16(LCPI1_0); addi r4, r1, -32; stfs f1, -16(r1); addi r5, r1, -16; lfs f0, lo16(LCPI1_0)(r3); stfs f0, -32(r1); lvx v2, 0, r4; lvx v3, 0, r5; vmrghw v3, v3, v2; vspltw v2, v2, 0; vmrghw v2, v2, v3; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:3642,Testability,test,test,3642,"ith different instructions. For example, we can generate; ""vsplti -1"" with ""vcmpequw R,R"" and 1,1,1,1 with ""vsubcuw R,R"", and 0,0,0,0 with; ""vsplti 0"" or ""vxor"", each of which use different execution units, thus could; help scheduling. This is probably only reasonable for a post-pass scheduler. //===----------------------------------------------------------------------===//. For this function:. void test(vector float *A, vector float *B) {; vector float C = (vector float)vec_cmpeq(*A, *B);; if (!vec_any_eq(*A, *B)); *B = (vector float){0,0,0,0};; *A = C;; }. we get the following basic block:. 	...; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp v4, v3, v2; vcmpeqfp. v2, v3, v2; bne cr6, LBB1_2 ; cond_next. The vcmpeqfp/vcmpeqfp. instructions currently cannot be merged when the; vcmpeqfp. result is used by a branch. This can be improved. //===----------------------------------------------------------------------===//. The code generated for this is truly aweful:. vector float test(float a, float b) {; return (vector float){ 0.0, a, 0.0, 0.0}; ; }. LCPI1_0: ; float; .space 4; .text; .globl _test; .align 4; _test:; mfspr r2, 256; oris r3, r2, 4096; mtspr 256, r3; lis r3, ha16(LCPI1_0); addi r4, r1, -32; stfs f1, -16(r1); addi r5, r1, -16; lfs f0, lo16(LCPI1_0)(r3); stfs f0, -32(r1); lvx v2, 0, r4; lvx v3, 0, r5; vmrghw v3, v3, v2; vspltw v2, v2, 0; vmrghw v2, v2, v3; mtspr 256, r2; blr. //===----------------------------------------------------------------------===//. int foo(vector float *x, vector float *y) {; if (vec_all_eq(*x,*y)) return 3245; ; else return 12;; }. A predicate compare being used in a select_cc should have the same peephole; applied to it as a predicate compare used by a br_cc. There should be no; mfcr here:. _foo:; mfspr r2, 256; oris r5, r2, 12288; mtspr 256, r5; li r5, 12; li r6, 3245; lvx v2, 0, r4; lvx v3, 0, r3; vcmpeqfp. v2, v3, v2; mfcr r3, 2; rlwinm r3, r3, 25, 31, 31; cmpwi cr0, r3, 0; bne cr0, LBB1_2 ; entry; LBB1_1: ; entry; mr r6, r5; LBB1_2: ; e",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:7413,Testability,test,test,7413,"121, i8 122, i8 123, i8 124, i8 125, i8 126, i8 127, i8 112>, <16 x i8>* %b, align 16; %0 = load <16 x i8>* %a, align 16; %1 = load <16 x i8>* %b, align 16; %2 = call <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8> %0, <16 x i8> %1); ret <16 x i8> %2; }. ; Function Attrs: nounwind readnone; declare <16 x i8> @llvm.ppc.altivec.crypto.vpmsumb(<16 x i8>, <16 x i8>) #1. Produces the following code with -mtriple=powerpc64-unknown-linux-gnu:; # %bb.0: # %entry; addis 3, 2, .LCPI0_0@toc@ha; addis 4, 2, .LCPI0_1@toc@ha; addi 3, 3, .LCPI0_0@toc@l; addi 4, 4, .LCPI0_1@toc@l; lxvw4x 0, 0, 3; addi 3, 1, -16; lxvw4x 35, 0, 4; stxvw4x 0, 0, 3; ori 2, 2, 0; lxvw4x 34, 0, 3; addi 3, 1, -32; stxvw4x 35, 0, 3; vpmsumb 2, 2, 3; blr; .long 0; .quad 0. The two stxvw4x instructions are not needed.; With -mtriple=powerpc64le-unknown-linux-gnu, the associated permutes; are present too. //===----------------------------------------------------------------------===//. The following example is found in test/CodeGen/PowerPC/vec_add_sub_doubleword.ll:. define <2 x i64> @increment_by_val(<2 x i64> %x, i64 %val) nounwind {; %tmpvec = insertelement <2 x i64> <i64 0, i64 0>, i64 %val, i32 0; %tmpvec2 = insertelement <2 x i64> %tmpvec, i64 %val, i32 1; %result = add <2 x i64> %x, %tmpvec2; ret <2 x i64> %result. This will generate the following instruction sequence:; std 5, -8(1); std 5, -16(1); addi 3, 1, -16; ori 2, 2, 0; lxvd2x 35, 0, 3; vaddudm 2, 2, 3; blr. This will almost certainly cause a load-hit-store hazard. ; Since val is a value parameter, it should not need to be saved onto; the stack, unless it's being done set up the vector register. Instead,; it would be better to splat the value into a vector register, and then; remove the (dead) stores to the stack. //===----------------------------------------------------------------------===//. At the moment we always generate a lxsdx in preference to lfd, or stxsdx in; preference to stfd. When we have a reg-immediate addressing mode, this i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt:9989,Testability,log,logic,9989,"----------------------------------------------------===//. There is a utility program called PerfectShuffle that generates a table of the; shortest instruction sequence for implementing a shufflevector operation on; PowerPC. However, this was designed for big-endian code generation. We could; modify this program to create a little endian version of the table. The table; is used in PPCISelLowering.cpp, PPCTargetLowering::LOWERVECTOR_SHUFFLE(). //===----------------------------------------------------------------------===//. Opportunies to use instructions from PPCInstrVSX.td during code gen; - Conversion instructions (Sections 7.6.1.5 and 7.6.1.6 of ISA 2.07); - Scalar comparisons (xscmpodp and xscmpudp); - Min and max (xsmaxdp, xsmindp, xvmaxdp, xvmindp, xvmaxsp, xvminsp). Related to this: we currently do not generate the lxvw4x instruction for either; v4f32 or v4i32, probably because adding a dag pattern to the recognizer requires; a single target type. This should probably be addressed in the PPCISelDAGToDAG logic. //===----------------------------------------------------------------------===//. Currently EXTRACT_VECTOR_ELT and INSERT_VECTOR_ELT are type-legal only; for v2f64 with VSX available. We should create custom lowering; support for the other vector types. Without this support, we generate; sequences with load-hit-store hazards. v4f32 can be supported with VSX by shifting the correct element into; big-endian lane 0, using xscvspdpn to produce a double-precision; representation of the single-precision value in big-endian; double-precision lane 0, and reinterpreting lane 0 as an FPR or; vector-scalar register. v2i64 can be supported with VSX and P8Vector in the same manner as; v2f64, followed by a direct move to a GPR. v4i32 can be supported with VSX and P8Vector by shifting the correct; element into big-endian lane 1, using a direct move to a GPR, and; sign-extending the 32-bit result to 64 bits. v8i16 can be supported with VSX and P8Vector by shifting the ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_ALTIVEC.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:8927,Performance,perform,perform,8927,"snmsubadp. . isCommutable = 1; // xsmaddqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, f128:$vTi))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsmsubqp; [(set f128:$vT, (fma f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmaddqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmsubqp; [(set f128:$vT, (fneg (fma f128:$vA, f128:$vB, (fneg f128:$vTi))))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. - Round to Odd of QP (Negative) Multiply-{Add/Subtract}:; xsmaddqpo xsmsubqpo xsnmaddqpo xsnmsubqpo; . Similar to xsrsqrtedp??. . Define DAG Node in PPCInstrInfo.td:; def PPCfmarto: SDNode<""PPCISD::FMARTO"", SDTFPTernaryOp, []>;. It looks like we only need to define ""PPCfmarto"" for these instructions,; because according to PowerISA_V3.0, these instructions perform RTO on; fma's result:; xsmaddqp(o); v  bfp_MULTIPLY_ADD(src1, src3, src2); rnd  bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v); result  bfp_CONVERT_TO_BFP128(rnd). xsmsubqp(o); v  bfp_MULTIPLY_ADD(src1, src3, bfp_NEGATE(src2)); rnd  bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v); result  bfp_CONVERT_TO_BFP128(rnd). xsnmaddqp(o); v  bfp_MULTIPLY_ADD(src1,src3,src2); rnd  bfp_NEGATE(bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v)); result  bfp_CONVERT_TO_BFP128(rnd). xsnmsubqp(o); v  bfp_MULTIPLY_ADD(src1, src3, bfp_NEGATE(src2)); rnd  bfp_NEGATE(bfp_ROUND_TO_BFP128(RO, FPSCR.RN, v)); result  bfp_CONVERT_TO_BFP128(rnd). DAG patterns of each instruction (PPCInstrVSX.td):; . isCommutable = 1; // xsmaddqpo; [(set f128:$vT, (PPCfmarto f128:$vA, f128:$vB, f128:$vTi))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsmsubqpo; [(set f128:$vT, (PPCfmarto f128:$vA, f128:$vB, (fneg f128:$vTi)))]>,; RegConstraint<""$vTi = $vT"">, NoEncode<""$vTi"">,; AltVSXFMARel;. // xsnmaddqpo; [(set f128:$vT, (fneg (PPCfmarto f128:$vA, f128:$vB, f128:$v",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17365,Performance,load,load,17365,"4:$XT, (fminnum f64:$XA, f64:$XB)). . xsmaxjdp xsminjdp; (set f64:$XT, (int_ppc_vsx_xsmaxjdp f64:$XA, f64:$XB)); (set f64:$XT, (int_ppc_vsx_xsminjdp f64:$XA, f64:$XB)). - Vector Byte-Reverse H/W/D/Q Word: xxbrh xxbrw xxbrd xxbrq; . Use intrinsic; (set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfr",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17614,Performance,load,load,17614,"(set v8i16:$XT, (int_ppc_vsx_xxbrh v8i16:$XB)); (set v4i32:$XT, (int_ppc_vsx_xxbrw v4i32:$XB)); (set v2i64:$XT, (int_ppc_vsx_xxbrd v2i64:$XB)); (set v1i128:$XT, (int_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17845,Performance,load,load,17845,"nt_ppc_vsx_xxbrq v1i128:$XB)). - Vector Permute: xxperm xxpermr; . I have checked ""PPCxxswapd"" in PPCInstrVSX.td, but they are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). -",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:17885,Performance,load,load,17885,"y are different; . Use intrinsic; (set v16i8:$XT, (int_ppc_vsx_xxperm v16i8:$XA, v16i8:$XB)); (set v16i8:$XT, (int_ppc_vsx_xxpermr v16i8:$XA, v16i8:$XB)). - Vector Splat Immediate Byte: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (out",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:18149,Performance,load,load,18149,"te: xxspltib; . Similar to XXSPLTW:; def XXSPLTW : XX2Form_2<60, 164,; (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),; ""xxspltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:18189,Performance,load,load,18189,"ltw $XT, $XB, $UIM"", IIC_VecPerm, []>;. . No SDAG, intrinsic, builtin are required?. - Load/Store Vector: lxv stxv; . Has likely SDAG match:; (set v?:$XT, (load ix16addr:$src)); (set v?:$XT, (store ix16addr:$dst)). . Need define ix16addr in PPCInstrInfo.td; ix16addr: 16-byte aligned, see ""def memrix16"" in PPCInstrInfo.td. - Load/Store Vector Indexed: lxvx stxvx; . Has likely SDAG match:; (set v?:$XT, (load xoaddr:$src)); (set v?:$XT, (store xoaddr:$dst)). - Load/Store DWord: lxsd stxsd; . Similar to lxsdx/stxsdx:; def LXSDX : XX1Form<31, 588,; (outs vsfrc:$XT), (ins memrr:$src),; ""lxsdx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (load xoaddr:$src))]>;. . (set f64:$XT, (load iaddrX4:$src)); (set f64:$XT, (store iaddrX4:$dst)). - Load/Store SP, with conversion from/to DP: lxssp stxssp; . Similar to lxsspx/stxsspx:; def LXSSPX : XX1Form<31, 524, (outs vssrc:$XT), (ins memrr:$src),; ""lxsspx $XT, $src"", IIC_LdStLFD,; [(set f32:$XT, (load xoaddr:$src))]>;. . (set f32:$XT, (load iaddrX4:$src)); (set f32:$XT, (store iaddrX4:$dst)). - Load as Integer Byte/Halfword & Zero Indexed: lxsibzx lxsihzx; . Similar to lxsiwzx:; def LXSIWZX : XX1Form<31, 12, (outs vsfrc:$XT), (ins memrr:$src),; ""lxsiwzx $XT, $src"", IIC_LdStLFD,; [(set f64:$XT, (PPClfiwzx xoaddr:$src))]>;. . (set f64:$XT, (PPClfiwzx xoaddr:$src)). - Store as Integer Byte/Halfword Indexed: stxsibx stxsihx; . Similar to stxsiwx:; def STXSIWX : XX1Form<31, 140, (outs), (ins vsfrc:$XT, memrr:$dst),; ""stxsiwx $XT, $dst"", IIC_LdStSTFD,; [(PPCstfiwx f64:$XT, xoaddr:$dst)]>;. . (PPCstfiwx f64:$XT, xoaddr:$dst). - Load Vector Halfword*8/Byte*16 Indexed: lxvh8x lxvb16x; . Similar to lxvd2x/lxvw4x:; def LXVD2X : XX1Form<31, 844,; (outs vsrc:$XT), (ins memrr:$src),; ""lxvd2x $XT, $src"", IIC_LdStLFD,; [(set v2f64:$XT, (int_ppc_vsx_lxvd2x xoaddr:$src))]>;. . (set v8i16:$XT, (int_ppc_vsx_lxvh8x xoaddr:$src)); (set v16i8:$XT, (int_ppc_vsx_lxvb16x xoaddr:$src)). - Store Vector Halfword*8/Byte*16 Indexed: stxvh8x stxvb16x; . Similar to stxv",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:5743,Security,access,accessed,5743,"oned/Signed-QWord:; bcdcfn. bcdcfz. bcdctn. bcdctz. bcdcfsq. bcdctsq.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdcfno v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdcfzo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdctno v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdctzo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdcfsqo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdctsqo v1i128:$vB)). - Decimal Copy-Sign/Set-Sign: bcdcpsgn. bcdsetsgn.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdcpsgno v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsetsgno v1i128:$vB, i1:$PS)). - Decimal Shift/Unsigned-Shift/Shift-and-Round: bcds. bcdus. bcdsr.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsro v1i128:$vA, v1i128:$vB, i1:$PS)). . Note! Their VA is accessed only 1 byte, i.e. VA.byte[7]. - Decimal (Unsigned) Truncate: bcdtrunc. bcdutrunc.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:6017,Security,access,accessed,6017,"; (set v1i128:$vD, (int_ppc_altivec_bcdctzo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdcfsqo v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcdctsqo v1i128:$vB)). - Decimal Copy-Sign/Set-Sign: bcdcpsgn. bcdsetsgn.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdcpsgno v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsetsgno v1i128:$vB, i1:$PS)). - Decimal Shift/Unsigned-Shift/Shift-and-Round: bcds. bcdus. bcdsr.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)); (set v1i128:$vD, (int_ppc_altivec_bcdsro v1i128:$vA, v1i128:$vB, i1:$PS)). . Note! Their VA is accessed only 1 byte, i.e. VA.byte[7]. - Decimal (Unsigned) Truncate: bcdtrunc. bcdutrunc.; . Use instrinstics:; (set v1i128:$vD, (int_ppc_altivec_bcdso v1i128:$vA, v1i128:$vB, i1:$PS)); (set v1i128:$vD, (int_ppc_altivec_bcduso v1i128:$vA, v1i128:$vB)). . Note! Their VA is accessed only 2 byte, i.e. VA.hword[3] (VA.bit[48:63]). VSX:; - QP Copy Sign: xscpsgnqp; . Similar to xscpsgndp; . (set f128:$vT, (fcopysign f128:$vB, f128:$vA). - QP Absolute/Negative-Absolute/Negate: xsabsqp xsnabsqp xsnegqp; . Similar to xsabsdp/xsnabsdp/xsnegdp; . (set f128:$vT, (fabs f128:$vB)) // xsabsqp; (set f128:$vT, (fneg (fabs f128:$vB))) // xsnabsqp; (set f128:$vT, (fneg f128:$vB)) // xsnegqp. - QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqp xsdivqp xsmulqp xssubqp xssqrtqp; . Similar to xsadddp; . isCommutable = 1; (set f128:$vT, (fadd f128:$vA, f128:$vB)) // xsaddqp; (set f128:$vT, (fmul f128:$vA, f128:$vB)) // xsmulqp. . isCommutable = 0; (set f128:$vT, (fdiv f128:$vA, f128:$vB)) // xsdivqp; (set f128:$vT, (fsub f128:$vA, f128:$vB)) // xssubqp; (set f128:$vT, (fsqrt f128:$vB))) // xssqrtqp. - Round to Odd of QP Add/Divide/Multiply/Subtract/Square-Root:; xsaddqpo xsdivqpo xsmulqpo xssubqpo xssqrtqpo; . Similar to xsrsqrtedp??; def XSRSQRTEDP : XX2Form<60, 74,; (outs v",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt:13253,Testability,test,testing,13253,"64:$XB)) // xscvudqp. - (Round &) Convert DP <-> HP: xscvdphp xscvhpdp; . Similar to XSCVDPSP; . No SDAG, intrinsic, builtin are required??. - Vector HP -> SP: xvcvhpsp xvcvsphp; . Similar to XVCVDPSP:; def XVCVDPSP : XX2Form<60, 393,; (outs vsrc:$XT), (ins vsrc:$XB),; ""xvcvdpsp $XT, $XB"", IIC_VecFP, []>;; . No SDAG, intrinsic, builtin are required??. - Round to Quad-Precision Integer: xsrqpi xsrqpix; . These are combination of ""XSRDPI"", ""XSRDPIC"", ""XSRDPIM"", .., because you; need to assign rounding mode in instruction; . Provide builtin?; (set f128:$vT, (int_ppc_vsx_xsrqpi f128:$vB)); (set f128:$vT, (int_ppc_vsx_xsrqpix f128:$vB)). - Round Quad-Precision to Double-Extended Precision (fp80): xsrqpxp; . Provide builtin?; (set f128:$vT, (int_ppc_vsx_xsrqpxp f128:$vB)). Fixed Point Facility:. - Exploit cmprb and cmpeqb (perhaps for something like; isalpha/isdigit/isupper/islower and isspace respectivelly). This can; perhaps be done through a builtin. - Provide testing for cnttz[dw]; - Insert Exponent DP/QP: xsiexpdp xsiexpqp; . Use intrinsic?; . xsiexpdp:; // Note: rA and rB are the unsigned integer value.; (set f128:$XT, (int_ppc_vsx_xsiexpdp i64:$rA, i64:$rB)). . xsiexpqp:; (set f128:$vT, (int_ppc_vsx_xsiexpqp f128:$vA, f64:$vB)). - Extract Exponent/Significand DP/QP: xsxexpdp xsxsigdp xsxexpqp xsxsigqp; . Use intrinsic?; . (set i64:$rT, (int_ppc_vsx_xsxexpdp f64$XB)) // xsxexpdp; (set i64:$rT, (int_ppc_vsx_xsxsigdp f64$XB)) // xsxsigdp; (set f128:$vT, (int_ppc_vsx_xsxexpqp f128$vB)) // xsxexpqp; (set f128:$vT, (int_ppc_vsx_xsxsigqp f128$vB)) // xsxsigqp. - Vector Insert Word: xxinsertw; - Useful for inserting f32/i32 elements into vectors (the element to be; inserted needs to be prepared); . Note: llvm has insertelem in ""Vector Operations""; ; yields <n x <ty>>; <result> = insertelement <n x <ty>> <val>, <ty> <elt>, <ty2> <idx>. But how to map to it??; [(set v1f128:$XT, (insertelement v1f128:$XTi, f128:$XB, i4:$UIMM))]>,; RegConstraint<""$XTi = $XT"">, NoEncode<""$XTi"">,",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/README_P9.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:249,Energy Efficiency,allocate,allocate,249,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:1166,Performance,load,load,1166,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:1188,Performance,load,load,1188,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt:421,Usability,clear,clear,421,"To-do; -----. * Keep the address of the constant pool in a register instead of forming its; address all of the time.; * We can fold small constant offsets into the %hi/%lo references to constant; pool addresses as well.; * When in V9 mode, register allocate %icc[0-3].; * Add support for isel'ing UMUL_LOHI instead of marking it as Expand.; * Emit the 'Branch on Integer Register with Prediction' instructions. It's; not clear how to write a pattern for this though:. float %t1(int %a, int* %p) {; %C = seteq int %a, 0; br bool %C, label %T, label %F; T:; store int 123, int* %p; br label %F; F:; ret float undef; }. codegens to this:. t1:; save -96, %o6, %o6; 1) subcc %i0, 0, %l0; 1) bne .LBBt1_2 ! F; nop; .LBBt1_1: ! T; or %g0, 123, %l0; st %l0, [%i1]; .LBBt1_2: ! F; restore %g0, %g0, %g0; retl; nop. 1) should be replaced with a brz in V9 mode. * Same as above, but emit conditional move on register zero (p192) in V9; mode. Testcase:. int %t1(int %a, int %b) {; %C = seteq int %a, 0; %D = select bool %C, int %a, int %b; ret int %D; }. * Emit MULX/[SU]DIVX instructions in V9 mode instead of fiddling; with the Y register, if they are faster. * Codegen bswap(load)/store(bswap) -> load/store ASI. * Implement frame pointer elimination, e.g. eliminate save/restore for; leaf fns.; * Fill delay slots. * Use %g0 directly to materialize 0. No instruction is required.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:920,Energy Efficiency,schedul,scheduling,920,"//===---------------------------------------------------------------------===//; // Random notes about and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:3254,Energy Efficiency,allocate,allocate,3254,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1071,Modifiability,extend,extend,1071,"and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:1086,Modifiability,variab,variable-length,1086,"and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2020,Modifiability,extend,extended,2020,"e MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:674,Performance,load,load,674,"//===---------------------------------------------------------------------===//; // Random notes about and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:878,Performance,perform,performance,878,"//===---------------------------------------------------------------------===//; // Random notes about and ideas for the SystemZ backend.; //===---------------------------------------------------------------------===//. The initial backend is deliberately restricted to z10. We should add support; for later architectures at some point. --. If an inline asm ties an i32 ""r"" result to an i64 input, the input; will be treated as an i32, leaving the upper bits uninitialised.; For example:. define void @f4(i32 *%dst) {; %val = call i32 asm ""blah $0"", ""=r,0"" (i64 103); store i32 %val, i32 *%dst; ret void; }. from CodeGen/SystemZ/asm-09.ll will use LHI rather than LGHI.; to load 103. This seems to be a general target-independent problem. --. The tuning of the choice between LOAD ADDRESS (LA) and addition in; SystemZISelDAGToDAG.cpp is suspect. It should be tweaked based on; performance measurements. --. There is no scheduling support. --. We don't use the BRANCH ON INDEX instructions. --. We only use MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet f",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2029,Performance,load,loads,2029,"e MVC, XC and CLC for constant-length block operations.; We could extend them to variable-length operations too,; using EXECUTE RELATIVE LONG. MVCIN, MVCLE and CLCLE may be worthwhile too. --. We don't use CUSE or the TRANSLATE family of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2237,Performance,load,load,2237,"mily of instructions for string; operations. The TRANSLATE ones are probably more difficult to exploit. --. We don't take full advantage of builtins like fabsl because the calling; conventions require f128s to be returned by invisible reference. --. ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2465,Performance,optimiz,optimizations,2465,". ADD LOGICAL WITH SIGNED IMMEDIATE could be useful when we need to; produce a carry. SUBTRACT LOGICAL IMMEDIATE could be useful when we; need to produce a borrow. (Note that there are no memory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-by",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2679,Performance,load,loading,2679,"ory forms of; ADD LOGICAL WITH CARRY and SUBTRACT LOGICAL WITH BORROW, so the high; part of 128-bit memory operations would probably need to be done; via a register.). --. We don't use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to suppo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2920,Performance,load,load,2920,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:2952,Performance,load,load,2952,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:3077,Performance,load,load,3077,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:3552,Security,access,access,3552,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt:3297,Usability,simpl,simpler,3297,"use ICM, STCM, or CLM. --. We don't use ADD (LOGICAL) HIGH, SUBTRACT (LOGICAL) HIGH,; or COMPARE (LOGICAL) HIGH yet. --. DAGCombiner doesn't yet fold truncations of extended loads. Functions like:. unsigned long f (unsigned long x, unsigned short *y); {; return (x << 32) | *y;; }. therefore end up as:. sllg %r2, %r2, 32; llgh %r0, 0(%r3); lr %r2, %r0; br %r14. but truncating the load would give:. sllg %r2, %r2, 32; lh %r2, 0(%r3); br %r14. --. Functions like:. define i64 @f1(i64 %a) {; %and = and i64 %a, 1; ret i64 %and; }. ought to be implemented as:. lhi %r0, 1; ngr %r2, %r0; br %r14. but two-address optimizations reverse the order of the AND and force:. lhi %r0, 1; ngr %r0, %r2; lgr %r2, %r0; br %r14. CodeGen/SystemZ/and-04.ll has several examples of this. --. Out-of-range displacements are usually handled by loading the full; address into a register. In many cases it would be better to create; an anchor point instead. E.g. for:. define void @f4a(i128 *%aptr, i64 %base) {; %addr = add i64 %base, 524288; %bptr = inttoptr i64 %addr to i128 *; %a = load volatile i128 *%aptr; %b = load i128 *%bptr; %add = add i128 %a, %b; store i128 %add, i128 *%aptr; ret void; }. (from CodeGen/SystemZ/int-add-08.ll) we load %base+524288 and %base+524296; into separate registers, rather than using %base+524288 as a base for both. --. Dynamic stack allocations round the size to 8 bytes and then allocate; that rounded amount. It would be simpler to subtract the unrounded; size from the copy of the stack pointer and then align the result.; See CodeGen/SystemZ/alloca-01.ll for an example. --. If needed, we can support 16-byte atomics using LPQ, STPQ and CSDG. --. We might want to model all access registers and use them to spill; 32-bit values. --. We might want to use the 'overflow' condition of eg. AR to support; llvm.sadd.with.overflow.i32 and related instructions - the generated code; for signed overflow check is currently quite bad. This would improve; the results of using -ftrapv.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:4315,Availability,redundant,redundant,4315,"le pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms before register coloring,; particularly when duplicating code, to allow register coloring to be aware of; the duplication. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:825,Deployability,integrat,integrated,825,"//===-- README.txt - Notes for WebAssembly code gen -----------------------===//. The object format emitted by the WebAssembly backed is documented in:. * https://github.com/WebAssembly/tool-conventions/blob/main/Linking.md. The C ABI is described in:. * https://github.com/WebAssembly/tool-conventions/blob/main/BasicCABI.md. For more information on WebAssembly itself, see the home page:. * https://webassembly.github.io/. Emscripten provides a C/C++ compilation environment based on clang which; includes standard libraries, tools, and packaging for producing WebAssembly; applications that can run in browsers and other environments. wasi-sdk provides a more minimal C/C++ SDK based on clang, llvm and a libc based; on musl, for producing WebAssembly applications that use the WASI ABI. Rust provides WebAssembly support integrated into Cargo. There are two; main options:; - wasm32-unknown-unknown, which provides a relatively minimal environment; that has an emphasis on being ""native""; - wasm32-unknown-emscripten, which uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instruc",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:6092,Energy Efficiency,schedul,schedule,6092,"----------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify is currently a greedy algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:825,Integrability,integrat,integrated,825,"//===-- README.txt - Notes for WebAssembly code gen -----------------------===//. The object format emitted by the WebAssembly backed is documented in:. * https://github.com/WebAssembly/tool-conventions/blob/main/Linking.md. The C ABI is described in:. * https://github.com/WebAssembly/tool-conventions/blob/main/BasicCABI.md. For more information on WebAssembly itself, see the home page:. * https://webassembly.github.io/. Emscripten provides a C/C++ compilation environment based on clang which; includes standard libraries, tools, and packaging for producing WebAssembly; applications that can run in browsers and other environments. wasi-sdk provides a more minimal C/C++ SDK based on clang, llvm and a libc based; on musl, for producing WebAssembly applications that use the WASI ABI. Rust provides WebAssembly support integrated into Cargo. There are two; main options:; - wasm32-unknown-unknown, which provides a relatively minimal environment; that has an emphasis on being ""native""; - wasm32-unknown-emscripten, which uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instruc",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2592,Integrability,depend,dependencies,2592,"-----------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMa",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2713,Integrability,depend,dependencies,2713,"the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:1739,Modifiability,extend,extend,1739,"ng WebAssembly applications that use the WASI ABI. Rust provides WebAssembly support integrated into Cargo. There are two; main options:; - wasm32-unknown-unknown, which provides a relatively minimal environment; that has an emphasis on being ""native""; - wasm32-unknown-emscripten, which uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===--------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3195,Modifiability,extend,extending,3195,"---------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3907,Modifiability,variab,variables,3907,"ed hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms befo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:6078,Modifiability,config,configured,6078,"----------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify is currently a greedy algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:6421,Modifiability,extend,extended,6421,"algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it needs an FP register, but later it; turns out to be unneeded, so one could either approach this by being more; clever about not inserting code for an FP in the first place, or optimizing; away the copy later. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2028,Performance,optimiz,optimizations,2028,"h uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===----------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2073,Performance,optimiz,optimizations,2073,"h uses Emscripten internally and; provides standard C/C++ libraries, filesystem emulation, GL and SDL; bindings; For more information, see:; * https://www.hellorust.com/. The following documents contain some information on the semantics and binary; encoding of WebAssembly itself:; * https://github.com/WebAssembly/design/blob/main/Semantics.md; * https://github.com/WebAssembly/design/blob/main/BinaryEncoding.md. Some notes on ways that the generated code could be improved follow:. //===---------------------------------------------------------------------===//. Br, br_if, and br_table instructions can support having a value on the value; stack across the jump (sometimes). We should (a) model this, and (b) extend; the stackifier to utilize it. //===---------------------------------------------------------------------===//. The min/max instructions aren't exactly a<b?a:b because of NaN and negative zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===----------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:2907,Performance,optimiz,optimization-related,2907,"ve zero; behavior. The ARM target has the same kind of min/max instructions and has; implemented optimizations for them; we should do similar optimizations for; WebAssembly. //===---------------------------------------------------------------------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same co",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3246,Performance,optimiz,optimization,3246,"---------===//. AArch64 runs SeparateConstOffsetFromGEPPass, followed by EarlyCSE and LICM.; Would these be useful to run for WebAssembly too? Also, it has an option to; run SimplifyCFG after running the AtomicExpand pass. Would this be useful for; us too?. //===---------------------------------------------------------------------===//. Register stackification uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3485,Performance,optimiz,optimizeSelect,3485,"ion uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3501,Performance,optimiz,optimizeCompareInstr,3501,"ion uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3523,Performance,optimiz,optimizeCondBranch,3523,"ion uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:3544,Performance,optimiz,optimizeLoadInstr,3544,"ion uses the VALUE_STACK physical register to impose; ordering dependencies on instructions with stack operands. This is pessimistic;; we should consider alternate ways to model stack dependencies. //===---------------------------------------------------------------------===//. Lots of things could be done in WebAssemblyTargetTransformInfo.cpp. Similarly,; there are numerous optimization-related hooks that can be overridden in; WebAssemblyTargetLowering. //===---------------------------------------------------------------------===//. Instead of the OptimizeReturned pass, which should consider preserving the; ""returned"" attribute through to MachineInstrs and extending the; MemIntrinsicResults pass to do this optimization on calls too. That would also; let the WebAssemblyPeephole pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebA",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:4338,Performance,optimiz,optimized,4338,"le pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms before register coloring,; particularly when duplicating code, to allow register coloring to be aware of; the duplication. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:5184,Performance,load,loads,5184,"-----------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms before register coloring,; particularly when duplicating code, to allow register coloring to be aware of; the duplication. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify is currently a greedy algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===--------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:7233,Performance,optimiz,optimizing,7233,"algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it needs an FP register, but later it; turns out to be unneeded, so one could either approach this by being more; clever about not inserting code for an FP in the first place, or optimizing; away the copy later. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:4315,Safety,redund,redundant,4315,"le pass clean up dead defs for such calls, as it does; for stores. //===---------------------------------------------------------------------===//. Consider implementing optimizeSelect, optimizeCompareInstr, optimizeCondBranch,; optimizeLoadInstr, and/or getMachineCombinerPatterns. //===---------------------------------------------------------------------===//. Find a clean way to fix the problem which leads to the Shrink Wrapping pass; being run after the WebAssembly PEI pass. //===---------------------------------------------------------------------===//. When setting multiple local variables to the same constant, we currently get; code like this:. i32.const $4=, 0; i32.const $3=, 0. It could be done with a smaller encoding like this:. i32.const $push5=, 0; local.tee $push6=, $4=, $pop5; local.copy $3=, $pop6. //===---------------------------------------------------------------------===//. WebAssembly registers are implicitly initialized to zero. Explicit zeroing is; therefore often redundant and could be optimized away. //===---------------------------------------------------------------------===//. Small indices may use smaller encodings than large indices.; WebAssemblyRegColoring and/or WebAssemblyRegRenumbering should sort registers; according to their usage frequency to maximize the usage of smaller encodings. //===---------------------------------------------------------------------===//. Many cases of irreducible control flow could be transformed more optimally; than via the transform in WebAssemblyFixIrreducibleControlFlow.cpp. It may also be worthwhile to do transforms before register coloring,; particularly when duplicating code, to allow register coloring to be aware of; the duplication. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify could use AliasAnalysis to reorder loads and stores more; aggressively. //===---------------------------------------------------------------------===//. WebAssemblyRe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt:6815,Testability,test,test,6815,"algorithm. This means that, for; example, a binary operator will stackify with its user before its operands.; However, if moving the binary operator to its user moves it to a place where; its operands can't be moved to, it would be better to leave it in place, or; perhaps move it up, so that it can stackify its operands. A binary operator; has two operands and one result, so in such cases there could be a net win by; preferring the operands. //===---------------------------------------------------------------------===//. Instruction ordering has a significant influence on register stackification and; coloring. Consider experimenting with the MachineScheduler (enable via; enableMachineScheduler) and determine if it can be configured to schedule; instructions advantageously for this purpose. //===---------------------------------------------------------------------===//. WebAssemblyRegStackify currently assumes that the stack must be empty after; an instruction with no return values, however wasm doesn't actually require; this. WebAssemblyRegStackify could be extended, or possibly rewritten, to take; full advantage of what WebAssembly permits. //===---------------------------------------------------------------------===//. Add support for mergeable sections in the Wasm writer, such as for strings and; floating-point constants. //===---------------------------------------------------------------------===//. The function @dynamic_alloca_redzone in test/CodeGen/WebAssembly/userstack.ll; ends up with a local.tee in its prolog which has an unused result, requiring; an extra drop:. global.get $push8=, 0; local.tee $push9=, 1, $pop8; drop $pop9; [...]. The prologue code initially thinks it needs an FP register, but later it; turns out to be unneeded, so one could either approach this by being more; clever about not inserting code for an FP in the first place, or optimizing; away the copy later. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:387,Deployability,patch,patches,387,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend: FP stack related stuff; //===---------------------------------------------------------------------===//. //===---------------------------------------------------------------------===//. Some targets (e.g. athlons) prefer freep to fstp ST(0):; http://gcc.gnu.org/ml/gcc-patches/2004-04/msg00659.html. //===---------------------------------------------------------------------===//. This should use fiadd on chips where it is profitable:; double foo(double P, int *I) { return P+*I; }. We have fiadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are ha",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1382,Deployability,patch,patches,1382,"tches/2004-04/msg00659.html. //===---------------------------------------------------------------------===//. This should use fiadd on chips where it is profitable:; double foo(double P, int *I) { return P+*I; }. We have fiadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Currently the x86 codegen isn't very good at mixing SSE and FPStack; code:. unsigned int foo(double x) { return x; }. foo:; 	subl $20, %esp; 	movsd 24(%esp), %xmm0; 	movsd %xmm0, 8(%esp)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1761,Deployability,patch,patches,1761,"iadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Currently the x86 codegen isn't very good at mixing SSE and FPStack; code:. unsigned int foo(double x) { return x; }. foo:; 	subl $20, %esp; 	movsd 24(%esp), %xmm0; 	movsd %xmm0, 8(%esp); 	fldl 8(%esp); 	fisttpll (%esp); 	movl (%esp), %eax; 	addl $20, %esp; 	ret. This just requires being smarter when custom expanding fptoui. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1818,Deployability,patch,patches,1818,"iadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Currently the x86 codegen isn't very good at mixing SSE and FPStack; code:. unsigned int foo(double x) { return x; }. foo:; 	subl $20, %esp; 	movsd 24(%esp), %xmm0; 	movsd %xmm0, 8(%esp); 	fldl 8(%esp); 	fisttpll (%esp); 	movl (%esp), %eax; 	addl $20, %esp; 	ret. This just requires being smarter when custom expanding fptoui. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1212,Energy Efficiency,reduce,reduce,1212,"----------===//. //===---------------------------------------------------------------------===//. Some targets (e.g. athlons) prefer freep to fstp ST(0):; http://gcc.gnu.org/ml/gcc-patches/2004-04/msg00659.html. //===---------------------------------------------------------------------===//. This should use fiadd on chips where it is profitable:; double foo(double P, int *I) { return P+*I; }. We have fiadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Cur",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt:1191,Usability,simpl,simple,1191,"----------===//. //===---------------------------------------------------------------------===//. Some targets (e.g. athlons) prefer freep to fstp ST(0):; http://gcc.gnu.org/ml/gcc-patches/2004-04/msg00659.html. //===---------------------------------------------------------------------===//. This should use fiadd on chips where it is profitable:; double foo(double P, int *I) { return P+*I; }. We have fiadd patterns now but the followings have the same cost and; complexity. We need a way to specify the later is more profitable. def FpADD32m : FpI<(ops RFP:$dst, RFP:$src1, f32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (extloadf64f32 addr:$src2)))]>;; // ST(0) = ST(0) + [mem32]. def FpIADD32m : FpI<(ops RFP:$dst, RFP:$src1, i32mem:$src2), OneArgFPRW,; [(set RFP:$dst, (fadd RFP:$src1,; (X86fild addr:$src2, i32)))]>;; // ST(0) = ST(0) + [mem32int]. //===---------------------------------------------------------------------===//. The FP stackifier should handle simple permutates to reduce number of shuffle; instructions, e.g. turning:. fld P	->		fld Q; fld Q			fld P; fxch. or:. fxch	->		fucomi; fucomi			jl X; jg X. Ideas:; http://gcc.gnu.org/ml/gcc-patches/2004-11/msg02410.html. //===---------------------------------------------------------------------===//. Add a target specific hook to DAG combiner to handle SINT_TO_FP and; FP_TO_SINT when the source operand is already in memory. //===---------------------------------------------------------------------===//. Open code rint,floor,ceil,trunc:; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02006.html; http://gcc.gnu.org/ml/gcc-patches/2004-08/msg02011.html. Opencode the sincos[f] libcall. //===---------------------------------------------------------------------===//. None of the FPStack instructions are handled in; X86RegisterInfo::foldMemoryOperand, which prevents the spiller from; folding spill code into the instructions. //===---------------------------------------------------------------------===//. Cur",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-FPStack.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3200,Availability,mask,mask,3200,"ith unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2071,Deployability,patch,patches,2071," A[0]+A[1]+A[2]+A[3];; }. Instead we get this:. _f32: ## @f32; 	pshufd	$1, %xmm0, %xmm1 ## xmm1 = xmm0[1,0,0,0]; 	addss	%xmm0, %xmm1; 	pshufd	$3, %xmm0, %xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3318,Deployability,update,update,3318,"----------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:22263,Deployability,pipeline,pipelined,22263,"; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {; entry:; %tmp18 = extractvalue %0 %0, 0 ; <double> [#uses=1]; %tmp19 = bitcast double %tmp18 to i64 ; <i64> [#uses=1]; %tmp20 = zext i64 %tmp19 to i128 ; <i128> [#uses=1]; %tmp10 = lshr i128 %tmp20, 32 ; <i128> [#uses=1]; %tmp11 = trunc i128 %tmp10 to i32 ; <i32> [#uses=1]; %tmp12 = bitcast i32 %tmp11 to float ; <float> [#uses=1]; %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]; store float %tmp12, float* %tmp5; ret void; }. Compiles to:. _test: ## @test; 	movd	%xmm0, %rax; 	shrq	$32, %rax; 	movl	%eax, 4(%rdi); 	ret. This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and; doing a shuffle from v[1] to v[0] then a float store. //===---------------------------------------------------------------------===//. [UNSAFE FP]. void foo(double, double, double);; void norm(double x, double y, double z) {; double scale = __builtin_sqrt(x*x + y*y + z*z);; foo(x/scale, y/scale, z/scale);; }. We currently generate an sqrtsd and 3 divsd instructions. This is bad, fp div is; slow and not pipelined. In -ffast-math mode we could compute ""1.0/scale"" first; and emit 3 mulsd in place of the divs. This can be done as a target-independent; transform. If we're dealing with floats instead of doubles we could even replace the sqrtss; and inversion with an rsqrtss instruction, which computes 1/sqrt faster at the; cost of reduced accuracy. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:4445,Energy Efficiency,schedul,scheduling,4445,"--------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%reg1037 = MOV32rm %reg1024, 1, %noreg, 40; 	%reg1080 = IMUL32rr %reg1079, %reg1037; 	%reg1081 = MOV32rm %reg1058, 1, %noreg, 0; 	%reg1038 = LEA32r %reg1081, 1, %reg1080, -3; 	%reg1036 = MOV32rm %reg1024, 1, %noreg, 32; 	%reg1082 = SHL32ri %reg1038, 4; 	%reg1039 = ADD32rr %reg1036, %reg1082; 	%reg1083 = MOVAPSrm %reg1059, 1, %noreg, 0; 	%reg1034 = SHUFPSrr %reg1083, %reg1083, 170; 	%reg1032 = SHUFPSrr %reg1083, %reg1083, 0; 	%reg1035 = SHUFPSrr %reg1083, %reg1083, 255; 	%reg1033 = SHUFPSrr %reg1083, %reg1083, 85; 	%reg1040 = MOV32rr %reg1039; 	%reg1084 = AND32ri8 %reg1039, 15; 	CMP32ri8 %reg1084, 0; 	JE mbb<cond_next204,0xa914d30>. Still ok. After register allocation:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%eax = MOV32ri -3; 	%edx = MOV32rm %stack.3, 1, %noreg, 0; 	ADD32rm %eax<def&use>, %edx, 1, %noreg, 0; 	%edx = MOV32rm %s",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:17549,Energy Efficiency,power,powers,17549,"---------------===//. We compile vector multiply-by-constant into poor code:. define <4 x i32> @f(<4 x i32> %i) nounwind {; 	%A = mul <4 x i32> %i, < i32 10, i32 10, i32 10, i32 10 >; 	ret <4 x i32> %A; }. On targets without SSE4.1, this compiles into:. LCPI1_0:					## <4 x i32>; 	.long	10; 	.long	10; 	.long	10; 	.long	10; 	.text; 	.align	4,0x90; 	.globl	_f; _f:; 	pshufd	$3, %xmm0, %xmm1; 	movd	%xmm1, %eax; 	imull	LCPI1_0+12, %eax; 	movd	%eax, %xmm1; 	pshufd	$1, %xmm0, %xmm2; 	movd	%xmm2, %eax; 	imull	LCPI1_0+4, %eax; 	movd	%eax, %xmm2; 	punpckldq	%xmm1, %xmm2; 	movd	%xmm0, %eax; 	imull	LCPI1_0, %eax; 	movd	%eax, %xmm1; 	movhlps	%xmm0, %xmm0; 	movd	%xmm0, %eax; 	imull	LCPI1_0+8, %eax; 	movd	%eax, %xmm0; 	punpckldq	%xmm0, %xmm1; 	movaps	%xmm1, %xmm0; 	punpckldq	%xmm2, %xmm0; 	ret. It would be better to synthesize integer vector multiplication by constants; using shifts and adds, pslld and paddd here. And even on targets with SSE4.1,; simple cases such as multiplication by powers of two would be better as; vector shifts than as multiplications. //===---------------------------------------------------------------------===//. We compile this:. __m128i; foo2 (char x); {; return _mm_set_epi8 (1, 0, 0, 0, 0, 0, 0, 0, 0, x, 0, 1, 0, 0, 0, 0);; }. into:; 	movl	$1, %eax; 	xorps	%xmm0, %xmm0; 	pinsrw	$2, %eax, %xmm0; 	movzbl	4(%esp), %eax; 	pinsrw	$3, %eax, %xmm0; 	movl	$256, %eax; 	pinsrw	$7, %eax, %xmm0; 	ret. gcc-4.2:; 	subl	$12, %esp; 	movzbl	16(%esp), %eax; 	movdqa	LC0, %xmm0; 	pinsrw	$3, %eax, %xmm0; 	addl	$12, %esp; 	ret; 	.const; 	.align 4; LC0:; 	.word	0; 	.word	0; 	.word	1; 	.word	0; 	.word	0; 	.word	0; 	.word	0; 	.word	256. With SSE4, it should be; movdqa .LC0(%rip), %xmm0; pinsrb $6, %edi, %xmm0. //===---------------------------------------------------------------------===//. We should transform a shuffle of two vectors of constants into a single vector; of constants. Also, insertelement of a constant into a vector of constants; should also result in a vector of con",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:22592,Energy Efficiency,reduce,reduced,22592,"; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {; entry:; %tmp18 = extractvalue %0 %0, 0 ; <double> [#uses=1]; %tmp19 = bitcast double %tmp18 to i64 ; <i64> [#uses=1]; %tmp20 = zext i64 %tmp19 to i128 ; <i128> [#uses=1]; %tmp10 = lshr i128 %tmp20, 32 ; <i128> [#uses=1]; %tmp11 = trunc i128 %tmp10 to i32 ; <i32> [#uses=1]; %tmp12 = bitcast i32 %tmp11 to float ; <float> [#uses=1]; %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]; store float %tmp12, float* %tmp5; ret void; }. Compiles to:. _test: ## @test; 	movd	%xmm0, %rax; 	shrq	$32, %rax; 	movl	%eax, 4(%rdi); 	ret. This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and; doing a shuffle from v[1] to v[0] then a float store. //===---------------------------------------------------------------------===//. [UNSAFE FP]. void foo(double, double, double);; void norm(double x, double y, double z) {; double scale = __builtin_sqrt(x*x + y*y + z*z);; foo(x/scale, y/scale, z/scale);; }. We currently generate an sqrtsd and 3 divsd instructions. This is bad, fp div is; slow and not pipelined. In -ffast-math mode we could compute ""1.0/scale"" first; and emit 3 mulsd in place of the divs. This can be done as a target-independent; transform. If we're dealing with floats instead of doubles we could even replace the sqrtss; and inversion with an rsqrtss instruction, which computes 1/sqrt faster at the; cost of reduced accuracy. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:10834,Modifiability,extend,extended,10834,"----------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n);; }. compile to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:404,Performance,load,load,404,"//===---------------------------------------------------------------------===//; // Random ideas for the X86 backend: SSE-specific stuff.; //===---------------------------------------------------------------------===//. //===---------------------------------------------------------------------===//. SSE Variable shift can be custom lowered to something like this, which uses a; small table + unaligned load + shuffle instead of going through memory. __m128i_shift_right:; 	.byte	 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15; 	.byte	 -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1. ...; __m128i shift_right(__m128i value, unsigned long offset) {; return _mm_shuffle_epi8(value,; _mm_loadu_si128((__m128 *) (___m128i_shift_right + offset)));; }. //===---------------------------------------------------------------------===//. SSE has instructions for doing operations on complex numbers, we should pattern; match them. For example, this should turn into a horizontal add:. typedef float __attribute__((vector_size(16))) v4f32;; float f32(v4f32 A) {; return A[0]+A[1]+A[2]+A[3];; }. Instead we get this:. _f32: ## @f32; 	pshufd	$1, %xmm0, %xmm1 ## xmm1 = xmm0[1,0,0,0]; 	addss	%xmm0, %xmm1; 	pshufd	$3, %xmm0, %xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2561,Performance,load,load,2561,"loat B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2787,Performance,load,load,2787,"2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2800,Performance,load,load,2800,"2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2816,Performance,load,load,2816,"2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8304,Performance,load,load,8304,"), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8327,Performance,load,load,8327,"), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===--------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:9454,Performance,load,load,9454," %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:9463,Performance,load,loads,9463," %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:10114,Performance,load,load,10114,"sing and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n);; }. compile to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11443,Performance,load,loads,11443,"le to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p),",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11481,Performance,load,load,11481,"le to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p),",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11568,Performance,load,loads,11568,"m1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11781,Performance,load,load,11781," is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11807,Performance,load,load,11807," is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11906,Performance,load,load,11906,"uld be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11925,Performance,load,load,11925,"uld be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11957,Performance,load,load,11957,"uld be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11988,Performance,load,load,11988,"uld be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12049,Performance,load,load,12049,"nclude <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp)",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12150,Performance,load,loads,12150,"%xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12241,Performance,load,load,12241,"%xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12438,Performance,load,load,12438,"aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't expo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12466,Performance,load,load,12466,"aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't expo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12516,Performance,load,load,12516,"aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't expo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13209,Performance,load,load,13209,"ra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments i",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13318,Performance,load,loads,13318,"mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===-----------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13350,Performance,load,load,13350,"mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===-----------------------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13445,Performance,load,load,13445,"ore, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-m",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:15175,Performance,load,loads,15175,"y number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-mem, and mem-reg are; legal, it'll just take a few extra patterns written in the .td file. Note: this is not a code quality issue; the custom lowered code happens to be; right, but we shouldn't have to custom lower anything. This is probably related; to <2 x i64> ops being so bad. //===---------------------------------------------------------------------===//. LLVM currently generates stack realignment code, when it is not necessary; needed. The problem is that we need to know about stack alignment too early,; before RA runs. At that point we don't know, whether there will be vector spill, or not.; Stack realignment logic is overly conservative here, but otherwise we can; produce unaligned loads/stores. Fixing this will require some huge RA changes. Testcase:; #include <emmintrin.h>. typedef short vSInt16 __attribute__ ((__vector_size__ (16)));. static const vSInt16 a = {- 22725, - 12873, - 22725, - 12873, - 22725, - 12873,; - 22725, - 12873};;. vSInt16 madd(vSInt16 b); {; return _mm_madd_epi16(a, b);; }. Generated code (x86-32, linux):; madd:; pushl %ebp; movl %esp, %ebp; andl $-16, %esp; movaps .LCPI1_0, %xmm1; pmaddwd %xmm1, %xmm0; movl %ebp, %esp; popl %ebp; ret. //===---------------------------------------------------------------------===//. Consider:; #include <emmintrin.h> ; __m128 foo2 (float x) {; return _mm_set_ps (0, 0, x, 0);; }. In x86-32 mode, we generate this spiffy code:. _foo2:; 	movss	4(%esp), %xmm0; 	pshufd	$81, %xmm0, %xmm0; 	ret. in x86-64 mode, we generate this code, which could be better:. _foo2:; 	xorps	%xmm1, %xmm1; 	movss	%xmm0, %xmm1; 	pshufd	$81, %xmm1, %xmm0; 	ret. In sse4 mode, we could use insertps t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:20172,Performance,load,load,20172,"d of movsd to implement (scalar_to_vector (loadf64)); when code size is critical. movlps is slower than movsd on core2 but it's one; byte shorter. //===---------------------------------------------------------------------===//. We should use a dynamic programming based approach to tell when using FPStack; operations is cheaper than SSE. SciMark montecarlo contains code like this; for example:. double MonteCarlo_num_flops(int Num_samples) {; return ((double) Num_samples)* 4.0;; }. In fpstack mode, this compiles into:. LCPI1_0:					; 	.long	1082130432	## float 4.000000e+00; _MonteCarlo_num_flops:; 	subl	$4, %esp; 	movl	8(%esp), %eax; 	movl	%eax, (%esp); 	fildl	(%esp); 	fmuls	LCPI1_0; 	addl	$4, %esp; 	ret; ; in SSE mode, it compiles into significantly slower code:. _MonteCarlo_num_flops:; 	subl	$12, %esp; 	cvtsi2sd	16(%esp), %xmm0; 	mulsd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp); 	fldl	(%esp); 	addl	$12, %esp; 	ret. There are also other cases in scimark where using fpstack is better, it is; cheaper to do fld1 than load from a constant pool for example, so; ""load, add 1.0, store"" is better done in the fp stack, etc. //===---------------------------------------------------------------------===//. These should compile into the same code (PR6214): Perhaps instcombine should; canonicalize the former into the later?. define float @foo(float %x) nounwind {; %t = bitcast float %x to i32; %s = and i32 %t, 2147483647; %d = bitcast i32 %s to float; ret float %d; }. declare float @fabsf(float %n); define float @bar(float %x) nounwind {; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define voi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:20216,Performance,load,load,20216,"d of movsd to implement (scalar_to_vector (loadf64)); when code size is critical. movlps is slower than movsd on core2 but it's one; byte shorter. //===---------------------------------------------------------------------===//. We should use a dynamic programming based approach to tell when using FPStack; operations is cheaper than SSE. SciMark montecarlo contains code like this; for example:. double MonteCarlo_num_flops(int Num_samples) {; return ((double) Num_samples)* 4.0;; }. In fpstack mode, this compiles into:. LCPI1_0:					; 	.long	1082130432	## float 4.000000e+00; _MonteCarlo_num_flops:; 	subl	$4, %esp; 	movl	8(%esp), %eax; 	movl	%eax, (%esp); 	fildl	(%esp); 	fmuls	LCPI1_0; 	addl	$4, %esp; 	ret; ; in SSE mode, it compiles into significantly slower code:. _MonteCarlo_num_flops:; 	subl	$12, %esp; 	cvtsi2sd	16(%esp), %xmm0; 	mulsd	LCPI1_0, %xmm0; 	movsd	%xmm0, (%esp); 	fldl	(%esp); 	addl	$12, %esp; 	ret. There are also other cases in scimark where using fpstack is better, it is; cheaper to do fld1 than load from a constant pool for example, so; ""load, add 1.0, store"" is better done in the fp stack, etc. //===---------------------------------------------------------------------===//. These should compile into the same code (PR6214): Perhaps instcombine should; canonicalize the former into the later?. define float @foo(float %x) nounwind {; %t = bitcast float %x to i32; %s = and i32 %t, 2147483647; %d = bitcast i32 %s to float; ret float %d; }. declare float @fabsf(float %n); define float @bar(float %x) nounwind {; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define voi",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2202,Safety,unsafe,unsafemath,2202,"xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===----",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:9623,Safety,unsafe,unsafe-fp-path,9623,"2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to commute some CMPP operations. //===---------------------------------------------------------------------===//. Apply the same transformation that merged four float into a single 128-bit load; to loads from constant pool. //===---------------------------------------------------------------------===//. Floating point max / min are commutable when -enable-unsafe-fp-path is; specified. We should turn int_x86_sse_max_ss and X86ISD::FMIN etc. into other; nodes which are selected to max / min instructions that are marked commutable. //===---------------------------------------------------------------------===//. We should materialize vector constants like ""all ones"" and ""signbit"" with ; code like:. cmpeqps xmm1, xmm1 ; xmm1 = all-ones. and:; cmpeqps xmm1, xmm1 ; xmm1 = all-ones; psrlq xmm1, 31 ; xmm1 = all 100000000000... instead of using a load from the constant pool. The later is important for; ABS/NEG/copysign etc. //===---------------------------------------------------------------------===//. These functions:. #include <xmmintrin.h>; __m128i a;; void x(unsigned short n) {; a = _mm_slli_epi32 (a, n);; }; void y(unsigned n) {; a = _mm_slli_epi32 (a, n);; }. compile to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3718,Security,expose,exposed,3718,"eing lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%reg1037 = MOV32rm %reg1024, 1, %noreg, 40; 	%reg1080 = IMUL32rr %reg1079, %reg1037; 	%reg1081 = MOV32rm %reg1058, 1, %noreg, 0; 	%reg1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:6551,Security,expose,exposes,6551," 	%esi = MOV32rm %esi, 1, %noreg, 0; 	MOV32mr %stack.4, 1, %noreg, 0, %esi; 	%eax = LEA32r %esi, 1, %eax, -3; 	%esi = MOV32rm %stack.7, 1, %noreg, 0; 	%esi = MOV32rm %esi, 1, %noreg, 32; 	%edi = MOV32rr %eax; 	SHL32ri %edi<def&use>, 4; 	ADD32rr %edi<def&use>, %esi; 	%xmm0 = MOVAPSrm %ecx, 1, %noreg, 0; 	%xmm1 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm1<def&use>, %xmm1, 170; 	%xmm2 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm2<def&use>, %xmm2, 0; 	%xmm3 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm3<def&use>, %xmm3, 255; 	SHUFPSrr %xmm0<def&use>, %xmm0, 85; 	%ebx = MOV32rr %edi; 	AND32ri8 %ebx<def&use>, 15; 	CMP32ri8 %ebx, 0; 	JE mbb<cond_next204,0xa914d30>. This looks really bad. The problem is shufps is a destructive opcode. Since it; appears as operand two in more than one shufps ops. It resulted in a number of; copies. Note icc also suffers from the same problem. Either the instruction; selector should select pshufd or The register allocator can made the two-address; to three-address transformation. It also exposes some other problems. See MOV32ri -3 and the spills. //===---------------------------------------------------------------------===//. Consider:. __m128 test(float a) {; return _mm_set_ps(0.0, 0.0, 0.0, a*a);; }. This compiles into:. movss 4(%esp), %xmm1; mulss %xmm1, %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Because mulss doesn't modify the top 3 elements, the top elements of ; xmm1 are already zero'd. We could compile this to:. movss 4(%esp), %xmm0; mulss %xmm0, %xmm0; ret. //===---------------------------------------------------------------------===//. Here's a sick and twisted idea. Consider code like this:. __m128 test(__m128 a) {; float b = *(float*)&A;; ...; return _mm_set_ps(0.0, 0.0, 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13462,Security,expose,exposed,13462,"ore, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-m",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:2500,Testability,test,testcase,2500,"loat B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===---------------------------------------------------------------------===//. This testcase should have no SSE instructions in it, and only one load from; a constant pool:. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } o",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3707,Testability,test,test,3707,"eing lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%reg1037 = MOV32rm %reg1024, 1, %noreg, 40; 	%reg1080 = IMUL32rr %reg1079, %reg1037; 	%reg1081 = MOV32rm %reg1058, 1, %noreg, 0; 	%reg1",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:4352,Testability,test,testb,4352,"o movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%reg1037 = MOV32rm %reg1024, 1, %noreg, 40; 	%reg1080 = IMUL32rr %reg1079, %reg1037; 	%reg1081 = MOV32rm %reg1058, 1, %noreg, 0; 	%reg1038 = LEA32r %reg1081, 1, %reg1080, -3; 	%reg1036 = MOV32rm %reg1024, 1, %noreg, 32; 	%reg1082 = SHL32ri %reg1038, 4; 	%reg1039 = ADD32rr %reg1036, %reg1082; 	%reg1083 = MOVAPSrm %reg1059, 1, %noreg, 0; 	%reg1034 = SHUFPSrr %reg1083, %reg1083, 170; 	%reg1032 = SHUFPSrr %reg1083, %reg1083, 0; 	%reg1035 = SHUFPSrr %reg1083, %reg1083, 255; 	%reg1033 = SHUFPSrr %reg1083, %reg1083, 85; 	%reg1040 = MOV32rr %reg1039; 	%reg1084 = AND32ri8 %reg1039, 15; 	CMP32ri8 %reg1084, 0; 	JE mbb<cond_next204,0xa914d30>. Still ok. After register allocation:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%eax = MOV32ri -3; 	%edx = MOV32rm %stack.3, 1, %n",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:6710,Testability,test,test,6710,"m %esi, 1, %noreg, 32; 	%edi = MOV32rr %eax; 	SHL32ri %edi<def&use>, 4; 	ADD32rr %edi<def&use>, %esi; 	%xmm0 = MOVAPSrm %ecx, 1, %noreg, 0; 	%xmm1 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm1<def&use>, %xmm1, 170; 	%xmm2 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm2<def&use>, %xmm2, 0; 	%xmm3 = MOVAPSrr %xmm0; 	SHUFPSrr %xmm3<def&use>, %xmm3, 255; 	SHUFPSrr %xmm0<def&use>, %xmm0, 85; 	%ebx = MOV32rr %edi; 	AND32ri8 %ebx<def&use>, 15; 	CMP32ri8 %ebx, 0; 	JE mbb<cond_next204,0xa914d30>. This looks really bad. The problem is shufps is a destructive opcode. Since it; appears as operand two in more than one shufps ops. It resulted in a number of; copies. Note icc also suffers from the same problem. Either the instruction; selector should select pshufd or The register allocator can made the two-address; to three-address transformation. It also exposes some other problems. See MOV32ri -3 and the spills. //===---------------------------------------------------------------------===//. Consider:. __m128 test(float a) {; return _mm_set_ps(0.0, 0.0, 0.0, a*a);; }. This compiles into:. movss 4(%esp), %xmm1; mulss %xmm1, %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Because mulss doesn't modify the top 3 elements, the top elements of ; xmm1 are already zero'd. We could compile this to:. movss 4(%esp), %xmm0; mulss %xmm0, %xmm0; ret. //===---------------------------------------------------------------------===//. Here's a sick and twisted idea. Consider code like this:. __m128 test(__m128 a) {; float b = *(float*)&A;; ...; return _mm_set_ps(0.0, 0.0, 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. mo",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:7194,Testability,test,test,7194,"d. The problem is shufps is a destructive opcode. Since it; appears as operand two in more than one shufps ops. It resulted in a number of; copies. Note icc also suffers from the same problem. Either the instruction; selector should select pshufd or The register allocator can made the two-address; to three-address transformation. It also exposes some other problems. See MOV32ri -3 and the spills. //===---------------------------------------------------------------------===//. Consider:. __m128 test(float a) {; return _mm_set_ps(0.0, 0.0, 0.0, a*a);; }. This compiles into:. movss 4(%esp), %xmm1; mulss %xmm1, %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Because mulss doesn't modify the top 3 elements, the top elements of ; xmm1 are already zero'd. We could compile this to:. movss 4(%esp), %xmm0; mulss %xmm0, %xmm0; ret. //===---------------------------------------------------------------------===//. Here's a sick and twisted idea. Consider code like this:. __m128 test(__m128 a) {; float b = *(float*)&A;; ...; return _mm_set_ps(0.0, 0.0, 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8251,Testability,test,test,8251,", 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/documentation/Performance/Conceptual/; Accelerate_sse_migration/index.html. e.g. SSE select using and, andnot, or. Various SSE compare translations. //===---------------------------------------------------------------------===//. Add hooks to ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:11408,Testability,test,tests,11408,"le to ( -O3 -static -fomit-frame-pointer):; _x:; movzwl 4(%esp), %eax; movd %eax, %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret; _y:; movd 4(%esp), %xmm0; movaps _a, %xmm1; pslld %xmm0, %xmm1; movaps %xmm1, _a; ret. ""y"" looks good, but ""x"" does silly movzwl stuff around into a GPR. It seems; like movd would be sufficient in both cases as the value is already zero ; extended in the 32-bit stack slot IIRC. For signed short, it should also be; save, as a really-signed value would be undefined for pslld. //===---------------------------------------------------------------------===//. #include <math.h>; int t1(double d) { return signbit(d); }. This currently compiles to:; 	subl	$12, %esp; 	movsd	16(%esp), %xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p),",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:12236,Testability,stub,stub,12236,"%xmm0; 	movsd	%xmm0, (%esp); 	movl	4(%esp), %eax; 	shrl	$31, %eax; 	addl	$12, %esp; 	ret. We should use movmskp{s|d} instead. //===---------------------------------------------------------------------===//. CodeGen/X86/vec_align.ll tests whether we can turn 4 scalar loads into a single; (aligned) vector load. This functionality has a couple of problems. 1. The code to infer alignment from loads of globals is in the X86 backend,; not the dag combiner. This is because dagcombine2 needs to be able to see; through the X86ISD::Wrapper node, which DAGCombine can't really do.; 2. The code for turning 4 x load into a single vector load is target ; independent and should be moved to the dag combiner.; 3. The code for turning 4 x load into a vector load can only handle a direct ; load from a global or a direct load from the stack. It should be generalized; to handle any load from P, P+4, P+8, P+12, where P can be anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub;",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:13171,Testability,stub,stub,13171," anything.; 4. The alignment inference code cannot handle loads from globals in non-static; mode because it doesn't look through the extra dyld stub load. If you try; vec_align.ll without -relocation-model=static, you'll see what I mean. //===---------------------------------------------------------------------===//. We should lower store(fneg(load p), q) into an integer load+xor+store, which; eliminates a constant pool load. For example, consider:. define i64 @ccosf(float %z.0, float %z.1) nounwind readonly {; entry:; %tmp6 = fsub float -0.000000e+00, %z.1		; <float> [#uses=1]; %tmp20 = tail call i64 @ccoshf( float %tmp6, float %z.0 ) nounwind readonly; ret i64 %tmp20; }; declare i64 @ccoshf(float %z.0, float %z.1) nounwind readonly. This currently compiles to:. LCPI1_0:					# <4 x float>; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; 	.long	2147483648	# float -0; _ccosf:; 	subl	$12, %esp; 	movss	16(%esp), %xmm0; 	movss	%xmm0, 4(%esp); 	movss	20(%esp), %xmm0; 	xorps	LCPI1_0, %xmm0; 	movss	%xmm0, (%esp); 	call	L_ccoshf$stub; 	addl	$12, %esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:15100,Testability,log,logic,15100,"y number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-mem, and mem-reg are; legal, it'll just take a few extra patterns written in the .td file. Note: this is not a code quality issue; the custom lowered code happens to be; right, but we shouldn't have to custom lower anything. This is probably related; to <2 x i64> ops being so bad. //===---------------------------------------------------------------------===//. LLVM currently generates stack realignment code, when it is not necessary; needed. The problem is that we need to know about stack alignment too early,; before RA runs. At that point we don't know, whether there will be vector spill, or not.; Stack realignment logic is overly conservative here, but otherwise we can; produce unaligned loads/stores. Fixing this will require some huge RA changes. Testcase:; #include <emmintrin.h>. typedef short vSInt16 __attribute__ ((__vector_size__ (16)));. static const vSInt16 a = {- 22725, - 12873, - 22725, - 12873, - 22725, - 12873,; - 22725, - 12873};;. vSInt16 madd(vSInt16 b); {; return _mm_madd_epi16(a, b);; }. Generated code (x86-32, linux):; madd:; pushl %ebp; movl %esp, %ebp; andl $-16, %esp; movaps .LCPI1_0, %xmm1; pmaddwd %xmm1, %xmm0; movl %ebp, %esp; popl %ebp; ret. //===---------------------------------------------------------------------===//. Consider:; #include <emmintrin.h> ; __m128 foo2 (float x) {; return _mm_set_ps (0, 0, x, 0);; }. In x86-32 mode, we generate this spiffy code:. _foo2:; 	movss	4(%esp), %xmm0; 	pshufd	$81, %xmm0, %xmm0; 	ret. in x86-64 mode, we generate this code, which could be better:. _foo2:; 	xorps	%xmm1, %xmm1; 	movss	%xmm0, %xmm1; 	pshufd	$81, %xmm1, %xmm0; 	ret. In sse4 mode, we could use insertps t",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:16169,Testability,test,testcase,16169,"stores. Fixing this will require some huge RA changes. Testcase:; #include <emmintrin.h>. typedef short vSInt16 __attribute__ ((__vector_size__ (16)));. static const vSInt16 a = {- 22725, - 12873, - 22725, - 12873, - 22725, - 12873,; - 22725, - 12873};;. vSInt16 madd(vSInt16 b); {; return _mm_madd_epi16(a, b);; }. Generated code (x86-32, linux):; madd:; pushl %ebp; movl %esp, %ebp; andl $-16, %esp; movaps .LCPI1_0, %xmm1; pmaddwd %xmm1, %xmm0; movl %ebp, %esp; popl %ebp; ret. //===---------------------------------------------------------------------===//. Consider:; #include <emmintrin.h> ; __m128 foo2 (float x) {; return _mm_set_ps (0, 0, x, 0);; }. In x86-32 mode, we generate this spiffy code:. _foo2:; 	movss	4(%esp), %xmm0; 	pshufd	$81, %xmm0, %xmm0; 	ret. in x86-64 mode, we generate this code, which could be better:. _foo2:; 	xorps	%xmm1, %xmm1; 	movss	%xmm0, %xmm1; 	pshufd	$81, %xmm1, %xmm0; 	ret. In sse4 mode, we could use insertps to make both better. Here's another testcase that could use insertps [mem]:. #include <xmmintrin.h>; extern float x2, x3;; __m128 foo1 (float x1, float x4) {; return _mm_set_ps (x2, x1, x3, x4);; }. gcc mainline compiles it to:. foo1:; insertps $0x10, x2(%rip), %xmm0; insertps $0x10, x3(%rip), %xmm1; movaps %xmm1, %xmm2; movlhps %xmm0, %xmm2; movaps %xmm2, %xmm0; ret. //===---------------------------------------------------------------------===//. We compile vector multiply-by-constant into poor code:. define <4 x i32> @f(<4 x i32> %i) nounwind {; 	%A = mul <4 x i32> %i, < i32 10, i32 10, i32 10, i32 10 >; 	ret <4 x i32> %A; }. On targets without SSE4.1, this compiles into:. LCPI1_0:					## <4 x i32>; 	.long	10; 	.long	10; 	.long	10; 	.long	10; 	.text; 	.align	4,0x90; 	.globl	_f; _f:; 	pshufd	$3, %xmm0, %xmm1; 	movd	%xmm1, %eax; 	imull	LCPI1_0+12, %eax; 	movd	%eax, %xmm1; 	pshufd	$1, %xmm0, %xmm2; 	movd	%xmm2, %eax; 	imull	LCPI1_0+4, %eax; 	movd	%eax, %xmm2; 	punpckldq	%xmm1, %xmm2; 	movd	%xmm0, %eax; 	imull	LCPI1_0, %eax; 	movd	%ea",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:21153,Testability,test,test,21153,"r to do fld1 than load from a constant pool for example, so; ""load, add 1.0, store"" is better done in the fp stack, etc. //===---------------------------------------------------------------------===//. These should compile into the same code (PR6214): Perhaps instcombine should; canonicalize the former into the later?. define float @foo(float %x) nounwind {; %t = bitcast float %x to i32; %s = and i32 %t, 2147483647; %d = bitcast i32 %s to float; ret float %d; }. declare float @fabsf(float %n); define float @bar(float %x) nounwind {; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {; entry:; %tmp18 = extractvalue %0 %0, 0 ; <double> [#uses=1]; %tmp19 = bitcast double %tmp18 to i64 ; <i64> [#uses=1]; %tmp20 = zext i64 %tmp19 to i128 ; <i128> [#uses=1]; %tmp10 = lshr i128 %tmp20, 32 ; <i128> [#uses=1]; %tmp11 = trunc i128 %tmp10 to i32 ; <i32> [#uses=1]; %tmp12 = bitcast i32 %tmp11 to float ; <float> [#uses=1]; %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]; store float %tmp12, float* %tmp5; ret void; }. Compiles to:. _test: ## @test; 	movd	%xmm0, %rax; 	shrq	$32, %rax; 	movl	%eax, 4(%rdi); 	ret. This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and; doing a shuffle from v[1] to v[0] then a float store. //===---------------------------------------------------------------------===//. [UNSAFE FP]. void foo(double, double, double);; void norm(double x, double y, double z) {; double scale = __builtin_sqrt(x*x + y*y + z*z);; foo(x/scale, y/scale, z",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:21711,Testability,test,test,21711,"; %d = call float @fabsf(float %x); ret float %d; }. //===---------------------------------------------------------------------===//. This IR (from PR6194):. target datalayout = ""e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128""; target triple = ""x86_64-apple-darwin10.0.0"". %0 = type { double, double }; %struct.float3 = type { float, float, float }. define void @test(%0, %struct.float3* nocapture %res) nounwind noinline ssp {; entry:; %tmp18 = extractvalue %0 %0, 0 ; <double> [#uses=1]; %tmp19 = bitcast double %tmp18 to i64 ; <i64> [#uses=1]; %tmp20 = zext i64 %tmp19 to i128 ; <i128> [#uses=1]; %tmp10 = lshr i128 %tmp20, 32 ; <i128> [#uses=1]; %tmp11 = trunc i128 %tmp10 to i32 ; <i32> [#uses=1]; %tmp12 = bitcast i32 %tmp11 to float ; <float> [#uses=1]; %tmp5 = getelementptr inbounds %struct.float3* %res, i64 0, i32 1 ; <float*> [#uses=1]; store float %tmp12, float* %tmp5; ret void; }. Compiles to:. _test: ## @test; 	movd	%xmm0, %rax; 	shrq	$32, %rax; 	movl	%eax, 4(%rdi); 	ret. This would be better kept in the SSE unit by treating XMM0 as a 4xfloat and; doing a shuffle from v[1] to v[0] then a float store. //===---------------------------------------------------------------------===//. [UNSAFE FP]. void foo(double, double, double);; void norm(double x, double y, double z) {; double scale = __builtin_sqrt(x*x + y*y + z*z);; foo(x/scale, y/scale, z/scale);; }. We currently generate an sqrtsd and 3 divsd instructions. This is bad, fp div is; slow and not pipelined. In -ffast-math mode we could compute ""1.0/scale"" first; and emit 3 mulsd in place of the divs. This can be done as a target-independent; transform. If we're dealing with floats instead of doubles we could even replace the sqrtss; and inversion with an rsqrtss instruction, which computes 1/sqrt faster at the; cost of reduced accuracy. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:1430,Usability,simpl,simple,1430," through memory. __m128i_shift_right:; 	.byte	 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15; 	.byte	 -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1. ...; __m128i shift_right(__m128i value, unsigned long offset) {; return _mm_shuffle_epi8(value,; _mm_loadu_si128((__m128 *) (___m128i_shift_right + offset)));; }. //===---------------------------------------------------------------------===//. SSE has instructions for doing operations on complex numbers, we should pattern; match them. For example, this should turn into a horizontal add:. typedef float __attribute__((vector_size(16))) v4f32;; float f32(v4f32 A) {; return A[0]+A[1]+A[2]+A[3];; }. Instead we get this:. _f32: ## @f32; 	pshufd	$1, %xmm0, %xmm1 ## xmm1 = xmm0[1,0,0,0]; 	addss	%xmm0, %xmm1; 	pshufd	$3, %xmm0, %xmm2 ## xmm2 = xmm0[3,0,0,0]; 	movhlps	%xmm0, %xmm0 ## xmm0 = xmm0[1,1]; 	movaps	%xmm0, %xmm3; 	addss	%xmm1, %xmm3; 	movdqa	%xmm2, %xmm0; 	addss	%xmm3, %xmm0; 	ret. Also, there are cases where some simple local SLP would improve codegen a bit.; compiling this:. _Complex float f32(_Complex float A, _Complex float B) {; return A+B;; }. into:. _f32: ## @f32; 	movdqa	%xmm0, %xmm2; 	addss	%xmm1, %xmm2; 	pshufd	$1, %xmm1, %xmm1 ## xmm1 = xmm1[1,0,0,0]; 	pshufd	$1, %xmm0, %xmm3 ## xmm3 = xmm0[1,0,0,0]; 	addss	%xmm1, %xmm3; 	movaps	%xmm2, %xmm0; 	unpcklps	%xmm3, %xmm0 ## xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1]; 	ret. seems silly when it could just be one addps. //===---------------------------------------------------------------------===//. Expand libm rounding functions inline: Significant speedups possible.; http://gcc.gnu.org/ml/gcc-patches/2006-10/msg00909.html. //===---------------------------------------------------------------------===//. When compiled with unsafemath enabled, ""main"" should enable SSE DAZ mode and; other fast SSE modes. //===---------------------------------------------------------------------===//. Think about doing i64 math in SSE regs on x86-32. //===-----------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:3588,Usability,clear,clear,3588,":. double %test3(bool %B) {; %C = select bool %B, double 123.412, double 523.01123123; ret double %C; }. Currently, the select is being lowered, which prevents the dag combiner from; turning 'select (load CPI1), (load CPI2)' -> 'load (select CPI1, CPI2)'. The pattern isel got this one right. //===---------------------------------------------------------------------===//. Lower memcpy / memset to a series of SSE 128 bit move instructions when it's; feasible. //===---------------------------------------------------------------------===//. Codegen:; if (copysign(1.0, x) == copysign(1.0, y)); into:; if (x^y & mask); when using SSE. //===---------------------------------------------------------------------===//. Use movhps to update upper 64-bits of a v4sf value. Also movlps on lower half; of a v4sf value. //===---------------------------------------------------------------------===//. Better codegen for vector_shuffles like this { x, 0, 0, 0 } or { x, 0, x, 0}.; Perhaps use pxor / xorp* to clear a XMM register first?. //===---------------------------------------------------------------------===//. External test Nurbs exposed some problems. Look for; __ZN15Nurbs_SSE_Cubic17TessellateSurfaceE, bb cond_next140. This is what icc; emits:. movaps (%edx), %xmm2 #59.21; movaps (%edx), %xmm5 #60.21; movaps (%edx), %xmm4 #61.21; movaps (%edx), %xmm3 #62.21; movl 40(%ecx), %ebp #69.49; shufps $0, %xmm2, %xmm5 #60.21; movl 100(%esp), %ebx #69.20; movl (%ebx), %edi #69.20; imull %ebp, %edi #69.49; addl (%eax), %edi #70.33; shufps $85, %xmm2, %xmm4 #61.21; shufps $170, %xmm2, %xmm3 #62.21; shufps $255, %xmm2, %xmm2 #63.21; lea (%ebp,%ebp,2), %ebx #69.49; negl %ebx #69.49; lea -3(%edi,%ebx), %ebx #70.33; shll $4, %ebx #68.37; addl 32(%ecx), %ebx #68.37; testb $15, %bl #91.13; jne L_B1.24 # Prob 5% #91.13. This is the llvm code after instruction scheduling:. cond_next140 (0xa910740, LLVM BB @0xa90beb0):; 	%reg1078 = MOV32ri -3; 	%reg1079 = ADD32rm %reg1078, %reg1068, 1, %noreg, 0; 	%re",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:8000,Usability,simpl,simplify,8000," %xmm0, %xmm0; ret. //===---------------------------------------------------------------------===//. Here's a sick and twisted idea. Consider code like this:. __m128 test(__m128 a) {; float b = *(float*)&A;; ...; return _mm_set_ps(0.0, 0.0, 0.0, b);; }. This might compile to this code:. movaps c(%esp), %xmm1; xorps %xmm0, %xmm0; movss %xmm1, %xmm0; ret. Now consider if the ... code caused xmm1 to get spilled. This might produce; this code:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... xorps %xmm0, %xmm0; movaps c2(%esp), %xmm1; movss %xmm1, %xmm0; ret. However, since the reload is only used by these instructions, we could ; ""fold"" it into the uses, producing something like this:. movaps c(%esp), %xmm1; movaps %xmm1, c2(%esp); ... movss c2(%esp), %xmm0; ret. ... saving two instructions. The basic idea is that a reload from a spill slot, can, if only one 4-byte ; chunk is used, bring in 3 zeros the one element instead of 4 elements.; This can be used to simplify a variety of shuffle operations, where the; elements are fixed zeros. //===---------------------------------------------------------------------===//. This code generates ugly code, probably due to costs being off or something:. define void @test(float* %P, <4 x float>* %P2 ) {; %xFloat0.688 = load float* %P; %tmp = load <4 x float>* %P2; %inFloat3.713 = insertelement <4 x float> %tmp, float 0.0, i32 3; store <4 x float> %inFloat3.713, <4 x float>* %P2; ret void; }. Generates:. _test:; 	movl	8(%esp), %eax; 	movaps	(%eax), %xmm0; 	pxor	%xmm1, %xmm1; 	movaps	%xmm0, %xmm2; 	shufps	$50, %xmm1, %xmm2; 	shufps	$132, %xmm2, %xmm0; 	movaps	%xmm0, (%eax); 	ret. Would it be better to generate:. _test:; movl 8(%esp), %ecx; movaps (%ecx), %xmm0; 	xor %eax, %eax; pinsrw $6, %eax, %xmm0; pinsrw $7, %eax, %xmm0; movaps %xmm0, (%ecx); ret. ?. //===---------------------------------------------------------------------===//. Some useful information in the Apple Altivec / SSE Migration Guide:. http://developer.apple.com/doc",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:14195,Usability,simpl,simple,14195,"esp; 	ret. Note the load into xmm0, then xor (to negate), then store. In PIC mode,; this code computes the pic base and does two loads to do the constant pool ; load, so the improvement is much bigger. The tricky part about this xform is that the argument load/store isn't exposed; until post-legalize, and at that point, the fneg has been custom expanded into ; an X86 fxor. This means that we need to handle this case in the x86 backend; instead of in target independent code. //===---------------------------------------------------------------------===//. Non-SSE4 insert into 16 x i8 is atrociously bad. //===---------------------------------------------------------------------===//. <2 x i64> extract is substantially worse than <2 x f64>, even if the destination; is memory. //===---------------------------------------------------------------------===//. INSERTPS can match any insert (extract, imm1), imm2 for 4 x float, and insert; any number of 0.0 simultaneously. Currently we only use it for simple; insertions. See comments in LowerINSERT_VECTOR_ELT_SSE4. //===---------------------------------------------------------------------===//. On a random note, SSE2 should declare insert/extract of 2 x f64 as legal, not; Custom. All combinations of insert/extract reg-reg, reg-mem, and mem-reg are; legal, it'll just take a few extra patterns written in the .td file. Note: this is not a code quality issue; the custom lowered code happens to be; right, but we shouldn't have to custom lower anything. This is probably related; to <2 x i64> ops being so bad. //===---------------------------------------------------------------------===//. LLVM currently generates stack realignment code, when it is not necessary; needed. The problem is that we need to know about stack alignment too early,; before RA runs. At that point we don't know, whether there will be vector spill, or not.; Stack realignment logic is overly conservative here, but otherwise we can; produce unaligned loads/stores. F",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt:17510,Usability,simpl,simple,17510,"---------------===//. We compile vector multiply-by-constant into poor code:. define <4 x i32> @f(<4 x i32> %i) nounwind {; 	%A = mul <4 x i32> %i, < i32 10, i32 10, i32 10, i32 10 >; 	ret <4 x i32> %A; }. On targets without SSE4.1, this compiles into:. LCPI1_0:					## <4 x i32>; 	.long	10; 	.long	10; 	.long	10; 	.long	10; 	.text; 	.align	4,0x90; 	.globl	_f; _f:; 	pshufd	$3, %xmm0, %xmm1; 	movd	%xmm1, %eax; 	imull	LCPI1_0+12, %eax; 	movd	%eax, %xmm1; 	pshufd	$1, %xmm0, %xmm2; 	movd	%xmm2, %eax; 	imull	LCPI1_0+4, %eax; 	movd	%eax, %xmm2; 	punpckldq	%xmm1, %xmm2; 	movd	%xmm0, %eax; 	imull	LCPI1_0, %eax; 	movd	%eax, %xmm1; 	movhlps	%xmm0, %xmm0; 	movd	%xmm0, %eax; 	imull	LCPI1_0+8, %eax; 	movd	%eax, %xmm0; 	punpckldq	%xmm0, %xmm1; 	movaps	%xmm1, %xmm0; 	punpckldq	%xmm2, %xmm0; 	ret. It would be better to synthesize integer vector multiplication by constants; using shifts and adds, pslld and paddd here. And even on targets with SSE4.1,; simple cases such as multiplication by powers of two would be better as; vector shifts than as multiplications. //===---------------------------------------------------------------------===//. We compile this:. __m128i; foo2 (char x); {; return _mm_set_epi8 (1, 0, 0, 0, 0, 0, 0, 0, 0, x, 0, 1, 0, 0, 0, 0);; }. into:; 	movl	$1, %eax; 	xorps	%xmm0, %xmm0; 	pinsrw	$2, %eax, %xmm0; 	movzbl	4(%esp), %eax; 	pinsrw	$3, %eax, %xmm0; 	movl	$256, %eax; 	pinsrw	$7, %eax, %xmm0; 	ret. gcc-4.2:; 	subl	$12, %esp; 	movzbl	16(%esp), %eax; 	movdqa	LC0, %xmm0; 	pinsrw	$3, %eax, %xmm0; 	addl	$12, %esp; 	ret; 	.const; 	.align 4; LC0:; 	.word	0; 	.word	0; 	.word	1; 	.word	0; 	.word	0; 	.word	0; 	.word	0; 	.word	256. With SSE4, it should be; movdqa .LC0(%rip), %xmm0; pinsrb $6, %edi, %xmm0. //===---------------------------------------------------------------------===//. We should transform a shuffle of two vectors of constants into a single vector; of constants. Also, insertelement of a constant into a vector of constants; should also result in a vector of con",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-SSE.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:1469,Availability,down,down,1469,"ing branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2843,Availability,redundant,redundant,2843,"lee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 le",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2893,Availability,mask,mask,2893,"value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 leaves the high word zero extended and to; be able to recognize the zero extend. T",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2981,Availability,mask,mask,2981,"value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 leaves the high word zero extended and to; be able to recognize the zero extend. T",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:1366,Energy Efficiency,allocate,allocated,1366," //===---------------------------------------------------------------------===//. Are we better off using branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR r",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5988,Energy Efficiency,reduce,reduce,5988,"the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack used by half. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:3885,Modifiability,extend,extended,3885,"e this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 leaves the high word zero extended and to; be able to recognize the zero extend. This could also presumably be implemented; if we have whole-function selectiondags. //===---------------------------------------------------------------------===//. Take the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:3932,Modifiability,extend,extend,3932,"e this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 leaves the high word zero extended and to; be able to recognize the zero extend. This could also presumably be implemented; if we have whole-function selectiondags. //===---------------------------------------------------------------------===//. Take the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:144,Performance,optimiz,optimizing,144,"//===- README_X86_64.txt - Notes for X86-64 code gen ----------------------===//. AMD64 Optimization Manual 8.2 has some nice information about optimizing integer; multiplication by a constant. How much of it applies to Intel's X86-64; implementation? There are definite trade-offs to consider: latency vs. register; pressure vs. code size. //===---------------------------------------------------------------------===//. Are we better off using branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Ca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:295,Performance,latency,latency,295,"//===- README_X86_64.txt - Notes for X86-64 code gen ----------------------===//. AMD64 Optimization Manual 8.2 has some nice information about optimizing integer; multiplication by a constant. How much of it applies to Intel's X86-64; implementation? There are definite trade-offs to consider: latency vs. register; pressure vs. code size. //===---------------------------------------------------------------------===//. Are we better off using branches instead of cmove to implement FP to; unsigned i64?. _conv:; 	ucomiss	LC0(%rip), %xmm0; 	cvttss2siq	%xmm0, %rdx; 	jb	L3; 	subss	LC0(%rip), %xmm0; 	movabsq	$-9223372036854775808, %rax; 	cvttss2siq	%xmm0, %rdx; 	xorq	%rax, %rdx; L3:; 	movq	%rdx, %rax; 	ret. instead of. _conv:; 	movss LCPI1_0(%rip), %xmm1; 	cvttss2siq %xmm0, %rcx; 	movaps %xmm0, %xmm2; 	subss %xmm1, %xmm2; 	cvttss2siq %xmm2, %rax; 	movabsq $-9223372036854775808, %rdx; 	xorq %rdx, %rax; 	ucomiss %xmm1, %xmm0; 	cmovb %rcx, %rax; 	ret. Seems like the jb branch has high likelihood of being taken. It would have; saved a few instructions. //===---------------------------------------------------------------------===//. It's not possible to reference AH, BH, CH, and DH registers in an instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Ca",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2259,Performance,optimiz,optimization,2259," instruction; requiring REX prefix. However, divb and mulb both produce results in AH. If isel; emits a CopyFromReg which gets turned into a movb and that can be allocated a; r8b - r15b. To get around this, isel emits a CopyFromReg from AX and then right shift it; down by 8 and truncate it. It's not pretty but it works. We need some register; allocation magic to make the hack go away (e.g. putting additional constraints; on the result of the movb). //===---------------------------------------------------------------------===//. The x86-64 ABI for hidden-argument struct returns requires that the; incoming value of %rdi be copied into %rax by the callee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5311,Performance,optimiz,optimizations,5311,"the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack used by half. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5660,Performance,optimiz,optimizations,5660,"the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack used by half. //===---------------------------------------------------------------------===//; ",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:2843,Safety,redund,redundant,2843,"lee upon return. The idea is that it saves callers from having to remember this value,; which would often require a callee-saved register. Callees usually; need to keep this value live for most of their body anyway, so it; doesn't add a significant burden on them. We currently implement this in codegen, however this is suboptimal; because it means that it would be quite awkward to implement the; optimization for callers. A better implementation would be to relax the LLVM IR rules for sret; arguments to allow a function with an sret argument to have a non-void; return type, and to have the front-end to set up the sret argument value; as the return value of the function. The front-end could more easily; emit uses of the returned struct value to be in terms of the function's; lowered return value, and it would free non-C frontends from a; complication only required by a C-based ABI. //===---------------------------------------------------------------------===//. We get a redundant zero extension for code like this:. int mask[1000];; int foo(unsigned x) {; if (x < 10); x = x * 45;; else; x = x * 78;; return mask[x];; }. _foo:; LBB1_0:	## entry; 	cmpl	$9, %edi; 	jbe	LBB1_3	## bb; LBB1_1:	## bb1; 	imull	$78, %edi, %eax; LBB1_2:	## bb2; 	movl	%eax, %eax <----; 	movq	_mask@GOTPCREL(%rip), %rcx; 	movl	(%rcx,%rax,4), %eax; 	ret; LBB1_3:	## bb; 	imull	$45, %edi, %eax; 	jmp	LBB1_2	## bb2; ; Before regalloc, we have:. %reg1025 = IMUL32rri8 %reg1024, 45, implicit-def %eflags; JMP mbb<bb2,0x203afb0>; Successors according to CFG: 0x203afb0 (#3). bb1: 0x203af60, LLVM BB @0x1e02310, ID#2:; Predecessors according to CFG: 0x203aec0 (#0); %reg1026 = IMUL32rri8 %reg1024, 78, implicit-def %eflags; Successors according to CFG: 0x203afb0 (#3). bb2: 0x203afb0, LLVM BB @0x1e02340, ID#3:; Predecessors according to CFG: 0x203af10 (#1) 0x203af60 (#2); %reg1027 = PHI %reg1025, mbb<bb,0x203af10>,; %reg1026, mbb<bb1,0x203af60>; %reg1029 = MOVZX64rr32 %reg1027. so we'd have to know that IMUL32rri8 le",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:4945,Testability,test,testcase,4945,"uld also presumably be implemented; if we have whole-function selectiondags. //===---------------------------------------------------------------------===//. Take the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registe",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt:5011,Testability,test,test,5011,"ctiondags. //===---------------------------------------------------------------------===//. Take the following code; (from http://gcc.gnu.org/bugzilla/show_bug.cgi?id=34653):; extern unsigned long table[];; unsigned long foo(unsigned char *p) {; unsigned long tag = *p;; return table[tag >> 4] + table[tag & 0xf];; }. Current code generated:; 	movzbl	(%rdi), %eax; 	movq	%rax, %rcx; 	andq	$240, %rcx; 	shrq	%rcx; 	andq	$15, %rax; 	movq	table(,%rax,8), %rax; 	addq	table(%rcx), %rax; 	ret. Issues:; 1. First movq should be movl; saves a byte.; 2. Both andq's should be andl; saves another two bytes. I think this was; implemented at one point, but subsequently regressed.; 3. shrq should be shrl; saves another byte.; 4. The first andq can be completely eliminated by using a slightly more; expensive addressing mode. //===---------------------------------------------------------------------===//. Consider the following (contrived testcase, but contains common factors):. #include <stdarg.h>; int test(int x, ...) {; int sum, i;; va_list l;; va_start(l, x);; for (i = 0; i < x; i++); sum += va_arg(l, int);; va_end(l);; return sum;; }. Testcase given in C because fixing it will likely involve changing the IR; generated for it. The primary issue with the result is that it doesn't do any; of the optimizations which are possible if we know the address of a va_list; in the current function is never taken:; 1. We shouldn't spill the XMM registers because we only call va_arg with ""int"".; 2. It would be nice if we could sroa the va_list.; 3. Probably overkill, but it'd be cool if we could peel off the first five; iterations of the loop. Other optimizations involving functions which use va_arg on floats which don't; have the address of a va_list taken:; 1. Conversely to the above, we shouldn't spill general registers if we only; call va_arg on ""double"".; 2. If we know nothing more than 64 bits wide is read from the XMM registers,; we can change the spilling code to reduce the amount of stack",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README-X86-64.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:16522,Availability,redundant,redundant,16522,"6 %tmp98 to i32		; <i32> [#uses=1]; 	%tmp585 = sub i32 32, %tmp583584		; <i32> [#uses=1]; 	%tmp614615 = sext i16 %tmp101 to i32		; <i32> [#uses=1]; 	%tmp621622 = sext i16 %tmp104 to i32		; <i32> [#uses=1]; 	%tmp623 = sub i32 32, %tmp621622		; <i32> [#uses=1]; 	br label %bb114. produces:. LBB3_5:	# bb114.preheader; 	movswl	-68(%ebp), %eax; 	movl	$32, %ecx; 	movl	%ecx, -80(%ebp); 	subl	%eax, -80(%ebp); 	movswl	-52(%ebp), %eax; 	movl	%ecx, -84(%ebp); 	subl	%eax, -84(%ebp); 	movswl	-70(%ebp), %eax; 	movl	%ecx, -88(%ebp); 	subl	%eax, -88(%ebp); 	movswl	-50(%ebp), %eax; 	subl	%eax, %ecx; 	movl	%ecx, -76(%ebp); 	movswl	-42(%ebp), %eax; 	movl	%eax, -92(%ebp); 	movswl	-66(%ebp), %eax; 	movl	%eax, -96(%ebp); 	movw	$0, -98(%ebp). This appears to be bad because the RA is not folding the store to the stack ; slot into the movl. The above instructions could be:; 	movl $32, -80(%ebp); ...; 	movl $32, -84(%ebp); ...; This seems like a cross between remat and spill folding. This has redundant subtractions of %eax from a stack slot. However, %ecx doesn't; change, so we could simply subtract %eax from %ecx first and then use %ecx (or; vice-versa). //===---------------------------------------------------------------------===//. This code:. 	%tmp659 = icmp slt i16 %tmp654, 0		; <i1> [#uses=1]; 	br i1 %tmp659, label %cond_true662, label %cond_next715. produces this:. 	testw	%cx, %cx; 	movswl	%cx, %esi; 	jns	LBB4_109	# cond_next715. Shark tells us that using %cx in the testw instruction is sub-optimal. It; suggests using the 32-bit register (which is what ICC uses). //===---------------------------------------------------------------------===//. We compile this:. void compare (long long foo) {; if (foo < 4294967297LL); abort();; }. to:. compare:; subl $4, %esp; cmpl $0, 8(%esp); setne %al; movzbw %al, %ax; cmpl $1, 12(%esp); setg %cl; movzbw %cl, %cx; cmove %ax, %cx; testb $1, %cl; jne .LBB1_2 # UnifiedReturnBlock; .LBB1_1: # ifthen; call abort; .LBB1_2: # UnifiedReturnBlock; addl $4, %esp",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt:40575,Availability,redundant,redundant,40575,"add32carry:; 	leal	(%rsi,%rdi), %eax; 	cmpl	%esi, %eax; 	adcl	$0, %eax; 	ret. //===---------------------------------------------------------------------===//. The hot loop of 256.bzip2 contains code that looks a bit like this:. int foo(char *P, char *Q, int x, int y) {; if (P[0] != Q[0]); return P[0] < Q[0];; if (P[1] != Q[1]); return P[1] < Q[1];; if (P[2] != Q[2]); return P[2] < Q[2];; return P[3] < Q[3];; }. In the real code, we get a lot more wrong than this. However, even in this; code we generate:. _foo: ## @foo; ## %bb.0: ## %entry; 	movb	(%rsi), %al; 	movb	(%rdi), %cl; 	cmpb	%al, %cl; 	je	LBB0_2; LBB0_1: ## %if.then; 	cmpb	%al, %cl; 	jmp	LBB0_5; LBB0_2: ## %if.end; 	movb	1(%rsi), %al; 	movb	1(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.3: ## %if.end38; 	movb	2(%rsi), %al; 	movb	2(%rdi), %cl; 	cmpb	%al, %cl; 	jne	LBB0_1; ## %bb.4: ## %if.end60; 	movb	3(%rdi), %al; 	cmpb	3(%rsi), %al; LBB0_5: ## %if.end60; 	setl	%al; 	movzbl	%al, %eax; 	ret. Note that we generate jumps to LBB0_1 which does a redundant compare. The; redundant compare also forces the register values to be live, which prevents; folding one of the loads into the compare. In contrast, GCC 4.2 produces:. _foo:; 	movzbl	(%rsi), %eax; 	cmpb	%al, (%rdi); 	jne	L10; L12:; 	movzbl	1(%rsi), %eax; 	cmpb	%al, 1(%rdi); 	jne	L10; 	movzbl	2(%rsi), %eax; 	cmpb	%al, 2(%rdi); 	jne	L10; 	movzbl	3(%rdi), %eax; 	cmpb	3(%rsi), %al; L10:; 	setl	%al; 	movzbl	%al, %eax; 	ret. which is ""perfect"". //===---------------------------------------------------------------------===//. For the branch in the following code:; int a();; int b(int x, int y) {; if (x & (1<<(y&7))); return a();; return y;; }. We currently generate:; 	movb	%sil, %al; 	andb	$7, %al; 	movzbl	%al, %eax; 	btl	%eax, %edi; 	jae	.LBB0_2. movl+andl would be shorter than the movb+andb+movzbl sequence. //===---------------------------------------------------------------------===//. For the following:; struct u1 {; float x, y;; };; float foo(struct u1 u) {; retu",MatchSource.DOCS,interpreter/llvm-project/llvm/lib/Target/X86/README.txt,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/README.txt
